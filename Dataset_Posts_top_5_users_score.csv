Post ID,Post Body,Post Score,Post Type ID,User ID,User Reputation
"608","<p>I have just learned about regularisation as an approach to control over-fitting, and I would like to incorporate the idea into a simple implementation of backpropagation and <a href=""http://en.wikipedia.org/wiki/Multilayer_perceptron"" rel=""noreferrer"">Multilayer perceptron</a> (MLP) that I put together.</p>

<p>Currently to avoid over-fitting, I cross-validate and keep the network with best score so far on the validation set. This works OK, but adding regularisation would benefit me in that correct choice of the regularisation algorithm and parameter would make my network converge on a non-overfit model more systematically.</p>

<p>The formula I have for the update term (from Coursera ML course) is stated as a batch update e.g. for each weight, after summing all the applicable deltas for the whole training set from error propagation, an adjustment of <code>lambda * current_weight</code> is added as well before the combined delta is subtracted at the end of the batch, where <code>lambda</code> is the regularisation parameter.</p>

<p>My implementation of backpropagation uses per-item weight updates. I am concerned that I cannot just copy the batch approach, although it looks OK intuitively to me. <em>Does a smaller regularisation term per item work just as well?</em> </p>

<p>For instance <code>lambda * current_weight / N</code> where N is size of training set - at first glance this looks reasonable. I could not find anything on the subject though, and I wonder if that is because regularisation does not work as well with a per-item update, or even goes under a different name or altered formula.</p>
","9","1","836","25573"
"711","<p>This question is in response to a comment I saw on another question.</p>
<p>The comment was regarding the Machine Learning course syllabus on Coursera, and along the lines of &quot;SVMs are not used so much nowadays&quot;.</p>
<p>I have only just finished the relevant lectures myself, and my understanding of SVMs is that they are a robust and efficient learning algorithm for classification, and that when using a kernel, they have a &quot;niche&quot; covering number of features perhaps 10 to 1000 and number of training samples perhaps 100 to 10,000. The limit on training samples is because the core algorithm revolves around optimising results generated from a square matrix with dimensions based on number of training samples, not number of original features.</p>
<p>So does the comment I saw refer some real change since the course was made, and if so, what is that change: A new algorithm that covers SVM's &quot;sweet spot&quot; just as well, better CPUs meaning SVM's computational advantages are not worth as much? Or is it perhaps opinion or personal experience of the commenter?</p>
<p>I tried a search for e.g. &quot;are support vector machines out of fashion&quot; and found nothing to imply they were being dropped in favour of anything else.</p>
<p>And Wikipedia has this: <a href=""http://en.wikipedia.org/wiki/Support_vector_machine#Issues"" rel=""nofollow noreferrer"">http://en.wikipedia.org/wiki/Support_vector_machine#Issues</a> . . . the main sticking point appears to be difficulty of interpreting the model. Which makes SVM fine for a black-box predicting engine, but not so good for generating insights. I don't see that as a major issue, just another minor thing to take into account when picking the right tool for the job (along with nature of the training data and learning task etc).</p>
","69","1","836","25573"
"2366","<p>You are altering weights in the wrong direction for the negative cases.</p>

<p>The line</p>

<pre><code>w = w + eta*Xtr(j,:)';
</code></pre>

<p>should be</p>

<pre><code>w = w + eta*Xtr(j,:)'*ytr(j);
</code></pre>

<p>With that change I got 12% error.</p>
","1","2","836","25573"
"5734","<p>A ""dead"" ReLU always outputs the same value (zero as it happens, but that is not important) for any input. Probably this is arrived at by learning a large negative bias term for its weights.</p>

<p>In turn, that means that it takes no role in discriminating between inputs. For classification, you could visualise this as a decision plane <em>outside</em> of all possible input data.</p>

<p>Once a ReLU ends up in this state, it is unlikely to recover, because the function gradient at 0 is also 0, so gradient descent learning will not alter the weights. ""Leaky"" ReLUs with a small positive gradient for negative inputs (<code>y=0.01x</code> when x &lt; 0 say) are one attempt to address this issue and give a chance to recover.</p>

<p>The sigmoid and tanh neurons can suffer from similar problems as their values saturate, but there is always at least a small gradient allowing them to recover in the long term.</p>
","166","2","836","25573"
"5825","<p>I would not apply convolutional neural networks to your problem (at least from what I can gather from the description).</p>

<p>Convolutional nets' strengths and weaknesses are related to a core assumption in the model class: Translating patterns of features in a regular way either has a minor impact on the outcome, or has a specific useful meaning. So a pattern <code>1 0 1</code> seen in features 9,10,11 is similar in some way to the same pattern seen in features 15,16,17. Having this assumption built in to the model allows you to train a network with far fewer free parameters when dealing with e.g. image data, where this is a key property of data captured by scanners and cameras. </p>

<p>With one-hot encoding of features, you assign a feature vector index from a value or category essentially at random (via some hashing function). There is no meaning to translations between indices of the feature vectors. The patterns <code>0 0 1 0 1 0 0</code>  and <code>0 0 0 1 0 1 0</code> can represent entirely different things, and any associations between them are purely by chance. You can treat a sparse one-hot encoding as an image if you wish, but there is no good reason to do so, and models that assume translations can be made whilst preserving meaning will not do well.</p>

<p>For such a small sparse feature vector, assuming you want to try a neural network model, use a simple fully-connected network.</p>
","11","2","836","25573"
"5979","<p>Generally with multiple classes you have to make a distinction between exclusive and inclusive groups. The simplest cases are ""all classes are exclusive"" (predict only one class), and ""all classes are compatible"" (predict list of classes that apply).</p>

<p>Either way, label the classes as you would want your trained model to predict them. If you expect your classifier to predict an example is in both <code>garbage</code> and <code>footpath</code>, then you should label such an example with both. If you want it to disambiguate between them, then label with a single correct class.</p>

<p>To train a classifier to predict multiple target classes at once, it is usually just a matter of picking the correct objective function and a classifier with architecture that can support it. </p>

<p>For example, with a neural network, you would avoid using a ""softmax"" output which is geared towards predicting a single class - instead you might use a regular ""sigmoid"" function and predict class membership on a simple threshold on each output.</p>

<p>You can get also more sophisticated perhaps with a pipeline model if your data can be split into several exclusive groups - predict the group in the first stage, and have multiple group-specific models predicting the combination of classes in each group in a second stage. This may be overkill for your problem, although it may still be handy if it keeps your individual models simple (e.g. they could all be logistic regression, and the first stage may gain some accuracy if the groups are easier to separate).</p>
","3","2","836","25573"
"6395","<p>It seems standard in many neural network packages to pair up the objective  function to be minimised with the activation function in the output layer.</p>

<p>For instance, for a linear output layer used for regression it is standard (and often only choice) to have a squared error objective function. Another usual pairing is logistic output and log loss (or cross-entropy). And yet another is softmax and multi log loss.</p>

<p>Using notation, $z$ for pre-activation value (sum of weights times activations from previous layer), $a$ for activation, $y$ for ground truth used for training, $i$ for index of output neuron.</p>

<ul>
<li><p>Linear activation $a_i=z_i$ goes with squared error $\frac{1}{2} \sum\limits_{\forall i} (y_i-a_i)^2$</p></li>
<li><p>Sigmoid activation $a_i = \frac{1}{1+e^{-z_i}}$ goes with logloss/cross-entropy objective $-\sum\limits_{\forall i} (y_i*log(a_i) + (1-y_i)*log(1-a_i))$</p></li>
<li><p>Softmax activation $a_i = \frac{e^{z_i}}{\sum_{\forall j} e^{z_j}}$ goes with multiclass logloss objective $-\sum\limits_{\forall i} (y_i*log(a_i))$</p></li>
</ul>

<p>Those are the ones I know, and I expect there are many that I still haven't heard of.</p>

<p>It seems that log loss would only work and be numerically stable when the output and targets are in range [0,1]. So it may not make sense to try linear output layer with a logloss objective function. Unless there is a more general logloss function that can cope with values of $y$ that are outside of the range?</p>

<p>However, it doesn't seem quite so bad to try sigmoid output with a squared error objective. It should be stable and converge at least.</p>

<p>I understand that some of the design behind these pairings is that it makes the formula for $\frac{\delta E}{\delta z}$ - where $E$ is the value of the objective function - easy for back propagation. But it should still be possible to find that derivative using other pairings. Also, there are many other activation functions that are not commonly seen in output layers, but feasibly could be, such as <code>tanh</code>, and where it is not clear what objective function could be applied.</p>

<p>Are there any situations when designing the architecture of a neural network, that you would or should use ""non-standard"" pairings of output activation and objective functions?</p>
","10","1","836","25573"
"6644","<p>The question is asking you to make the following mapping between old representation and new representation:</p>

<pre><code>Represent    Old                     New
0            1 0 0 0 0 0 0 0 0 0     0 0 0 0 
1            0 1 0 0 0 0 0 0 0 0     0 0 0 1 
2            0 0 1 0 0 0 0 0 0 0     0 0 1 0 

3            0 0 0 1 0 0 0 0 0 0     0 0 1 1 
4            0 0 0 0 1 0 0 0 0 0     0 1 0 0 
5            0 0 0 0 0 1 0 0 0 0     0 1 0 1 

6            0 0 0 0 0 0 1 0 0 0     0 1 1 0 
7            0 0 0 0 0 0 0 1 0 0     0 1 1 1 
8            0 0 0 0 0 0 0 0 1 0     1 0 0 0 

9            0 0 0 0 0 0 0 0 0 1     1 0 0 1
</code></pre>

<p>Because the old output layer has a simple form, this is quite easy to achieve. Each output neuron should have a positive weight between itself and output neurons which should be on to represent it, and a negative weight between itself and output neurons that should be off. The values should combine to be large enough to cleanly switch on or off, so I would use largish weights, such as +10 and -10.</p>

<p>If you have sigmoid activations here, the bias is not that relevant. You just want to simply saturate each neuron towards on or off. The question has allowed you to assume very clear signals in the old output layer.</p>

<p>So taking example of representing a 3 and using zero-indexing for the neurons in the order I am showing them (these options are not set in the question), I might have weights going from activation of old output $i=3$, $A_3^{Old}$ to logit of new outputs $Z_j^{New}$, where $Z_j^{New} = \Sigma_{i=0}^{i=9} W_{ij} * A_i^{Old}$ as follows:</p>

<p>$$W_{3,0} = -10$$
$$W_{3,1} = -10$$
$$W_{3,2} = +10$$
$$W_{3,3} = +10$$</p>

<p>This should clearly produce close to <code>0 0 1 1</code> output when only the old output layer's neuron representing a ""3"" is active. In the question, you can assume 0.99 activation of one neuron and &lt;0.01 for competing ones in the old layer. So, if you use the same magnitude of weights throughout, then relatively small values coming from +-0.1 (0.01 * 10) from the other old layer activation values will not seriously affect the +-9.9 value, and the outputs in the new layer will be saturated at very close to either 0 or 1.</p>
","18","2","836","25573"
"6653","<p>Normalise using your training data statistics. Save the values used (e.g. mean and sd per feature), treating them as part of your model. Once you have used these values to transform input, they become fixed translate/scale factors in the model. </p>

<p>Use the same values to normalise test data or new inputs as required. You do not need to calculate new normalisation constants for new data. In fact doing so will most likely reduce the effectiveness of your model.</p>

<p>The same principle applies to interpreting output values if you need to scale those into range that your model produces. Scale according to your training data.</p>
","2","2","836","25573"
"6688","<p>A layer in a neural network does not process logic of the kind you present. It cannot ""check the input pattern with a simple if else pattern"". You could make a model that processed your example inputs to example outputs as you show, but it would not be a neural network.</p>

<p>In machine-learning, neural networks have a formal, numeric structure. There are a few variations, but essentially in your diagram:</p>

<ul>
<li><p>Each circle is a ""neuron"" which may have a numeric output value, also called its <em>activation</em>.</p></li>
<li><p>Each arrow is a connection between neurons that shows where values are passed between neurons. Each connection has a strength or <em>weight</em> associated with it.</p></li>
<li><p>When you have many arrows going into a neuron, you can calculate its new <em>activation</em> starting with the sum of <em>weight</em>s times the activation values of neurons that they are connected from. Call this sum $z$. Then the activation of the neuron you are looking at is $a = f(z)$ where f is an ""activation function"" (also called ""transfer function"").</p></li>
</ul>

<p>The above very roughly covers how many neural networks are constructed. As you can see though, there is no room in that for having your if/else statement.</p>

<p>Instead, neural networks like the one you show (technically a <em>feed forward</em> network also called a <em>multi-layer perceptron</em> or MLP) are most often <em>trained by example</em>. This involves repeatedly giving the network inputs where you know the output you want. To start with the network will give completely incorrect outputs, and you then adjust the weights to try and make the result closer. This is repeated very many times until the network becomes the best model it can for your purpose. A very common algorithm used to train a neural network in this way is <a href=""https://en.wikipedia.org/wiki/Backpropagation"" rel=""nofollow"">backpropagation and gradient descent</a>.</p>

<p>For your problem as described, a neural network is <em>not</em> a good model, and would not be used in the real world. That is because you already know the full logic of a simpler model that covers all possible inputs. At best with a neural network you will get an approximation to that already-perfect model.</p>

<p>However, if this is for the purposes of understanding neural networks, your approach would be roughly this:</p>

<ol>
<li><p>Generate some training data with example inputs and outputs. Most example outputs will be the 23,23 default values, but when inputs are the special values 6,7,8 then the training outputs should be 3 and 4.</p></li>
<li><p>Build an initial network. Normally you would use an appropriate library - it is important you learn how the library works and understand any limitations. E.g. many activation functions have a maximum output of 1, so cannot learn the values 3,4, or 23.</p></li>
<li><p>Train the network to fit the example data. There are very many options there, depending on the library you are using. Usually it is just a matter of selecting parameters for the learning that help it progress smoothly. You may have to run training for a large number of repeats over the whole dataset (called epochs)</p></li>
<li><p>Test that your network works as you expect. Even if you have trained it well, the response of the network is never as precise as your problem statement. If you gave it inputs of 5.8,7.2,8.1 (i.e. close to 6,7,8) then it would likely output something like 7.2,8.5 and not 23,23 .  . . it will interpolate between values.</p></li>
</ol>
","2","2","836","25573"
"6711","<p>In policy iteration, you define a starting policy and iterate towards the best one, by estimating the state value associated with the policy, and making changes to action choices. So the policy is explicitly stored and tracked on each major step. After each iteration of the policy, you re-calculate the value function for that policy to within a certain precision. That means you also work with value functions that measure actual policies. If you halted the iteration just after the value estimate, you would have a non-optimal policy and the value function for that policy.</p>

<p>In value iteration, you implicitly solve for the state values under an ideal policy. There is no need to define an actual policy during the iterations, you can derive it at the end from the values that you calculate. You could if you wish, after any iteration, use the state values to determine what ""current"" policy is predicted. The values will likely not approximate the value function for that predicted policy, although towards the end they will probably be close.</p>
","0","2","836","25573"
"6739","<p>Outside of context of NLG (thus not a direct answer to your whole question, but an answer to your question's title): Generating words from a character-level model has been done using RNNs exposed to large corpora of text, such as Wikipedia content, and trained to predict text <em>character-by-character</em>.</p>

<p>Used to generate content, the model is normally fed a few starting characters and asked to predict the next one. A choice is made from its most-likely predictions and fed back to it to continue the sequence. </p>

<p><a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow"">Here is a blog showing some examples trained on some Shakespear and Wikipedia.</a></p>

<p>Such a network can and does generate nonsense words, although they are often fitting and might read like e.g. a noun or verb as you could expect depending on context. The sentence structure and grammar can come out sort of right, but the semantic content is usually complete gibberish.</p>
","3","2","836","25573"
"6923","<p>In dropout as described in <a href=""http://www.cs.toronto.edu/%7Ehinton/absps/JMLRdropout.pdf"" rel=""nofollow noreferrer"">here</a>, weights are <em>not</em> masked. Instead, the neuron activations are masked, <em>per example as it is presented for training</em> (i.e. the mask is randomised for each run forward and gradient backprop, not ever repeated).</p>
<p>The activations are masked during forward pass, and gradient calculations use the same mask during back-propagation of that example. This can be implemented as a modifier within a layer description, or as a separate dropout layer.</p>
<p>During weight update phase, typically applied on a mini-batch (where each example would have had different mask applied) there is no further use of dropout masks. The gradient values used for update have already been affected by masks applied during back propagation.</p>
<p>I found a useful reference for learning how dropout works, for maybe implementing yourself, is the <a href=""https://github.com/rasmusbergpalm/DeepLearnToolbox"" rel=""nofollow noreferrer"">Deep Learn Toolbox</a> for Matlab/Octave.</p>
","6","2","836","25573"
"8009","<p>I am researching to implement RMSProp in a neural network project I am writing.</p>

<p>I have not found any published paper to refer for a canonical version - I first stumbled across the idea from a <a href=""https://www.coursera.org/course/neuralnets"" rel=""noreferrer"">Coursera class presented by Geoffrey Hinton</a> (lecture 6 I think). I don't think the approach has ever been formally published, despite many gradient-descent optimisation libraries having an option called ""RMSProp"". In addition, my searches are showing up a few variations of the original idea, and it is not clear why they differ, or whether there is a clear reason to use one version over another.</p>

<p>The general idea behind RMSProp is to scale learning rates by a moving average of current gradient magnitude. On each update step, the existing squared gradients are averaged into a running average (which is ""decayed"" by a factor) and when the network weight params are updated, the updates are divided by the square roots of these averaged squared gradients. This seems to work by stochastically ""feeling out"" the second order derivatives of the cost function.</p>

<p>Naively, I would implement this as follows:</p>

<p>Params:</p>

<ul>
<li>$\gamma$ geometric rate for averaging in [0,1]</li>
<li>$\iota$ numerical stability/smoothing term to prevent divide-by-zero, usually small e.g 1e-6</li>
<li>$\epsilon$ learning rate</li>
</ul>

<p>Terms:</p>

<ul>
<li>$W$ network weights</li>
<li>$\Delta$ gradients of weights i.e. $\frac{\partial E}{\partial W}$ for a specific mini-batch</li>
<li>$R$ RMSProp matrix of running average squared weights</li>
</ul>

<p>Initialise:</p>

<ul>
<li>$R \leftarrow 1$ (i.e. all matrix cells set to 1)</li>
</ul>

<p>For each mini-batch:</p>

<ul>
<li>$R \leftarrow (1-\gamma)R + \gamma \Delta^2$ (element-wise square, not matrix multiply)</li>
<li>$W = W - \epsilon \frac{\Delta}{\sqrt{R + \iota}}$ (all element-wise)</li>
</ul>

<p>I have implemented and used a version similar to this before, but that time around I did something different. Instead of updating $R$ with a single $\Delta^2$ from the mini-batch (i.e. gradients summed across the mini-batch, then squared), I summed up each individual example gradient squared from the mini-batch. Reading up on this again, I'm guessing that's wrong. But it worked reasonably well, better than simple momentum. Probably not a good idea though, because of all those extra element-wise squares and sums needed, it will be less efficient if not required.</p>

<p>So now I am discovering further variations that seem to work. They  call themselves RMSProp, and none seems to come with much rationale beyond ""this works"". For example, the Python <code>climin</code> library <a href=""http://climin.readthedocs.org/en/latest/rmsprop.html"" rel=""noreferrer"">seems to implement what I suggest above</a>, but then suggests a further combination with momentum with the teaser ""In some cases, adding a momentum term β is beneficial"", with a partial explanation about adaptable step rates - I guess I'd need to get more involved in that library before fully understanding what they are. In another example the <a href=""http://downhill.readthedocs.org/en/stable/generated/downhill.adaptive.RMSProp.html"" rel=""noreferrer"">downhill library's RMSProp implementation</a> combines two moving averages - one is the same as above, but then another, the average of gradients without squaring is also tracked (it is squared and taken away from the average of squared weights).</p>

<p>I'd really like to understand more about these alternative RMSProp versions. Where have they come from, where is the theory or intuition that suggests the alternative formulations, and why do these libraries use them? Is there any evidence of better performance?</p>
","5","1","836","25573"
"8061","<p>There are lots of function optimising routines that could be applied, based on the description so far. Random search, grid search, hill-climbing, gradient descent, genetic algorithms, simulated annealing, particle swarm optimisation are all possible contenders that I have heard of, and I am probably missing a few.</p>

<p>The trouble is, starting with next to zero knowledge of the black box, it is almost impossible to guess a good candidate from these search options. All of them have strengths and weaknesses. To start with, you seem to have no indication of <em>scale</em> - should you be trying input parameters in any particular ranges? So you might want to try very crude searches through a range of magnitudes (positive and negative values) to find the area worth searching. Such a grid search is expensive - if you have $k$ dimensions and want to search $n$ different magnitudes, then you need to call your black box $n^k$ times. </p>

<p>This can be done in parallel though, and given you are confident that the function is roughly unimodal, you can start with a relatively low number of n (maybe check -10, -1, 0, +1, +10 for 15625 calls to your function taking roughly 8 hours 40 mins using 5 boxes). You may need to repeat with other params once you know whether you have found a bounding box for the mode or need to try yet more values, so this process could take a while longer - potentially days if the optimal value for param 6 is more like 20,000. You could also refine more closely, once you have a potential mode you might want to define another grid of values to search based around. This basic grid search might be my first point of attack on a black box system where I had no clue about parameter meaning, but some confidence that the black box output had a rough unimodal form. </p>

<p>Given the speed of response you should be storing all input and output values in a database for faster lookup and better model building later. No point repeating a call taking 10 seconds when a cache could look it up in 1 millisecond.</p>

<p>Once you have some range of values you think that a mode might be in, then it is time to pick a suitable optimiser.</p>

<p>Given the information so far, I would be tempted to run either more grid search (with a separate linear scaling between values of each param) and/or a random search, constrained roughly to the boxes defined by the set of $2^6$ corner points <em>around</em> the best result found in initial order-of-magnitude search.</p>

<p>At that point you could also consider graphing the data, to see if there is any intuition about which other algorithms could perform well. </p>

<p>With the possibility of parallel calls, then gradient descent might be a reasonable guess, because you can get approximate gradients by adding a small offset to each param and dividing difference that causes in the output by it. In addition, gradient descent (or simple hill climbing) has some chance of optimising with less calls to evaluate the function than approaches that rely on many iterations (simulated annealing) or lots of work in parallel (particle swarm or genetic algorithms). Gradient descent optimisers as used in neural networks, with additions like Nesterov momentum or RMSProp, can cope with changes in function output ""feature scale"" such as different sizes and heights of peaks, ridges, saddle points.</p>

<p>However, gradient descent and hill climbing algorithms are not robust against all function shapes. A graph or several of what your explorations are seeing may help you to decide on a different approach. So keep all the data and graph it in case you can get clues.</p>

<p>Finally, don't rule out random brute-force search, and being able to just accept ""best so far"" under time constraints. With low knowledge of the internals of the black box, it is a reasonable strategy.</p>
","3","2","836","25573"
"8232","<p>This is normal behaviour of most classifiers. You are not guaranteed 100% accuracy in machine learning, and a direct consequence is that classifiers make mistakes. Different classifiers, even if trained on the same data, can make different mistakes. Neural networks with different starting weights will often converge to slightly different results each time.</p>

<p>Also, perhaps in your problem the classification is an artificial construct over some spectrum (e.g. ""car"" vs ""van"" or ""safe"" vs ""dangerous"") in which case the mistake in one case is entirely reasonable and expected?</p>

<p>You should use the value from the classifier that you trust the most. To establish which one that is, use cross-validation on a hold-out set (where you know the true labels), and use the classifier with the best accuracy, or other metric, such as logloss or area under ROC. Which metric you should prefer depends on the nature of your problem, and the consequences of making a mistake.</p>

<p>Alternatively, you could look at averaging the class probabilities to determine the best prediction - perhaps one classifier is really confident in the class assignment, and the other is not, so an average will go with the first classifier. Some kind of model aggregation will often boost accuracy, and is common in e.g. Kaggle competitions when you want the highest possible score and don't mind the extra effort and cost. However, if you want to use aggregation to solve your problem, again you should test your assumptions using validation and a suitable metric so you know whether or not it is really an improvement. </p>
","2","2","836","25573"
"8348","<p>You do not need additional learning algorithms to perform reinforcement learning in simple systems where you can explore all states. For those, simple iterative <a href=""https://en.wikipedia.org/wiki/Q-learning"" rel=""nofollow noreferrer"">Q-learning</a> can do very well - as well as a variety of similar techniques, such as Temporal Difference, SARSA. All these can be used without neural networks, provided your problem is not too big (typically under a few million state/action pairs). </p>

<p>The simplest form of Q-learning just stores and updates a table of <code>&lt;state, action&gt; =&gt; &lt;estimated reward&gt;</code> pairs. There is no deeper statistical model inside that. Q-learning relies on estimates of reward from this table in order to take an action and then updates it with a more refined estimate after each action. </p>

<p>Q-learning and related techniques such as Temporal Difference are sometimes called <em>model free</em>. However, this does not refer to the absence of a statistical model such as a neural net. Instead, it means that you do not need to have a model of the system you are learning to optimise available, such as knowing all the probabilities of results and consequences of actions in a game. In model free RL, all learning can be done simply by experiencing the system as an agent (if you do have a model then it may still be used for simulation or planning). When considering whether or not you need a neural network, then the term <em>tabular</em> is used for systems that work with explicit value estimates for every possible state or state/action pair. And the term <em>function approximation</em> is used to describe how a neural network is used in the context of RL.</p>

<p>For large, complex problems, which may even have infinite possible states, it is not feasible to use tabular methods, and you need good generalised value estimates based on some function of the state. In those cases, you can use a neural network to create a function approximator, that can estimate the rewards from similar states to those already seen. The neural network replaces the function of the simple table in tabular Q-Learning. However, the neural network (or other supervised ML algorithm) does not perform the learning process by itself, you still need an ""outer"" RL method that explores states and actions in order to provide data for the NN to learn.</p>
","8","2","836","25573"
"8417","<p>There is not really a ""right way"" to use machine learning outside of running the algorithms, any more than there is a ""right way"" to use a sorting algorithm, or any other kind of complex automation.</p>

<p>Some guidelines/thoughts:</p>

<ul>
<li><p>You can start using your model to make predictions, once those predictions are useful to you. This may be once they pass some threshold in performance that would make the predictions the best way of achieving some other goal. That may just mean ""better than the last ML model"", but could also mean ""cheaper and faster than having some people doing it, even if it is less accurate"".</p></li>
<li><p>Depending on how your training data is collected, and how the model works, it may be possible to <em>both</em> use your model for predictions, and to continue training it on new data as it arrives.</p></li>
<li><p>However, there is no <em>requirement</em> for continuous learning. Learning systems that continuously adapt to new data are useful in some situations (example might be recommendation systems), but might be inappropriate in others (example might be automatic driver for a vehicle, where you don't want to risk reduction in performance whilst in use).</p></li>
</ul>

<p>One thing that <em>is</em> considered very standard practice is to demonstrate how the machine learning system is performing after a period of learning by using a test metric. The simplest variation is to keep some of your precious data to one side, deliberately not training on it, then measure the performance. Often two such sets are kept aside - a <em>cross validation</em> set is used to help decide automatically between variations of your learning algorithm, and a <em>test</em> set is used to assess the end result of that selection.</p>
","2","2","836","25573"
"8499","<p>I wouldn't give any reason to make an choice <em>a priori</em>, based on such a broad description.</p>

<p>Instead I would build models for NN and SVM (or any other that you were considering), train them on the data then compare using cross-validation and a suitable metric for your problem - such as accuracy, <a href=""https://en.wikipedia.org/wiki/F1_score"" rel=""nofollow"">F1 score</a>, <a href=""https://en.wikipedia.org/wiki/Receiver_operating_characteristic"" rel=""nofollow"">AUROC</a> or log loss. You may also want to note characteristics other than performance metric if you care about costs such as time to train, amount of computing resource required etc.</p>

<p>Experience may tell you likely best choices from the problem domain. For instance, if you are classifying photographic or scanned images, then currently a convolutional neural network would be a very strong candidate, as these have repeatedly shown themselves as best performers with real-world image data.</p>
","3","2","836","25573"
"8817","<p>The equation that you show <em>calculates</em> (the negative of) a gradient to the objective function. The value $\delta_j$ is the rate of change of the objective function for an isolated change of output of a neuron <em>before</em> applying the activation function for the neuron. The factor of $f′(net_i)$ is required because it is <em>correct</em> - it is due to <a href=""https://en.wikipedia.org/wiki/Chain_rule"" rel=""nofollow"">the chain rule</a> applied twice - once due to the linear relationship linking weights between layers, and again to get the gradient pre-activation function in the next layer down. </p>

<p>An interpretation of $\delta_i$ is ""the (negative) gradient of objective function with respect to a pre-activation value of a neuron the network"". If, somehow, the pre-activation value could be changed - ignoring <em>how</em> it could be changed - then $\delta_i$ tells you the linear scale of the impact on the objective function, at least for very small changes. </p>

<p>This value is not normally used directly in weight updates, but is fed deeper backwards into the networks layers if there are yet lower layers to update, because it can be used for weight updates in those layers using the same rule.</p>

<p>The property of low gradients associated with strong activations is because you previously chose <code>tanh</code> as an activation function, and is not because of the way you calculate the gradient. Once you chose <code>tanh</code>, you were not really free to choose a different backpropagation gradient calculation. Or rather, there is not much to gain - and potentially a lot to lose - by doing so.</p>

<blockquote>
  <p>What is the purpose of this for weight updating? </p>
</blockquote>

<p>The value shown is not used directly, but used to derive $\frac{\partial J}{\partial W_{ij}}$ gradients for weights in deeper layers (where $J$ is value of your objective function).</p>

<p>In terms of your equations</p>

<p>$$\frac{\partial J}{\partial W_{ij}} = -\delta_j y_i$$</p>

<p>This is very simple, so it has been folded in to your delta update rule. However, doing that is hiding something important. Out of all the intermediate values you calculate during back propagation, $\frac{\partial J}{\partial W_{ij}}$ is the key one. It is the gradient of the objective function <em>with respect to parameters that you can change</em>. </p>

<p>Once you have a set of gradients for  $\frac{\partial J}{\partial W_{ij}}$, this can be used in various ways.  Using this gradient <em>directly</em> for weight updates using the delta rule (as in your equation $\Delta W_{ij} = \eta \delta_j y_i$) is a basic approach. The purpose of this rule is to reduce the value of the objective function by taking a small step in the direction that you have just calculated <em>will</em> reduce the value of objective function. The step size needs to be small (multiplied by a factor $\eta$) because you have only measured the gradient at one point, and don't know for what step size the relationship still holds - complicated by the fact that you are making a similar step in multiple dimensions at once.</p>

<p>The delta update rule is a relatively weak update method compared to e.g. Nesterov momentum, Adagrad, Rmsprop optimisers which will use the calculated gradient as input, but adjust the update weights based on history of previous gradients. These optimisers can deal with low but consistent gradient values and make large updates.</p>

<blockquote>
  <p>How would the networks behave differently 
  if one would remove or change this factor?</p>
</blockquote>

<p>This is not advisable as effectively you would be optimising against a <em>different</em> network architecture than the feed-forward network you had constructed. The most likely way they would behave is to either diverge or settle on a larger value of your objective function than you would otherwise see. Making small changes (such as a minimum absolute gradient) can work, but usually such a tweak is best done consistently, by changing the activation function as well.</p>

<p>In a few edge cases, you might consider ""lying"" in the gradient calculation. A trivial case could be considered the ReLU activation function ($y = 0 (if x \lt 0), y = x (otherwise)$) where the gradient is technically undefined at $x=0$, but can safely be treated as either 0 or 1.</p>

<p>An alteration to updates that is more usually done (by e.g. momentum variants, Adagrad, RMSProp et al) is to use the gradient as-is, but then multiply update amounts by various factors based on history or estimates of higher-order derivatives, because you can guess that the gradient will not switch sign if you update a weight by a smaller or larger amount. These approaches replace the delta update rule with update mechanisms that are more robust when gradients become small.</p>
","4","2","836","25573"
"8856","<p>No it is not possible to learn those meta parameters from a data set using a learning algorithm. There is no way to calculate a gradient of any objective function with respect to pool_size and stride params. Even if there were, they are typically discrete values and relatively small integers, so cannot be updated in the same way as e.g. the weights are.</p>

<p>What you can do is search through different values looking for optimal ones, using cross-validation to pick the best. The size and stride meta parameters can also have a large effect on memory use and speed of learning, so it may be the case you are looking to find a good compromise. With stride in lower layers for instance, it may be more effective (in terms of training time required for similar level of accuracy) to have a higher stride value but increase the overall training set size by adding random x/y translations to the training set images.</p>
","4","2","836","25573"
"9032","<p>In a neural network model, you can use <a href=""https://en.wikipedia.org/wiki/Autoencoder"" rel=""nofollow"">autoencoders</a>.</p>

<p>The basic idea of an autoencoder is to learn a hidden layer of features by creating a network that simply copies the input vector at the output. So the training features and training ""labels"" are initially identical, no supervised labels are required. This can work using a classic <em>triangular</em> network architecture with progressively smaller layers that capture a compressed and hopefully useful set of derived features. The network's hidden layers learn representations based on the larger unsupervised data set. These layers can then be used to initialise a regular supervised learning network to be trained using the actual labels. </p>

<p>A similar idea is pre-training layers using a <a href=""https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine"" rel=""nofollow"">Restricted Boltzmann Machine</a>, which can be used in a very similar way, although based on different principles. </p>
","0","2","836","25573"
"9311","<p>The first logloss formula you are using is for multiclass log loss, where the $i$ subscript enumerates the different classes in an example. The formula <em>assumes</em> that a single $y_i'$ in each example is 1, and the rest are all 0.</p>

<p>That means the formula only captures error on the target class. It discards any notion of errors that you might consider ""false positive"" and does not care how predicted probabilities are distributed other than predicted probability of the true class.</p>

<p>Another assumption is that $\sum_i y_i = 1$ for the predictions of each example. A softmax layer does this automatically - if you use something different you will need to scale the outputs to meet that constraint.</p>

<h2>Question 1</h2>

<blockquote>
  <p>Isn't it a problem that the $y_i$ (in $log(y_i)$) could be 0?</p>
</blockquote>

<p>Yes that can be a problem, but it is usually not a practical one. A randomly-initialised softmax layer is extremely unlikely to output an exact <code>0</code> in any class. But it is possible, so worth allowing for it. First, don't evaluate $log(y_i)$ for any $y_i'=0$, because the negative classes always contribute 0 to the error. Second, in practical code you can limit the value to something like <code>log( max( y_predict, 1e-15 ) )</code> for numerical stability - in many cases it is not required, but this is sensible defensive programming.</p>

<h2>Question 2</h2>

<blockquote>
  <p>I've learned that cross-entropy is defined as $H_{y'}(y) := - \sum_{i} ({y_i' \log(y_i) + (1-y_i') \log (1-y_i)})$</p>
</blockquote>

<p>This formulation is often used for a network with one output predicting two classes (usually positive class membership for 1 and negative for 0 output). In that case $i$ may only have one value - you can lose the sum over $i$.</p>

<p>If you modify such a network to have two opposing outputs and use softmax plus the first logloss definition, then you can see that in fact it <em>is the same error measurement</em> but folding the error metric for two classes into a single output.</p>

<p>If there is more than one class to predict membership of, and the classes are <em>not exclusive</em> i.e. an example could be any or all of the classes at the same time, then you will need to use this second formulation. For digit recognition that is not the case (a written digit should only have one ""true"" class)</p>
","28","2","836","25573"
"9776","<p>I don't think that your assumption $w' = w^T$ holds. Or rather is not necessary, and if it is done, it is not in order to somehow automatically reverse the calculation to create the hidden layer features. It is not possible to reverse the compression in general, going from n to smaller m, directly in this way. If that was the goal, then you would want a form of matrix inversion, not simple transpose.</p>

<p>Instead we just want $w_{ij}$ for the compressed higher-level feature representation, and will discard $w'_{ij}$ after auto-encoder is finished. </p>

<p>You <em>can</em> set $w' = w^T$ and tie the weights. This can help with regularisation - helping the autoencoder generalise. But it is not necessary.</p>

<p>For the autoencoder to function it doesn't actually matter what activation function you use after the layer that you are pre-training, provided the last layer of the autoencoder can express the range of possible inputs. However, you may get varying quality of results depending on what you use, as normal for a neural network. </p>

<p>It is quite reasonable to use the same activation function that you are building the pre-trained layer for, as it is the simplest choice. </p>

<p>Using an inverse function is possible too, but not advisable for sigmoid or tanh, because e.g. arctanh is not defined &lt; -1 or > 1, so likely would not be numerically stable.</p>
","5","2","836","25573"
"10052","<p>Your choices of <code>activation='softmax'</code> in the last layer and compile choice of <code>loss='categorical_crossentropy'</code> are good for a model to predict multiple mutually-exclusive classes.</p>

<p>Regarding more general choices, there is rarely a ""right"" way to construct the architecture. Instead that should be something you test with different meta-params (such as layer sizes, number of layers, amount of drop-out), and should be results-driven (including any limits you might have on resource use for training time/memory use etc). </p>

<p>Use a cross-validation set to help choose a suitable architecture. Once done, to get a more accurate measure of your model's general performance, you should use a separate test set. Data held out from your training set separate to the CV set should be used for this. A reasonable split might be 60/20/20 train/cv/test, depending on how much data you have, and how much you need to report an accurate final figure.</p>

<p>For Question #2, you can either just have two outputs with a softmax final similar to now, or you can have <em>final</em> layer with one output, <code>activation='sigmoid'</code> and <code>loss='binary_crossentropy'</code>.</p>

<p>Purely from a gut feel from what might work with this data, I would suggest trying with <code>'tanh'</code> or <code>'sigmoid'</code> activations in the hidden layer, instead of <code>'relu'</code>, and I would also suggest increasing the number of hidden neurons (e.g. 100) and reducing the amount of dropout (e.g. 0.2). Caveat: Gut feeling on neural network architecture is not scientific. Try it, and test it.</p>
","37","2","836","25573"
"10122","<p>You do not <em>have</em> to normalise features used in logistic regression, but sometimes it can help.</p>

<p>You should normalise features used in logistic regression, if you are using a gradient-based optimiser (e.g. SGD) to find the optimum weights. That is because the optimiser will perform better when partial derivatives of the cost function are of similar magnitude in each direction. When the derivatives vary too much, you will need a lower learning rate to compensate (making learning slower, and more likely to get stuck) or the optimiser will not converge - it may oscillate or start to diverge instead.</p>
","3","2","836","25573"
"10684","<p>The simplest solution would be to average the predictions from ""Team A vs Team B"" and ""Team B vs Team A"" as inputs, and consider that part of your algorithm. You are already doing something similar when augmenting training data by considering both arrangements. This solution is also used by image classification models, which may output predictions averaged over more than one random patch from an input image, to get better translation invariance.</p>

<p>Inherently the problem you face is one of dimensionality - the input features are linked by some symmetry (e.g. Team A's defensive strength is equivalent to Team B's defensive strength), but this is not encoded into the learning model. Depending on your model class, you might be able to link weights or make adjustments to the features to better encode the symmetry. But this is a lot more work than creating a simple meta-model that predicts on both input arrangements.</p>

<p>Another simple alternative is to have a sorting algorithm apply between the features, such that which is Team A and which Team B as presented to the ML and prediction is decided by some function of the feature values (e.g. always make the team with highest value of a specific predictive feature the first team). This makes the training and prediction deterministic and avoids the issue of needing to treat the teams equivalently. In effect it breaks the symmetry, instead of ""Team A"" and ""Team B"" features you have ""Team with highest X"" and ""Team with lowest X"" features, and predict whether ""Team with highest X"" will win. A disadvantage of this approach is that which feature or rule you pick to sort by becomes a meta-param of your ML routine. It may help to pick a feature with a known strong influence on the result (sorting by team name is not likely to be effective). </p>
","0","2","836","25573"
"10922","<p>It appears that AlphaGo did <em>not</em> rate the move as a best possible move for Lee Sedol, just as one that was within its search space. To put into context the board is 19x19, so a 1 in 10000 chance of a move is much lower than chance of the square being picked at random. That likely makes the move that it ""found"" not worth exploring much deeper.</p>

<p>It is important to note too that the probabilities assigned to moves are equivalent to AlphaGo's rating for quality of that move - i.e. AlphaGo predicted that this was a bad choice for its opponent. Another way of saying this, is ""there is a probability p that this move is the best possible one, and therefore worth investigating further"". There is no separate quality rating - AlphaGo does not model ""opponent's chance of making a move"" separately from ""opponent's chance of gaining the highest score from this position if he/she makes that move"". There is just one probability covering both those meanings <sup>1</sup></p>

<p>As I understand it, AlphaGo rates the probabilities of <em>all possible moves</em> at each game board state that it considers (starting with the current board), and employs the most search effort for deeper searches on the highest rated ones. I don't know the ratios or how many nodes are visited in a typical search, but expect that a 1 in 10000 rating would not have been explored in much detail if at all. </p>

<p>It is not surprising to see the probability calculation in the system logs, as the logs likely contain the ratings for <em>all</em> legal next moves, as well as ratings for things that didn't actually happen in the game but AlphaGo considered in its deeper searches.</p>

<p>It is also not surprising that AlphaGo failed to rate the move correctly. The neural network is not expected to be a perfect oracle that rates all moves perfectly (if it was, then there would be no need to search). In fact, the opposite could be said to be the case - it is surprising (and of course an amazing feat of engineering) just how good the predictions are, good enough to beat a world-class champion. This is not the same as solving the game though. Go remains ""unsolved"", even if machines can beat humans, there is an unknown amount of additional room for better and better players - and in the immediate future that could be human or machine.</p>

<hr>

<ol>
<li>There are in fact two networks evaluating two different things - the ""policy network"" evaluates potential moves, and the output of that affects the Monte Carlo search. There is also a ""value network"" which assesses board states to score the end point of the search. It is the policy network that predicted the low probability of the move, which meant that the search had little or no chance of exploring game states past Lee Sedol's move (if it had, maybe the value network would of detected a poor end result from playing that through). In reinforcement learning, a <em>policy</em> is set of rules, based on known state, that decide between actions that an agent can take.</li>
</ol>
","5","2","836","25573"
"10996","<p>Yes you can. There are no hard rules against having different activation functions in any layer, and combining these two types should give no numerical difficulties.</p>

<p>In fact it can be a good choice to have tanh in hidden layers and sigmoid on the last layer, if your goal is to predict membership of a single class or non-exclusive multiple class probabilities. The sigmoid output lends itself well to predicting an independent probability (using e.g. a logloss (aka cross-entropy) objective function).</p>

<p>Whether or not it is better than using sigmoid on all layers will depend on other features of your network, the data and the problem you are trying solve. Usually the best way to find out which is better - at least in terms of accuracy - is to try out some variations and see which scores best on a cross-validation data set. In my experience, there often is a small difference between using tanh or sigmoid in the hidden layers.</p>
","5","2","836","25573"
"11101","<p>Although many solutions in production systems still use a sliding window as described below in this answer, the field of computer vision is moving quickly. Recent advances in this field include <a href=""https://arxiv.org/abs/1506.01497"" rel=""noreferrer"">R-CNN</a> and <a href=""http://pjreddie.com/media/files/papers/yolo.pdf"" rel=""noreferrer"">YOLO</a>.</p>

<hr>

<p>Detecting object matches in an image, when you already have an object classifier trained, is usually a matter of brute-force scanning through image patches.</p>

<p>Start with the largest expected patch size. E.g. if your image is 1024 x 768, but always a distance shot of a road maybe you do not expect any car to take up more than 80 x 80 pixels in the image. So you take an 80x80 block of pixels from one corner of the image, and ask your classifier what chance there is a car in that corner. Then take the next patch - perhaps move by 20 pixels. </p>

<p>Repeat for all possible positions, and decide which patches are most likely to contain cars.</p>

<p>Next, take a block size down (maybe 60 x 60, moving 15 pixels at a times) and repeat the same exercise again. Repeat this until you have hit the expected smallest block size for your goal.</p>

<p>Eventually you will have a list of areas within the image, with the probability that each contains a car.</p>

<p>Overlapped blocks both with high probability are most likely the <em>same</em> car, so the logic needs to have thresholds for merging blocks - usually taking the overlapped area with the highest probability score - and declaring there is only one car in that area.</p>

<p>As usual with ML approaches, you will need to experiment with correct meta-params - in this case block sizes, step sizes and rules for merging/splitting areas - in order to get the most accurate results.</p>
","11","2","836","25573"
"11108","<p>Support vector machines have one built-in ""layer"" that helps with having an interpretation of the data - the kernel. You could even use output from some other image classifier, including a neural network, as the kernel. E.g. you could measure how far apart two images are in ""classifier space"" from a trained neural network (maybe one trained against different targets from the ones you want to classify with the SVM)</p>

<p>There is also an implied ""layer"" in image feature construction.  Common image features are histograms of pixel values, histograms of pixel differences (edge detection), bags of ""visual words"" and there are many more possible transformations and statistics. </p>

<p>Typically if you are using a classifier other than a deep neural network (which is supposed to discover the features automatically), then you will pre-process the image into a set of features. That pre-processing covers the majority of an approach being ""good at images"".</p>
","2","2","836","25573"
"11206","<p>The purpose of this algorithm appears to be about reducing cost. So a metric which includes the <em>financial risk</em> associated with defaults should be preferred over one that simply measures number of defaults.</p>

<p>A weighted <code>false_positive * loss_on_default</code> metric would seem to be ideal, where you set the probability cutoff of positive class (auto-approve) so that 80% of loans were approved regardless of absolute probability of the prediction. Conceptually this is similar to picking a point on the ROC curve and assessing the algorithm at that point.</p>

<p>This metric could not be used directly as an objective function, you'd probably use want something like logloss weighted per sample by the loss_on_default in order to train a model.</p>

<p>I did a quick search for similar metrics, and could not find anything (it is also not something I have done before, so do please test the idea carefully, it is mostly conjecture on my part). There are plenty of metrics in e.g. SciKit learn however that take an array <code>sample_weight</code> that does something similar - e.g. <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss"" rel=""nofollow"">Sklearn's Zero-One loss</a> is pretty close except it will include false negatives as well.</p>
","1","2","836","25573"
"11227","<p>In a large neural network there can be millions of free parameters, and assessing the current heuristic means evaluating the network for a significant number of training records, assuming supervised or semi-supervised learning.</p>

<p>Gradient-based methods can cope with this amount of parameters in a reasonable time. The gradient can be approximated for all parameters in a single pass through a subset of the training data, and a single step will update all of parameters at once. There is theoretical support showing gradient-based changes to weights should converge to a better solution (caveat being local minima). There is no theoretical result that shows that a set of weights with optimal values for <em>some</em> of its parameters (in some local subset of all the weights) should be better at a task than a set of random weights. This puts search-based methods that look to build good local choices and combine them at a disadvantage in that we really don't know whether they have a chance of working for a specific problem.</p>

<p>Genetic algorithms have successfully been used to train small/medium networks, e.g. <a href=""https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies"" rel=""nofollow"">NEAT</a>. This works well in small control systems with immediate feedback (although reinforcement learning may be a better choice in many cases), and also commonly seen in a-life simulations. But this approach does not scale up to complex networks, or supervised learning scenarios with large amounts of data. The cost of assessing the GA against the training data for each member of the population, and the size of population/number of assessments required for a search for a genome that might be several megabytes, makes GA for large networks computationally too expensive.</p>

<p>Simulated annealing cannot be used directly to search weights in a standard feed-forward network, since it needs a heuristic calculated (e.g. from a significant portion of training data) for every tiny ""move"" mode in parameter space - the cost of each move needs to be fast to compute. However, Boltzmann Machine training is closely related to simulated annealing, and <a href=""https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine"" rel=""nofollow"">Restricted Boltzmann Machines</a> can be used to pre-train deep neural networks, amongst other things, so there are cases where SA is sort-of used.</p>
","3","2","836","25573"
"11296","<p>Your design makes some sense, but there is no need to limit connections even if you expect to represent probabilities of upper/lower case separately, because they will interact usefully. E.g if the character could most likely be one of <code>o, O, Q, G</code> then this might be useful information to choose the correct one. </p>

<p>If you went ahead, you would need to train this network without the final layer (so that it learns the representations you expect, not some other group of 52 features), then add the final layer later, with no need for special connection rules, just use existing ones. Initially you would training the new layer separately from the full output of the 52-class net i.e. probability values, not selected class. Then you would combine with the existing net and fine-tune the result by running a few more epochs with a low learning rate on the final model.</p>

<p>That all seems quite complex, and IMO unlikely to gain you much accuracy (although I am guessing, it could be great - so if you have time to explore ideas, you could still try). Personally I would not take your hidden layer idea further. The full 52-class version with simple logic to combine results is I think simpler. This is also not necessary, the neural net can learn to have two different-looking images be in the same class quite easily, provided you supply examples of them in training. However, it may give you useful insights into categorisation failures in training or testing.</p>

<p>It is not clear from the question, but if you are not already using convolutional neural network for lower layers, then you should do so. This will make the largest impact on your results by far.</p>
","0","2","836","25573"
"11596","<p>If you are using basic gradient descent (with no other optimisation, such as momentum), and a minimal network 2 inputs, 2 hidden neurons, 1 output neuron, then it is definitely possible to train it to learn XOR, but it can be quite tricky and unreliable.</p>

<ul>
<li><p>You may need to adjust learning rate. Most usual mistake is to set it too high, so the network will oscillate or diverge instead of learn.</p></li>
<li><p>It can take a surprisingly large number of epochs to train the minimal network using batched or online gradient descent. Maybe several thousand epochs will be required.</p></li>
<li><p>With such a low number of weights (only 6), sometimes random initialisation can create a combination that gets stuck easily. So you may need to try, check results and then re-start. I suggest you use a seeded random number generator for initialisation, and adjust the seed value if error values get stuck and do not improve.</p></li>
</ul>
","4","2","836","25573"
"11689","<p>If you were fitting a large number of different models, and you had sequences of training data for each different model (with the params already known in those cases), then you might be able to use a recursive neural network (RNN) to provide param estimates for new data sequences. However, that does not seem to be your situation.</p>

<p>As I understand your question, you have a mathematical model with some free parameters, and <em>one</em> set of data that you would like to use to estimate those parameters.</p>

<p>A neural network is not really of any use to you here. That is because the neural network would <em>replace</em> your semi-empirical model with the NN, and the free parameters become the weights of the NN. The trained NN would not match your desired model, but would generically fit to the data. It could be used to predict more function outputs given the inputs - the NN might even do better than your preferred model, which would be an interesting result implying that your model is incomplete.</p>

<p>Often we don't know (or perhaps even care) about an underlying parsimonious model. In this case, ML techniques that fit params of a generic model to data using some objective function like squared error can be very useful tools. However, when we do have a good idea about a simple or explanatory  model, those same techniques are less useful, and instead we can use optimisers direct on the model - in some cases these could be the same optimisers used to train neural networks, provided the model is differentiable.</p>
","1","2","836","25573"
"11705","<p><code>mnist.load_data()</code> supplies the MNIST digits with structure <code>(nb_samples, 28, 28)</code> i.e. with 2 dimensions per example representing a greyscale image 28x28.</p>

<p>The Convolution2D layers in Keras however, are designed to work with 3 dimensions per example. They have 4-dimensional inputs and outputs. This covers colour images <code>(nb_samples, nb_channels, width, height)</code>, but more importantly, it covers deeper layers of the network, where each example has become a set of feature maps i.e. <code>(nb_samples, nb_features, width, height)</code>.</p>

<p>The greyscale image for MNIST digits input would either need a different CNN layer design (or a param to the layer constructor to accept a different shape), or the design could simply use a standard CNN and you must explicitly express the examples as 1-channel images. The Keras team chose the latter approach, which needs the re-shape.</p>
","11","2","836","25573"
"11736","<p>You need to limit the network model to one which naturally would generalise your problem.</p>

<p>For a simple multiplication, this would be a single linear neuron. You can also train bias so that the network represents <code>y = Ax + B</code>.</p>

<p>If you take that very simple network give it just 2 or 3 examples to learn from, and train it using least squares error, it will quickly converge to the function you want. A single layer linear network like this can generalise to <code>y = Ax + B</code> where x, y and B are vectors, and A is a matrix - i.e. it can solve simultaneous linear equations provided you supply enough examples to do so (usually number of examples equals number of dimensions of the vectors).</p>

<p>This is actually really trivial, and if you train such a simple network using any modern NN library, it will be very fast to learn your linear function. However, in the general case of having a specific non-linear function and wanting the network to learn it in a general sense, it is not possible. The network can be made to learn to generalise reasonably well near where you have provided example inputs and outputs, but will output incorrect results when you give it an <code>x</code> value far away from the training examples.</p>

<p>It is important to note that the network does not learn the math formula for your function. It finds the best approximation given its own internal model. But by picking a network where the internal model is a good match to the function you want to learn, it is going to generalise well to it, and may be able to get it exactly if there is no noise in the examples and enough of them.</p>
","1","2","836","25573"
"11811","<p>This looks like almost an ideal use case for a Recurrent Neural Network (RNN) model. I say almost, as your output variable is 2 separate class vectors, that part is slightly fiddly. </p>

<p>The main advantage of using a RNN is that the model would be built on the assumption that <code>s1_var1</code> is of the same type as <code>s2_var1</code> etc. Whilst with other models you <em>could</em> flatten the time series into a 10 x 20 = 200 long vector, and the output likewise into a 50 long vector, and treat the whole problem as a single step supervised learning, such a model would not include the same assumption.</p>

<p>For your problem, the RNN would have 10 inputs (corresponding to the 5 classes of each of 2 variables) and 10 outputs. You would train it by presenting your input sequence one sample at a time in the correct order 1 to 20, then continue running the network 5 more steps, comparing the outputs against expected. When predicting, you do similar, but then just read off the outputs as predictions, one at a time.</p>

<p>There are a few different Python libraries that support RNN models, and there are different internal choices (such as using LSTM layer versus GRU) which you might want to explore. I am currently learning the <a href=""http://keras.io/"" rel=""nofollow"">Keras</a> framework, which supports some options for RNNs. Probably the example code which <a href=""https://github.com/fchollet/keras/blob/master/examples/stateful_lstm.py"" rel=""nofollow"">learns to predict an exponentially-decaying sine wave</a> would be a good place to start trying to understand the Keras model - although that's a regression, it should give you some insight into how RNNs can be built for this kind of series-based data.</p>

<hr>

<p>An alternative to RNN here might be a <a href=""https://en.wikipedia.org/wiki/Hidden_Markov_model"" rel=""nofollow"">hidden markov model</a> - if you have some insight into the internal state of the system, that could be a better option.</p>
","1","2","836","25573"
"11819","<p>Your error calculation looks wrong:</p>

<pre><code>if output1 &lt; 0.99:
    error += (output1)**2
if output2 &gt; 0.01:
    error += (1-output2)**2
</code></pre>

<p>If I am reading this correctly, output1 = 1.0 is a correct classification, since you treat any value above 0.99 as no error at all. However, you then measure the squared error as being <code>output1 ** 2</code> which means the largest error is at 0.99. This is going to confuse the network, it will treat classifications above the line as perfect 1.0 if they score > 0.99, or otherwise want to adjust weights to make the classification 0.0. The opposite is true for your scoring of points under the line. </p>

<p>You need to either</p>

<ol>
<li>Swap the conditionals (change <code>&lt; 0.99</code> to <code>&gt; 0.01</code> and vice-versa)</li>
</ol>

<p>or</p>

<ol start=""2"">
<li>Replace <code>(output1)</code> with <code>(1-output1)</code> and <code>(1-output2)</code> with <code>(output2)</code> </li>
</ol>

<p>in order to make the logic self-consistent. Which you choose depends on whether above the line points are positive class, or it is the negative ones. If above the line is positive you should use option 2.</p>

<p>In addition, as comments have pointed out, you might be a little ambitious with your approach. I spot the following things that make your job harder:</p>

<ul>
<li><p>Using genetic algorithm search. Whilst it is possible to train small NNs like yours using a GA, it is less efficient. GAs might be a good choice for control systems or for a-life scenarios, but are far behind cutting edge for supervised learning.</p></li>
<li><p>Using mean squared error metric for classification. You should use <a href=""https://en.wikipedia.org/wiki/Cross_entropy"" rel=""nofollow"">logloss</a>, which more heavily penalises bad guesses. With a mean squared error, there is a higher chance the network will settle for a few bad misclassifications if it also gets some other reasonable ones.</p></li>
<li><p>Ambitious function to separate. A neural network should be able to classify your function, but that's a lot of curvature to learn for a starting problem. Reduce it to just a couple of cycles to start with should help you debug practical issues.</p></li>
<li><p>No input normalisation. This may lead to saturated values in the network - close to 1.0 or 0.0, making it very hard to discriminate between good and bad weights. You don't show your weight initialisations - large initial weights can also cause this effect (although using a GA may actually help here, depending on how you are mutating weights).</p></li>
</ul>
","1","2","836","25573"
"11856","<p>You are right there are just 10 params in your example.</p>

<p>For determining gradients, you just add up all the deltas from backpropagation in each location - i.e. you run backpropagation 30x30 = 900 times, for each position the 3x3 kernel is used, for every example in your batch (or just for one example if you are running most simple onine stochastic gradient descent), and for each position you add those delta values into a suitably-sized buffer (10 values for weight deltas, or 9 values for previous layer activation deltas). You will end up with one set of summed deltas matching your single 3x3 filter (plus a delta bias term). You then apply the summed version to update the weights of your single filter + bias.</p>

<p>Note this is a general rule you can apply whenever multiple gradient sources from backpropagation can be applied to any parameter - they just add. This occurs in RNNs too, or in any structure where you can set an objective function for non-output neurons.</p>
","2","2","836","25573"
"11870","<p>Bias operates per virtual neuron, so there is no value in having multiple bias inputs where there is a single output - that would equivalent to just adding up the different bias weights into a single bias.</p>

<p>In the feature maps that are the output of the first hidden layer, the colours are no longer kept separate*. Effectively each feature map is a ""channel"" in the next layer, although they are usually visualised separately where the input is visualised with channels combined. Another way of thinking about this is that the separate RGB channels in the original image are 3 ""feature maps"" in the input.</p>

<p>It doesn't matter how many channels or features are in a previous layer, the output to each feature map in the next layer is a single value in that map. One output value corresponds to a single virtual neuron, needing one bias weight.</p>

<p>In a CNN, as you explain in the question, the same weights (including bias weight) are shared at each point in the output feature map. So each feature map has its own bias weight as well as <code>previous_layer_num_features x kernel_width x kernel_height</code> connection weights.</p>

<p>So yes, your example resulting in <code>(3 x (5x5) + 1) x 32</code> weights total for the first layer is correct for a CNN with first hidden layer processing RGB input into 32 separate feature maps.</p>

<hr>

<p><sup>*</sup> You may be getting confused by seeing visualisation of CNN <em>weights</em> which can be separated into the colour channels that they operate on.</p>
","10","2","836","25573"
"11922","<p>There are not any strong, well-documented principles to help you decide between types of regularisation in neural networks. You can even combine regularisation techniques, you don't have to choose just one.</p>

<p>A workable approach can be based on experience, and following literature and other people's results to see what gave good results in different problem domains. Bearing this in mind, dropout has proved very successful for a broad range of problems, and you can probably consider it a good first choice almost regardless of what you are attempting.</p>

<p>Also sometimes just picking a option you are familiar with can help - working with techniques you understand and have experience with may get you better results than trying a whole grab bag of different options where you are not sure what order of magnitude to try for a parameter. A key issue is that the techniques can interplay with other network parameters - for instance, you may want to increase size of layers with dropout depending on the dropout percentage.</p>

<p>Finally, it may not matter hugely <em>which</em> regularisation techniques you are using, just that you understand your problem and model well enough to spot when it is overfitting and could do with more regularisation. Or vice-versa, spot when it is underfitting and that you should scale back the regularisation.</p>
","7","2","836","25573"
"11955","<p>The <code>75^6</code> option is not only bad for speed, but it is a very difficult representation to train, because the NN doesn't ""understand"" that any of the output categories are related. You would need an immense amount of data to train such a network, because ideally you need at least a few examples in any category that you expect the network to predict. Unless you had literally billions of examples to train from, the chances are certain combinations will never occur in your training set, thus could never be predicted with any confidence.</p>

<p>Therefore I would probably use 75 outputs, one for each object representing the probability that it would be chosen. This is easy to create training data for, if you have training examples with the 6 favoured objects - just a 1 for the objects chosen and 0 for all others as a 75-wide label.</p>

<p>For prediction, select the 6 objects with the highest probabilities. If these choices are part of a recommender system (i.e. may be presented to same person as being predicted for), then you can select items randomly using the outputs as weights. You may even find that this weighted Monte Carlo selection works well for predicting bulk user behaviour as well (e.g. for predictions fed into stock purchases). In addition, this stochastic approach <em>can</em> be made to predict duplicates (but not accurately, except perhaps averaged over many predictions).</p>

<p>A sigmoid transfer function on the output layer is good for representing non-exclusive probability. The logloss objective function can be used to generate the error values and train the network.</p>

<p>If you want to accurately predict duplicate choices out of the 6 items chosen, then you will need plenty of examples where duplicates happened and have some way to represent that in the output layer. For example, you could have double the number of output neurons, with two assigned to each object. The first probability would then be probability of selecting the item once, and the second probability would be for selecting it twice.</p>

<hr>

<p>The question has since been updated, and it appears there are strong relationships between items making the choice of a set of items potentially very recipe-like. That may reduce the effectiveness of the ideas outlined above in this answer.</p>

<p>However, using 75 outputs may still work better than other approaches, and is maybe the simplest setup, so I suggest still giving it a try, even if just to establish a benchmark for other ideas. This will work best when decisions are driven heavily by the feature data available, and when in practice there are lots of valid choices for combining items so there is a strong element of player preference. It will work less well if there is a large element of game mastery and logic in player decisions in order to combine items.</p>
","9","2","836","25573"
"11963","<p>There should be a bias weight for each virtual neuron as it controls the threshold at which the neuron responds to combined input. So if your hidden layer has 100 neurons, that is 100 bias weights for that layer. Same applies to each layer.</p>

<p>There are usually two different approaches taken when implementing bias. You can do one or the other:</p>

<ol>
<li><p>As a separate vector of bias weights for each layer, with different (slightly cut down) logic for calculating gradients.</p></li>
<li><p>As an additional column in the weights matrix, with a matching column of 1's added to input data (or previous layer outputs), so that the exact same code calculates bias weight gradients and updates as for connection weights.</p></li>
</ol>

<p>In both cases, you only do backpropagation calculation from neuron activation deltas to the bias weight deltas, you don't need to calculate the ""activation"" delta for bias, because it is not something that can change, it is always 1.0. Also the bias does not contribute deltas back further to anything else.</p>
","11","2","836","25573"
"12030","<p>The partial derivative is used precisely because it separates concerns about how the value is calculated (from all the other parameters and outputs in the network) from how the value affects the output. This is purely by definition so you can do the calculation.</p>

<p>The ""normal"" derivative of the error function can be expressed as the sum of any ""complete"" set of independent partial derivatives. There are a few such sets possible - in a simple feed-forward network one for each layer plus one for all the weights combined. </p>

<p>So in essence you <em>are</em> calculating the ""normal"" derivative w.r.t. the network weights, but due to the nature of the problem this is done by calculating the partial derivatives in multiple steps.</p>

<hr>

<p>Caveat: I am probably not using those maths terms 100% accurately. For instance, I am ignoring the role of the training data and treating it as a constant.</p>
","2","2","836","25573"
"12118","<p>Yes it is definitely possible to calculate optimised weightings provided you have some training examples where you know the document fields, the query, and either the outcome (relevant/not-relevant) or the desired score. </p>

<p>I think your training feature set should be the query score in range [0.0,1.0] for each field of each example. The training label should be either relevance 0 or 1 for each example, or the relevance score that the example has.</p>

<p><strong>If you have a target score for each example</strong></p>

<p>You want to determine the weights $W_i$ to use for each field $i$. Your calculated relevance score would be $\hat{y} = \sum_{i=1}^{N_{fields}} W_i * X_i$ where the caret signifies this is the estimate from your function and $N_{fields}$ is the number of field. Note I am ignoring your original idea of dividing by the sum of all $W_i$, because it makes things more complex. You can either add that term or force the sum to be equal to 1.0 if you wish (I am not going to show you how though, as this answer would get too long, and it probably won't help you much)</p>

<p>With a target score and training data, the simplest approach is to find the weights which cause the lowest error when used with the training data. This is a very common goal in supervised learning. You need a <a href=""https://en.wikipedia.org/wiki/Loss_function"" rel=""nofollow"">loss function</a>. Having a target scalar value means you can use difference from target and a very common loss function for this kind of regression problem is the mean squared error:</p>

<p>$$E = \frac{1}{N_{examples}} \sum_{j=1}^{N_{examples}} (\hat{y}_j - y_j)^2$$</p>

<p>Where $\hat{y}_j$ is your calculated relevance score for example $j$ and $y_j$ is your training label for the same example.</p>

<p>There are a few different ways to solve for lowest $E$ with this loss function, and it is one of the simplest to solve. If you express your weights as a vector $W$ length $N_{fields}$ your example features as a matrix $X$ size $N_{examples} \times N_{fields}$ and the labels as a vector $Y$ length $N_{examples}$ then you can get an exact solution to minimise loss using the <a href=""https://en.wikipedia.org/wiki/Least_squares#Linear_least_squares"" rel=""nofollow"">linear least squares equation</a></p>

<p>$$W = (X^TX)^{-1}X^TY$$</p>

<p>There are other approaches that work too - gradient descent or other function optimisers. You can look these up and see which you would prefer to use for your problem. Most programming languages will have a library with this already implemented.</p>

<p>Note that you will likely get scores greater than 1.0 or less than 0.0 from some document/query pairs.</p>

<p>You will have to use a adjust the technique if you want to divide by total of all weights or want sum of all weights equal to 1 in your scoring system.</p>

<p><strong>If you have a relevance 0 or 1 for each example</strong></p>

<p>You have a classification problem, relevant or not are your two classes. This can still be made to work, but you will want to change how you calculate your weighted score and use <a href=""https://en.wikipedia.org/wiki/Logistic_regression"" rel=""nofollow"">logistic regression</a>. </p>

<p>Your weighted score under logistic regression would be:</p>

<p>$$\hat{y} = \frac{1}{1 + e^{-(b + \sum_{i=1}^{N_{fields}} W_i * X_i)}}$$ </p>

<p>Where $b$ is a <em>bias</em> term. This looks complicated, but really it is just the same as before but mapped by a sigmoid function to better represent class probabilities - the result is always between 0 and 1.</p>

<p>You can look up solvers for logistic regression, and most stats or ML libraries will have functions ready to use.</p>

<p><strong>Caveats</strong></p>

<p>You have made a starting assumption that a simple combined relevance score will lead to a useful end result for your users performing search. This has led to simple linear models looking like a good solution. However, this may not be the case in practice, and you may need to re-visit that assumption. </p>
","6","2","836","25573"
"12170","<p>I have done the opposite of your problem - I have written code to <em>implement</em> shipping costs for e-commerce sites which runs on their sites.</p>

<p>Shipping cost rules can be almost completely arbitrary logic. In the general case, you have no good choice but to implement them as logic that is assessed per order. That means your optimiser will have to run that logic for every combination of purchases it considers. Which makes this a planning/combinatorics problem.</p>

<p>This may not be so bad - e.g. for purchasing 10 items across 5 stores you have $5^{10}$ combinations which is 9765625. That is possible by brute force.</p>

<p>For larger orders, or more choices of store, you may want to look at dynamic solutions to optimise cost, such as simulated annealing, genetic algorithms. Reinforcement learning run per order may also work.</p>
","2","2","836","25573"
"12181","<p>A CNN could be a good choice for this task if you expect variation in the original image scale, rotation lighting etc, and also have a <em>lot</em> of training data.</p>

<p>The usual CNN architecture is to have convolutional layers close to the input, and fully-connected layers in the output. Those fully-connected layers can have the output arranged for different classification or regression tasks as you see fit. Predicting the values of parameters describing the image is a regression task.</p>

<p>If you want accurate measures of size, you may need to avoid using max pooling layers. Unfortunately, not using pooling will make your network larger and harder to train - you <em>might</em> get away with strided convolution instead if that is a problem for you.</p>

<p>If your input images are very simple and clear (because they are always computer generated), then other approaches may be more reliable. You may be able to reverse-engineer image production and derive simple rules such as identifying lines, corners, circles and other easy-to-filter image components, and make direct measurements. There may also be a middle ground in complexity where extracting this data as features and using it to train a simple NN (or other ML model) will have good performance.</p>
","8","2","836","25573"
"12198","<p>Recurrent neural networks (RNNs) can work with series as input or output or both.</p>

<p>Even a simple one-layer RNN is effectively ""deep"" because it has to solve similar problems as multi-layer non-recursive networks. That is because backpropagation logic in a RNN has to account for delay between input and target, which is solved by <a href=""https://en.wikipedia.org/wiki/Backpropagation_through_time"" rel=""noreferrer"">backpropagation through time</a> - essentially adding a layer to the network for every time step of delay between first input and last output.</p>

<p>RNN architecture has become more sophisticated in recent years by using ""gating"" techniques such as <a href=""https://en.wikipedia.org/wiki/Gated_recurrent_unit"" rel=""noreferrer"">Gated Recurrent Units</a> (GRU) or <a href=""https://en.wikipedia.org/wiki/Long_short-term_memory"" rel=""noreferrer"">Long Short Term Memory</a> (LSTM). These have multiple trainable params - 3 or 4 - per neuron, and the schematics are more complicated than feed-forward networks. They have been demonstrated as <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""noreferrer"">very effective in practice</a>, so this extra complexity does seem to pay off.</p>

<p>Although you can research and implement RNNs yourself in a library like Theano or Tensor Flow, several neural network libraries already implement RNN architectures (e.g. <a href=""http://keras.io/"" rel=""noreferrer"">Keras</a>, <a href=""https://github.com/jcjohnson/torch-rnn"" rel=""noreferrer"">torch-rnn</a>)</p>
","6","2","836","25573"
"12227","<p>A couple of important points:</p>

<ul>
<li><p>Sentiment analysis is not an exact science. Two people, reading the same text in different contexts will come to different conclusions about sentiment, especially on borderline cases. Perhaps text has complex grammar, or has a metaphor or simile in it where it helps to understand what is actually being compared.</p></li>
<li><p>The <em>ground truth</em> for sentiment data sets is established by people. The best you can hope for from any trained ML classifier is that it closely matches the opinion of those people when it predicts on new data.</p></li>
<li><p>Your different sentiment analysis tools could well vary in quality, because they will have been built with different technologies at different times, and with different training data.</p></li>
</ul>

<p>There are a couple of ways I can think of that you could assess the classifiers for your purpose:</p>

<p><strong>1. Test against your own labelled test data</strong></p>

<p>It is important to source this yourself to ensure it does not overlap with training data used by any of the models you want to compare (if it did overlap then it gives a big advantage to any model that trained using it). You will need to collect ground truth data on sentiment for all the texts.</p>

<p>Before you test, it is a good idea to pick a metric that you care about for your intended use. Accuracy is not the only metric, but it is probably fine for your purpose. You might choose other metrics if there are different costs associated with incorrect classifications - e.g. if assigning negative sentiment needs to be done cautiously.</p>

<p>Then run each classifier across your test set and calculate the metric. The classifier with the best metric is your best guess at the one you should use.</p>

<p>If you have a large enough test set, you can split it into parts and get a measure of the estimate error in your metric. This will help you see whether differences in the test metrics are significant.</p>

<p>This can be the best approach, especially if you can source data that you know matches your project goals. However, it would take a long time and a lot of effort.</p>

<p>If you are in a hurry, you could just search for and use someone else's dataset e.g I found these ones on a quick web search: <a href=""https://inclass.kaggle.com/c/si650winter11"" rel=""noreferrer"">a Kaggle competition</a> and <a href=""http://www.sananalytics.com/lab/twitter-sentiment/"" rel=""noreferrer"">a free sample hosted by Niek Sanders</a> (I have no idea of the quality of these - you should sample them and see if the data is useful for you). There is a risk though that these data sets were used to train one of your classifiers, so it would give you a false high rating.</p>

<p><strong>2. Read up on associated reports and papers</strong></p>

<p>Each of the sentiment analysers should explain the model class and data used to train it in documentation. Quite often you can find papers comparing the different approaches and quoting accuracy scores on a standard data set. If you are lucky, you will find enough comparisons that you get some sense of which algorithms are considered ""cutting edge"" and which are out-dated.</p>

<p><strong>3. Collect text that is classified differently by algorithms you are considering, and get feedback</strong></p>

<p>A variation of (1), you can do this at any stage, and it might work well in the context of a live product. For all the data that is classified, log how different classifiers respond to it, and where there is discrepancy, save it for later assessment. You will need some people to read and help classify the text (ideally without seeing what the classifiers thought), and you can collect metric data over time and use the classifier that does best once you have collected enough examples to make a clear decision.</p>

<hr>

<p>Ultimately what matters is the results of using your classifier in a project. Just labelling data has no inherent purpose. It is the consequences of those labels that matter. So you need to be driven by results. If you use one classifier and get feedback that the product is not performing well, you may be able to trial another one. If your product is used by many people, and enough are giving feedback of quality, you could even A/B test a couple of classifiers in production.</p>
","5","2","836","25573"
"12555","<p>Being good at a subject does not automatically make someone a good teacher, and it looks like Roman's answer on Quora has fallen into a trap of thinking everything he knows is simple and could be picked up quickly. Also making the potential student attempt things outside of a beginner level - effectively just by research on the web following a 2 paragraph pointer - is going to make progress slow and frustrating. Despite no doubt good intentions, the advice there is likely to give a very poor learning experience.</p>

<p>There are full structured tutorials available on free Massive Online Open Courses (MOOCs) available for learning data science topics. These have been put together by professionals who know how to teach, and the effort put into designing any of these courses, providing materials etc, totally dwarfs the effort that went into the advice on Quora.</p>

<p>Ignore the Quora answer, and sign up for one or more of these MOOCs. You can find them at <a href=""https://www.coursera.org/"" rel=""noreferrer"">Coursera</a>, <a href=""https://www.udacity.com/"" rel=""noreferrer"">Udacity</a>, <a href=""https://www.edx.org/"" rel=""noreferrer"">edX</a> and other similar places. If you want to cram a lot of study into a short time, look for ""at your own pace"" courses where all the materials are available to you immediately.</p>

<p>The almost canonical start point to test the waters would be <a href=""https://www.coursera.org/learn/machine-learning"" rel=""noreferrer"">Andrew Ng's Machine Learning course on Coursera</a>. Machine Learning is one of the more immediately accessible and fun parts of data science, and the course goes into theory and practice with additional sections for beginners at the start covering necessary maths and programming. Following that entire course is probably around 100 hours total effort. You won't come out of it knowing data science, but you will gain useful practical skills, and get a real taster for the machine learning side of the subject.</p>
","9","2","836","25573"
"12705","<p>You could be right that ignoring top part of image would benefit the CNN. However, there is very little point in trying to <em>architect</em> this - if your premise that the CNN will ignore irrelevant details in the top half is correct, then that will occur anyway and there is no standard NN architecture that will help that other than disconnecting the top half of network, which is going to be logically exactly the same as programmatically slicing the image, with the disadvantage of storing and calculating with twice as many parameters.</p>

<p>You should either programatically cut the image in half or do nothing to the image and rely on the CNN's inherent ability to give low weights to irrelevant details. If you do the latter, you may be able to get around the learning of incorrect details in the top half by augmenting your data - e.g. add some noise to images*, especially in the irrelevant top half. Perhaps horizontally flipping a few images (and reverse relevant targets for the control class) might be another useful augmentation. Some augmentations could also be useful if you just take the lower half of the image.</p>

<p>* Noise should be something close to variations that could be seen when in use. E.g. slurring pixels left or right might be reasonable. Inserting ""static"" probably is not.</p>
","1","2","836","25573"
"12735","<p>AUC (or most often AUROC = ""area under <a href=""https://en.wikipedia.org/wiki/Receiver_operating_characteristic"" rel=""noreferrer"">receiver operating characteristic</a>"") and accuracy are different measures, but used for same purpose - to objectively measure performance of a simple binary classifier.</p>

<p>The two measures can be equal at extreme values of 0 and 1 for perfect classifiers - or inverse perfect classifiers (you can just invert the output to get a perfect classifier). It might be possible for the values to be numerically equal at other times, but if so it would be a coincidence with no specific meaning.</p>

<p>Both these metrics can be used with a simple classifier that only outputs true or false values for class membership. However, the AUROC metric requires some kind of parameter you can vary in order to plot the ROC curve. Usually this is a threshold for classification, used against the class probability output of a classifier.</p>

<p>There are other possible metrics. For example, <a href=""https://en.wikipedia.org/wiki/F1_score"" rel=""noreferrer"">F1 score</a> and <a href=""https://en.wikipedia.org/wiki/Cross_entropy"" rel=""noreferrer"">cross entropy</a>. The F1 score again has 1.0 for a perfect classifier and 0.0 for a bad classifier, but cross entropy scores lower for better classifiers - 0.0 for perfect and no upper limit for bad output. Again, the values of these might be equal to other metrics at some points, but if so it is not meaningful.</p>

<p>If you are comparing two classifiers for a particular task, then it is important to compare them using the same metric, on the same test data. The metric and test data you choose should relate to your original problem.</p>

<p>If you are reading other people's published results, and want to compare them, it is not really possible if one person has used AUC and the other accuracy.</p>
","5","2","836","25573"
"12749","<p>Yes the ReLU second order derivative is 0. Technically, neither $\frac{dy}{dx}$ nor $\frac{d^2y}{dx^2}$ are defined at $x=0$, but we ignore that - in practice an exact $x=0$ is rare and not especially meaningful, so this is not a problem. Newton's method does not work on the ReLU transfer function because it has no stationary points. It also doesn't work meaningfully on most other common transfer functions though - they cannot be minimised or maximised for finite inputs.</p>

<p>When you combine multiple ReLU functions with layers of matrix multiplications in a structure such as a neural network, and wish to minimise an objective function, the picture is more complicated. This combination does have stationary points. Even a single ReLU neuron and a mean square error objective will have different enough behaviour such that the second-order derivative of the single weight will vary and is not guaranteed to be 0.</p>

<p>Nonlinearities when multiple layers combine is what creates a more interesting optimisation surface. This also means that it is harder to calculate useful second-order partial derivatives (or <a href=""https://en.wikipedia.org/wiki/Hessian_matrix"" rel=""noreferrer"">Hessian matrix</a>), it is not just a matter of taking second order derivatives of the transfer functions. </p>

<p>The fact that $\frac{d^2y}{dx^2} = 0$ for the transfer function will make some terms zero in the matrix (for the second order effect from same neuron activation), but the majority of terms in the Hessian are of the form $\frac{\partial^2E}{\partial x_i\partial x_j}$ where E is the objective and $x_i$, $x_j$ are <em>different</em> parameters of the neural network. A fully-realised Hessian matrix will have $N^2$ terms where $N$ is number of parameters - with large neural networks having upwards of 1 million parameters, even with a simple calculation process and many terms being 0 (e.g. w.r.t. 2 weights in same layer) this may not be feasible to compute.</p>

<p>There are techniques to estimate effects of second-order derivatives used in some neural network optimisers. RMSProp can be viewed as roughly estimating second-order effects, for example. The ""Hessian-free"" optimisers more explicitly calculate the impact of this matrix.</p>
","5","2","836","25573"
"12800","<p>You can use the dataframe's <code>.values</code> method to access raw data once you have manipulated the columns as you need them.</p>

<p>E.g.</p>

<pre><code>train = pd.read_csv(""train.csv"")
target = train['target']
train = train.drop(['ID','target'],axis=1)
test = pd.read_csv(""test.csv"")
test = test.drop(['ID'],axis=1)

xgtrain = xgb.DMatrix(train.values, target.values)
xgtest = xgb.DMatrix(test.values)
</code></pre>

<p>Obviously you may need to change which columns you drop or use as the training target. The above was for a Kaggle competition, so there was no target data for <code>xgtest</code> (it is held back by the organisers).</p>
","23","2","836","25573"
"12809","<p>One way of stating what you are looking for is to find a simple mathematical model to explain your data.</p>

<p>One thing about neural networks is that (once they have more than 2 layers, and enough neurons total) they can in theory emulate any function, no matter how complex. This is useful for machine learning as often the function we want to predict is complex and cannot be expressed simply with a few operators. However, it is kind of the opposite of what you want - the neural network behaves like a ""black box"" and you don't get a simple function out, even if there is one driving the data.</p>

<p>You can try to fit a model (any model) to your data using very simple forms of regression, such as <a href=""https://en.wikipedia.org/wiki/Linear_regression"" rel=""nofollow"">linear regression</a>. So if you are reasonably sure that your system is a cubic equation $y= ax^3 + bx^2 +cx +d$ then you could create a table like this:</p>

<pre><code>  bias   |   x  |  x*x  |  x*x*x  |     y
     1       0       0         0        0
     1       2       4         8        4
     1       3       9        27        9
     1       .         .        .       .
     1     100   1000000    10000   10000
</code></pre>

<p>and then use a <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"" rel=""nofollow"">linear regression optimiser</a> (sci-kit learn's SGD optimiser linked). With the above data this should quickly tell you $b=1, a,c,d=0$. But what it won't tell you is whether your model is the best possible or somehow ""correct"". You can scan for more possible formulae by creating more columns - any function of any combination of inputs (if there is more than one) that could be feasible. </p>

<p><em>However</em>, the more columns you add in this way, the more likely it is you will find an incorrect <em>overfit</em> solution that matches all your data using a clever combination of parameters, but which is not a good general predictor. To address this, you will need to add regularisation - a simple L1 or L2 regularisation of the parameters will do (in the link I gave to scikit-learn, the <code>penalty</code> argument can control this), which will penalise large parameters and help you home in on a simple formula if there is one.</p>
","1","2","836","25573"
"12857","<p>Probably you need to do one or more of:</p>

<ul>
<li><p>Decrease learning rate. Diverging loss is often a symptom of learning rate too high.</p></li>
<li><p>Increase number of hidden neurons. The output function will be the combination of many ""patches"" each created by a neuron that has learnt a different bias. The shape and quality of each patch is determined by the activation functions, but almost any nonlinear activation function used in an NN library should work to make a universal function approximator.</p></li>
<li><p>Normalise inputs. If you are training with high values for $x_1$ or $x_2$, this could make it harder to train unless you normalise your training data.</p></li>
</ul>

<p>For your purposes, it might be an idea to skip the need for normalisation by training with $x_1$ and $x_2$ in range -2.0 to 2.0. It doesn't change your goal much to do this, and removes one potential problem.</p>

<p>You should note that a network trained against an open-ended function like this (where there are no logical bounds on the input params) will not learn to extrapolate. It never ""learns"" the function itself, but an approximation of it close to the supplied examples. When you supply  a $(x_1, x_2)$ input far from the training examples, the output is likely to completely mismatch your original function.</p>
","3","2","836","25573"
"13281","<p>It is not possible to tell whether a machine learning algorithm is overfitting based purely on the training set accuracy. </p>

<p>You could be right, that using more features with a small data set increases sampling error and reduces the generalisation of the SVM model you are building. It is a valid concern, but you cannot say that for sure with only this worry and the training accuracy to look at.</p>

<p>The usual solution to this is to keep some data aside to test your model. When you see a high training accuracy, but a low test accuracy, that is a classic sign of over-fitting. </p>

<p>Often you are searching for the best hyper-parameters to your model. In your case you are trying to discover the best number of features to use. When you start to do that, you will need to make multiple tests in order to pick the best hyper-parameter values. At that point, a single test set becomes weaker measure of true generalisation (because you have had several attempts and picked best value - just by selection process you will tend to over-estimate the generalisation). So it is common practice to split the data three ways - training set, cross-validation set and test set. The cross-validation set is used to check accuracy as you change the parameters of your model, you pick the best results and then finally use the test set to measure accuracy of your best model. A common split ratio for this purpose is 60/20/20.</p>

<p>Taking a pragmatic approach when using the train/cv/test split, it matters less that you are over or under fitting than simply getting the best result you can with your data and model class. You can use the feedback on whether you are over-fitting (high training accuracy, low cv accuracy) in order to change model parameters  - increase regularisation when you are over-fitting for example.</p>

<p>When there are a small number of examples, as in your case, then the cv accuracy measure is going to vary a lot depending on which items are in the cv set. This makes it hard to pick best hyper-params, because it may just be noise in the data that makes one choice better than another. To reduce the impact of this, you can use <a href=""https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation"" rel=""nofollow"">k-fold cross-validation</a> - splitting your train/cv data multiple times and taking an average measure of the accuracy (or whatever metric you want to maximise).</p>

<hr>

<p>In your confusion matrices, there is no evidence of over-fitting. A training accuracy of 100%* and testing accuracy of 93.8% are suggestive of some degree of over-fit, but the sample size is too low to read anything into it. You should bear in mind that balance between over- and under- fit is very narrow and most models will do one or the other to some degree. </p>

<p>* A training accuracy of 100% is nearly always <em>suggestive</em> of overfit. Matched with e.g. 99% test accuracy, you may not be too concerned. The question is at worst ""could I do better by increasing regularisation a little""? However, matched with ~60% test accuracy it is clear you have actually overfit - even then you might be forced to accept the situation if that's the best you could achieve after trying many different hyperparameter values (including some attempts with increased regularisation).</p>
","5","2","836","25573"
"13326","<p>I took a deeper look at this problem with some example data (that the OP provided) and some simplifications. I just predicted brick height param as opposed to the 17 example params in the question. I used the Keras library in Python to explore a few different architectures.</p>

<p>Initially I replicated the problem, simple CNNs were predicting identical values for all heights, and the best loss (around 0.0053 on the sample data) was when this predicted the mean very precisely for all inputs. For some reason the network could not find the visual structure that is obviously in the data.</p>

<p>After some experimentation, I had a network that functioned as intended. It was surprisingly deep for such a simple problem. My best guess at the reason why is that the combined overlap of all the convolutions needs to be of the same size order as the features being detected.</p>

<p>Some important details:</p>

<ul>
<li><p>Check that the order of the dimensions of the images when read with the image library you are using match the expected structure that the CNN is working on. If the dimensions are in a different order, this will not cause any error, but it scrambles the interpretation used by the CNN and prevents it learning anything but the mean.</p></li>
<li><p>Avoid using pooling layers, as they obscure pixel locations of features from further layers.</p></li>
<li><p>Check that your parameters are not ambiguous. The two ""brick width"" parameters seem interchangeable for instance. That makes it much harder for the network to predict them, and it is likely to output a rough mean for the example even if you avoid the problems with architecture.</p></li>
<li><p>The problem is harder to find a working architecture for than I first thought. I am not entirely sure why that is the case, but am guessing it is to do with the size of the features that need to be isolated in order for the network to ""measure"" them. Eventual working architectures were quite deep e.g. 10 convolutional layers, despite the pixel-level structures being quite obvious and clear in the training data.</p></li>
</ul>

<p><a href=""https://gist.github.com/neilslater/40201a6c63b4462e6c6e458bab60d0b4"" rel=""nofollow"">Here is a gist of my solution in Python/Keras.</a> Summary of architecture in the gist: 4 times Conv2d layers with 8 feature maps and 5x5 kernel. Dropout 25%. 4 more times Conv2d layers with 16 feature maps and 5x5 kernel. Dropout 50%. Fully connected layer with 256 outputs. Dropout 50%. Fully connected later with 128 outputs. Dropout 50%. Single output neuron. All layers use ReLU activation, except output which is linear.</p>

<p>The example re-sizes images to 50x50 for speed. However, using 100x100 images works fine. I added a couple of convolutional layers to it for that (one each for the 8 features and 16 features section), and get a reasonable loss of ~0.00025 on a validation set. It looks possible to get a lower loss still with more training examples, more epochs and perhaps more feature maps. Many of the predictions were very good, close to the actual value. </p>

<p>Interestingly, the worst errors were for short bricks predicting close to a 2:1 or 3:2 ratio with the ground truth, implying that the network had found correct structure but was being confused by the periodic nature of the bricks. I think this is something that could be resolved with more training examples (and perhaps more features in higher layers) and it should be possible to get much lower loss metric on brick height.</p>

<p>I think that the corresponding tiny-cnn structure looks like this:</p>

<pre><code>net &lt;&lt; convolutional_layer&lt;relu&gt;(100, 100, 5, 3, 8)
    &lt;&lt; convolutional_layer&lt;relu&gt;(96, 96, 5, 8, 8)
    &lt;&lt; convolutional_layer&lt;relu&gt;(92, 92, 5, 8, 8)
    &lt;&lt; convolutional_layer&lt;relu&gt;(88, 88, 5, 8, 8)
    &lt;&lt; convolutional_layer&lt;relu&gt;(84, 84, 5, 8, 8)
    &lt;&lt; dropout(80 * 80 * 8, 0.25)
    &lt;&lt; convolutional_layer&lt;relu&gt;(80, 80, 5, 8, 16)
    &lt;&lt; convolutional_layer&lt;relu&gt;(76, 76, 5, 16, 16)
    &lt;&lt; convolutional_layer&lt;relu&gt;(72, 72, 5, 16, 16)
    &lt;&lt; convolutional_layer&lt;relu&gt;(68, 68, 5, 16, 16)
    &lt;&lt; convolutional_layer&lt;relu&gt;(64, 64, 5, 16, 16)
    &lt;&lt; dropout(60 * 60 * 16, 0.5)
    &lt;&lt; fully_connected_layer&lt;relu&gt;(57600, 256)
    &lt;&lt; dropout(256, 0.5)
    &lt;&lt; fully_connected_layer&lt;relu&gt;(256, 128)
    &lt;&lt; dropout(128, 0.5)
    &lt;&lt; fully_connected_layer&lt;identity&gt;(128, 1)
</code></pre>
","3","2","836","25573"
"13361","<blockquote>
  <p>I believe reason for using these variants of SGD is to solve ""bad"" local minima problem</p>
</blockquote>

<p>That is not accurate. The variants are mostly about accelerating steps of gradient descent when faced with shallow or rapidly changing gradients, or with gradients that need adaptive learning rates because some parts of the network get stronger gradient signals than others. They do this by weighting or adjusting the step sizes in each dimension, based on additional knowledge to the current gradients, such as the history of previous gradients.</p>

<p>Shapes like saddle points or curving gullies, in the cost function ""landscape"" can cause difficulties for basic SGD. Take a saddle point as an example - there is no ""bad"" minima, a saddle point can be quite high up in a cost function. But the gradient values can be very low, even if they pick up again if you can take steps away from the saddle point. The trouble for SGD is that using just the gradient is likely to make updates oscillate up and down the steep parts of the saddle, and not move in the shallow direction away from the saddle point. Other difficult shapes can cause similar problems.</p>

<p>For a visualisation of the difference in behaviours of some of the optimisers at a saddle point, <a href=""http://sebastianruder.com/content/images/2016/01/saddle_point_evaluation_optimizers.gif"" rel=""nofollow"">take a look at this animation</a> (I'd like to find a creative-commons variant of this and include in the answer), and <a href=""http://sebastianruder.com/optimizing-gradient-descent/"" rel=""nofollow"">a blog that references it</a>.</p>

<p>In addition, for deep learning network, there is a problem that gradients can ""explode"" or ""vanish"" as you work back through the layers. Using ReLU activation or similar can help with that, but is not always possible (think of RNNs which need sigmoid activations inside LSTM modules). An optimiser like RMSprop deals with this by normalising gradients used by the weight update steps based on recent history. Whilst SGD could more easily get stuck and fail to update the lower layer weights (weights closer to the input) - either not updating them much at all, or taking too large steps.</p>
","3","2","836","25573"
"13524","<p>The ""same angle hyperplane"" does not have the same cost. It is the same decision boundary as you describe it, but perpendicular distances to it are larger wrt the norm of the weights. In effect with higher weights in the same ratio (i.e. without any regularisation effect), the classifier will be more confident in all of its decisions. That means the classifier will be more sensitive to getting as many observations as possible in the training set on the ""right"" side of the boundary. In turn this makes it sensitive to noise in the observations.</p>

<p>Your estimated probability for being in the positive class is:</p>

<p>$$p(y=1|X) = \frac{1}{1+e^{-W^TX}}$$</p>

<p>This includes $w_0$ and fixed value 1 for $x_0$. If you take the midpoint, decision line where $W^TX$ is zero (and the output is at the threshold value 0.5), that defines your decision hyperplane in $X$ space.</p>

<p>When $W$ has the same factors but a larger norm with $w_0$ compensating to make the same hyperplane, then $X$ values that were on the decision hyperplane still give threshold value of 0.5. However, $X$ values away from the hyperplane will deviate more strongly. If instead of 0 you had $W^TX=1.0$ and doubled the weights keeping the same hyperplane, you would get $W^TX=2.0$ for that example. Which changes your confidence from 0.73 to 0.88.</p>

<p>The usual cost function without regularisation for logistic regression with example vectors $X_j$ and targets $y_j$ is:</p>

<p>$$J = - \sum_{\forall j} y_jlog(\frac{1}{1+e^{-W^TX_j}}) + (1 -y_j)(1  - log(\frac{1}{1+e^{-W^TX_j}}))$$</p>

<p>The cost is more sensitive to distances from the hyperplane for larger weight values. Looking your example for the imaginary item (with 0.73 or 0.88 confidence), when the categorisation is correct (i.e. y=1) the score would improve by 0.19 for that example if the weights doubled. When the categorisation is wrong (y=0) then the score would worsen by 0.81. In other words for <em>higher</em> weights, with same weight ratio, the same miscategorisations are punished more than correct categorisations are rewarded.</p>

<p>When training, weights will converge to specific balanced weight vector for the minimum cost, <strong>not</strong> to a specific ratio that forms a ""best decision hyperplane"". That's because the hyperplane does not correspond to a single value of the cost function. </p>

<p>You can demonstrate this effect. Train a logistic regression classifier - without any regularisation to show it has nothing to do with that. Take the weight vector and multiply by some factor e.g. 0.5.Then re-train starting with those weights. You will end up with the same weights as before. The cost function minimum clearly defines <em>specific</em> weight values, not a ratio.</p>

<p>When you add regularisation, that changes the cost and how the weights will converge. Higher regularisation in effect makes the classifier prefer a boundary with lower confidence in all its predictions, it penalises ""near misses"" less badly because the weights are forced down where possible. When viewed as a hyperplane, the boundary will likely be different.</p>
","3","2","836","25573"
"13637","<p>Technically with most languages you could pass in integer features for the input layer, since the weights will be floats, and multiplying a float by an integer will give you a float. Also, you don't usually care about partial derivatives of the input data, so it doesn't matter that the values are discrete.</p>

<p>However:</p>

<ul>
<li><p>For all weights and neuron activations, if you are using a method based on backpropagation for training updates, then you need a data type that approximates real numbers, so that you can apply fractional updates based on differentiation. Best weight values are often going to be fractional, non-whole numbers. Non-linearities such as sigmoid are also going to output floats. So after the input layer you have matrices of float values anyway. There is not much speed advantage multiplying integer matrix with float one (possibly even slightly slower, depending on type casting mechanism). So the input may as well be float.</p></li>
<li><p>In addition, for efficient training, the neural network inputs should be normalised to a specific roughly unit range (-1.0 to 1.0) or to mean 0, standard deviation 1.0. Both of these require float representation. If you have input data in 0-255 range - float or not - you will usually find the network will learn less effectively.</p></li>
</ul>

<p>There are exceptions to these rules in some architectures and learning algorithms, where perhaps an integer-based input would work, but for the most common NN types, including MLP and ""deep"" feed-forward networks, it is simpler and easier to use float data type.</p>
","4","2","836","25573"
"13698","<p>Activation ""linear"" is identical to ""no activation function"". The term ""linear output layer"" also means precisely ""the last layer has no activation function"". Whether you use one or the other term might be down to how your NN library implements it. You may also see it described either way around in documents, but it is exactly the same thing mathematically:</p>

<p>$$a^{out}_j = b^{out}_j + \sum_{i=1}^{N^{hidden}} W_{ij}a^{hidden}_i$$</p>

<p>Where $a$ values are activation, $b$ are biases, $W$ is weight matrix.  </p>

<p>For a regression problem with a mean squared error objective, this is the most common approach.</p>

<p>There is nothing stopping you using other activation functions. They might help you if they match the target variable distribution. About the only rule is that your network output should be able to cover possible values of the target variable. So if the target variable is always between -1.0 and 1.0, with higher density around 0.0, perhaps tanh could also work for you.</p>
","3","2","836","25573"
"14095","<p>No, area under receiver operating characteristic (AUROC) is just one metric amongst very many possibilities, even assuming you just want to pick a standard approach. There are too many to list in a simple Stack Exchange answer. You can take a look at this list extracted from <a href=""http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics"" rel=""nofollow"">scikit learn documentantion on metrics</a> for example, which is not by any means exhaustive:</p>

<pre><code>Accuracy classification score.
Area Under the Curve (AUC) using the trapezoidal rule
Average precision (AP) from prediction scores
The Brier score.
The F1 score, also known as balanced F-score or F-measure
The F-beta score
The average Hamming loss.
Average hinge loss (non-regularized)
Jaccard similarity coefficient score
Log loss, aka logistic loss or cross-entropy loss.
The Matthews correlation coefficient (MCC) for binary classes
Area Under the Curve (AUC) from prediction scores
Zero-one classification loss.
</code></pre>

<p>What you should choose depends entirely on your model class and goals for the learning work. AUROC is a good choice for a binary classifier when you have different business costs associated with false positives and false negatives. That is because it gives you a sense of how well the classifier can be tuned to be more or less sensitive, and get the best outcomes by changing the class threshold.</p>

<p>There is no minimum AUC or other metric required to select a model in practice. It depends on performance of existing solutions (including non-ML ones) and what the costs vs benefits would be of using the model. Clearly a poorly-performing model (e.g. with AUROC 0.5, no better than random guessing on a balanced data set) is unlikely to gain much from being implemented in a production environment, and you would want to take a new look at the original problem and see if was reasonable to expect anything from ML at all.</p>

<p>To decide which metric to use, you need to define some goal for the eventual model. An ideal metric is one that can be simply converted to terms that the end users will care about. Failing that, one that can be compared to known results from other ML work in the subject area (domain) of the problem.</p>
","3","2","836","25573"
"14113","<p>There is a key difference:</p>

<ul>
<li><p>Softmax regression provides class probabilities for mutually exclusive classes.</p></li>
<li><p>Logistic regression treats class membership for each class separately. Classes do not need to be mutually exclusive.</p></li>
</ul>

<p>The two are equivalent for a scenario with two mutually exclusive classes - e.g. a ""positive"" and ""negative"" class - where softmax would have two outputs summing to 1, and logistic regression would have one output giving probability of the ""positive"" class.</p>

<p>I am not certain of the results comparing logistic regression one-vs-all (taking max output) with softmax regression on the same multi-class problem. I would expect the performance to be quite similar. Neither model copes well with non-linear relationships between input and target classes.</p>
","6","2","836","25573"
"14133","<p>Max pooling doesn't down-sample the image. It down-samples the features (such as edges) that you have just extracted. Which means you get more approximately where those edges or other features are. Often this is just what the network needs for generalisation - in order to classify it doesn't need to know there is a vertical edge running from 10,5 to 10,20, but that there is an approximately vertical edge about 1/3 from left edge about 2/3 height of the image. </p>

<p>These rougher categories of features inherently cover more variations in the input image for very little cost, and the reduction in size of the feature map is a nice side effect too, making the network faster.</p>

<p>For this to work well, you still need to extract features to start with, which max pooling does not do, so the convolutional layer is necessary. You should find you can down-sample the original image (to 14x14) instead of using the first max-pooling layer, and you will still get pretty reasonable accuracy. How much pooling to do, and where to add those layers is yet another hyper-parameter problem when building a deep neural network.</p>
","16","2","836","25573"
"14137","<blockquote>
  <p>Are there any statistical/otherwise tests that I can run to understand the quality of the labeling I get from my code.</p>
</blockquote>

<p>Yes. You can treat your automated labelling as a model in its own right (it essentially is an ""expert"" model, that takes in additional features). Collect ground truth data with known accurate labels, and use a metric such as <a href=""https://en.wikipedia.org/wiki/Accuracy_and_precision"" rel=""nofollow"">accuracy</a>, <a href=""https://en.wikipedia.org/wiki/Receiver_operating_characteristic"" rel=""nofollow"">AUROC</a>, <a href=""https://en.wikipedia.org/wiki/F1_score"" rel=""nofollow"">F1 score</a> to determine how well your expert model works. Usually you should pick one metric that makes sense to you based on the problem you are trying to solve. Most statistics libraries will allow you to assess these metrics on arbitrary data, so it should be possible to find the relevant part of the API and feed it your synthetic labels to compare with ground truth data you have collected.</p>

<p>But wait! That is exactly what you would need to do in order to train and assess an ML model. About the best you can hope is that you will need less ground truth data to assess your expert than you would to train the ML version from scratch. For complex ML models such as deep neural networks, this could well be true.</p>

<p>Note that you may hit a limitation from this approach. Say that you assess from ground truth data that your expert is 85% accurate. You will not be able to use it to train an ML solution that is better than 85% accurate, because the ML will learn to copy the expert's predictions as best it can.</p>

<p>One thing to watch out for - if you analyse why the expert got some values wrong and then adjust it to give correct answers for those labels, you will need to remove the labels you used to do this from your test data - and ideally collect more, or take more from some unused pool, to replace them. Otherwise you will fit your expert to the test data and get incorrect reporting of how well the expert generalises to unseen data. Same reasoning means you should not use tweets and labels that you analysed when developing your expert model in order to test it.</p>
","2","2","836","25573"
"14227","<p>The different layers you describe can all have gradients calculated using the same back propagation equations as for a simpler MLP. It is still the same recursive process, but it is altered by the parameters of each layer in turn. </p>

<p>There are some details worth noting:</p>

<ul>
<li><p>If you want to <em>understand</em> the correct formula to use, you will need to study the <a href=""https://theclevermachine.wordpress.com/2014/09/06/derivation-error-backpropagation-gradient-descent-for-neural-networks/"" rel=""nofollow"">equations of back propagation</a> using the chain rule (note I have picked one example worked through, there are plenty to choose from - including <a href=""https://github.com/neilslater/ru_ne_ne/blob/master/tutorials/BACKPROP.md"" rel=""nofollow"">some notes I made myself</a> for a now defunct software project).</p></li>
<li><p>When feed-forward values overlap (e.g. convolutional) or are selected (e.g. dropout, max pooling), then the combinations are usually logically simple and easy to understand:</p>

<ul>
<li><p>For overlapped and combined weights, such as with convolution, then gradients simply add. When you back propagate the the gradients from each feature ""pixel"" in a higher layer, they add into the gradients for the shared weights in the kernel, and also add into the gradients for the feature map ""pixels"" in the layer below (in each case before starting calculation, you might create an all-zero matrix to sum up the final gradients into).</p></li>
<li><p>For a selection mechanisms, such as the max pooling layer, you only backprop the gradient to the selected output neuron in the previous layer. The others do not affect the output, so by definition increasing or decreasing their value has no effect - they have a gradient of 0 for the example being calculated.</p></li>
</ul></li>
<li><p>In the case of a feed-forward network, each layer's processing is independent from the next, so you only have a complex rule to follow if you have a complex layer. You can write the back propagation equations down so that they relate gradients in one layer to the already-calculated gradients in the layer above (and ultimately to the loss function evaluated in the output layer). It doesn't directly matter what the activation function was in the output layer after you backpropagate the gradient from it - at that point the only difference is numeric, the equations relating deeper layer gradients to each other do not depend on the output at all.</p></li>
<li><p>Finally, if you want to just use a neural network library, you don't need to worry much about this, it is usually just done for you. All the standard activation functions and layer architectures are covered by existing code. It is only when creating your own implementations from scratch, or when making use of unusual functions or structure, that you might need to go as far as deriving the values directly.</p></li>
</ul>
","1","2","836","25573"
"14232","<p>There are two approaches that can be taken:</p>

<ul>
<li><p>Only use valid indices. Matrix C will then be smaller than matrix I, in your example it would be a 7x7 matrix (9 - 3 + 1 = 7). You may see this in neural network libraries as a convolution working with ""valid"" border.</p></li>
<li><p>Use synthetic values, usually just 0, for out-of-bounds indices in matrix I in order to calculate C. This produces the same size output as input, so some neural network libraries note this with ""same"" border.</p></li>
</ul>

<p>Your formula suggests ""same"" border is being used*, because it would be unusual to work with a matrix C starting from an offset corner. If you do use ""same"" borders, it is important to normalise your input image data pixels to mean 0, otherwise the synthetic border will appear as a strong edge to the kernels.</p>

<p>If you are developing your own CNN code, I would suggest using ""valid"" border mode, because it is simpler, and a more common approach. It will work just fine for image classification tasks.</p>

<hr>

<p>* Actually I think the formula you have posted is over-simplified or even wrong. I would expect to see the term $I(x+a,y+b)$ so that the top left index of C stays as (1,1) for a ""valid"" border. For a ""same"" border, I would expect to see a centered kernel.</p>
","2","2","836","25573"
"14665","<p>Yes this is possible by treating the audio as a sequence into a <a href=""https://deeplearning4j.konduit.ai/getting-started/tutorials/recurrent-networks"" rel=""nofollow noreferrer"">Recurrent Neural Network (RNN)</a>. You can train a RNN against a target that is correct at the end of a sequence, or even to predict another sequence offset from the input.</p>
<p>Do note however that there is <a href=""https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/"" rel=""nofollow noreferrer"">a bit to learn about options that go into the construction and training of a RNN</a>, that you will not already have studied whilst looking at simpler layered feed-forward networks. Modern RNNs make use of layer designs which include memory gates - the two most popular architectures are LSTM and GRU, and these add more trainable parameters into each layer as the memory gates need to learn weights in addition to the weights between and within the layer.</p>
<p>RNNs are used extensively to predict from audio sequences that have already been processed in MFCC or similar feature sets, because they can handle sequenced data as input and/or output, and this is a desirable feature when dealing with variable length data such as <a href=""https://arxiv.org/abs/1402.1128"" rel=""nofollow noreferrer"">spoken word</a>, music etc.</p>
<p>Some other things worth noting:</p>
<ul>
<li><p>RNNs can work well for <em>sequences</em> of data that are variable length, and where there is a well-defined dimension over which the sequences evolve. But they are less well adapted for variable-sized sets of features where there is no clear order or sequence.</p>
</li>
<li><p>RNNs can get state-of-the-art results for signal processing, NLP and related tasks, but only when there is a very large amount of training data. Other, simpler, models can work just as well or better if there is less data.</p>
</li>
<li><p>For the specific problem of generating MFCCs from raw audio samples: Whilst it should be possible to create a RNN that predicts MFCC features from raw audio, this might take some effort and experimentation to get right, and could take a lot of processing power to make an RNN powerful enough to cope with very long sequences at normal audio sample rates. Whilst creating MFCC from raw audio using the standard approach starting with FFT will be a lot simpler, and is guaranteed to be accurate.</p>
</li>
</ul>
","11","2","836","25573"
"14833","<p>My understanding of the question is that you are using a basic MLP, feed-forward neural network, to work with data that is a 3-dimensional vector which you are calling $(x,y,z)$ - although note that most notation you read would call the whole observation $x$ which would be a vector $(x_1, x_2, x_3)$. You want to accept an input of 1st, 2nd, 3rd observations of this data (a window on some possibly longer series), and predict the 4th observation.</p>

<p>For the given architecture, the prediction is made immediately on the supplied input - a MLP has no ""memory"" for previous inputs - so you will need 3 x 3 = 9 inputs to represent each (x,y,z) in the series. For a MLP it doesn't really matter how you arrange this input - e.g. you could have $(x^{(1)}, y^{(1)}, z^{(1)},x^{(2)}, y^{(2)}, z^{(2)},x^{(3)}, y^{(3)}, z^{(3)})$ or $(x^{(1)}, x^{(2)}, x^{(3)},y^{(1)}, y^{(2)}, y^{(3)},z^{(1)}, z^{(2)}, z^{(3)})$ or any other arrangement as long as you are consistent for every training example and prediction. </p>

<p>You don't usually want or need to somehow combine the x, y and z values to create a pre-processed layer with 3 inputs, or create a special layer that combines $x^{(1)}, x^{(2)}, x^{(3)}$. You <em>might</em> do that if your understanding of how the sequence worked allowed you to come up with features that modelled something important. However, the more usual approach is to allow the neural network to figure out how the multiple inputs combine from the training data.</p>

<p>You may want to look at alternative architecture of Recurrent Neural Network (RNN), which has more options for dealing with predictions of series. For a RNN, you would have 3 input neurons, and you would run the network 3 times in sequence and read the prediction after running it the third time. </p>

<p>You have noticed correctly that a MLP has no ""understanding"" internally that your x in (x,y,z) at step 1 is the same type of variable as the x in steps 2 and 3. The MLP would have 9 separate inputs and would ignore any direct relations, or even that there is a sequence being predicted. Whilst in a RNN, both those details are built into the model as core assumptions. In some cases that could be an important advantage to the RNN. But it is still worth trying the simpler MLP first and testing whether its predictions are good enough for purpose.</p>
","1","2","836","25573"
"14938","<p>Available compute power does not directly affect the accuracy of a neural network. If your different runs of the network have:</p>

<ul>
<li>identical architecture and meta-params </li>
<li>identical code (including library code)</li>
<li>all training data is identical </li>
<li>all stochastic parts of training use the same random seed and generator</li>
<li>all data types are identical precision (e.g. all vectors and matrices are 32-bit or 64-bit floats)</li>
</ul>

<p>then the behaviour of neural network training in each run is fully deterministic and repeatable. Having a faster processor will just get you to the result faster*.</p>

<p>The most likely difference between your tests is due to not seeding the random number generators used in the training the process. For you this includes weight initialisation, possibly train/test split and possibly shuffling training data in each epoch. As you did not use any regularisation, then accuracy of the trained network can vary quite a bit due to over-fitting.</p>

<p>To verify this, you can train a second or third time on each CPU. I expect you will see a lot of variation in final accuracy, regardless of which machine you run it on.</p>

<hr>

<p>* This does mean that having a faster machine can result in you having a more accurate final network in practice when you are tuning the parameters, because you can try more variations of meta-params with multiple training sessions.</p>
","1","2","836","25573"
"15002","<p>The genes shown in the diagrams have been simplified so that they only show topology and innovation index number. In addition, the full genome contains weights for each connection and bias for each node. </p>

<p>The innovation numbers are not tracking changes with connection weights or biases, just the rough time of inserting a specific connection between two nodes. So each parent can have matching connections/innovations, but with different weights associated. The full genome is still subject to cross-over and mutation, the excerpt you quote helps explain how the genome is aligned between parents with different architectures, so that GA crossover and mutation can occur in each connection and node.</p>

<p>Which parent is used to create each offspring gene matters because of the additional connection weight data associated with each parent's gene.</p>
","1","2","836","25573"
"15038","<p>There is no native <code>.dot_product</code> method. However, a dot product between two vectors is just element-wise multiply summed, so the following example works:</p>



<pre><code>import tensorflow as tf

# Arbitrarity, we'll use placeholders and allow batch size to vary,
# but fix vector dimensions.
# You can change this as you see fit
a = tf.placeholder(tf.float32, shape=(None, 3))
b = tf.placeholder(tf.float32, shape=(None, 3))

c = tf.reduce_sum( tf.multiply( a, b ), 1, keep_dims=True )

with tf.Session() as session:
    print( c.eval(
        feed_dict={ a: [[1,2,3],[4,5,6]], b: [[2,3,4],[5,6,7]] }
    ) )
</code></pre>

<p>The output is:</p>

<pre><code>[[ 20.]
 [ 92.]]
</code></pre>
","11","2","836","25573"
"15105","<p>When you create a Numpy array like this:</p>

<pre><code>x_data = np.array( [[1,2],[4,5,6],[1,2,3,4,5,6]])
</code></pre>

<p>The internal Numpy dtype is ""object"":</p>

<pre><code>array([[1, 2], [4, 5, 6], [1, 2, 3, 4, 5, 6]], dtype=object)
</code></pre>

<p>and this cannot be used as a Tensor in TensorFlow. In any case, Tensors must have same size in each dimension, they cannot be ""ragged"" and must have a shape defined by a single number in each dimension. TensorFlow basically assumes this about all its data types. Although the designers of TensorFlow could write it in theory make it accept ragged arrays and include a conversion function, that kind of auto-casting is not always a good idea, because it might hide a problem in the input code.</p>

<p>So you need to pad the input data to make it a usable shape. On a quick search, I found <a href=""https://stackoverflow.com/questions/32037893/numpy-fix-array-with-rows-of-different-lengths-by-filling-the-empty-elements-wi"">this approach in Stack Overflow</a>, replicated as a change to your code:</p>



<pre><code>import tensorflow as tf
import numpy as np

x = tf.placeholder( tf.int32, [3,None] )
y = x * 2

with tf.Session() as session:
    x_data = np.array( [[1,2],[4,5,6],[1,2,3,4,5,6]] )

    # Get lengths of each row of data
    lens = np.array([len(x_data[i]) for i in range(len(x_data))])

    # Mask of valid places in each row
    mask = np.arange(lens.max()) &lt; lens[:,None]

    # Setup output array and put elements from data into masked positions
    padded = np.zeros(mask.shape)
    padded[mask] = np.hstack((x_data[:]))

    # Call TensorFlow
    result = session.run(y, feed_dict={x:padded})

    # Remove the padding - the list function ensures we 
    # create same datatype as input. It is not necessary in the case
    # where you are happy with a list of Numpy arrays instead
    result_without_padding = np.array(
       [list(result[i,0:lens[i]]) for i in range(lens.size)]
    )
    print( result_without_padding )
</code></pre>

<p>Output is:</p>

<pre><code>[[2, 4] [8, 10, 12] [2, 4, 6, 8, 10, 12]]
</code></pre>

<p>You don't have to remove the padding at the end - only do this if you require to show your output in the same ragged array format. Also note that when you feed the resulting <code>padded</code> data to more complex routines, the zeros - or other padding data if you change it - may get used by whatever algorithm you have implemented.</p>

<p>If you have many short arrays and just one or two very long ones, then you might want to consider using a <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/sparse_ops.html"" rel=""noreferrer"">sparse tensor representation</a> to save memory and speed up calculations.</p>
","6","2","836","25573"
"15144","<p><a href=""https://en.wikipedia.org/wiki/Hyperparameter_optimization"" rel=""nofollow noreferrer"">Wikipedia lists some well-known approaches</a> to hyper-parameter searches.</p>

<p>The brute-force scan/search, or a grid search across multiple parameters, is still a very common and workable approach. As is random search, just trying some variations of parameters automatically and picking the best result from cross validation. </p>

<p>A guided search by intuitive feel of what might best work is also still quite common approach, despite being characterised as ""optimisation by graduate descent"" (a pun based on the fact that a senior researcher will hand off the tuning work to their smart junior researchers who make educated guesses on hyperparameter values) - for regularisation parameters you can look at training curves and difference between training loss vs cv loss to help decide where to look. You generally want to increase regularisation if there is a large difference and decrease it if the values become close but not very good. What counts and as ""large difference"" and ""not very good"" is subjective though, and typically you only get a feel for what works after a few random attempts and immersing yourself in the problem so that you recognise good vs bad behaviour of a model.</p>

<p>There are some claimed ""smart"" schemes that apply <a href=""https://en.wikipedia.org/wiki/Bayesian_optimization"" rel=""nofollow noreferrer"">Bayesian optimisation</a> or similar, and even some automated services or paid tools that attempt to make these available for tuning models automatically. However, I don't see these used much in practice.</p>

<p>Unless you are trying to win a Kaggle competition, you quickly hit diminishing returns attempting to fine-tune your parameter. I've rarely found tuning the second significant digit of a meta-param like L2 loss to be worth much attention. Maybe with the exception of testing between e.g. 1.0 and 1.5 - often useful to think in terms of geometric progression, not linear, even when fine-tuning. </p>

<p>You also have to be careful that you are not simply tuning to your cross-validation set, with no meaningful or justifiable improvement for test cases or in production. You can mitigate this in part by using k-fold cross validation (which can reduce error on metrics estimates effectively by using more data to estimate them), although of course that increases time/CPU costs.</p>
","6","2","836","25573"
"15156","<p>If your classes arre not mutually exlcusive, then you just have multiple sigmoid outputs (instead of softmax function as seen in example MNIST classifiers). Each output will be a separate probability that the network assigns to membership in that class.</p>

<p>For a matching loss function, in TensorFlow, you could use the built-in <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sigmoid_cross_entropy_with_logits"" rel=""nofollow noreferrer"">tf.nn.sigmoid_cross_entropy_with_logits</a> - note that it works on the logits - the inputs to the sigmoid function - for efficiency. The link explains the maths involved. </p>

<p>You will still want a sigmoid function on the output layer too, for when you read off the predictions, but you apply the loss function above to the <em>input</em> of the sigmoid function. Note this is not a requirement of your problem, you can easily write a loss function that works from the sigmoid outputs, just the TensorFlow built-in has been written differently to get a small speed boost. </p>
","1","2","836","25573"
"15191","<p>I tried this and got same result.</p>

<p>It is because the gradient of <code>.abs</code> is harder for a simple optimiser to follow to the minima, unlike squared difference where gradient approaches zero slowly, the gradient of the absolute difference has a fixed magnitude which abruptly reverses, which tends to make the optimiser oscillate around the minimum point. Basic gradient descent is very sensitive to magnitude of the gradient, and to the learning rate, which is essentially just a multiplier of the gradient for step sizes.</p>

<p>The simplest fix is to reduce the learning rate e.g. change line</p>

<pre><code>optimizer = tf.train.GradientDescentOptimizer(0.5)
</code></pre>

<p>to</p>

<pre><code>optimizer = tf.train.GradientDescentOptimizer(0.05)
</code></pre>

<p>Also, have a play with different optimisers. Some will be able to cope with <code>.abs</code>-based loss better.</p>
","17","2","836","25573"
"15278","<p>Train with the same kinds of inputs as you wish the network to recognise. There is no easy way to add logical combinations that you think should help the network, because deep neural networks, despite the hype as the best thing to hit AI, are essentially just really dumb complex statistical analysers.</p>

<p>Provided you supply training images, the CNN will learn how to statistically combine components to decide if overall an image is a chair or not. For instance if the image is of a person wearing a padded jacket, perhaps that will look a little like the back of a chair. The NN might output a 0.75 confidence that a back of a chair is in the image. If you manually combine decisions from components, you will need to figure out from that whether to say there is a chair in the image. If you leave it to the network, the CNN itself will combine this data with the additional detection there is a 0.2 confidence that the person's legs are chair legs (and they are in the wrong relative place) and decide - hopefully - that there is no chair.</p>

<p>You don't need to explicitly label components of objects for the NN to learn that there are components and what they look like. The outputs of Deep Dream and other CNN visualisations demonstrate that internally the CNN builds up its own structures that represent parts of objects. They are not necessarily the same parts as a person would focus on, but by allowing the training process to choose, you should be able to  get a good recognition rate.</p>

<p>So, if you give the network a lot of chair parts to recognise separately, it will get better at labelling those chair parts accurately. But that will not really help the network recognise a chair. In fact it could be worse at the main task if the training images are too different from images it is expected to recognise. Plus, it will be your task to then decide how to re-combine the classes to detect the chair later.</p>

<p>There probably <em>are</em> architectures and image-processing pipelines that can be used to combine logical knowledge of parts and their poses into whole objects. It is something I very much expect to be an active area of research in computer vision systems. However, these approaches will not use a simple CNN and altering training data as the solution.</p>
","1","2","836","25573"
"15421","<p>The reward function in the Chapter 2 test bed is simply the ""true"" mean value for the chosen action, plus a ""noise term"" which is normal distribution with mean 0, standard deviation 1. </p>

<p>The noise has the same distribution as the initial setting of ""true"" values. The difference is you set the true values at the start and do not change them, then add noise on evaluation of each reward. The goal for the learner is then to find the best ""true"" value whilst only seeing the reward.</p>

<p>This matches your understanding as I read it from the question. You could write it like this:</p>

<p>Initialisation:</p>

<ul>
<li>$\forall a \in A: q_*(a) \leftarrow N(0,1)$</li>
</ul>

<p>Evaluation:</p>

<ul>
<li>$R_t = r(A_t) = q_*(A_t) + N(0,1)$</li>
</ul>

<p>Where $N(\mu,\sigma)$ is a sample from the normal distribution, mean $\mu$, standard deviation $\sigma$</p>

<blockquote>
  <p>Why is the reward function chosen this way in the test bed and how does the reward function affect the value estimations and the graphs ?</p>
</blockquote>

<p>For a bandit problem to be non-trivial, the reward function needs to be stochastic, such that it is not possible to immediately discover the best action, there should be some uncertainty on what the best action to take is, even after taking many samples.</p>

<p>So the noise is there to provide at least some difficulty - without it finding the best action would be a trivial $argmax$ over the 10 possible actions. The noise does <em>not</em> represent uncertainty in the sensing (although that could also be a real world issue), but variability of the environment in response to an action. The test examples could have almost <em>any</em> distribution (e.g. $p(-1.0|a=1) = 0.9, p(9.0|a=1) = 0.1$ for $q_*(a=1) = 0.0$), the authors made a choice that was concise to describe and useful for exploring the different techniques in the chapter.</p>

<p>The specific reward function will affect the learning graphs. The test bed has been chosen so that ratio of noise to magnitude of ""true"" values is high. In turn, this means that value estimates will converge relatively slowly (as a ratio to the true values), and this exposes differences between different sampling and estimation techniques when they are plotted by time step.</p>

<p>To answer your concern:</p>

<blockquote>
  <p>I guess the reason i'm asking this question is because i'm not 
  completely clear how a reward looks like in the real world where i
  don't know anything about q*(a).</p>
</blockquote>

<p>In the real world you may need to <em>sense</em> or receive the reward from the environment. Obviously that complicates the test scenario, and doesn't add anything to the understanding of the maths, so the test bed just generates some imaginary distribution for the environment inside the problem. The ""sensing"" in the test is just assumed with a reward amount defined by the test. </p>

<p>To qualify as a simple (and static) bandit problem, the reward has to be immediately apparent on taking the action, and have no dependency on current state or history. That constrains the problem somewhat - it is not the <em>full</em> reinforcement problem. So real-world examples tend to be about gambling with limited choice on independent repeatable events.</p>
","2","2","836","25573"
"15453","<p>A lot of people are suggesting holding 2016 data, but what you should hold out as a test set depends on what is being predicted. If defective works do not depend on date/time, and the date/time is not being used in the model, it may make sense to hold a random sample (at least w.r.t. date/time) for your tests.</p>

<p>If you have features that you want to avoid extrapolating from, then split by those features e.g. if there is some common ""location"" property for your items, you may want to split by identity of location, because you want to use the model to make predictions in new locations (and the location as a one-hot feature would not have any predictive value for a new location, even if your target class was correlated with existing locations).</p>

<p>Ideally the consultancy firm will be helping you here to identify correlations in the data set that you don't want to use in predictions, because they could affect the generalisation you want when using the model in production. If any such thing is identified, then it clearly points to using that feature to split test data by. If there is no such issue, then you may as well split randomly.</p>
","1","2","836","25573"
"15491","<p>NB This advice assumes your goal is to recognise expression in pictures of any person, and not just people from your training data.</p>

<blockquote>
  <p>Should the validation set consists of unseen subjects as well? </p>
</blockquote>

<p>Yes. This will give you the most accurate measure of performance in the task you want to use the network in, in order to choose the best generalisation and take it forward to testing.</p>

<p>You would only use a simple random split if the end goal of your trained network is to recognise expressions from images of the people in the training set.</p>

<blockquote>
  <p>Or can I shuffle the whole training set and use a part of it (10-20%) as validation set?</p>
</blockquote>

<p>No. If you take random samples where the same face appears in training and cv, you will get an over-estimate of generalisation. I have seen this effect first hand in the Kaggle <a href=""https://www.kaggle.com/c/state-farm-distracted-driver-detection"" rel=""nofollow noreferrer"">State Farm Distracted Driver</a> contest, and you should see it discussed in the forums there. It might be helpful for ideas to improve performance too.</p>

<blockquote>
  <p>if I use 10-20% of the training set as my validation set, my model learns with reasonable accuracy (45-50%) using a 3-layer CNN </p>
</blockquote>

<p>This is a <em>data leak</em> between train and cross validation - the network has learnt to correctly classify expressions in images of people it has already seen, and that is what you are measuring by taking a split in this way. It is not surprising that the test result does not match the promising values from cross validation.</p>

<blockquote>
  <p>I'm just wondering which is correct since using a validation set of unseen subjects, my model starts to overfit after 3-5 epochs and doesn't learn at all</p>
</blockquote>

<p>You are over-fitting regardless of how you split train,CV. When you split the CV incorrectly, then in addition to over-fitting, the data leak gives you bad guidance.</p>

<p>It looks like that you have very little diversity in the training data, whilst training an image classifier from scratch requires a <em>lot</em> of data. Consider:</p>

<ul>
<li><p>Collecting more labelled data, perhaps some other dataset you can download.</p></li>
<li><p>Adding a lot of regularisation to your model (e.g. multiple dropout layers)</p></li>
<li><p>Using a pre-trained image classifier network (e.g. VGG-19 or Inception) as a starting point and fine-tuning it for your classification task.</p></li>
<li><p>Use full k-fold cross-validation regardless of the computational cost, to mitigate problems of using small training set. This won't help solve your training problems directly, but will give you a better shot at tuning your network hyper-parameters once you solve that issue.</p></li>
</ul>
","3","2","836","25573"
"15522","<p>There are many factors which influence choice of classifier algorithm. Number of target classes does not generally have an influence, compared to the nature of input features. </p>

<p>As one example, if your input data is natural audio or image, then regardless of the number of classes, a deep convolutional neural network is very likely going to have the best performance.</p>

<blockquote>
  <p>What algorithms generally make the best predictions with the smallest amount of data at each of these tiers?</p>
</blockquote>

<p>There is no a priori best approach based on number of output classes. The ""best predictions"" vs ""smallest amount of data"" is also a trade-off, where simpler models will perform better than complex ones on small amounts of data, but the more complex models will cope better with larger amounts of data, and will then give you better predictions. At some point you might have enough data so that sampling more will not improve your trained models, but you need to establish that empirically.</p>

<p>Most algorithms allow you to explore the trade-off within them by varying hyper-parameters to make the model simpler for smaller data sets and more complex when there is more training data. </p>

<blockquote>
  <p>I know at some point it makes more sense to use regression rather than a classifier. How many potential class labels would this be?</p>
</blockquote>

<p>That is not strictly true. In general, the distinction between classifying and regression is a hard line. If you are classifying hand-written symbols into an alphabet for instance, it doesn't really matter if you are doing this for 10, 100 or 1000 classes, there is not a <em>practical</em> point at which the symbols turn from being a set of objects into a continuous space to perform regression over.</p>

<p>It could be true if your target class represents a range within some continuous variable (e.g. classifying an event by some of its properties into which year it occurred). But in that case the problem is inherently a regression problem to start with. In fact you may be better off training a regression algorithm in this case even for small number of target classes, and simply binning the predictions into relevant classes.</p>

<p>It could also be true that your target class represents a rank or sequence within an ordered set. In which case this does look more like a regression problem when you have a longer sequence. In general, if you can <em>arrange</em> your target classes into a meaningful sequence, then you might be able to perform some kind of <a href=""https://en.wikipedia.org/wiki/Ordinal_regression"" rel=""nofollow noreferrer"">ordinal regression</a> which could be a better choice than using a classifier. However, classifying symbols/alphabets does not work this way, because the sequence in those is arbitrary.</p>

<p>Finally, you might be facing such a large number of classes that a single classifier model is overwhelmed and you need to approach the problem differently. </p>

<p>For an example of this last case, consider a classifier for images of pets. If it had three classes (cats, dogs, rabbits), then you'd clearly use standard classification approach. Even when classifying by breed - 100s of classes - then this approach still works well enough, as seen in <a href=""http://www.image-net.org/"" rel=""nofollow noreferrer"">ImageNet</a> competitions. However, once you decide to try and detect the <em>identity</em> of each pet (still technically a class) you hit a problem using simple classifier techniques - in that case the structure of the solution needs more thought. One possible solution is a regression algorithm trained to extract biometric data from the image (nose length, distance between eyes, angle subtended between centre of jaw and ears) and move the classification stage into KNN based on a database of biometric data for observed individuals. This is how some face identifying algorithms work, by mapping images of faces into an easy-to-classify continuous space first (typically using a deep CNN), then using a simpler classifier that scales well across that space.</p>
","1","2","836","25573"
"15591","<blockquote>
  <p>Essentially what I would love to do is create an A.I. app that will be fed the same data that the ""experts"" had and see if I can create something more accurate and beat them at it. Is this a viable approach?</p>
</blockquote>

<p>Sure, you can use one or more supervised learning techniques to train a model here. You have features, a target variable and ground truth for that variable. </p>

<p>In addition to applying ML you have learned, all you need to do to test your application fairly is reserve some of the data you have with expert predictions for comparison as test data (i.e. do not train using it). </p>

<p>I would caveat that with some additional thoughts:</p>

<ul>
<li><p>You haven't really outlined an ""approach"" here, other than mentioning use of ML.</p></li>
<li><p>Be careful not to leak future data back into the predictive model when building a test version.</p></li>
<li><p>Predicting stock and markets is hard, because they react to their own predictability and many professional organisations trade on the slightest advantage they can calculate, with experienced and highly competent staff both gathering and analysing data. </p></li>
</ul>

<hr>

<p>Not directly part of the answer, but to anyone just starting out and discovering machine learning, and finding this Q&amp;A:</p>

<p>Please don't imagine rich rewards from predicting markets using stats at home, it doesn't happen. If you think that this is a route to ""beating the market"" be aware that you are far from the first to think of doing this, and such a plan can be summarised like this:</p>

<ol>
<li>Market Data + ML</li>
<li>???</li>
<li>Profit</li>
</ol>

<p>You can fill in the ??? by learning <em>loads</em> about financial markets - i.e. essentially by becoming one of the experts. ML is not a short-cut, but it might be a useful tool if you are, or plan to be, a market analyst.</p>
","5","2","836","25573"
"15690","<blockquote>
  <p>in what sense applying the transpose is like ""moving the error backwards""?</p>
</blockquote>

<p>It isn't. Or at least you shouldn't think too hard about the analogy. </p>

<p>What you are actually doing is calculating the gradient - or partial derivative - of the error/cost term with respect to the activations and weights in the network. This is done by repeatedly applying the <a href=""https://en.wikipedia.org/wiki/Chain_rule"" rel=""nofollow noreferrer"">chain rule</a> to terms that depend on each other in the network, until you have the gradient for all the variables that you control in the network (usually you are interested in changing the weights between neurons, but you can alter other things if they are under your control - this is exactly how deep dreaming works too, instead of treating the weights as a variable, you treat the input image as one).</p>

<p>The analogy is useful in that it explains the progression and goal of the repeated calculations across layers. It is also correct in that values of the gradient in one layer depend critically on values in ""higher"" layers. This can lead to intuitions about managing problems with gradient values in deep networks.</p>

<p>There is no relation between taking the matrix transpose and ""moving the error backwards"". Actually you are taking sums over terms connected by the weights in the NN model, and that happens to be the same <em>operation</em> as the matrix multiplication given. The matrix multiplication is a concise way of writing the relationship (and is useful when looking at optimisations), but is not fundamental to understanding neural networks.</p>

<p>The statement ""moving the error backwards"" is in my opinion a common cause of problems when developers who have not studied the theory carefully try to implement it. I have seen quite a few attempts at implementing back propagation that cargo cult in error terms and multiplications without comprehension. It does not help here that some combinations of cost functions and activation functions in the output layer have been deliberately chosen to have very simple derivatives, so that it <em>looks like</em> the neural network is copying error values around with almost magical multiplications by activation value etc. Actually deriving those simple terms takes a reasonably competent knowledge of basic calculus.</p>
","4","2","836","25573"
"15741","<p>You don't get a full data dictionary with this dataset. Instead the explanation (from your link) is:</p>

<blockquote>
  <p>90 attributes, 12 = timbre average, 78 = timbre covariance 
  The first value is the year (target), ranging from 1922 to 2011. 
  Features extracted from the 'timbre' features from The Echo Nest API. 
  We take the average and covariance over all 'segments', each segment 
  being described by a 12-dimensional timbre vector.</p>
</blockquote>

<p>This is basically features derived direct from the audio. This is typical for problems in signal analysis, and the UCI dataset has been cut down so that it is purely about matching audio summaries to year of production.</p>

<p><a href=""http://labrosa.ee.columbia.edu/millionsong/pages/example-track-description"" rel=""nofollow noreferrer"">The data dictionary from the original MSD dataset</a> shows much more metadata. There are 54 feature <em>types</em>, but actually thousands of features - each type can be repeated many times. The UCI subset is not only subset by row, but by columns too, and only uses some of the MSD data.</p>
","1","2","836","25573"
"15751","<p>You probably just need a deeper network to learn the cross test example. Convolutional layers need to have multiple feature channels and be stacked in order to learn anything more complex that edge/corner counting. Look at example networks for e.g. MNIST and you will usually see typically 2 or 3 CNN layers, with multiple channels (e.g. 32 or 64 channels, each of which is associated with previous layer channels amount of NxN kernels) plus 1 or 2 fully connected layers before classifier.</p>

<p>For a binary choice, yes train on object present/not present classes. There is no major difference between a single sigmoid output or softmax with two outputs. Use cross-entropy loss function, multiclass version if you use softmax. </p>

<p>I suggest just adapt an MNIST example network to your problem by changing input shape and number of outputs. So that would use softmax and multiclass cross-entropy loss.</p>

<blockquote>
  <p>Is it a problem to have a large MaxPooling layer</p>
</blockquote>

<p>It could work in some situations, but by having a single layer, single channel CNN, you have made the network too simple to classify even moderately complex shapes. Reducing the activations from this layer to just the max is going to limit the network to essentially classifying by strongest matching 5x5 template to the filter in the image. With enough training examples it should be able to create such a filter, but it would not cope very well with noise, rotation or any partial matches.</p>

<p>You might get something more effective with a large pooling layer over say a 2-deep multi-channel CNN, and then a small fully-connected layer over those max channel outputs. It would still be an unusual choice of architecture, but I suspect it could learn problems like your cross test.</p>
","3","2","836","25573"
"15994","<p>The theory of backpropagation <em>should</em> work with complex numbers, it doesn't make any assumptions about all numbers being real. However, most NN libraries would not be able to calculate with them. Your weights and biases would also need to be complex. I have no idea if you could reasonably expect convergence using gradient descent - many activation functions have periodic output and numerically unstable regions when calculating with complex numbers, which makes me think it would be risky.</p>

<p>The simplest thing to do would be to split each complex number up into its components at the input, representing as a 2-dimensional vector of real numbers - either using cartesian or polar approach. Neural networks should cope just fine with splitting features up like this.</p>
","3","2","836","25573"
"16067","<p>There are multiple ways to approach solving game playing problems. Some games can be solved by search algorithms for example. This works well for card and board games up to some level of complexity. For instance, <a href=""https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)"" rel=""nofollow noreferrer"">IBM's Deep Blue</a> was essentially a fast heuristic-driven search for optimal moves.</p>

<p>However, probably the most generic machine learning algorithm for training an agent to perform a task optimally is <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow noreferrer"">reinforcement learning</a>. Technically it is not one algorithm, but an extended family of related algorithms that all solve a specific formalisation of the learning problem.</p>

<p>Informally, Reinforcement Learning (RL) is about finding optimal solutions to problems defined in terms of an <em>agent</em> that can observe the <em>state</em> of an <em>environment</em>, take <em>actions</em> in that environment and experience <em>rewards</em> which are somehow related to the state and action. RL solvers need to be designed to cope with situations where rewards are received later than when important actions were taken, and this is usually achieved by the algorithm learning an internal expectation of later rewards associated with state and/or state-action pairs.</p>

<p>Here are some resources for studying Reinforcement Learning:</p>

<ul>
<li><p><a href=""http://incompleteideas.net/book/the-book-2nd.html"" rel=""nofollow noreferrer"">Reinforcement Learning: An Introduction (Second Edition)</a></p></li>
<li><p><a href=""https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf"" rel=""nofollow noreferrer"">Algorithms for Reinforcement Learning (PDF)</a></p></li>
<li><p><a href=""https://eu.udacity.com/course/reinforcement-learning--ud600"" rel=""nofollow noreferrer"">Udacity Reinforcement Learning course</a></p></li>
<li><p><a href=""http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"" rel=""nofollow noreferrer"">David Silver UCL lectures on Reinforcement Learning</a></p></li>
</ul>

<p>You will find the subject itself is quite large as more and more sophisticated variations of the algorithms are necessary as the problem to solve becomes harder.</p>

<p>Starting games for studying reinforcement learning might include:</p>

<ul>
<li><p>Tik-tac-toe (aka Noughts and crosses) - this can be solved easily using search, but it makes for a simple toy problem to solve using basic RL techniques.</p></li>
<li><p>Mazes - in the reinforcement learning literature, there are many examples of ""grid world"" games where an agent moves in single N,E,S,W steps on a small board that can be populated with hazards and goals.</p></li>
<li><p>Blackjack (aka 21)</p></li>
</ul>

<p>If you want to work with agents for playing video games, you will also want to learn about <a href=""https://en.wikipedia.org/wiki/Artificial_neural_network"" rel=""nofollow noreferrer"">neural networks</a> and probably in some detail - you will need deep, convolutional neural networks to process screen graphics.</p>

<p>A relatively new resource for RL is <a href=""https://universe.openai.com/"" rel=""nofollow noreferrer"">OpenAI Universe</a>. They have done a lot of work to package up environments ready to train agents against, meaning you can concentrate on studying the learning algorithms, as opposed to the effort of setting up the environment.</p>

<hr>

<p>Regarding your list of current skills: None of them are directly relevant to reinforcement learning. However:</p>

<ul>
<li><p>If you can understand the maths and theory from your previous course, then you should also be able to understand reinforcement learning theory.</p></li>
<li><p>If you have studied any online or batch supervised learning techniques, then these can be used as components inside a RL framework. Typically they can be used to approximate a <em>value function</em> of the game state, based on feedback from successes and failures so far.</p></li>
</ul>
","8","2","836","25573"
"16077","<p>The terms are different:</p>

<ul>
<li><p><em>Equivariant to translation</em> means that a translation of input features results in an equivalent translation of outputs. So if your pattern 0,3,2,0,0 on the input results in 0,1,0,0 in the output, then the pattern 0,0,3,2,0 might lead to 0,0,1,0</p></li>
<li><p><em>Invariant to translation</em> means that a translation of input features doe not change the outputs at all. So if your pattern 0,3,2,0,0 on the input results in 0,1,0 in the output, then the pattern 0,0,3,2,0 would also lead to 0,1,0</p></li>
</ul>

<p>For feature maps in convolutional networks to be useful, they typically need both properties in some balance. The equivariance allows the network to generalise edge, texture, shape detection in different locations. The invariance allows precise location of the detected features to matter less. These are two complementary types of generalisation for many image processing tasks.</p>
","43","2","836","25573"
"16258","<p>Text-to-speech suffers from problems similar to the drivers of ""<a href=""http://www.tsdconference.org/tsd2014/download/preprints/673.pdf"" rel=""nofollow noreferrer"">uncanny valley</a>"" effects when viewing faces of humanoid robots and computer renderings of faces. Our ability to distinguish depth of meaning, emotional content and other subtle cues makes humans very sensitive to small details in audio containing spoken language. Note this is not quite the same as claiming text-to-speech actually <em>has</em> an ""uncanny valley""; there doesn't appear to be much analysis of that claim either way around.</p>

<p>Existing text-to-speech systems have two basic generative processes: <em>Concatenative</em> models are essentially databases of phoneme samples that the system strings together. These suffer from lack of flexibility. <em>Parametric</em> models attempt to represent sound generation at a lower level to improve on this, but it is quite a challenge to create a model that includes all the fine detail that we notice. For instance, we notice many non-verbal cues including imperfections such as mouth noise, breathing etc.</p>

<p>There has been recent progress in this area by researchers running a very detailed generative model - one that literally generates sound, sample by sample, after consuming a large amount of training data.</p>

<p><a href=""https://deepmind.com/blog/wavenet-generative-model-raw-audio/"" rel=""nofollow noreferrer"">This report on WaveNet by DeepMind team</a> explains the different synthesis techniques used to date and shows off the capabilities of the new approach.</p>

<p>The process creates state-of-the-art results, and it is easy to hear the improvement from examples on the site. However, it is too computationally intensive to be used in real-time systems yet. Give it a few years to be refined, and it could be the basis of far better text-to-speech systems.</p>

<hr>

<p>The above written in January 2017. In the intervening time (it is now October 2017) the DeepMind team have been working on the efficiency of their model, and <a href=""https://deepmind.com/blog/wavenet-launches-google-assistant/"" rel=""nofollow noreferrer"">it is now much faster and sounds even better</a>. This is close to becoming a solved problem, albeit with some proprietary ownership. Give it a little while longer though and this breakthrough will allow real-time and natural-sounding parametric voice models in many applications.</p>
","6","2","836","25573"
"16307","<p>The value of $inv(X)$ or $inv(X^T)$ cannot be calculated for non-square matrices, and usually your number of samples and features is different, so your simplification is not generic.</p>

<p>However, you can simplify the functional notation in a very similar way to your suggestion using a <a href=""https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse#Linear_least-squares"" rel=""noreferrer"">pseudo-inverse</a> - note that the link here is direct to the part of the article that describes this inverse as a direct solution to the linear least squares problem. Noting the pseudo inverse as $pinv(X)$ then:</p>

<p>$$\theta = pinv(X)y$$</p>

<p>However, this is simply hiding the more basic derivation behind a complex function. The computation required is essentially the same.</p>

<p>In practice if you are using matrix library that supports inverses and pseudo-inverses, you can choose to use a longer formulation using inverses or the pseudo-inverse version and it will give near identical results.</p>
","4","2","836","25573"
"16554","<p>If you are performing regression, you would usually have a final layer as linear.</p>

<p>Most likely in your case - although you do not say - your target variable has a range outside of (-1.0, +1.0). Many standard activation functions have restricted output values. For example a sigmoid activation can only output values in range (0.0, 1.0) and a ReLU activation can only output positive values. If the target value is outside of their range, they will never be able to match it and loss values will be high.</p>

<p>The lesson to learn here is that it <em>is</em> important which activation function you use in the output layer. The function must be able to output the full range of values of the target variable.</p>
","2","2","836","25573"
"16602","<p>There is no requirement for specific pixel dimensions for convolutional neural networks to function normally. It is likely the values have been chosen for pragmatic reasons - such as a compromise between using image details vs number of parameters and training set size required. </p>

<p>In addition, if source data has a range of different aspect ratios, some portrait, some landscape, with the target object usually in the centre, then taking a square crop from the middle could be a reasonable compromise.</p>

<p>When you increase the input image size, you will also increase the amount of noise and variance that the network will need to deal with in order to process that input. That could mean more layers - both convolutional and pooling. It could also mean that you need more training examples, and of course each training example will be larger. Together, these increase the computation resources you need to complete training. However, if you can overcome this requirement, it is possible that you will end up with a more accurate model, for any task where the extra pixels could make a difference. </p>

<p>One possible rule of thumb for whether you would want higher resolution is if, for goal of your network, a human expert could make use of the extra resolution and perform better at the task. This might be the case in regression systems, where the network is deriving some numerical quantities from the image - e.g. for face recognition extracting biometrics such as distance between facial features. It might also be desirable for image-processing tasks such as automated masking - state of the art results for these tasks may still be lower resolution than the commercial images where we would like to apply them in practice.</p>
","12","2","836","25573"
"16621","<p>In all the implementations for CNNs processing images that I have seen, the output in any layer is</p>

<pre><code>Width x Height x Channels
</code></pre>

<p>or some permutation. This is the same number of dimensions as the input, no additional dimensions are added by the convolutional layers. Each feature map channel in the output of a CNN layer is a ""flattened"" 2D array created by adding the results of multiple 2D kernels (one for each channel in the input layer).</p>

<p>Usually even greyscale input images are expected to be represented as <code>Width x Height x 1</code> so that they fit the same pattern and the same layer model can be used.</p>

<p>It is entirely feasible to build a layer design which converts a standard 2D+channels input layer into a 3D+channels layer. It is not something I have seen done before, but you can never rule out that it could be useful in a specific problem.</p>

<p>You may also see 3D+channels convolutions in CNNs applied to video, but in that case, the structure will be some variation of</p>

<pre><code>Width x Height x Frames x Channels
</code></pre>
","7","2","836","25573"
"16641","<p>Probably not. Modern encryption systems are designed around cryptographic random number generators, their output is designed to be statistically indistinguishable from true randomness. Machine learning is generally based on discovering statistical patterns in the data, and with truly random data there is none. Even for flawed crypto where there is some small pattern to be found, the large amount of randomness in the input will overwhelm any direct attempt to decrypt the ciphertext.</p>

<p>In addition, there is no heuristic you can use to tell if you are getting close to a correct decryption - a single bit out in a guess at a key for example will completely scramble the output (blame Hollywood for when it shows decryption on screen like some crossword puzzle where the correct letters drop into place). That all-or-nothingness rules out discovering algorithms via a machine learning process, even when you have the encryption key. The best you can do is brute-force all known algorithms. If you don't have the key, then you have to brute-force all possible keys too.</p>

<p>You could explore how difficult the problem is by attempting to guess the seed value used for a random number generator. Using the Mersenne Twister RNG (the standard one used in e.g. Python), then the input could be the bit pattern for 624 32-bit unsigned integers, and the output could be the 32 bits of the seed used to generate that series. The reason I suggest those specific numbers is because it <em>is</em> in fact possible to crack Mersenne Twister with that much data. However, I still think that ML approaches would be entirely the wrong tool to do so.</p>

<p>Another simple variant would be to see if you can teach a network to either produce or reverse a cryptographic hash. You could start with a known broken one such as MD5. Input and output could be 80 bits, which simplifies the architecture and pipeline enough that you could put together this test in a few hours. Even though MD5 is known to be compromised, I think there is zero chance you could teach a neural network to find any pattern.</p>

<hr>

<p>One important detail: If you want to ""crack"" an encryption, you will not be able to use the key as a known value. Chances are though, that even if you provide the key to an ML process, it will be unable to learn how to decrypt.</p>
","13","2","836","25573"
"16679","<p>For a first pass at this problem, I suggest just use a simple document search/classifier feature set such as <a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow noreferrer"">bag of words</a>, or maybe <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">tf-idf</a> against each full job description, and see what results you can get from a basic classifier. </p>

<p>You can train the model based on your girlfriend short-listing or rejecting each item. These features can also be used more or less directly to find and order other documents by degree of similarity.</p>

<p>A bag of words model is not very sophisticated. However, it is simple to implement, and has reasonable chance of being trained with limited amount of data. With this project you are more interested in optimising a human's search time than it getting the best possible accuracy. Any accuracy better that random chance should be helpful.</p>
","1","2","836","25573"
"16719","<p>There's more than one type of generative network. However, I am not aware of a generic approach that can take a trained RNN-based network and essentially run it backwards to sample an input that is expected to produce a given output. So I am suggesting a couple of generative approaches that I have seen working, but that will require that you construct and train a new network. </p>

<p>You can bring in some knowledge about the typical size of network that learns the regression model, but you cannot AFAIK directly re-use the regression model and somehow reverse it.</p>

<p>A caveat: Although I have played briefly with both types of generative network, I have never constructed one conditioned on desired goal like the one you want to work with.</p>

<hr>

<p>With a purely RNN-based approach, you might do OK by making your network predict the next letter/word in the title - a classifier - whilst taking the title so far (or X characters/words of it) and the rating (normalised) as inputs. </p>

<p>Then, once trained, you can sample from the RNN randomly to generate new strings. This is the technique used by e.g. Karpathy in his now famous blog ""<a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""noreferrer"">The Unreasonable Effectiveness of Recurrent Neural Networks</a>"". There are many examples of such sequence sampling generators available to study.</p>

<p>You could also take the output of such a model and see if it matches your regression model from earlier. But I don't think there is much you can do if it does not, except perhaps filter out generated titles if they don't meet expectations - e.g. generate many titles with intended rating of 10, and only display one when your initial model also agrees with a close to 10 rating.</p>

<p>The most relevant Keras example for this I could find is <a href=""https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py"" rel=""noreferrer"">lstm_text_generation.py</a></p>

<hr>

<p>A RNN-based <a href=""https://en.wikipedia.org/wiki/Generative_adversarial_networks"" rel=""noreferrer"">generative adversarial network (GAN)</a> might also be able to achieve what you want. However, please note that GANs are notoriously fiddly to train.</p>

<p>A GAN is actually 2 networks. You create a discriminator and a generator and train them in parallel. The generator takes a small completely random vector (e.g. 10 numbers sampled from N(0,1)), plus the rating you want to achieve. Then it generates a text sequence output. The discriminator would take a text input, and a rating, and outputs 1 if it is real, or 0 if it is fake. You present either real training data or output form the generator to the discriminator, and use these to train it. You train the generator based on whether it fools the discriminator.</p>

<p>The tricky part is to maintain balance between the two components - if either becomes too good relative to the other, training will stall.</p>

<p>However, if you can get it to work, you will have a true generative model, which samples from a population space (the noise vector that you input) plus is conditioned on the rating.</p>

<p>The most relevant Keras example for this I could find is <a href=""https://github.com/fchollet/keras/blob/master/examples/mnist_acgan.py"" rel=""noreferrer"">mnist_acgan.py</a> which generates an image, not text sequence, but hopefully should give a start.</p>
","4","2","836","25573"
"16740","<p>Using a low-level library, such as Theano or TensorFlow, it is likely that you can construct new schemes for reducing tensors (maybe via some learnable weight vector etc). In TensorFlow, you also get automated gradient calculations, so you should be able to still define a cost function and use existing optimisers, without needing to analyse the new design yourself or re-write back propagation formulae.</p>

<p><a href=""https://en.wikipedia.org/wiki/Universal_approximation_theorem"" rel=""noreferrer"">The universal approximation theorem</a> essentially states that in order to have a network that <em>could</em> learn a specific function, you don't need anything more than one hidden layer using the standard matrix multiplying 
plus non-linear activation function. So, invention and use of new architectures needs some reason. The reason is usually that these additions improve the speed or scope of what can be learned, they generalise better from training data, or they model something from a problem domain really well.</p>

<p>No doubt there have been explorations of all sorts of variations to the standard NN model over time. The ones that have become popular have all proven themselves on some task or other, and often have associated papers demonstrating their usefulness. </p>

<p>For example Convolutional Neural Networks have proven themselves good at image-based tasks. The design of a 2-dimensional CNN layer has a logical match to how pixels in an image relate to each other locally - defining edges, textures etc, so the architecture in the model nicely matches to some problem domains.</p>

<p>If you can find a good match from your vector multiplication model to a specific problem domain, then that's an indication that it may be worth implementing in order to test that theory. If you just want to explore other NN structures to see whether they work, you can still do so, but without a specific target you will be searching for a problem to solve (unless by chance you stumble upon something generally useful that has been overlooked before).</p>

<p>To address the question in the title:</p>

<blockquote>
  <p>Why neural networks models do not allow for multiplication of inputs?</p>
</blockquote>

<p>Some low-level ones do (e.g. TensorFlow). However, this is not an architecture that has proven itself useful, so authors of higher-level libraries (e.g. Keras) have no reason to implement or support it. There is the possibility that this situation is an oversight, and the idea is generally useful. In which case, once someone can show this you will find it would get support across actively-developed libraries pretty quickly since it seems straightforward to implement.</p>
","6","2","836","25573"
"16763","<p>Your padded vector should be OK as a starting model. You could take it a step further and have a specific ""no character"" encoding, as that will be a clearer learning signal.</p>

<p>You say that you are using a feed-forward neural network. In that case, there is a possible change and improvement to your model, which is to use <a href=""https://en.wikipedia.org/wiki/Recurrent_neural_network"" rel=""nofollow noreferrer"">Recurrent Neural Network (RNN)</a>, probably a <a href=""https://en.wikipedia.org/wiki/Recurrent_neural_network#Long_short-term_memory"" rel=""nofollow noreferrer"">long short-term memory (LSTM)</a> variation, because that has had some success in language models. </p>

<p>In a RNN, instead of having the character encoding repeated N times, you have it represented just once, and run the network N times for each sample, feeding in each character in turn. Instead of a ""no character"" symbol, you would have an ""end of sequence"" symbol. For an open-ended model with no specific target, it is common to have the output of the network be a prediction of the next character. This allows you to explore what the model has learnt by sampling from its predictions - a lot like using n-grams to generate realistic-looking words.</p>

<p>The advantage of a RNN for character-level language modelling is that the model is built around the assumption that each input is somehow equivalent - the <em>same</em> encoding is used at each time step. This is clearly true for character strings, so it can lead to better models from less training data.</p>

<p>The main disadvantage of using a RNN is that it is more complex to understand, build and train such a model.</p>
","1","2","836","25573"
"16883","<blockquote>
  <p>Why is stochastic gradient descent so much worse then batch GD for MNIST task?</p>
</blockquote>

<p>It isn't inherently worse. Instead, by changing just one parameter on its own you have adjusted the example outside of where it has been ""tuned"" to work, because it is a simplified example for learning purposes, and it is missing some features that most users of NNs would consider standard.</p>

<p>The batch size of 1 is performing just fine. In fact, although it takes longer to process the same number of epochs, each epoch actually has more weight updates. You get 100 times as many weight updates, although each one has far more noise in it than a batch size of 100. It is these extra weight updates, plus extra time spent running interpreted Python code for 100 times as many batches, which adds the large amount of time. </p>

<p>The problem with accuracy is that the example network has no protection from <a href=""https://en.wikipedia.org/wiki/Overfitting"">overfitting</a>. By running so many more weight updates, the training starts to learn the <em>precise</em> image data to match each digit, in order to get the best score. And by doing that exclusively it learns rules that work really well on the training data but generalise really badly to new data in the test set.</p>

<p>Try batch size of 1 and number of epochs = 3 (I tried this and got accuracy of 94.32%). Basically that is using early stopping as a form of regularisation. It's not the best form of regularisation, but it is quick to try and often effective - the problem is how to tell when to stop, so you need to measure a test set (often separate to final test set, called cross-validation set) at any potential stopping point, and save the best model so far. That will obviously involve adjusting the example code.</p>

<p>Probably the 15 epochs in the original example has been chosen carefully so as to not make overfitting a problem with batch size of 100, but as soon as you change batch size, without any other form of regularisation, the network is very likely to over-fit. In general neural networks strongly tend to over-fit, and you have to spend time and effort to understand and defend against this.</p>

<p>Have a look at <a href=""http://www.ritchieng.com/machine-learning/deep-learning/tensorflow/regularization/"">regularisation in TensorFlow</a> for other options. For this kind of problem, <a href=""https://www.tensorflow.org/tutorials/mnist/pros/"">dropout</a> (explained lower down page in the link) is highly recommended, which is not purely regularisation, but works to improve generalisation for many neural network problems.</p>
","7","2","836","25573"
"16963","<p>In reinforcement learning, you should avoid scoring interim results based on heuristics. Unlike supervised learning, or a search algorithm, you are not trying to guide the behaviour, just reward good results. For an inverted pendulum a good result might simply be ""has not fallen over so far"", although there is nothing <em>inherently</em> wrong with a cost function which expresses cost in terms of minimising differences from an ideal, you do have to take more care with the values used.</p>

<p>Assuming you are using discounting, and continuous (not episodic) approach, then reward can be 0 for ""not falling over"" and -1 for ""it fell over"", followed by a re-set/continue. You can check for falling by measuring whether the pendulum has reached some large angle to the vertical (e.g. 45 degrees or more).</p>

<p>For an episodic approach, it is more natural to have +1 ""ok"" and 0 for the end state ""fell over"", although the 0/-1 scheme would also work. However, you want to avoid having negative values for any state which is ""ok"", because that is basically telling the agent to hurry up and end the episode. In your case, ending the episode is bad, so you don't want that.</p>

<p>If you do want to reward ""perfection"" in your episodic approach, then your formula might work better if you added a positive offset, so that the agent has an incentive to continue the episode if possible. You should choose a value such that recoverable states are positive.</p>

<hr>

<p>Note that the above analysis applies only to certain episode-based approaches. It depends critically on what you count as an episode, and whether the agent is able to take an action which ends the episode.</p>
","0","2","836","25573"
"17000","<blockquote>
  <p>What is this ""something""? Is it dst.data? I stepped through a debugger and found that dst.data was just a matrix of zeros right after the assignment and then filled with values after the backward pass.</p>
</blockquote>

<p>Yes. <code>dst.data</code> is the working contents of the layer inside the CNN that you are trying to maximise. The idea is that you want to generate an image that has a high neuron activation in this layer by making changes to the input. If I understand this correctly though, it should be populated immediately after the forward pass here: <code>net.forward(end=end)</code></p>

<blockquote>
  <p>Anyways, assuming it finds how ""off"" the result of the forward pass is, why does it try to do a backwards propagation? I thought the point of deep dream wasn't to further train the model but ""morph"" the input image into whatever the original model's layer represents.</p>
</blockquote>

<p>It is not training. However, like with training we cannot directly measure how ""off"" the source that we want to change is from an ideal value, so instead we calculate how to move toward a better value by taking gradients. Back propagation is the usual method for figuring out gradients to parameters in the CNN. There are some main differences with training:</p>

<ul>
<li><p>Instead of trying to minimise a cost, we want to increase a metric which summarises how excited the target layer is - we are not trying to find any stationary point (e.g. the maximum possible value), and instead Deep Dream usually just stops arbitrarily after a fixed number of iterations.</p></li>
<li><p>We back propagate further than usual, all the way to the input layer. That's not something you do normally during training.</p></li>
<li><p>We don't use any of the gradients to the weights. The weights in the neural network are never changed. We are only interested in the gradients at the input - but to get them we need to calculate all the others first.</p></li>
</ul>

<blockquote>
  <p>What exactly does src.data[:] += step_size/np.abs(g).mean() * g do? It seems like applying whatever calculation was done above to the original image. Is this line what actually ""morphs"" the image?</p>
</blockquote>

<p>It takes a single step in the image data along the gradients that we have calculated will trigger more activity in the target layer. Yes this alters the input image. It should be repeated a few times, according to how extreme you want the Deep Dream effect to be.</p>
","1","2","836","25573"
"17132","<p>It depends a little on the specific model.</p>

<p>If your model is using a softmax output layer, then the values are usually interpreted as mutually-exclusive class probabilities and should sum to 1. </p>

<p>If your model is using a sigmoid output layer, then the values for a classification problem can be interpreted as the individual, <em>non-exclusive</em> probabilities. In that case, it is possible for an example to not be in either of your classes (an output close to [0,0]), or to definitely be in both classes (an output close to [1,1]) - if that is not the case in the problem that you have trained the NN to model, then you might want to consider altering the model.</p>

<p>So, if your classification <em>should</em> be mutually-exclusive, then the person asking you the question has a valid point. The probabilities should sum to 1, and you may need to fix the model. In that case, you should use a softmax output layer, and would not have this problem because the probabilities would always sum to 1.</p>

<p>Whether it is worth fixing the model, depends on your understanding of mutually-exclusive vs non-exclusive classes in the problem, whether the current model meets its goals (it is not a big issue in practice, if the model is not a strict theoretical match to the problem, but has a perfectly good accuracy) and how much effort it would take to alter your code and re-train the model.</p>

<p>Your statement:</p>

<blockquote>
  <p>the probability stands for <em>confidence</em> concept in mathematics.</p>
</blockquote>

<p>is technically correct, but not useful as an answer to your colleague's query. For classification, the terms <em>confidence</em> and <em>probability</em> are almost interchangeable, so you cannot really use one word to explain the other. If someone asked why one of the numbers was larger than another in your model, you would not answer ""that is because x is greater than y"" - essentially this is what you have done here. So your statement is not wrong as such, but neither does it explain the result.</p>
","3","2","836","25573"
"17185","<p>This problem is not complex enough to justify a large convolutional network. However, if you are determined to use a CNN, then you could, just try to keep the architecture very simple. Just one convolutional layer (probably SAME/padded), no pooling or dropout, only a few feature maps (e.g. no more than 4, maybe just 2 will do) - and a softmax fully-connected layer for the output.</p>

<p>Bear in mind that it can take more epochs, and perhaps more tuning of hyper-params, in order to get a more complex model to fit a simple problem. If you follow the plan in your earlier problem, and train against the whole population of valid states and moves, then you don't need to worry about over-fitting.</p>

<p>You should bear in mind that tic-tac-toe is simple enough that you can use tabular methods (i.e. approaches that simply enumerate and score all possible game states) to find an optimal policy. The network is being used in your case as a policy function approximation, and is a bit like using a sledgehammer to crack a nut. This makes sense if you are learning a technique used on more sophisticated grid-based games, using a toy problem. However, if your goal was more directly to learn a policy for a tic-tac-toe playing bot, then you would be better off not using any supervised learning model for the policy.</p>
","1","2","836","25573"
"17198","<p>If you want to identify and remove problematic data, then this is much better done <em>before</em> training.</p>

<p>You might have some luck with your approach, but you have no guarantees that the neural network will help you isolate the problem entries based on error values, if it has been trained on the ""good"" and ""bad"" entries at the same time. It depends on the nature of the problem data. You take a significant risk that the model will fit to the problem data well enough to cause you to reject good data.</p>

<p>You should instead try to think of a way to identify the bad data more directly, and prior to training for the predictive task.</p>

<p>Here's one idea: Are you able to identify enough good and bad data manually - enough to train and test a classifier? Then label some of the data ""good"" or ""bad"", train a classifier and test it - the test should be on some held out labeled values, to help you assess accuracy. If your classifier has a good accuracy, you can have some confidence to use it to filter the remainder of your data, and only use the ""good"" classified data for your original training goal.</p>
","1","2","836","25573"
"17249","<p>In short, there is nothing special about number of dimensions for convolution. Any dimensionality of convolution could be considered, if it fit a problem. </p>

<p>The number of dimensions is a property of the problem being solved. For example, 1D for audio signals, 2D for images, 3D for movies . . .</p>

<p>Ignoring number of dimensions briefly, the following can be considered <em>strengths</em> of a convolutional neural network (CNN), compared to fully-connected models, when dealing with certain types of data:</p>

<ol>
<li><p>The use of shared weights for each location that the convolution processes significantly reduces the number of parameters that need to be learned, compared to the same data processed through fully-connected network.</p></li>
<li><p>Shared weights is a form of regularisation.</p></li>
<li><p>The structure of a convolutional model makes strong assumptions about local relationships in the data, which when true make it a good fit to the problem.</p>

<p>3.1 Local patterns provide good predictive data (and/or can be usefully combined into more complex predictive patterns in higher layers)</p>

<p>3.2 The types of pattern found in the data can be found in multiple places. Finding the same pattern in a different set of data points is meaningful.</p></li>
</ol>

<p>These properties of CNNs are independent of the number of dimensions. One-dimensional CNNs work with patterns in one dimension, and tend to be useful in signal analysis over fixed length signals. They work well for analysis of audio signals, for instance. Also for some natural language processing - although recurrent neural networks, which allow for different sequence lengths, may be a better fit there, especially ones with memory gate arrangements such as LSTM or GRU. Still a CNN can be easier to manage, and you could simply pad the input to be fixed length.</p>
","16","2","836","25573"
"17290","<p>By transforming both your signal and kernel tensors into frequency space, <a href=""https://en.wikipedia.org/wiki/Convolution_theorem"" rel=""noreferrer"">a convolution becomes a single element-wise multiplication</a>, with no shifting or repeating.</p>

<p>So you can convert your data and kernel into frequencies using FFT, multiply them <em>once</em> then convert back with an inverse FFT. There are some fiddly details about aligning your data first, and correcting for gain caused by the conversion.</p>

<p>If you have a good FFT library, this can be very efficient, but there is overhead cost for running the Fourier transform and its inverse, so your convolution needs to be relatively large before it is worth looking at FFT.</p>

<p>I have explored this a while ago in a <a href=""https://github.com/neilslater/convolver"" rel=""noreferrer"">Ruby gem called convolver.</a> You can see some of the code for <a href=""https://github.com/neilslater/convolver/blob/master/lib/convolver.rb#L44-L59"" rel=""noreferrer"">an FFT-based convolution here</a> and the project includes unit tests that prove that direct convolution gets same numerical results as FFT-based convolution. There is also code that attempts to estimate when it would be more efficient to calculate convolutions directly by repeated multiplications or use FFT-based solution (that is rough and ready guesswork though, and implementation-dependent). </p>
","6","2","836","25573"
"17365","<blockquote>
  <p>My logic is that because these noise variables do NOT give maximum gain split at all, then they would never be selected thus they do not influence the tree growth.</p>
</blockquote>

<p>This is only perfectly correct for very large, near infinite data sets, where the number of samples in your training set gives good coverage of all variations. In practice, with enough dimensions you end up with a lot of sampling noise, because your coverage of possible examples is weaker the more dimensions your data has.</p>

<p>Noise on weak variables that ends up correlating <em>by chance</em> with the target variable can limit the effectiveness of boosting algorithms, and this can more easily happen on deeper splits in the decision tree, where the data being assessed has already been grouped into a small subset.</p>

<p>The more variables you add, the more likely it is that you will get weakly correlated variables that just happen to look good to the split selection algorithm for some specific combination, which then creates trees that learn this noise instead of the intended signal, and ultimately generalise badly.</p>

<p>In practise, I have found XGBoost quite robust to noise on a small scale. However, I have also found that it will sometimes select poor quality engineered variables, in preference to better-correlated data, for similar reasons. So it is not an algorithm where ""the more variables the better for XGBoost"" and you do need to care about possible low-quality features.</p>
","12","2","836","25573"
"17425","<p>In terms of generating an image ""layer"", that is just the same as generating an output image that can be overlaid on the input using standard graphics software. If you want pixel-level accuracy in the output then the output will need to be the same size as the input, otherwise it could be smaller, provided it is the same aspect ratio, and in which case it would need to be scaled up in order to be used as an overlay. In any case, as the output of a GAN can be an image (and often is), then this part is easy.</p>

<p>The ""G"" in GAN stands for Generative. The purpose of a generative network is to create samples from a population, where there are typically many possibilities. Those samples can be conditioned on some additional data, and that additional data <em>could</em> be an image, although many examples will simpler conditioning, such as a category that the training output is representative of.</p>

<p>One possibility for using a GAN, is where your population contains a range of traits, and you can calculate vectors that control that trait. So you can take an input image, reconstruct it in the GAN then modify it by adding/subtracting the trait-related vector. An interesting example of this is <a href=""https://arxiv.org/pdf/1702.01983.pdf"" rel=""nofollow noreferrer"">Face Aging With Conditional Generative Adversarial Networks</a>, and similar examples are around of adding/removing glasses etc. For this to work for you, you would literally need images that had your points-of interest in them and ones without them, and then you would be able to control addition/removal of points-of-interest. The network would not detect these points in the input, instead it would <em>add</em> them into the output. From reading your question this does not seem to be what you want.</p>

<p>A <a href=""https://arxiv.org/abs/1701.05957"" rel=""nofollow noreferrer"">similar paper uses a GAN to remove rain from photos, based on training many images with and without rain</a> then learning the ""rain vector"", encoding new images with rain in them into the GAN's internal representation and subtracting this ""rain vector"".</p>

<p>GANs conditioned on input images (as opposed to categories or internal embeddings) are also possible - this <a href=""http://cs231n.stanford.edu/reports2016/209_Report.pdf"" rel=""nofollow noreferrer"">example of image completion</a> might be closer to your goal. If your points of interest are variable with many options feasible, then it could work for you.</p>

<p>However, if your points of interest are supposed to always be the same pixels in each image, then your goal might be better defined by strict ground truth and become more like semantic segmentation, which can be attempted with variations on CNNs, such as <a href=""http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Dai_Convolutional_Feature_Masking_2015_CVPR_paper.pdf"" rel=""nofollow noreferrer"">described in this paper by Microsoft</a>. These are much easier to set up and train than GANs, so if you can reasonably frame your problem as pixel classification from the original image, this is probably the way to go.</p>
","3","2","836","25573"
"17441","<p>It is not really possible to alter input feature array size per example on normal CNNs. Instead this is fixed when the model is built for the first time, before you start training.</p>

<p>Depending on your goal, it might be possible to work around that using some kind of pipeline that worked with image patches (taking multiple slightly randomised patches to augment the training data can improve results and doing the same with prediction inputs can also drive up classifier accuracy). Or <a href=""https://arxiv.org/abs/1406.6247"" rel=""nofollow noreferrer"">a more complex variant using RNN/CNN hybrid</a> to consume an image as a sequence of parts, which might also be used for multi-object recognition.</p>

<p>However, these solutions are complex, and state-of-the-art results in image classification can be achieved by simpler techniques such as taking a centre crop and/or padding. Provided your training data is also treated the same way, and aspect ratios are not extreme this can work adequately.</p>
","2","2","836","25573"
"17446","<p>Yes you can use deep learning techniques to process non-image data. However, other model classes are still very competitive with neural networks outside of signal-processing and related tasks.</p>

<p>To use deep learning approaches on non-signal/non-sequence data, typically you use a simple feed-forward multi-layer network. No need for convolutional layers or pooling layers. The best architecture other than that needs to be explored with cross-validation, and can be time-consuming to discover as deep NNs take a lot of computation to train.</p>

<p>In my experience attempting to use deep(-ish, typically ~ 5 layers) neural networks in Kaggle competitions:</p>

<ul>
<li><p>Dropout is still highly effective for regularisation and improving accuracy</p></li>
<li><p>Input normalisation - usually to mean 0, standard deviaton 1, is important</p></li>
<li><p>Hidden layer activation functions can make a difference. Although ReLU reduces some problems with vanishing gradients, in my experience it is less robust with non-signal data and you will want some other form. If you have only a few layers, then sigmoid or tanh still work OK. Otherwise, look into leaky ReLU, PReLU, ELU and other ReLU variants that attempt to patch its problems with ""dead"" neurons.</p></li>
<li><p>Make use of optimisers designed for deep learning, such as Adam, Adagrad or RMSProp</p></li>
<li><p>Use a weight initialisation approach that works with deep learning, such as Glorot.</p></li>
<li><p>Consider using Batch Normalisation layers. Not something I have much experience with, but I have seen other people do well with this approach.</p></li>
</ul>

<p>Despite all this, XGBoost can routinely and easily beat deep NNs with minimal tuning and training effort in comparison (depending of course on the problem and the data you have). If accuracy is everything to you though, it is possible - although not guaranteed - that an ensemble of deep NNs and other model such as XGBoost will perform better than either singly.</p>
","11","2","836","25573"
"17517","<p>Sklearn's <code>LabelEncoder</code> module finds all classes and assigns each a numeric id starting from 0. This means that whatever your class representations are in the original data set, you now have a simple consistent way to represent each. It doesn't do one-hot encoding, although as you correctly identify, it is pretty close, and you can use those ids to quickly generate one-hot-encodings in other code.</p>

<p>If you want one-hot encoding, you can use <code>LabelBinarizer</code> instead. This works very similarly:</p>



<pre><code> from sklearn.preprocessing import LabelBinarizer
 encoder = LabelBinarizer()
 transfomed_label = encoder.fit_transform([""dog"", ""cat"", ""bird""])
 print(transfomed_label)
</code></pre>

<p>Output:</p>

<pre><code>[[0 0 1]
 [0 1 0]
 [1 0 0]]
</code></pre>
","19","2","836","25573"
"17563","<p>There are a few different factors involved here. It is difficult to tell, without getting heavily involved, which could be the most important. I will put them in the order I think worth looking at first.</p>

<ul>
<li><p>Your data is images from CCTV, so you likely have more than one image from each camera. From your results (reasonable training and CV scores, but bad training), it looks like you are over-fitting. But your CV approach is not spotting this. So I think that the test set is likely to be from a different set of cameras to the training set. In order to properly measure CV therefore, you have to split train/cv <em>by camera</em> - you cannot just use the 0.1 split, because a random split will include images which are correlated with training data and will give you too high an estimate, allowing you to overfit without noticing.</p>

<ul>
<li>It occurs to me it <em>might</em> just be your data augmentation causing a problem for you here. If you augment <em>first</em> then randomly split to train/CV, then your CV set will contain images very similar to training set, and will get too high a score. You can more easily check and fix this than split by camera, so give it a try.</li>
</ul></li>
<li><p>800 original images is not much to work with. You need to do something about that. Here are some ideas: </p>

<ul>
<li><p>Scale down the images. You probably don't need 340x340. Depending on the target object, maybe just 78x78 will do. You can assess this easily enough - scale down and check if you can still differentiate the classes easily by eye.</p></li>
<li><p>You don't have enough data to get best quality filters in the lower layers, which will limit the capabilities of the CNN. You might bootstrap from a pre-trained image model. Take a publicly available pre-trained CNN, such as VGG-19, use its weights in convolutional layers as a starting point, put different classifier layers on top and fine-tune your classifier starting from this. This might change the ideal image scale too - you want something that fits the pre-trained CNN.</p></li>
<li><p>Augmenting the data, which you have started. You could go a lot further. Take random patches from training examples, possibly flipped horizontally (if this maintains the object class). However, <em>don't</em> augment data used for cross validation, unless your model used for testing also includes augmentation - e.g. if it takes 8 random augmented variants of the test image and returns an average of predictions, then you can do similar for cross-validation.</p></li>
</ul></li>
<li><p>A 0.1 train/CV split run once on this much data is not going to give you an accurate assessment of the model. You need to run k-fold cross-validation. This is annoying because NNs take a long time to train, but if you want some confidence that you have really found some good parameters, you will need to do this. Remember to split by camera if you can.</p></li>
<li><p>The scoring appears to be strongly related to categorical cross-entropy, so you have the right loss function. You should optimise, using cross-validation, to find the model with the lowest loss, not the best accuracy.</p></li>
</ul>
","2","2","836","25573"
"17600","<p>The X (and sometimes Y) variables are matrices.</p>

<p>In some math notation, it is common practice to write vector variable names as lower case and matrix variable names as upper case. Often these are in bold or have other annotation, but that does not translate well to code. Either way, I believe that the practice has transferred from this notation. </p>

<p>You may also notice in code, when the target variable is a single column of values, it is written <code>y</code>, so you have <code>X, y</code></p>

<p>Of course, this has no special semantic meaning in Python and you are free to ignore the convention. However, because it has become a convention, it may be worth maintaining if you share your code.</p>
","21","2","836","25573"
"17658","<p>This sounds like a classic use for a <a href=""http://hunch.net/?p=298"" rel=""nofollow noreferrer"">contextual bandit</a> solver.</p>

<p>In essence you can run a simple online model (pretty much any regression model, or even a simple classifier like logistic regression if your reward signal is binary success/fail such as in your case) that learns to associate your demographic data with <em>expected reward</em> from each possible action - for you the reward can simply be 1 for a share link created or 0 for no share link.</p>

<p>Whilst the model is learning, you select the next action according to predicted reward from the model. There are choices between different workable strategies. For instance you could use an $\epsilon$-greedy approach: Pick the action with maximum predicted expected reward (or randomly choose between shared maximum values), but sometimes - with probability $\epsilon$ - you choose random content. There are other approaches and options that you can discover by researching contextual bandits and the simpler multi-armed bandit problems. </p>

<p>As an example, you could use a <a href=""https://en.wikipedia.org/wiki/Logistic_regression"" rel=""nofollow noreferrer"">logistic regression</a> model to predict expected reward from user demographics, with one such model per possible action. For a version that picks evenly to start, but prefers items that have been shared more over time, you can use a <a href=""https://en.wikipedia.org/wiki/Boltzmann_distribution"" rel=""nofollow noreferrer"">Boltzmann distribution (also called Gibbs distribution)</a> using the predicted rewards as the inverse ""energies"" for the actions, and lowering the temperature as you collect more data. You can also initialise the weights of your model to predict a small but optimistic positive reward to start with to encourage early exploration. Whenever a user views your page, you pick the action to take based on the predicted rewards, and afterwards take the user response (share or not share) as feedback to update the one model associated with that action.</p>

<p>In the above example, the logistic regression learning rate, temperature scheme and starting reward are hyper-parameters of your model, and you use them to trade off responsiveness to individual events versus long-term accuracy for selecting the best action.</p>
","2","2","836","25573"
"17667","<p>With 29 states and 841 possible transitions to track whilst reading a file with 2000 entries (word, tag), then you should not be experiencing a speed problem when using a dictionary of dictionaries.</p>

<p>Assuming your data structure as described called <code>transition_counts</code>, and receiving data in pairs, <code>(this_pos, next_pos)</code> then running 2000 times:</p>

<pre><code>transition_counts[this_pos][next_pos] += 1
</code></pre>

<p>takes only a fraction of a second. This is similar for code that calculates $p(POS_{t+1}|POS_t)$:</p>

<pre><code>total_from_pos_t =  sum(transition_counts[pos_t].values())
prob_pos_tplus_one = transition_counts[pos_t][pos_tplus_one] / total_from_pos_t  
</code></pre>

<p>This is very fast. Your problem is not with the representation.</p>
","0","2","836","25573"
"17685","<p>To make this question answerable, I have to assume that you have implemented back propagation correctly. That is, in terms of the underlying maths, the steps are in the right order, there is not any non-necessary repetition, and the values are calculated using correct formulae. </p>

<p>You can check whether the gradient values are correct by implementing <a href=""http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization"" rel=""nofollow noreferrer"">gradient checking for weights</a>, which in general you do by taking a random test case (any network architecture, random weights, inputs and target values), run your back propagation code to get your gradients, then for each weight in turn run the network forward, measure error, then adjust the weight by a very small delta, measure error again and use the difference in error to get an estimate for the gradient. If the estimate and your back propagation agree - approximately - for all weights, then your back propagation code is likely correct. If you are going to implement just one unit test for a NN implementation, this is the one to do.</p>

<p>To address the question directly: ""Is there a more efficient way to approach backpropagation?"", the answer is complicated.</p>

<p><em>The basic answer is ""no""</em>. You absolutely have to do all the calculations involved in back propagation, there are no shortcuts.</p>

<p>However, most neural network frameworks will perform much better than your implementation, where you give an example of a network that has layers <code>[39,39,39,1]</code> training on 10,000 examples and taking 28 minutes to process a single epoch.</p>

<p>As you don't link your implementation, it is not possible to figure out precisely what the difference is. However, ignoring the possibility of bugs, the following things will probably apply:</p>

<ul>
<li><p>Avoid calculating results using loops in an interpreted language. If you are looping through neuron or weight indices to multiple some values together and store back in an array in a high-level language, then this can be very inefficient. Most frameworks take advantage of fast loops and parallelisation that is possible if you frame the neural network processes (both forward and back propagation) in terms of matrix and vector manipulations. This is sometimes called <a href=""https://en.wikipedia.org/wiki/Array_programming"" rel=""nofollow noreferrer"">vectorizaton</a>, and most frameworks split this up so that the vector and matrix operations are black box routines implemented with strong attention to optimisation in a low-level language (like C), whilst passing the results around and deciding what to do with them can be done in higher-level language like Python or Matlab.</p></li>
<li><p>Mini-batches. You have to process forward and back propagation for each example seen by the network. But you don't have to apply weight updates for each example. It turns out that applying an update only once every so many examples is not only more efficient in terms of calculations, but also has benefits for convergence. Frameworks that support large amounts of parallel processing via GPU can also improve performance greatly from working in batches.</p></li>
</ul>

<p>It is unlikely you will find the time to explore optimisation to the extent that the better resourced public frameworks already have. However, you can get quite far by taking advantage of vectorisation options built into your language - for instance using matrix operations in Matlab, or Numpy features in Python. This might be worthwhile so you can continue to work on and understand low-level detail in neural network models. However, at some point, if you want to work on practical problems instead of your own NN library, you will find yourself naturally switching to a third-party library because the optimisation problems and many useful additions have been solved for you already.</p>
","1","2","836","25573"
"17840","<p>The ground truth is what you measured for your target variable for the training and testing examples.</p>

<p>Nearly all the time you can safely treat this the same as the label.</p>

<p>In some cases it is not precisely the same as the label. For instance if you augment your data set, there is a subtle difference between the ground truth (your actual measurements) and how the augmented examples relate to the labels you have assigned. However, this distinction is not usually a problem.</p>

<p>Ground truth can be wrong. It is a measurement, and there can be errors in it. In some ML scenarios it can also be a subjective measurement where it is difficult define an underlying objective truth - e.g.  expert opinion or analysis, which you are hoping to automate. Any ML model you train will be limited by the quality of the ground truth used to train and test it, and that is part of the explanation on the Wikipedia quote. It is also why published articles about ML should include full descriptions of how the data was collected.</p>
","37","2","836","25573"
"17860","<p>I am studying reinforcement learning and I am working methodically through <a href=""http://incompleteideas.net/book/the-book-2nd.html"" rel=""nofollow noreferrer"">Sutton and Barto's book</a> plus <a href=""https://www.youtube.com/playlist?list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT"" rel=""nofollow noreferrer"">David Silver's lectures</a>.</p>

<p>I have noticed a minor difference in how the Markov Decision Processes (MDPs) are defined in those two sources, that affects the formulation of the Bellman equations, and I wonder about the reasoning behind the differences and when I might choose one or the other.</p>

<p>In Sutton and Barto, the expected reward function is written $R^a_{ss'}$, whilst in David Silver's lectures it is written $R^a_{s}$. In turn this leads to slightly different formulations of all the Bellman equations. For instance, in Sutton and Barto, the equation for policy evaluation is given by:</p>

<p>\begin{align}
v_{\pi}(s) = \sum_a \pi(a|s) \sum_{s'} P_{ss'}^a(R_{ss'}^a + \gamma v_{\pi}(s'))
\end{align}</p>

<p>Whilst David Silver's lectures show:</p>

<p>\begin{align}
v_{\pi}(s) = \sum_a \pi(a|s) \left(R_{s}^a + \gamma \sum_{s'} P_{ss'}^a v_{\pi}(s') \right)
\end{align}</p>

<p>In both cases: </p>

<ul>
<li>$\pi(a|s)$ is policy function - probability of choosing action $a$ given state $s$. </li>
<li>$\gamma$ is discount factor.</li>
<li>$P_{ss'}^a$ is transition function, probability of state changing to $s'$ given $s, a$</li>
</ul>

<p>I understand that $R_{ss'}^a$ and $R_{s}^a$ are related (via $P_{ss'}^a$), so that these two sources are explaining the exact same thing. Note that the first equation can also be written as </p>

<p>\begin{align}
v_{\pi}(s) 
&amp;= \sum_a \pi(a|s) \sum_{s'} (P_{ss'}^aR_{ss'}^a + \gamma P_{ss'}^av_{\pi}(s'))\\
&amp;= \sum_a \pi(a|s) \left( \sum_{s'} P_{ss'}^aR_{ss'}^a + \sum_{s'} \gamma P_{ss'}^av_{\pi}(s') \right) \\
&amp;= \sum_a \pi(a|s) \left( \sum_{s'} P_{ss'}^aR_{ss'}^a + \gamma \sum_{s'}  P_{ss'}^av_{\pi}(s') \right)
\end{align}</p>

<p>Hence, it must be true that $R_{s}^a = \sum_{s'} P_{ss'}^a R_{ss'}^a$.</p>

<p>My question is whether there is any reason I should prefer to use one or the other notation? </p>

<p>I started with Sutton and Barto, and find that notation more intuitive - the reward may depend on the eventual state, and this is explicit in the equations. However, it looks like in practice that the notation used in the video lectures describes more efficient calculations (essentially $R_{s}^a = \sum_{s'} P_{ss'}^a R_{ss'}^a$ is cached, if the formula is translated <em>directly</em> to code). Is that all there is to it?</p>
","6","1","836","25573"
"17922","<p>For a fully-connected network the precise <strong>order of features does not matter initially</strong> (i.e. before you start to train), as long as it is consistent for each example. This is independent of whether you have an auto-encoder to train or some other fully-connected network. Processing images with pixels as features does not change this.</p>

<p>Some caveats:</p>

<ul>
<li><p>To succeed in training, you will <strong>need the pixel order to be the same for each example</strong>. So it can be randomly shuffled, but only if you keep the same shuffle for each and every example. </p>

<ul>
<li>As an aside, you will still get <em>some</em> training effect from fully random shuffling the variables, because for example writing an ""8"" has more filled pixels than writing a ""1"" on average. But the performance will be very bad, accuracy only a little better than guessing, for most interesting problem domains.</li>
</ul></li>
<li><p>To visualise what the auto-encoder has learned, your output needs to be unscrambled. You can actually input a (same shuffle each example) scrambled image and train the autoencoder to unscramble it - this will in theory get the same accuracy as training to match the scrambled input, showing again that pixel order is not important. You could also train autoencoder to match scrambled input to scrambled output and visualise it by reversing the scrambling effect (again this must be a consistent scramble, same for each example).</p></li>
</ul>

<p>In a fully-connected neural network, there is nothing in the model that represents the local differences between pixels, or even that they are somehow related. So the network will learn relations (such as edges) irrespective of how the image is presented. But it will also suffer from being unable to generalise. E.g. just because an edge between pixels 3 and 4 is important, the network will not learn that the same edge between pixels 31 and 32 is similar, unless lots of examples of both occur in the training data.</p>

<p>Addressing poor generalisation due to loss of knowledge about locality in the model is one of the motivations for convolutional neural networks (CNNs). You can have <a href=""https://blog.keras.io/building-autoencoders-in-keras.html"" rel=""nofollow noreferrer"">CNN autoencoders</a>, and for those, you intentionally preserve the 2D structure and local relationships between pixels - if you did not then the network would function very poorly or not at all.</p>
","2","2","836","25573"
"18065","<p>No it is not possible to preserve relative distances when reducing dimensions for arbitrary data. This is not due to a property of auto-encoders compared to e.g. PCA or T-SNE. It is due to geometry.</p>

<p>You can see this relatively easily by considering a reduction of dimensions from 3 to 2, and examining a tetrahedron where all four corner points are 1 unit apart. There is no two-dimensional shape that can place four points mutually equidistant (unless perhaps you consider non-Euclidean spaces). It should be clear that relative distances near those corner vertices would also be affected, and thus this special shape demonstrates a general property of dimension reduction.</p>
","2","2","836","25573"
"18097","<p>You are looking at the Keras code implementing dropout for training step. </p>

<p>In the Keras implementation, the output values are corrected during training (by dividing, in addition to randomly dropping out the values) instead of during testing (by multiplying). This is called ""inverted dropout"".</p>

<p>Inverted dropout is functionally equivalent to original dropout (as per your link to Srivastava's paper), with a nice feature that the network does not use dropout layers at all during test and prediction. This is explained a little in this <a href=""https://github.com/fchollet/keras/issues/3305"" rel=""nofollow noreferrer"">Keras issue</a>.</p>
","3","2","836","25573"
"18104","<p>This is unlikely to add much beyond your direct data collection efforts. </p>

<p>The quality of current GAN outputs (as of 2017) will not be high enough. The images produced by a GAN are typically small and can have unusual/ambiguous details and odd distortions. In the paper you linked, the images generated by the system from a sentence have believable blocks of colour given the subject matter, but without the sentence priming you what to expect most of them are not recognisable as <em>any</em> specific subject.</p>

<p>GANs with a less ambitious purpose than generating images from sentences (which is despite my criticism above, a truly remarkable feat IMO) should produce closer to photo-realistic images. But their scope will be less and probably not include your desired image type. Also, typically the output size is small e.g. 64x64 or 128x128*, and there are still enough distortions and ambiguities that original ground truth photos would be far preferable.</p>

<p>The GAN is itself limited by training library available - it will not do well if you attempt to generate images outside of the scope of its training data. The results shown in the research paper of course focus on the domain supplied by the training data. But you cannot just feed <em>any</em> sentence into this model and expect a result that would be useful elsewhere.</p>

<p>If you find a GAN that has been trained on a suitable data set for your problem, then you are most likely better off trying to source the same data directly for your project.</p>

<p>If you are facing a problem with limited ground truth data, then maybe a better approach to using a GAN would be to use a pre-trained classifier such as VGG-19 or Inception v5, replace the last few fully-connected layers, and fine tune it on your data. <a href=""https://gist.github.com/fchollet/7eb39b44eb9e16e59632d25fb3119975"" rel=""noreferrer"">Here is an example of doing that using Keras library in Python</a> - other examples can be found with searches like ""fine tune CNN image classifier"".</p>

<hr>

<p>* State-of-the art GANs have got better since I posted this answer. <a href=""http://research.nvidia.com/publication/2017-10_Progressive-Growing-of"" rel=""noreferrer"">A research team at Nvidia has had remarkable success creating 1024x1024 photo-realistic images</a>. However, this does not change the other points in my answer. GANs are not a reliable source of images for image classification tasks, except maybe for sub-tasks of whatever the GAN has already been trained on and is able to generate conditionally (or maybe more trivially, to provide source data for ""other"" categories in classifiers).</p>
","10","2","836","25573"
"18148","<p>The usual processing for your suggested layers:</p>

<pre><code>model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
</code></pre>

<p>would be (reading left to right)</p>

<pre><code>dense output -&gt; relu -&gt; apply dropout mask -&gt; apply ""inverse dropout"" divide by p  
</code></pre>

<p>The <em>precise</em> combination may vary depending upon optimisations, and can in theory be changed a little without affecting the result (it doesn't matter to the end result numerically if we scale then mask or mask then scale for instance). However when dealing with vectorised optimisations (like those found in TensorFlow and Theano), it is normal to accept a percentage of ""wasted"" processing, and just have that naive left-to-right processing happen. It is often harder to parallelise decision branches than to simply process all items, even repeated multiplying by and adding zeroes for a significant fraction of each array.</p>

<p>There is no ""normalization"" implied by activation functions, so this is not a concern. From your comments, it seems you are worried that dividing by <code>p</code> could mean that the output of a neuron that would be between 0 and 1 (because you were using sigmoid activation for example) would now be between 0 and 1/p - i.e. larger. That is true. Is this a problem? No it is not, and in fact it is required. The impact of the larger values is fully compensated for by the weights learned in the connections between layers. If you used ""vanilla"" dropout then the weights would be correspondingly larger, but you would need to scale the outputs down (new range would be 0 to p) during testing/prediction.</p>
","3","2","836","25573"
"18153","<p>A neural network can approximate any continuous function, provided it has at least one hidden layer and uses non-linear activations there. This has been proven by the <a href=""https://en.wikipedia.org/wiki/Universal_approximation_theorem"" rel=""noreferrer"">universal approximation theorem</a>.</p>

<p>So, there are no exceptions for specific functions. You ask:</p>

<blockquote>
  <p>I read somewhere on the StackExchange that a neural network can't approximate the Pi number as a function of circles length and radii.</p>
</blockquote>

<p>A neural network to approximate $\pi$ is very easy. Possibly what you read is that a neural network cannot generate new digits of $\pi$ that it has not already been shown. More on that later . . .</p>

<blockquote>
  <p>What about the sum or multiplication of any arbitrary numbers? </p>
</blockquote>

<p>Yes, a neural network can approximate that.</p>

<blockquote>
  <p>Are there any other specific functions neural networks can't approximate or not?</p>
</blockquote>

<p>No, there are no specific functions that a neural network cannot approximate.</p>

<p>However, there are some important caveats:</p>

<ul>
<li><p>Neural networks do not encode the actual functions, only numeric approximations. This means there are practical limits on the ranges of inputs for which you can achieve a good approximation.</p></li>
<li><p>A neural network being able to approximate a function in theory is not the same thing as you or I being able to construct a neural network that approximates that function. There is no known method to construct a neural network by analysis of a function alone (it can be done for specific simple functions such as xor).</p></li>
<li><p>The usual way to achieve approximation is to <em>train</em> a neural network by giving example data. The network will approximate to data it has been shown. There is no guarantee that this will generalise to new inputs that it has not been trained on and approximate the correct outputs. In fact for certain types of input/output it cannot possibly do so. For instance, it will not learn how to generate the 4th digit of $pi$ if it has been shown digits 1,2,3,5,6,7,8,9. The best generalisation results occur for functions that have smooth transitions between the training examples.</p></li>
<li><p>Neural networks do not extrapolate well to inputs outside of the data they have used for training. They ""fit"" to the training data (imagine a rubber sheet draped over all the points in the training set).</p></li>
<li><p>Neural networks do not learn to copy algorithms, only functions. So if you take a complex algorithm, such as AES encryption, and attempt to train a neural network to perform this given lots of input examples, has no real chance of working. Now, AES encryption can be considered a function e.g. $output = encrypt( input, key )$. So the NN <em>can</em> approximate it. But it will only do so for the specific inputs and outputs it has been shown. In addition AES does not respond well to approximation - a single bit wrong will cause it to be a bad encryption. So you won't see NNs used to encrypt or decrypt in cryptography.</p></li>
<li><p>The capability of a neural network to approximate is limited by the number of neurons and connections it has. More complex functions require larger networks. In order to train a larger network on a more complex function takes more time and more training data. You could in theory train a neural network to learn a random number generator function. However, that would take an impossible amount of resources - memory to store the network, and time to train it against the whole output of the RNG.</p></li>
</ul>
","14","2","836","25573"
"18173","<blockquote>
  <p>1) Is this even an appropriate algorithm to apply to this sort of task?</p>
</blockquote>

<p>No, you have selected an evaluation algorithm from chapter 9 of the book. None of the algorithms in chapter 9 are control algorithms. They are all designed to estimate the value function for a policy supplied as input. The corresponding control algorithms are discussed in chapter 10.</p>

<p>The current draft of the book does not give the corresponding TD(0) control algorithm with linear estimator. However, that algorithm does exist and might be suitable (with caveats). In fact in your case it could even have benefits over action-value based methods, because you reduce the scope of estimates needed by a factor of 3. This is something that you can take advantage of only if you have a full model of the environment, so can look ahead one time step to determine the best action. Without a model of the environment, or if you don't want to use the model in your agent, then you must use an action value based algorithm like Monte Carlo, SARSA or Q Learning.</p>

<blockquote>
  <p>2) If yes to 1), how do I choose an action?</p>
</blockquote>

<p>Well it was a no, but you could use the control version of TD(0). Then you have the problem of using your state value function to figure out the policy. The rule here is that to use state values you need to use a model of the environment. Specifically you need to be able to predict the next state and immediate reward given the current state and action. In the book, this is usually represented by the transition function $p(s′,r|s,a)$ which gives the probability of each possible successor state and reward. In a fully deterministic game, the probability is just 1.0 for one target state and reward caused by each action. In your case you have new rocks appearing randomly at the top. To be complete you would probably have to model this in detail (which would be painfully slow). However, given the very low influence of this top row, and how little planning the agent can do to deal with it, I'd be tempted to just sample it.</p>

<p>Assuming you want to choose the greedy action, then you can find a policy by taking $argmax_a$ over the next step. When you have a state value function, then you have to run the step forward in simulation to figure out the expectation over each action. This is the same calculation for greedy policy as used in dynamic programming (back in chapter 4 of the book):</p>

<p>$\pi(s) = argmax_a \sum_{s'} p(s',r|s,a) (r + \gamma \hat{v}(s',\theta))$</p>

<p>Of course this is a lot simpler if you used action values istead (e.g. in SARSA):</p>

<p>$\pi(s) = argmax_a \hat{q}(s,a,\theta)$</p>

<p>. . . so despite the fact this is less efficient, you might want to use action values for less effort in this part.</p>

<hr>

<p>One additional thing you are likely to have problems with: Your choice of state representation does not have good linear relationship with the true value function. The linear estimator is quite limited in what it can do. A better estimator here would be a neural network with at least one hidden layer. </p>

<p>However, you can probably tweak the representation slightly to get something that will work a little bit with a linear estimator - the trick is to have the rocks part of the state represented <em>relative to the agent</em> - i.e. don't have a grid of absolute positions of rocks, but make the grid relative to the agent. That way, a rock directly above the agent will always contribute the same to the current state value, which is important. Without this tweak, and using a linear approximator, your agent will not learn optimal control, it will instead learn a kind of fatalistic ""with these many rocks, I have roughly this long to live"" value function and probably just take random actions (if the distribution of rocks is not even it might learn to move to a particular column . . .)</p>
","5","2","836","25573"
"18210","<p>You are confusing ""<a href=""https://en.wikipedia.org/wiki/Dimension"" rel=""nofollow noreferrer"">dimensions</a>"" with ""<a href=""https://en.wikipedia.org/wiki/Tensor"" rel=""nofollow noreferrer"">order of tensor</a>"". A softmax with 256 different categories is a 256 dimensional vector, but is also a tensor with order 1 (whilst a matrix is a tensor of order 2). The paper is using the technical terms correctly, so the 256 dimensional vector is just a normal vector with 256 scalar entries.</p>

<p>Therefore a 256-dimensional softmax in TensorFlow is typically an output layer that looks something like this:</p>

<pre><code>y = tf.nn.softmax(tf.matmul(h, W) + b)
</code></pre>

<p>where <code>h</code> is the last hidden layer, <code>W</code> is the weight matrix n x 256, and <code>b</code> is the bias 1 x 256 vector.</p>

<blockquote>
  <p>In the paper, the candidate generation neural network model outputs a softmax with 256 dimensions, which acts as an ""output embedding"" of each of the 1M video classes</p>
</blockquote>

<p>That is a description of the training process that compresses 1M different inputs to 256-dimensional output for use as an embedding for recommendation matches. The softmax is at the output, and as far as I can see is just a normal softmax classifier output as seen in many other classifier networks (except the result is not technically being used to classify anything). I am not clear on what supervision data was used or on what the input representation was. However, I don't think it likely that 1M ""classes"" ever appear as e.g. 1-hot encoding, because that would not scale out usefully to the many other millions of videos - the point of the embedding is to turn disparate features of the videos into something that be used as a similarity measure, that can be run on any video stored in YouTube.</p>
","0","2","836","25573"
"18279","<p>Image shows a typical layer somewhere in a feed forward network:</p>

<p><a href=""https://i.stack.imgur.com/fwC9R.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fwC9R.png"" alt=""enter image description here""></a></p>

<p>$a_i^{(k)}$ is the activation value of the $i^{th}$ neuron in the $k^{th}$ layer. </p>

<p>$W_{ij}^{(k)}$ is the weight connecting $i^{th}$ neuron in the $k^{th}$ layer to the $j^{th}$ neuron in the $(k+1)^{th}$ layer.</p>

<p>$z_j^{(k+1)}$ is the pre-activation function value for the $j^{th}$ neuron in the $(k+1)^{th}$ layer. Sometimes this is called the ""logit"", when used with logistic functions.</p>

<p>The feed forward equations are as follows:</p>

<p>$z_j^{(k+1)} = \sum_i W_{ij}^{(k)}a_i^{(k)}$</p>

<p>$a_j^{(k+1)} = f(z_j^{(k+1)})$</p>

<p>For simplicity, bias is included as a dummy activation of 1, and implied used in iterations over $i$.</p>

<p>I can derive the equations for back propagation on a feed-forward neural network, using chain rule and identifying individual scalar values in the network (in fact I often do this as a paper exercise just for practice):</p>

<p>Given $\nabla a_j^{(k+1)} = \frac{\partial E}{\partial a_j^{(k+1)}}$ as gradient of error function with respect to a neuron output.</p>

<h3>1. $\nabla z_j^{(k+1)} = \frac{\partial E}{\partial z_j^{(k+1)}} = \frac{\partial E}{\partial a_j^{(k+1)}} \frac{\partial a_j^{(k+1)}}{\partial z_j^{(k+1)}} = \nabla a_j^{(k+1)} f'(z_j^{(k+1)})$</h3>

<h3>2. $\nabla a_i^{(k)} = \frac{\partial E}{\partial a_i^{(k)}} = \sum_j \frac{\partial E}{\partial z_j^{(k+1)}} \frac{\partial z_j^{(k+1)}}{\partial a_i^{(k)}} = \sum_j \nabla z_j^{(k+1)} W_{ij}^{(k)}$</h3>

<h3>3. $\nabla W_{ij}^{(k)} = \frac{\partial E}{\partial W_{ij}^{(k)}} = \frac{\partial E}{\partial z_j^{(k+1)}} \frac{\partial z_j^{(k+1)}}{\partial W_{ij}^{(k)}} = \nabla z_j^{(k+1)} a_{i}^{(k)}$</h3>

<p>So far, so good. However, it is often better to recall these equations using matrices and vectors to represent the elements. I can do that, but I am not able to figure out the ""native"" representation of the equivalent logic in the middle of the derivations. I can figure out what the end forms should be by referring back to the scalar version and checking that the multiplications have correct dimensions, but I have no idea <em>why</em> I should put the equations in those forms. </p>

<p>Is there actually a way of expressing the tensor-based derivation of back propagation, using only vector and matrix operations, or is it a matter of ""fitting"" it to the above derivation?</p>

<p>Using column vectors $\mathbf{a}^{(k)}$, $\mathbf{z}^{(k+1)}$, $\mathbf{a}^{(k+1)}$ and weight matrix $\mathbf{W}^{(k)}$ plus bias vector $\mathbf{b}^{(k)}$, then the feed-forward operations are:</p>

<p>$\mathbf{z}^{(k+1)} = \mathbf{W}^{(k)}\mathbf{a}^{(k)} + \mathbf{b}^{(k)}$</p>

<p>$\mathbf{a}^{(k+1)} = f(\mathbf{z}^{(k+1)})$</p>

<p>Then my attempt at derivation looks like this:</p>

<h3>1. $\nabla \mathbf{z}^{(k+1)} = \frac{\partial E}{\partial \mathbf{z}^{(k+1)}} = ??? = \nabla \mathbf{a}^{(k+1)} \odot f'(\mathbf{z}^{(k+1)})$</h3>

<h3>2. $\nabla \mathbf{a}^{(k)} = \frac{\partial E}{\partial \mathbf{a}^{(k)}} = ??? = {\mathbf{W}^{(k)}}^{T} \nabla \mathbf{z}^{(k+1)}$</h3>

<h3>3. $\nabla \mathbf{W}^{(k)} = \frac{\partial E}{\partial \mathbf{W}^{(k)}} = ??? =  \nabla\mathbf{z}^{(k+1)} {\mathbf{a}^{(k)}}^T $</h3>

<p>Where $\odot$ represents element-wise multiplication. I've not bothered showing equation for bias.</p>

<p>Where I have put ??? I am not sure of the correct way to go from the feed-forward operations and knowledge of linear differential equations to establish the correct form of the equations? I could just write out some partial derivative terms, but have no clue as to why some should use element-wise multiplication, others matrix multiplication, and why multiplication order has to be as shown, other than clearly that gives the correct result in the end.</p>

<p>I am not even sure if there is a purely tensor derivation, or whether it is all just a ""vectorisation"" of the first set of equations. But my algebra is not that good, and I'm interested to find out for certain either way. I feel it might do me some good comprehending work in e.g. TensorFlow if I had a better native understanding of these operations by thinking more with tensor algebra.</p>

<hr>

<p>Sorry about ad-hoc/wrong notation. I understand now that $\nabla a_j^{(k+1)}$ is more properly written $\nabla_{a_j^{(k+1)}}E$ thanks to Ehsan's answer. What I really wanted there is a short reference variable to substitute into the equations, as opposed to the verbose partial derivatives.</p>
","8","1","836","25573"
"18290","<p>The slide explains a limitation which applies to any linear model. It would equally apply to linear regression for example. </p>

<blockquote>
  <p>What does he mean by hand generated features? </p>
</blockquote>

<p>This means any features generated by analysis of the problem. For instance if you wanted to categorise a building you might have its height and width. A hand generated feature could be deciding to multiply height by width to get floor area, because it looked like a good match to the problem.</p>

<blockquote>
  <p>I don't get the binary input example and why it is a table look-up type problem and why it won't generalize?</p>
</blockquote>

<p>A table look-up solution is just the logical extreme of this approach. If you have a really complex classification, and your raw features don't relate directly (as a linear multiple of the target), you can craft very specific manipulations of them that give just the right answer for each input example. Essentially this is the same as marking each example in your training data with the correct answer, which has the same structure, conceptually, as a table of <code>input: desired output</code> with one entry per example.</p>

<p>In fact this <em>might</em> generalize, but only exactly as well as the crafted features do. In practice, when you have a complex problem and sample data that only partially explains your target variable (i.e. in most data science scenarios), then generating derived features until you find some that explain the data is strongly related to <a href=""https://en.wikipedia.org/wiki/Overfitting"" rel=""nofollow noreferrer"">overfitting</a>.</p>

<p>From your comment:</p>

<blockquote>
  <p>In his video lecture, he says ""Suppose for example we have binary input vectors. And we create a separate feature unit that gets activated by exactly one of those binary input vectors. We'll need exponentially many feature units. But now we can make any possible discrimination on binary input vectors. So for binary input vectors, there's no limitation if you're willing to make enough feature units."" 1.What feature? 2.Why are we creating this feature? And why adding exponential such features we can discriminate these vectors?</p>
</blockquote>

<p>Here is an example of the scheme that Geoffrey Hinton describes. Say you have 4 binary features, associated with one target value and see the following data:</p>

<pre><code>data 0 1 1 0 -&gt; class 1
data 1 1 1 0 -&gt; class 2 
data 0 1 0 1 -&gt; class 1
data 1 1 1 0 -&gt; class 2
data 0 1 1 1 -&gt; class 2
data 0 1 0 0 -&gt; class 1
</code></pre>

<p>It is possible to get a perceptron to predict the correct output values by crafting features as follows:</p>

<pre><code>data 0 1 1 0 -&gt; features 1 0 0 0 0 -&gt; class 1
data 1 1 1 0 -&gt; features 0 1 0 0 0 -&gt; class 2
data 0 1 0 1 -&gt; features 0 0 1 0 0 -&gt; class 1
data 1 1 1 0 -&gt; features 0 1 0 0 0 -&gt; class 2
data 0 1 1 1 -&gt; features 0 0 0 1 0 -&gt; class 2
data 0 1 0 0 -&gt; features 0 0 0 0 1 -&gt; class 1
</code></pre>

<p>Each unique set of original data gets a new one-hot-encoded category assigned. It is clear that ultimately if you had $n$ original features, you would need $2^n$ such derived categories - which is an exponential relationship to $n$. </p>

<p>Working like this, there is no generalisation possible, because any pattern you had not turned into a derived feature and learned the correct value for would not have any effect on the perceptron, it would just be encoded as all zeroes. However, it would learn to fit the training data very well, it could just associate each unique vector with a weight equal to the training output - this is effectively a table lookup.</p>

<p>The whole point of this description is to show that hand-crafted features to ""fix"" perceptrons are not a good strategy. Even though they can be made to work for training data, ultimately you would be fooling yourself.</p>
","3","2","836","25573"
"18347","<p>The test set and cross validation set have different purposes. If you drop either one, you lose its benefits:</p>

<ul>
<li><p>The cross validation set is used to help detect over-fitting and to assist in hyper-parameter search.</p></li>
<li><p>The test set is used to measure the performance of the model.</p></li>
</ul>

<p>You cannot use the cross validation set to measure performance of your model accurately, because you will deliberately <em>tune</em> your results to get the best possible metric, over maybe hundreds of variations of your parameters. The cross validation result is therefore likely to be too optimistic. </p>

<p>For the same reason, you cannot drop the cross validation set and use the test set for selecting hyper parameters, because then you are pretty much guaranteed to be overestimating how good your model is. In the ideal world you use the test set just once, or use it in a ""neutral"" fashion to compare different experiments. </p>

<p>If you cross validate, find the best model, then add in the test data to train, it is <em>possible</em> (and in some situations perhaps quite likely) your model will be improved. However, you have no way to be sure whether that has actually happened, and even if it has, you do not have any unbiased estimate of what the new performance is.</p>

<p>From witnessing many Kaggle competitions, my experience is that tuning to the test set by over-using it is a real thing, and it impacts those competitions in a large way. There is often a group of competitors who have climbed the public leaderboard and selected their best model in test (the public leaderboard is effectively a test set), whilst not being quite so thorough on their cross validation . . . these competitors drop down the leaderboard when a new test set is introduced at the end.</p>

<p>One approach that is reasonable is to re-use (train + cv) data to re-train using the hyper-params you have found, before testing. That way you do get to train on more data, and you still get an independent measure of performance at the end.</p>

<p>If you want to get more out of cross validation, the usual approach is <a href=""https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation"" rel=""noreferrer"">k-fold cross validation</a>. A common trick in Kaggle competitions is to use k-fold cross validation, and instead of re-combining the data into a larger (train + cv) training set, to ensemble or stack the cv results into a meta-model. </p>

<p>Finally, always check that your splits for validation and test are robust against possible correlation within your data set.</p>
","16","2","836","25573"
"18419","<p>As described, you have no data describing individual people (such as age, sex, shoe size), but are searching for an  optimum value of the mix for the whole population. So what you want is a mix with the maximum <em>expected</em> rating, if you chose a random person to rate it from the population. In principle, this expected rating is a function taking two parameters e.g. $f(n_{apple}, n_{orange})$ - the amount of the third juice type is a not a free choice, so you only have two dimensions.</p>

<p>You can break down your problem into two distinct parts:</p>

<ul>
<li><p>Taking samples from your population in order to find approximation to the function $f(n_{apple}, n_{orange})$</p></li>
<li><p>Using the approximation as it evolves to guide the search for an optimum value.</p></li>
</ul>

<p>For a simple approach, you could ignore the second bullet point and just randomly sample different mixes throughout the event. Then train a regression ML on the ratings (any algorithm would do, although you'll probably want something nonlinear, otherwise you'll just predict one of the pure juices as favourite) - finally graph its predictions and find the maximum rating at the end. This would probably be fine when pitched as a fun experiment. </p>

<p>However, there is a more sophisticated approach that is well-studied and used to make decisions when you want to optimise an expected value of an action whilst exploring options - it is usually called <a href=""https://en.wikipedia.org/wiki/Multi-armed_bandit"" rel=""noreferrer"">multi-armed bandit</a>. In your case, you would need variants of it that consider an ""arm space"" or parametric choice, as opposed to a finite number of choices that represent selecting between actions. This is important to you, since splitting your mix parameters up into e.g. in 5% steps, will give you too many options to explore given the number of samples you need to make. Instead, you will need to make an assumption that the expected rating function is relatively smooth - the expected rating for 35% Apple, 10% Orange, 55% Grape is correlated with the rating for  37% Apple, 9% Orange, 54% Grape . . . that seems at least reasonable to me, but you should make clear in any write-up that this is an assumption and/or find something published that supports it. If you make this assumption, you can then use a function approximator such as a neural network, a program like xgboost or maybe some Guassian kernels to predict expected rating from mix percentages.</p>

<p>In brief for a multi-armed bandit problem, you will use data collected as your experiment progresses to estimate the expected value for each choice, and on each step will make a new choice of mix. The choice itself will be guided by your current best approximation. However, you don't always sample the current top-rated value, you need to explore other mixes in order to refine your estimated function. You have choices here too - you could use $\epsilon$-greedy where e.g. 10% of the time you choose completely randomly to get other sample points. However, you might need something more sophisticated that explores more to start with and still converges quickly, such as <a href=""https://en.wikipedia.org/wiki/Gibbs_sampling"" rel=""noreferrer"">Gibbs sampling</a>.</p>

<p>One thing you don't say is at what level you are pitching this experiment. Studying the multi-armed bandit problem by yourself referring to blogs, tutorials and papers could be a bit too much work if this is for school science fair. If this all seems a bit too vague and a lot of work to study, then you can probably stick with a simple regression model from the data of a random experiment. </p>

<p>I suggest whichever approach you take, that you run some simulations of input data and see whether your approach works. Obviously there is a lot of guess work here. But the principle is: </p>

<ul>
<li><p>Create a ""true"" model function - e.g. pick an imaginary favourite mix and make it score higher. Make it a simple and probably quite subtle function - e.g. score 5 for best result, and take away euclidean distance in ""juice space"" times a small factor (maybe 1.5) from it.</p></li>
<li><p>Create a noisy sampler that imitates someone in your experiment giving a rating to a specific mix. Ensure that the mean value from this matches the ""true"" function.</p></li>
<li><p>Try out your sampling and learning strategies, see how well they find the favourite mix.</p></li>
</ul>

<p>I highly recommend this kind of dry run before putting your system to real use, otherwise you will have no confidence that your ML/approximator is working.</p>

<hr>

<p>One more piece of advice about your estimator: You are expecting a large amount of variance in your data, and will not have a lot of samples. So to avoid over-fitting you will want to have a relatively simple ML model. For a neural network for example, you will probably want only one hidden layer with very few neurons in it (e.g. 4 or 5 might be enough). Finding a model sophisticated enough to predict a curve, but simple enough that it doesn't overfit given very noisy target outputs might take a few tries - this is the main reason why I suggest performing trial runs with simulated data.</p>
","6","2","836","25573"
"18434","<p>This sounds entirely reasonable, and the usual name for this structure I have heard for this is just ""pipeline"" which also applies to other system-feeds-next-system structures - it might also be ""machine learning pipeline"" or ""data processing pipeline"".</p>

<p>There are ways to assess performance of a ML pipeline:</p>

<ul>
<li><p>You can of course compare the final accuracy or loss value, with the simpler model. Has turning the model into a more complex multi-stage one actually improved things? Sadly nothing is guaranteed, although I would be hopeful in your case initially - in part because you could apply adjustments available to classifier models used to deal with class imbalance issues.</p></li>
<li><p>You can decide which part of the pipeline will gain you the most benefit by switching between pipeline-so-far input to each unit and perfect input from the training data. Then you can see how much incremental difference is possible by perfecting that unit in the pipeline.</p></li>
</ul>

<p>In your case you have a two stage pipeline, so you can check whether it is worth focusing more effort on the classifier or regression parts by comparing the incremental improvements between:</p>

<ol>
<li><p>The unadjusted output of the whole pipeline run end-to-end.</p></li>
<li><p>The output of the regression (or zero) assuming that the classifier was perfect.</p></li>
<li><p>A perfect score.</p></li>
</ol>

<p>Whichever of the two differences gives you the largest difference (2) - (1), or (3) - (2) points at work being most rewarded for working on the classifier or regression stage respectively.</p>

<p>You can see a worked example of this per-stage analysis in <a href=""http://cs229.stanford.edu/materials/ML-advice.pdf"" rel=""nofollow noreferrer"">Advice for Applying Machine Learning</a> (slides 21, 22), amongst other places.</p>
","3","2","836","25573"
"18504","<p>As you are building policies in simulation, and can avoid the need to use approximate methods (the state space is small enough to fit in a table in memory), then your goal is to converge on the optimal policy. With correct setup, <em>all</em> the methods you are comparing are guaranteed to converge for your problem, provided the assumptions of reinforcement learning are not broken.</p>

<blockquote>
  <p>With that being said, I cannot measure the performance based on the number of actions needed to reach the terminal state, as it will always be reached with the same number of actions.</p>
</blockquote>

<p>This is just the reward scheme for maze solvers. For those the goal is to solve the maze quickly.</p>

<p>You have a reward scheme. The best policy maximises the expected reward. That is the <em>only</em> measurement that a completed RL output (your policy) cares about. As all your chosen learners should learn the optimal policy, you cannot compare them on this matter. So you maybe need to look at efficiency - how quickly the algorithm converges to optimal policy.</p>

<blockquote>
  <p>The state-space is quite large actually, which is one of my concerns about how many epochs (episodes) I should have, its a 250*8*7*3000 space-action (3-state combination, 250,8,7 and 3000 possible actions per state).</p>
</blockquote>

<p>I would characterise this as a small state space. It will fit into memory as a table. Looping through the 40 million state/action pairs should be pretty fast unless the simulation step is expensive for some reason.</p>

<p>In fact, if you have a simple model for transitions and rewards at each step, I would encourage you to use <a href=""https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/node19.html"" rel=""nofollow noreferrer"">Value Iteration</a> (a dynamic programming method) instead of the sampling based approaches in SARSA or Q-Learning. To do so, you would need to be able to define code for state transitions and rewards given current state and action. Sometimes though, the simulation is much easier to code than the transitions/rewards model. If that's the case for you, stick with SARSA or Q-Learning.</p>

<p>As there are no consequences to you for bad decisions and low rewards during training stages - learning offline in simulations - then Q-Learning may be preferable as it learns the optimal policy whilst exploring. Compared to  SARSA you have to be concerned about how to reduce $\epsilon$ so as to converge on the optimal policy. For Q-Learning you can leave $epsilon$ at a relatively high value (e.g. 0.1) and still learn whilst refining estimates of alternative actions.</p>

<p>However, I am a little concerned about this: </p>

<blockquote>
  <p>the end-state reward depends on the actions selected in the other 249 days</p>
</blockquote>

<p>This is a problem, since it breaks the assumptions of the Markov Decision Process that all the algorithms are based upon. The reward at any step should <em>only</em> be based on current state/action. By doing this, you may invalidate the policy. It definitely would defeat dynamic programming (may prevent it converging) unless you included action history in the state (which would definitely make your state space large). <em>However</em>, it may still work for sampling methods with a high value of $\lambda$, because these are more robust when faced with environments which are not strictly MDPs.  </p>

<blockquote>
  <p>Any suggestions on how to compare the models?</p>
</blockquote>

<p>I suggest on CPU time required to learn the optimal model, assuming you intend to repeat this process on different data routinely. The way to find out is to experiment, as convergence will depend on properties of the problem, how fast your simulation code runs, and in your case you are stretching one of the assumptions behind the design of the RL solvers which could make strongly bootstrapping methods (like SARSA(0)) either fail to converge or converge very slowly.</p>

<p>If convergence takes too long, set a cutoff at some time limit (e.g. an hour) and find out expected reward from initial conditions when following the policy learned so far. You could Monte Carlo sample that for a fair comparison (and if results are stochastic even with a deterministic policy, then give plenty of time for 1000s of runs to get a good estimate).</p>
","1","2","836","25573"
"18608","<p>In <a href=""https://www.youtube.com/watch?v=5cKpzp358F4"" rel=""nofollow noreferrer"">your example</a>, you are seeing the effect of a stabilisation algorithm applied to otherwise jerky movement - and the effect is a moving ""window"" inside the video frame. </p>

<p>The actual video file has a fixed frame size. As far as I know there are no video formats that switch size frame to frame. You may have different areas of focus to worry about, but should never need to worry about frame size switching on you mid video. You can rely on your video processing library to present a consistent frame size for the whole of a video.</p>

<p>If you find that the border of the view inside the video frame interferes with the performance of your CNN once trained, then you could deal with it in a few ways. A couple of simple approaches might be:</p>

<p>1) Crop out a centre portion that always reliably has content in, and use just that portion for your input features.</p>

<p>2) Detect and flood-fill the black border with the mean pixel value, using an image/video library (like OpenCV) so that edges around it cause less activity in your CNN.</p>
","1","2","836","25573"
"18705","<p>Look at the <a href=""https://www.tensorflow.org/api_docs/python/tf/gradients"" rel=""nofollow noreferrer"">documentation of <code>tf.gradients</code></a> and you will see that the third parameter is used to weight the gradient calculation of first param wrt to second param. This is phrased in the documentation as </p>

<blockquote>
  <p>A user can provide their own initial grad_ys to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).</p>
</blockquote>

<p>So it essentially uses the <code>grad_ys</code> parameter as an element-wise multiplication.</p>

<p>The parameter update gradient calculation in the project code is feeding in the already-calculated value of $\frac{\partial \mu(s|\theta)}{\partial \theta}$ as a placeholder, and using it for this third param.</p>

<p>So the multiplication of the two gradient factors occurs <em>within</em> <code>tf.gradients</code>.</p>
","3","2","836","25573"
"18714","<p>In theory, this should not do any harm to the accuracy of the trained network on the data that you have. However, of course the network will have no ability to predict the three classes it has not been shown. If it is well trained, it should predict close to zero probability for all three unseen classes.</p>

<p>It might be a reasonable structure to have all thirteen classes defined for a case where you continue to train online, as data arrives - although the network might have unknown problems adjusting to new classes when they appear, you will have no measure of its ability to do this. Also, if your goal is to produce a model for someone else's use and then leave it with some simple notes on how to re-train it, then it could make sense to have known future requirements already coded.</p>

<p>However, if you intend to re-train from scratch with new data, or if you will be building any new model when there is new data, then it is not strictly necessary to add unseen classes in advance. Depending on training time for your network, you might view any hyper-parameter in the code as something you can adjust quickly. If you write the code with a constant defining the number of classes, and refer that in all code that needs to know this number, then it should only take seconds to adapt your model - that's no time at all compared to time spent on other parts of the problem.</p>
","0","2","836","25573"
"18717","<blockquote>
  <p>The problem is that training data will positionally biased by the fact that the probability of a click is correlated to the old model's prediction.</p>
</blockquote>

<p>It should be the case that many of your input variables are correlated in some way with the output, otherwise your model could not work. The main difference here is you are expecting a strong correlation from a single feature. This is not a problem - you could think of it as a complex form of feature engineering.</p>

<p>You are essentially <a href=""https://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning"">stacking the old model</a> with some new variables which you hope are predictive. You <em>should probably</em> in this case include all the existing/old variables so that the new model can more easily spot mistakes made by the old model.</p>

<blockquote>
  <p>My plan is to introduce a penalty factor on the original model's prediction to ensure that it doesn't dominate the new model </p>
</blockquote>

<p>I doubt this would be useful. However the correct way to assess this plan is to try it and measure the performance compared to the simpler version without any penalty.</p>
","1","2","836","25573"
"18728","<p>$f$ is not defined for $\mathbb{R}^M$, so as well as reducing $x \in \mathbb{R}^N$ to $y \in \mathbb{R}^M$ you are creating a function approximator $g(y) \approx f(x)$.</p>

<p>I suggest using a <a href=""https://en.wikipedia.org/wiki/Autoencoder"" rel=""nofollow noreferrer"">neural network autoencoder</a> with N dimension inputs and M dimension ""bottleneck"" layer. You will need to scale inputs and outputs. Using a standard auto-encoder, you will have to measure the fraction of preserved variance in $f(x)$ afterwards. The auto-encoder is not <em>directly</em> going to do that for you, instead it will, like PCA, attempt to use y to encode x (unlike PCA, it may do so nonlinearly). </p>

<p>You <em>could</em> take this further if $f(x)$ is differentiable. Instead of usual mean-squared error as the loss function, you could use $\frac{1}{k}\sum_{i=1}^{k} (f(x_{i}) - f(\hat{x_i}))^2$ - that will encourage the NN to preserve variance in $x$ that matters to $f(x)$. You will have to figure out the gradient function for this analytically. Also bear in mind I have not tried this, just attempted to match your requirement to some theory.</p>

<p>In addition, you will now have $g(y)$ as it will be $f(\hat{x})$. You can generate any $y$ from $x$ by running the first half of the autoencoder, and can generate any $\hat{x}$ from $y$ by running second half of the autoencoder.</p>

<hr>

<p>You <em>might</em> also be able to adapt t-SNE by using your function $f(x)$ to generate distances that need to be preserved when reducing dimensions.</p>
","2","2","836","25573"
"18771","<p>There are no rules to infer neural network hyper-parameters from a problem description. With only number of features, number of examples and the fact you have a binary classification problem, it is far too little to even make an educated guess.</p>

<p>First, are you sure you are ready to build a deep learning model for your data? Have you looked at the data, or scatter plotted it reduced to two dimensions using PCA or t-SNE, to get a feel for how easy your data is to separate (easy to separate raw data implies using simpler/shallower model)? You could try a basic model such as logistic regression or SVM in order to establish a benchmark so you can tell whether the deep model was doing anything useful.</p>

<p>Assuming you are ready to go ahead, the time-honoured approach is to try out variations, and measure the results using cross-validation. You can do this methodically, by for instance starting with one hidden layer with 64 neurons, and either adding/removing layers or adding/removing neurons in each layer. Generally these searches don't cover all possible variations, but stop once you have tried those that seem interesting and have reached a reasonable result for your problem.</p>

<p>Do note that other hyper-parameters will affect results and can interact with your choices for numbers of neurons and layers. You cannot isolate your choice of network depth from other choices, such as optimisation method, activation functions, regularisation. When starting to build a model, it is just as reasonable to spend a long time exploring these other factors as it is to look at network size/shape.</p>

<p>It is very easy to get a deep NN to over-fit your data. Cross-validation is therefore a necessity as you explore hyper-parameters for your problem. One reasonable method of searching against number of neurons is to increase them until you start to notice over-fitting, then adjust regularisation to stop the over-fit. At that point, with all other factors remaining the same, there is probably no need to explore networks with many more neurons (although deeper/shallower networks may still be worth exploring, and if you do so, you will once again want to explore number of neurons per layer).</p>
","2","2","836","25573"
"18799","<p>There is more than one way to do this in principle, but most CNNs, and most CNN libraries will do the following:</p>

<ul>
<li><p>Each layer has a <em>target</em> number of feature maps, which is what you have labelled <code>nb_filters</code>.</p></li>
<li><p>Each feature map is derived from <em>all</em> previous layer's feature maps.</p></li>
<li><p>The weights of the locally connected kernels <em>do</em> follow a <code>width x height x nb_input_features x nb_output_features</code> structure similar to your first example (with width and height being the convolutional kernel size), because each output feature in layer b is derived as the sum of convolutions over every feature in layer a.</p></li>
</ul>

<p>So if your output feature map with (channel) index $j$ in layer $k+1$ is $I^{(j, k+1)}$, and you are looking at connection to all input feature maps $I^{(i, k)}_i$ in layer $k$ with filters connecting each map $F^{(i,j,k)}$, then the maths looks a bit like this:</p>

<p>$$I^{(j,k+1)} = f(b^{(j,k)} + \sum_i I^{(i,k)} \ast F^{(i,j,k)})$$</p>

<p>. . .where $f(x)$ is the transfer or activation function e.g. ReLU. The bias is $b^{(j,k)}$ and the $\ast$ represents the convolution function. Summing over $i$ means iterating over all the input channels. NB I have indexed the items all with a superscript like this to more clearly show that all the variables (apart from $b$) are matrices <em>after</em> indexing - i.e. this is not the equation for a specific pixel, but for a whole feature map. </p>

<p>You can see that the kernel $F$ is indexed by $(i,j,k)$ i.e. there are <code>nb_input_features</code> times <code>nb_output_features</code> filters connecting each layer.</p>

<p>In comparison, your expected scheme would keep the derived features <em>separate</em> layer-by-layer, and create an exponential growth in number of feature maps as the layers increased, with each feature ""type"" being handled separately. Whereas what happens in practice is each feature map is a ""re-mix"" of all the feature maps in the previous layer, convolved each with its own filter then summed up. </p>

<p>This cross-referencing of features between layers helps create richer complex feature maps - e.g. it can combine edge and corner detectors to detect corners with specific angles. Whilst a scheme that refined each feature into multiple separate sub-features would not be able to do this mixing between features as freely (until the later fully-connected layers) - although it <em>might</em> do well at other tasks - e.g. detecting variations of extended structures of the same type longer lines, curves etc (NB this is just a guess on my part, I don't know for certain).</p>

<blockquote>
  <p>If it's not working that way, how do I know what filters are applied to what activation maps?</p>
</blockquote>

<p>You cannot say something like ""this feature map in layer N is derived only from this other feature map in layer N-1, using this single filter"", because each feature map in layer N is derived from <em>all</em> feature maps in layer N-1 and results of the filters are combined in a way that you cannot easily reverse.</p>

<p>However, the weights array will be arranged so that you can find which filters are connecting each input feature map to a specific output feature map. Which dimension you would use to isolate those depends on the library. If your library stores weights in a <code>[kernel_width, kernel_height, input_feature_map_id, output_feature_map_id]</code> form, then your selection (using Numpy-style syntax might be) <code>[:,:,:,n]</code> where <code>n</code> is the id of the output feature map that you want to fetch the filters for.</p>
","5","2","836","25573"
"18803","<p>The loss functions are only simple convex functions with respect to the weight parameters (and specific data) when there is a single layer. More exactly, they can proven to be always convex with respect to the weights in the simple models (linear or logistic regression), but <em>not</em> with respect to weights of deeper networks.</p>

<p>You can prove that there must be more than one minimum in a network with 2 or more layers - and thus the loss function cannot be convex - by considering swapping the weights around when you have found a minimum value. Unlike with a single layer network, it is possible to swap the weights around that feed into the hidden layer <strong>whilst maintaining the same output</strong>. For example, you can swap the weights between input and hidden layer so that values of neuron output 1 and neuron output 2 are reversed. Then you can also swap the weights feeding <em>out</em> of those neurons to the output so that the network still outputs the same value. The network would have a different set of weights, but generate the same outputs, and so this new permutation of weights is <em>also</em> at a minimum for the loss function. It is the ""same"" network, but the weight matrices are different. It is clear that there must be very many fully equivalent solutions all at the true minimum.  </p>

<p>Here's a worked example. If you have a network with 2 inputs, 2 neurons in the hidden layer, and a single output, and you found that the following weight matrices were a minimum:</p>

<p>$W^{(1)} = \begin{bmatrix} -1.5 &amp; 2.0 \\ 1.7 &amp; 0.4 \end{bmatrix}$</p>

<p>$W^{(2)} = \begin{bmatrix} 2.3 &amp; 0.8 \end{bmatrix}$</p>

<p>Then the following matrices provide the same solution (the network outputs the same values for all inputs):</p>

<p>$W^{(1)} = \begin{bmatrix} 1.7 &amp; 0.4 \\ -1.5 &amp; 2.0 \end{bmatrix}$</p>

<p>$W^{(2)} = \begin{bmatrix} 0.8 &amp; 2.3 \end{bmatrix}$</p>

<p>As we said the first set of 6 parameters was a solution/minimum, then the 
second set of 6 parameters must also be a solution (because it outputs the same). The loss function therefore has 2 minima with respect to the weights. In general for a MLP with one hidden layer containing $n$ neurons, there are $n!$ permutations of weights that produce identical outputs. That means that there are at least $n!$ minima.</p>

<p>Although this does not prove that there are worse local minima, it definitely shows that the loss surface must be much more complex than a simple convex function.</p>
","7","2","836","25573"
"18837","<p>Kaggle competitions with clean, anonymised and opaque numerical features are often popular. My opinion is they are popular because they are more universally accessible - all you need is to have studied at least one ML supervised learning approach, and maybe have a starter script that loads the data, and it is very easy to make a submission. The competitions become very focused on optimising parameters, picking best model implementations and ensembling techniques. The more advanced competitors will also refine and check their CV approaches very carefully, trying to squeeze the last iota of confidence out of them in order to beat the crowd climbing the public leaderboard.</p>

<p>Examples of historic Kaggle competitions with obfuscated data might be <a href=""https://www.kaggle.com/c/otto-group-product-classification-challenge/data"" rel=""noreferrer"">Otto Group Product Classification</a> or <a href=""https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/data"" rel=""noreferrer"">BNP Paribas Cardif CLaims Management</a>. For some of these competitions the data is adjusted for anonymity of the users who might otherwise be identified from the records. In other cases it is less clear what the sponsor's motivation is.</p>

<p>However, there are negative consequences (you will find these complained about in the same competitions):</p>

<ul>
<li><p>Use of insight from domain knowledge, or exploration/study of the underlying principles from the subject being predicted are effectively blocked. It is hard to assess the impact of this, but it is possible that the sponsors miss out on potentially better models.</p></li>
<li><p>Doing ""just"" the machine learning side can be a bit too mechanical and boring for some competitors, who may not not try as hard.</p></li>
</ul>

<blockquote>
  <p>How much do contestants reply on statistical analysis on the (raw, i.e. non-PCA-ed) input features? </p>
</blockquote>

<p>There are always data explorations and views of data published in forums (and Kaggle's scripts - called kernels), and many people view, upvote and presumably use the insights from them. I recall at least one competition forum thread where there was a lot of discussion about weird patterns appearing in data, which were probably an artefact of obfuscation (sorry I cannot find the thread now).</p>

<p>With obfuscated data, there can be attempts to de-obfuscate, and they have sometimes been partially successful.</p>
","5","2","836","25573"
"18869","<p>You decide what the output layer represents, so it should be OK (and probably easier to implement) to have a single fully connected layer of size $m_x + m_y + m_z$ and to interpret the output in your own code. E.g. $n_x = N_{(x)}, n_y = N_{(m_x + y)}, n_z = N_{(m_x + m_y + z)}$, where $N_{(i)}$ is the output of the $i^{th}$ neuron. In the case of regression, this should have very similar behaviour to 3 separate fully connected layers for each dimension. </p>

<p>However, once you have either structure, it is not clear that you will be estimating reward directly any more. There is definitely no guarantee that the rewards will be the same between $n_x, n_y, n_z$, in fact they will not be in most cases. You will need to define a function $\hat{r}(x, y, z)$ that combined the outputs of the network and have your loss and error gradient calculated from that. The function could just be $\hat{r}(x, y, z) = n_x + n_y + n_z$ or maybe $\hat{r}(x, y, z) = n_x \times n_y \times n_z$ </p>

<p>Clearly this approximation has limits built in - it will likely over-estimate and under-estimate rewards due to its structure. If the simplified structure matches your problem, then this will be fine - it may even speed convergence. But if not, it will limit performance.</p>

<p>An alternative, if you don't want to build a network that has a large number of outputs, is to have a single reward estimate as the output, and have the action choice as an input. That has the advantage of leaving the details of the approximation to the neural network. It has the disadvantage that you would need to run the network up to $m_x * m_y * m_z$ times in order to decide the policy in each state.</p>

<p>Because of the large number of possible actions in your case, you may want to look at a policy-based learning method, rather than DQN. There are deep NN-based versions of those algorithms. A recently-published <a href=""https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"" rel=""nofollow noreferrer"">algorithm called Asynchronous Advantage Actor-Critic (A3C)</a> has performed well in computer gaming tasks and might be appropriate for your problem.</p>
","1","2","836","25573"
"18917","<p>The area of computer science this most closely relates to is <a href=""https://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow noreferrer"">natural language processing</a>. There is a large body of work and ongoing research in this area.</p>

<p>You have a few barriers to achieving interest in your ideas from people studying the subject:</p>

<ul>
<li><p>The methods as you present them, are not tightly defined or easy to comprehend. </p></li>
<li><p>Where the methods are comprehensible, they appear to be a form of <a href=""https://en.wikipedia.org/wiki/Expert_system"" rel=""nofollow noreferrer"">expert system</a> which attempt to address questions about a piece of text. Expert systems are not new to NLP and are used in many text comprehension systems, but generally work at the level of text adventures, or in very limited domains (e.g. Amazon's Alexa might use such rules after analysing input, when deciding between playing some music that you requested or booking you a table at a local restaurant)</p></li>
<li><p>Some of the terms you use in your heuristics (such as ""opposite"") are not well defined for all inputs, and could be the subject of a large piece of research in their own right. A computer programmer would likely halt and ask you to define how to extract ""quality of X"" from a piece of content, before they could apply your rule.</p></li>
<li><p>Claims such as ""FORMULA FOR FINDING NEW THEORIES OF PHYSICS"" will not be well received by any research or academic community. Unless you have actually discovered, published and had verified a new formula for physics using your heuristics, you are most likely to receive negative attention for making such a claim. If you cannot defend this claim about the effectiveness of your methods <em>with proof</em>, I'd advise not making such a claim.</p></li>
</ul>

<p>Probably you could have some of your rules encoded as an expert system (perhaps with some existing NLP analysis to help with identifying the loose concepts in your rule system), and they would generate output for a range of inputs. They might be interesting in some contexts e.g. these rules, or ones like them, might be useful topic generators for <a href=""https://en.wikipedia.org/wiki/Chatbot"" rel=""nofollow noreferrer"">chatbots</a>. However, they don't appear to represent a novel approach to NLP, nor do they align well with open research topics in the subject.</p>
","2","2","836","25573"
"18948","<p>I think it might be a relatively trivial bug in your cost function for softmax: </p>

<pre><code>J = -(sum(sum((Y).*log(h))) + lambda*p/(2*m)) 
</code></pre>

<p>should be </p>

<pre><code>J = -sum(sum((Y).*log(h)))/m + lambda*p/(2*m) 
</code></pre>

<p>I.e. for softmax only, you have effectively subtracted the regularisation term from the cost function instead of adding it. Also, you forgot to divide the error term by the number of examples in the batch (and you are taking this average when calculating the gradients)</p>

<p>Your back propagation calculations look correct to me if you correct this miscalculation for <code>J</code>.</p>
","2","2","836","25573"
"18979","<p>When a neural network processes a batch, all activation values for each layer are calculated for each example (maybe in parallel per example if library and hardware support it). Those values are stored for possible later use - i.e. one value per activation per example in the batch, they are not aggregated in any way</p>

<p>During back propagation, those activation values are used as one of the numerical sources to calculate gradients, along with gradients calculated so far working backwards and the connecting weights. Like forward propagation, back propagation is applied per example, it does not work with averaged or summed values. Only when all examples have been processed do you work with the summed or averaged gradients for the batch.</p>

<p>This applies equally to max pool layers. Not only do you know what the output from the pooling layer for each example in the batch was, but you can look at the preceding layer and determine <em>which</em> input to the pool was the maximum. </p>

<p>Mathematically, and avoiding the need to define indices for NN layers and neurons, the rule can be expressed like this</p>

<ul>
<li><p>The forward function is $m = max(a,b)$</p></li>
<li><p>We know $\frac{\partial J}{\partial m}$ for some target function J (in the neural network that will be the loss function we want to minimise, and we are assuming we have backpropagated to this point already)</p></li>
<li><p>We want to know $\frac{\partial J}{\partial a}$ and $\frac{\partial J}{\partial b}$</p></li>
<li><p>If $a &gt; b$</p>

<ul>
<li><p><em>Locally</em>,* $m = a$. So $\frac{\partial J}{\partial a} = \frac{\partial J}{\partial m}$</p></li>
<li><p><em>Locally</em>,* $m$ does not depend on $b$. So $\frac{\partial J}{\partial b} = 0$</p></li>
</ul></li>
<li><p>Therefore $\frac{\partial J}{\partial a} = \frac{\partial J}{\partial m}$ if $a &gt; b$, else $\frac{\partial J}{\partial a} = 0$</p></li>
<li><p>and $\frac{\partial J}{\partial b} = \frac{\partial J}{\partial m}$ if $b &gt; a$, else $\frac{\partial J}{\partial b} = 0$</p></li>
</ul>

<p>When back propagation goes across a max pooling layer, the gradient is processed <em>per example</em> and assigned only to the input from the previous layer that was the maximum. Other inputs get zero gradient. When this is batched it is no different, it is just processed per example, maybe in parallel. Across a whole batch this can mean that more than one, maybe all, of the input activations to the max pool get some share of the gradient - each from a different subset of examples in the batch.</p>

<hr>

<p>* Locally -> when making only infinitesimal changes to $m$. </p>

<p>** Technically, if $a=b$ exactly then we have a discontinuity, but in practice we can ignore that without issues when training a neural network.</p>
","10","2","836","25573"
"19014","<p>In a scenario where <em>consequences</em> of prediction errors are not equivalent, you are usually still interested in training a model to predict accurately from the data set, and would not change the objective function in supervised learning. </p>

<p>Typically when consequences of FP and FN differ, you would:</p>

<ul>
<li><p>Use the <em>confidence</em> of the prediction given by the model. In XGBoost, that is <code>objective: ""binary:logistic""</code></p></li>
<li><p>Use <a href=""https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve"" rel=""nofollow noreferrer"">area under ROC</a> as a base metric for deciding best tuned model (this is <code>eval_metric: ""auc""</code> for XGBoost). This metric is a measure of how well sorted your classes are - the higher the value, the easier and more effective it is to tune the confidence level later.</p></li>
<li><p>Decide a weighting for your costs FP and FN - this is entirely up to you, you say that FP is worse than FN, but in order to make an optimal decision you have to turn that into a numerical statement. If you can assign a <em>relative</em> financial cost to a business for each kind of mistake, then that would be a good start.</p></li>
<li><p>Using the best tuned model, use the weightings to calculate costs at different cutoffs of class confidence before assigning positive class that you wish to act upon in some way. Predict against a test set to get class probabilities. Then use different cutoff points for positive class, count your FP and FN given that cutoff, and multiply the totals by the costs you chose later. The lowest-scoring cutoff point should be the one to use in production. If FP has a higher cost than FN, then you will likely find that a confidence level > 0.5 is required.</p></li>
</ul>

<p>You can also look into machine learning approaches that consider the consequences of actions - e.g. reinforcement learning - in order to achieve a similar result within a single learning framework. However, that is not something you can do solely within XGBoost, and as long as your problem remains balancing between FP and FN in a single step prediction, then the above approach should be OK.</p>
","3","2","836","25573"
"19023","<p>CNN (and RNN) models are not <em>general</em> improvements to the MLP design. They are specific choices that match certain types of problem. The CNN design works best when there is some local pattern in the data (which may repeat in other locations), and this is often the case when the inputs are images, audio or other similar signals.</p>

<p>The reuters example looks like a ""bag of words"" input. There is no local pattern or repeating relationships in that data that a CNN can take advantage of.</p>

<p>Your results with a CNN on this data set look reasonable to me. You have not made a mistake, but learned how a CNN really works on this data.</p>
","4","2","836","25573"
"19038","<p>Cosine is not a commonly used activation function.</p>

<p>Looking at the <a href=""https://en.wikipedia.org/wiki/Activation_function"" rel=""noreferrer"">Wikipedia page describing common activation functions</a>, it is not listed.</p>

<p>And one of the <em>desirable properties</em> of activation functions described on that page is:</p>

<blockquote>
  <p>Approximates identity near the origin: When activation functions have this property, the neural network will learn efficiently when its weights are initialized with small random values. When the activation function does not approximate identity near the origin, special care must be used when initializing the weights.</p>
</blockquote>

<p>$cos(0) = 1$, a basic cosine function does not have this property. Combined with its periodic nature, this makes it look like it could be particularly tricky to get correct starting conditions and other hyper-parameters in order to have a network learn whilst using it.</p>

<p>In addition, cosine is not monotonic, which means that error surface is likely to be more complex than for e.g. sigmoid.</p>

<p>I suggest trying with a low learning rate, and initialising all the bias values to $-\frac{\pi}{2}$. Maybe reduce the variance in initial weights a little too, just to start off with things close to zero. Essentially this is starting with $sin()$. Caveat: not that I have tried this myself, just an educated guess, so I would be interested to know if that helps at all with stability.</p>
","5","2","836","25573"
"19076","<p>It is likely that at least one of the stop words you have removed is predictive of the subreddit category. </p>

<p>This could be a post style thing - e.g. in a simple case the posts have different typical lengths depending on class, and counting high frequency words gives a good proxy for article length. You could test that by using article length as an additional feature whilst still removing the stop words, and see whether any of the 5% improvement is accounted for by that.</p>

<p>It is possible that certain stop words are really more associated with certain topics. Perhaps they occur in common phrases associated with the topic. You could check feature importance from the weights and see which of the stop words were being used most heavily by the model.</p>
","0","2","836","25573"
"19085","<p>The subject areas Artifical Intelligence and Machine Learning (plus Data Science) are loosely defined, such that it is hard to make strict statements about how they relate. In the general case, it seems that there are parts that overlap, but that they are quite far from being ""the same subject with two different names"" as suggested in the question.</p>

<p>The term <em>Artifical Intelligence</em> has many possible meanings and interpretations - which version to refer varies by time and by the source using it. Textbooks on artificial intelligence will often cover topics such as search algorithms, logical deduction and other things which are clearly not machine learning as it is practised today.</p>

<p>For instance, we could take it to refer to <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""noreferrer"">Artificial General Intelligence</a> (or ""hard AI""), and it should be clear in this case that at least some form of learning algorithm(s) would be required to meet the goals of AGI. However, it is far less clear how much of AGI can be solved by combining machine learning into complex structures.</p>

<p>The term <em>Machine Learning</em> has a few different working definitions, but this is a popular one:</p>

<blockquote>
  <p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.</p>
</blockquote>

<p>This is far more tightly defined than Artificial Intelligence, but still has a lot of scope. </p>

<p>The trend to conflate AI and ML appears to be a media and marketing issue, not a technical one. I suspect this is in part due to advances in the last 5-10 years in neural networks. Neural network models have made strong progress, especially in signal processing of images, video, audio. There is also an analogy with biological brains which can be compelling - especially when the subject matter is simplified for consumption by mainstream media. </p>

<p>It is worth mentioning Data Science too. Like Artificial Intelligence, the term is somewhat fuzzily defined. Also like AI, Data Science has more to it than just Machine Learning. To Data Science practitioners, ML is part of a toolkit to achieve goals - for some people it is a large part of what they do, for others it is just one part of a wider scope (actually training and refining a ML model might take only a small fraction of a professional data scientist, analyst or statistician's time). I think it is reasonable to state that Artificial Intelligence and Data Science relate to Machine Learning in a similar way.</p>
","10","2","836","25573"
"19101","<p>I agree with the comments on your question that you should look into a course, maybe <a href=""https://www.coursera.org/learn/machine-learning"" rel=""noreferrer"">Andrew Ng's Machine Learning on Coursera</a>, which is a highly regarded, free introductory course. This is a basic question about fundamentals of machine learning. As such I am not covering the maths in this answer, you can get that from many places, including that course.</p>

<blockquote>
  <p>where and how the values for Bias and Weight are determined?</p>
</blockquote>

<p>Weights and biases are the learnable parameters of your model. As well as neural networks, they appear with the same names in related models such as linear regression. Most machine learning algorithms include some learnable parameters like this.</p>

<p>The values of these parameters before learning starts are initialised randomly (this stops them all converging to a single value). Then when presented with data during training, they are adjusted towards values that have correct output.</p>

<blockquote>
  <p>Do we have to provide these values or does the TensorFlow library calculates these values automatically based on the training data set?</p>
</blockquote>

<p>You do not need to provide values before training, although you may want to decide things such as how many parameters there should be (in neural networks that is controlled by the size of each layer). TensorFlow calculates the values automatically, during training. When you have an already-trained model and want to re-use it, then you will want to set the values directly e.g. by loading them from file.</p>

<p>The specific code that handles changes to weights and biases from the tutorial is this:</p>

<pre><code>train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
</code></pre>

<p>and this:</p>

<pre><code>sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
</code></pre>

<p>The first line defines how the weights and values will be changed. You can read this almost literally as ""define a training function that uses the gradient descent optimizer to reduce the cross entropy of the supplied data"".</p>

<p>The second line invokes that function with a specific piece of data. Each time this second line is run, the weight and bias values are adjusted so that neural network outputs $y$ values a little bit closer to the correct association for each $x$ value.</p>
","14","2","836","25573"
"19303","<p>Essentially, each non-linear layer in a neural network is a map from $\mathbb{R}^n$ input to a $\mathbb{R}^m$ output. There is no requirement for these dimensions to be separate and uncorrelated, nor for the map to be reversible.</p>

<p>The best performance is usually found if each layer is roughly scaled to approximate a normal distribution with mean 0 and standard deviation 1. However, that is an independent issue to any kind of space-like mapping.</p>

<p>If you know a suitable mapping that relates to your problem and would simplify its expression, then you can apply a spacial transform between co-ordinate systems (typically by mapping the input features) and use it within the neural network. You could map co-ordinates to any curved space if you think it might help. Choosing between polar or cartesian co-ordinates for features is one choice that may crop up for example.</p>

<p>However, even without making this helpful mapping, you can analyse a neural network layer-by-layer, and see that it can effectively learn these kind of spacial mappings from the training data anyway. This is an intuition that has been called the ""<a href=""http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/"" rel=""nofollow noreferrer"">manifold hypothesis</a>"" - the link has some images showing how a neural network might learn to separate classes depending on how they are arranged in feature space, and <a href=""https://machinelearning.technicacuriosa.com/2017/03/19/a-darpa-perspective-on-artificial-intelligence/"" rel=""nofollow noreferrer"">this article from a DARPA researcher</a> has a nice series of images showing successive transformations of a 2D spiral that allow the last layer to perform linear separation.</p>
","2","2","836","25573"
"19332","<p>Working definitions of ReLU function and its derivative:</p>

<p><span class=""math-container"">$ReLU(x) = \begin{cases}
  0, &amp; \text{if } x &lt; 0, \\
  x, &amp; \text{otherwise}.
\end{cases}$</span></p>

<p><span class=""math-container"">$\frac{d}{dx} ReLU(x) = \begin{cases}
  0, &amp; \text{if } x &lt; 0, \\
  1, &amp; \text{otherwise}.
\end{cases}$</span></p>

<p>The derivative is the unit <a href=""https://en.wikipedia.org/wiki/Step_function"" rel=""noreferrer"">step function</a>. This does ignore a problem at <span class=""math-container"">$x=0$</span>, where the gradient is not strictly defined, but that is not a practical concern for neural networks. With the above formula, the derivative at 0 is 1, but you could equally treat it as 0, or 0.5 with no real impact to neural network performance.</p>

<hr>

<p><strong>Simplified network</strong></p>

<p>With those definitions, let's take a look at your example networks.</p>

<p>You are running regression with cost function <span class=""math-container"">$C = \frac{1}{2}(y-\hat{y})^2$</span>. You have defined <span class=""math-container"">$R$</span> as the output of the artificial neuron, but you have not defined an input value. I'll add that for completeness - call it <span class=""math-container"">$z$</span>, add some indexing by layer, and I prefer lower-case for the vectors and upper case for matrices, so <span class=""math-container"">$r^{(1)}$</span> output of the first layer, <span class=""math-container"">$z^{(1)}$</span> for its input and <span class=""math-container"">$W^{(0)}$</span> for the weight connecting the neuron to its input <span class=""math-container"">$x$</span> (in a larger network, that might connect to a deeper <span class=""math-container"">$r$</span> value instead). I have also adjusted the index number for the weight matrix - why that is will become clearer for the larger network. NB I am ignoring having more than neuron in each layer for now.</p>

<p>Looking at your simple 1 layer, 1 neuron network, the feed-forward equations are:</p>

<p><span class=""math-container"">$z^{(1)} = W^{(0)}x$</span></p>

<p><span class=""math-container"">$\hat{y} = r^{(1)} = ReLU(z^{(1)})$</span></p>

<p>The derivative of the cost function w.r.t. an example estimate is:</p>

<p><span class=""math-container"">$\frac{\partial C}{\partial \hat{y}} = \frac{\partial C}{\partial r^{(1)}} = \frac{\partial}{\partial r^{(1)}}\frac{1}{2}(y-r^{(1)})^2 = \frac{1}{2}\frac{\partial}{\partial r^{(1)}}(y^2 - 2yr^{(1)} + (r^{(1)})^2) = r^{(1)} - y$</span></p>

<p>Using the chain rule for back propagation to the pre-transform (<span class=""math-container"">$z$</span>) value:</p>

<p><span class=""math-container"">$\frac{\partial C}{\partial z^{(1)}} = \frac{\partial C}{\partial r^{(1)}} \frac{\partial r^{(1)}}{\partial z^{(1)}} = (r^{(1)} - y)Step(z^{(1)}) = (ReLU(z^{(1)}) - y)Step(z^{(1)})$</span></p>

<p>This <span class=""math-container"">$\frac{\partial C}{\partial z^{(1)}}$</span> is an interim stage and critical part of backprop linking steps together. Derivations often skip this part because clever combinations of cost function and output layer mean that it is simplified. Here it is not.</p>

<p>To get the gradient with respect to the weight <span class=""math-container"">$W^{(0)}$</span>, then it is another iteration of the chain rule:</p>

<p><span class=""math-container"">$\frac{\partial C}{\partial W^{(0)}} = \frac{\partial C}{\partial z^{(1)}} \frac{\partial z^{(1)}}{\partial W^{(0)}} = (ReLU(z^{(1)}) - y)Step(z^{(1)})x = (ReLU(W^{(0)}x) - y)Step(W^{(0)}x)x$</span></p>

<p>. . . because <span class=""math-container"">$z^{(1)} = W^{(0)}x$</span> therefore <span class=""math-container"">$\frac{\partial z^{(1)}}{\partial W^{(0)}} = x$</span></p>

<p>That is the full solution for your simplest network.</p>

<p>However, in a layered network, you also need to carry the same logic down to the next layer. Also, you typically have more than one neuron in a layer.</p>

<hr>

<p><strong>More general ReLU network</strong></p>

<p>If we add in more generic terms, then we can work with two arbitrary layers. Call them Layer <span class=""math-container"">$(k)$</span> indexed by <span class=""math-container"">$i$</span>, and Layer <span class=""math-container"">$(k+1)$</span> indexed by <span class=""math-container"">$j$</span>. The weights are now a matrix. So our feed-forward equations look like this:</p>

<p><span class=""math-container"">$z^{(k+1)}_j = \sum_{\forall i} W^{(k)}_{ij}r^{(k)}_i$</span></p>

<p><span class=""math-container"">$r^{(k+1)}_j = ReLU(z^{(k+1)}_j)$</span></p>

<p>In the output layer, then the initial gradient w.r.t. <span class=""math-container"">$r^{output}_j$</span> is still  <span class=""math-container"">$r^{output}_j - y_j$</span>. However, ignore that for now, and look at the <em>generic</em> way to back propagate, assuming we have already found <span class=""math-container"">$\frac{\partial C}{\partial r^{(k+1)}_j}$</span> - just note that this is ultimately where we get the output cost function gradients from. Then there are 3 equations we can write out following the chain rule:</p>

<p>First we need to get to the neuron input before applying ReLU: </p>

<ol>
<li><span class=""math-container"">$\frac{\partial C}{\partial z^{(k+1)}_j} = \frac{\partial C}{\partial r^{(k+1)}_j} \frac{\partial r^{(k+1)}_j}{\partial z^{(k+1)}_j} = \frac{\partial C}{\partial r^{(k+1)}_j}Step(z^{(k+1)}_j)$</span></li>
</ol>

<p>We also need to propagate the gradient to previous layers, which involves summing up all connected influences to each neuron:</p>

<ol start=""2"">
<li><span class=""math-container"">$\frac{\partial C}{\partial r^{(k)}_i} = \sum_{\forall j} \frac{\partial C}{\partial z^{(k+1)}_j} \frac{\partial z^{(k+1)}_j}{\partial r^{(k)}_i} = \sum_{\forall j} \frac{\partial C}{\partial z^{(k+1)}_j} W^{(k)}_{ij}$</span></li>
</ol>

<p>And we need to connect this to the weights matrix in order to make adjustments later:</p>

<ol start=""3"">
<li><span class=""math-container"">$\frac{\partial C}{\partial W^{(k)}_{ij}} = \frac{\partial C}{\partial z^{(k+1)}_j} \frac{\partial z^{(k+1)}_j}{\partial W^{(k)}_{ij}} = \frac{\partial C}{\partial z^{(k+1)}_j} r^{(k)}_{i}$</span></li>
</ol>

<p>You can resolve these further (by substituting in previous values), or combine them (often steps 1 and 2 are combined to relate pre-transform gradients layer by layer). However the above is the most general form. You can also substitute the <span class=""math-container"">$Step(z^{(k+1)}_j)$</span> in equation 1 for whatever the derivative function is of your current activation function - this is the only place where it affects the calculations.</p>

<hr>

<p>Back to your questions:</p>

<blockquote>
  <p>If this derivation is correct, how does this prevent vanishing?</p>
</blockquote>

<p>Your derivation was not correct. However, that does not completely address your concerns.</p>

<p>The difference between using sigmoid versus ReLU is just in the step function compared to e.g. sigmoid's <span class=""math-container"">$y(1-y)$</span>, applied once per layer. As you can see from the generic layer-by-layer equations above, the gradient of the transfer function appears in one place only. The sigmoid's best case derivative adds a factor of 0.25 (when <span class=""math-container"">$x = 0, y = 0.5$</span>), and it gets worse than that and saturates quickly to near zero derivative away from <span class=""math-container"">$x=0$</span>. The ReLU's gradient is either 0 or 1, and in a healthy network will be 1 often enough to have less gradient loss during backpropagation. This is not guaranteed, but experiments show that ReLU has good performance in deep networks.</p>

<blockquote>
  <p>If there's thousands of layers, there would be a lot of multiplication due to weights, then wouldn't this cause vanishing or exploding gradient?</p>
</blockquote>

<p>Yes this can have an impact too. This can be a problem regardless of transfer function choice. In some combinations, ReLU may help keep exploding gradients under control too, because it does not saturate (so large weight norms will tend to be poor direct solutions and an optimiser is unlikely to move towards them). However, this is not guaranteed.</p>
","21","2","836","25573"
"19379","<p>It looks like when you say ""way of assigning the weights"", that you mean ""what order are weights counted in, so that I know which weights connect between which neurons"".</p>

<p>There is no formal ""correct"" way of doing this for all neural networks. However, in practice for feed-forward networks like your diagram, you would choose to use a matrix, not a vector, to represent the weights connecting layers. That is how pretty much all standard libraries will represent weights.</p>

<p>A matrix uses two indices (call them $i,j$ in this case) to identify a single scalar value. If we call the weight matrix $W$, then an individual weight is $W_{ij}$.</p>

<p>To determine how the weights connect between neurons, then you index the input layer neuron with $i$ and the output layer neuron with $j$. </p>

<p>In math notation, looking at your diagram, call each input value $x_i$ and each hidden value $h_j$, then the formula for calculating a single $h_j$ would be:</p>

<p>$h_j = f(b_j + \sum_{i=1}^{N} W_{ij}x_{i})$</p>

<p>Where $f()$ is a transfer function (such as sigmoid), $N$ is the number of input features, and $b_j$ is the bias for hidden layer neuron $j$.</p>

<p>You will also often see this written using matrix notation:</p>

<p>$\mathbf{h} = f(W\mathbf{x} + \mathbf{b})$</p>

<p>. . . this is not only clear and simple notation, but using matrix maths like this to describe a neural network is what allows us to use high performance libraries on GPUs such as TensorFlow.</p>
","3","2","836","25573"
"19387","<p>Use a linear output and mean squared error loss, assuming you are predicting normalised pixel intensity values.</p>

<p>Cross-entropy over sigmoid output layer activations can do odd things when the values are not strictly in $\{0,1\}$, depending on implementation.</p>
","2","2","836","25573"
"19420","<p>There is no shortcut syntax that goes as far as accepting <code>[2,3,4,5]</code> as a param and create a model. However, it would be very simple for you to create this as a Python function yourself, provided in your case you already have made the decisions about activation functions, the types of layer etc. The need to make those decisions and have them available in the Keras API means that Keras itself does not offer such a short form build function.</p>

<p>You can make the model building slightly less verbose by using a list of layers when you instantiate the model, instead of adding them afterwards:</p>

<pre><code>model = Sequential([
  Dense(8, input_dim=2),
  Activation('tanh'),
  Dense(1),
  Activation('sigmoid')
])
</code></pre>

<p>However, if you want to try variations of a model, where the only things you change are the number and size of hidden layers, then you can write a short Python function to encapsulate that requirement. Here's an example (I'm sure you already know how to do this, just included for completeness):</p>

<pre><code>def build_model(hidden_layer_sizes):
  model = Sequential()

  model.add(Dense(hidden_layer_sizes[0], input_dim=2))
  model.add(Activation('tanh'))

  for layer_size in hidden_layer_sizes[1:]:
    model.add(Dense(layer_size))
    model.add(Activation('tanh'))

  model.add(Dense(1))
  model.add(Activation('sigmoid'))

  return model
</code></pre>

<p>If you take this approach, you may find you end up parametrising other choices such as input size (as you try some feature engineering), hidden layer activation function, whether to use a Dropout layer etc. Again, it is this need to define all the other choices in a typical network that lead to Keras' design. The best you can do is compress down the choices for your case, with a custom function.</p>

<p>I'd like to address this comment in your question:</p>

<blockquote>
  <p>Although the network above is quite small, the current implementation may become frustrating with deeper networks.</p>
</blockquote>

<p>In practice, I have not found Keras' design difficult to use for deep networks. I typically write a separate build function, and parametrise a few things, such as input dimensions. However, I typically <em>don't</em> loop through a list of different layer sizes as the main param of the build function. Instead, I find the more verbose approach just fine, even when trying variations of network size/shape (I guess that might change if I wanted to grid search including layer sizes). I think that is because I find the function names very easy to read - even with a screen full of <code>.add()</code> functions, I can see quite quickly what the NN structure is.</p>
","1","2","836","25573"
"19463","<p>It looks like you are training this as a multiclass classifier, to represent a binary choice. In which case, your <code>Y</code> value is wrong:</p>

<pre><code>Y = [[0, 1],
    [1, 1],
    [1, 0],
    [0, 1]]
</code></pre>

<p>Here your second label is not self-consistent, and thus it is impossible to predict using a softmax output layer (where the sum of all outputs must equal 1). The best it can do is <code>[0.5, 0.5]</code> to match that label and you can see actually it got close to that in your test.</p>

<p>You want this instead:</p>

<pre><code>Y = [[0, 1],
    [1, 0],
    [1, 0],
    [0, 1]]
</code></pre>

<hr>

<p>A few asides . . .</p>

<ul>
<li><p>Your example inputs all have the same third column (=1). This is redundant data, and you could drop it.</p></li>
<li><p>Your network is more complex than it needs to be for this task. A single hidden layer with only a few neurons in it should be sufficient.</p></li>
<li><p>For this specific task you could have chosen a single output neuron using a sigmoid activation (and need only one column in Y).</p></li>
</ul>
","1","2","836","25573"
"19581","<p>Your weights have diverged during training, and the network as a result is essentially broken. As it consists of ReLUs, I expect the huge loss in the first epoch caused an update which has zeroed out most of the ReLU activations. This is known as the <a href=""https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks"">dying ReLU problem</a>, although the issue here is not necessarily the choice of ReLU, you will probably get similar problems with other activations in the hidden layers.</p>

<p>You need to tone down some of the numbers that might be causing such a large initial loss, and maybe also make the weight updates smaller:</p>

<ul>
<li><p>Normalise your input data. The autoencoder is trying to match the input, and if the numbers are large here, this multiplies up to a large loss. If the input can have negative values (either naturally or due to the normalisation) then you should not have ReLU activation in the output layer otherwise it is not possible for the autoencoder to match the input and output values - in that case just have a linear output layer.</p></li>
<li><p>Reduce the learning rate - in Keras SGD has default <code>lr=0.01</code>, try lower e.g. <code>lr=0.0001</code>. Also consider a more sophisticated optimiser than plain SGD, maybe Adam, Adagrad or RMSProp.</p></li>
<li><p>Add some conservative weight initialisations. In Keras you can set the weight initialiser - see <a href=""https://keras.io/initializers/"" rel=""noreferrer"">https://keras.io/initializers/</a> - however, the default <code>glorot_uniform</code> should already be OK in your case, so maybe you will not need to do this.</p></li>
</ul>
","12","2","836","25573"
"19621","<blockquote>
  <p>What piece of knowledge am I missing here to use this dataset?</p>
</blockquote>

<p>By anonymising the data and altering the attribute names, the providers of this dataset have made it into an abstract machine learning exercise, where data has similar <em>qualities</em> to real-world credit approval data, so that approaches to train models that work well on it are likely to work well in a real-world scenario. However, those models could not be used in a production system. You are not in a position to train a model based on it and then input real world data you have collected elsewhere to make a prediction.</p>

<p>You can still train a model on this data to predict A16 (the class attribute). You can measure the accuracy - or any other metric - by holding out a test set. You can experiment with feature engineering, feature selection etc, in an abstract way without being able to apply much domain knowledge. You can try out different model classes, different hyper-parameters, different approaches to imputing missing values or cross-validation etc. </p>

<p>Your project can draw conclusions about the approach you have taken, based on test results. What you cannot do is explore real-world scenarios with simulated customer data. This might make it less compelling as a demonstration - for instance users could not explore exactly what factors might lead to an application being approved or not, instead you would just have to show some graphs that demonstrate you have selected a model with good performance.</p>
","2","2","836","25573"
"19694","<p>You have created a model pipeline and must run all trained models (""lower level"" ones first) in order to make a prediction on new data using the stack.</p>

<p>With test data set, it is slightly easier, since you can store the predictions from the ""level 1"" models when testing them, and only run the final model across this stored data.</p>

<p>In addition to your brief description, <em>usually</em> to avoid bias from re-using training data, you would use k-fold cross-validation or similar mechanism, and your training data for the final model should be the cv predictions from each model. You do not want to use the training predictions from those models, because they are likely to be overfit whilst ""level 1"" test and production predictions will not be, and this would introduce population differences between train and test data in your ""level 2"" model.</p>

<p>It is also quite a common variation to use the M new features from your ""level 1"" models alongside some or all of the original features. This gives the meta model more data to base its decision on when deciding the relative weights between the first stage models (assuming this top-level model is non-linear).</p>
","4","2","836","25573"
"19706","<p>Typically you would use a <a href=""https://en.wikipedia.org/wiki/Perplexity"" rel=""noreferrer"">perplexity</a> value. For example, if your LSTM model is word-based and you have a sentence $[x_1, x_2 . . . x_N]$, and your model predicts the words that appear in that sentence with probability $p(x_i|x_0..x_{i-1})$ (where $x_0$ is a ""start token"" or whatever you use to start your RNN prediction sequence). Then you might quote a per-word perplexity for that sentence under your model as</p>

<h2>$- \frac{1}{N}\sum_{i=1}^N log_2(p(x_i|x_0..x_{i-1}))$</h2>

<p>Using an LSTM to predict consecutive words, it is practical to construct an array of probabilities $[p_1, p_2 . . . p_N]$ by running the network on the sentence and noting the probabilities for the correct matching word - i.e. $p_i = $ the predicted probability of the correct class $x_i$ at each step, which simplifies the expression:</p>

<h2>$- \frac{1}{N}\sum_{i=1}^N log_2(p_i)$</h2>
","6","2","836","25573"
"19777","<p>The reason is that the two metrics, <a href=""https://en.wikipedia.org/wiki/Mean_absolute_percentage_error"" rel=""noreferrer"">mean absolute percentage error</a> (MAPE) and <a href=""https://en.wikipedia.org/wiki/Mean_squared_error"" rel=""noreferrer"">mean square error</a> (MSE) are optimising to different targets. Improving one can be done at the expense of the other.</p>

<p>As a simple example, consider this data:</p>

<pre><code>x = [ 0,  1,  2,  3,  4,  5]
y = [ 3,  5, 10, 10, 11, 15]
</code></pre>

<p>The best fit mean squared error (MSE) for a line on this data is $\hat{y} = 2.23x + 3.43$, which has MSE of $1.18$, and a mean absolute percentage error (MAPE) of $11.0$%.</p>

<p>The best fit mean absolute percentage loss for a line on this data is $\hat{y} = 2.35x + 2.99$, which has a MSE of $1.24$, and a MAPE of $8.34$%. </p>

<p>You can see that optimising for MAPE gives a worse MSE, and vice-versa.</p>

<p>The difference can get extreme when there is a large range for y values (in terms of orders of magnitude covered), because optimising for MAPE will favour being more accurate on small values at the expense of larger ones. So if we change y to be:</p>

<pre><code>y = [ 1,  2, 10, 10, 11, 20]
</code></pre>

<p>Then optimising mean abs percentage gives the line $\hat{y} = 3.78x + 1.05$ with MSE $7.09$ and MAPE $21.9$%. But optimising for mean square error gives $\hat{y} = 3.49x + 0.286$ with MSE  $4.56$ and MAPE $62.9$% - this is a larger difference, and I suspect that your data has a large range of target variable causing a similar effect.</p>

<hr>

<p>You can potentially get closer results using the Least Squares regressor by using a transformed target variable $z = log(y)$ and transforming back at the end. This still won't be quite the same, but it does reduce the difference significantly - in my last example if I try this, I get MAPE $24.2$% - compared to $21.9$% for optimising MAPE directly.</p>
","5","2","836","25573"
"19826","<ol>
<li><p>The sigmoid range $(0,1)$ is technically open, because no input value maps to $0$ or $1$. You can get arbitrarily close to $0$ or $1$ but never equal them.</p></li>
<li><p>Taking an exponent followed by a log can cause overflow issues during computation, because exponents grow quickly. Sigmoid also gets close to zero quickly and this can underflow (potentially rounding to 0). However, the end result of the combined function is likely to be something within normal calculation range, because of the log (this is similar to - but even more extreme - multiplying and dividing by a very large number). </p></li>
</ol>

<p>Although your quote suggests:</p>

<blockquote>
  <p>it is best to write the negative log-likelihood as a function of z</p>
</blockquote>

<p>There is no analytical form where $E = -log(\sigma(z))$ can be re-written as a simple function of z. In theory, it can be calculated numerically by expanding terms carefully (beyond me, so won't show here), and some libraries may include this kind of expansion. In practice, to avoid numerical instability, many machine learning libraries will simply cap the value like this: $E = -log( max(\epsilon, \sigma(z)) )$ with $\epsilon$ a small number, maybe $10^{-15}$</p>

<p>A more common issue where the calculations are done accurately in neural networks is when considering the gradient of a loss function. If you use a sigmoid output layer alongside a binary cross-entropy cost function $E = -(y log(\sigma(z)) + (1-y)log(1-\sigma(z))) $, then some of the terms cancel out, and then the gradient contribution from training is trivially $\frac{\partial{E}}{\partial z} = y - \sigma(z)$ - there is no need in that case to calculate any log values despite them being in the loss function.</p>
","1","2","836","25573"
"19862","<p>The book quotes in your question are the outline of a proof that shows single output sigmoid represents the same class probabilities as a two dimensional softmax.</p>

<blockquote>
  <ol>
  <li>What does it mean by ""impose a requirement that one element of $z$ be fixed."" I understand you can get the $n^{th}$ one if you know n-1 but ""fixed""?</li>
  </ol>
</blockquote>

<p>It doesn't mean the value has to be fixed. It means that we <em>can</em> choose any $z_n$ value, and have the other's adjusted to represent any valid target probabilities. So for argument's sake, <em>we</em> can pick any fixed value.</p>

<blockquote>
  <ol start=""2"">
  <li>How is it equivalent to having $z_1=0$?</li>
  </ol>
</blockquote>

<p>It is an equivalence between $\sigma(z)$ and $softmax(z)_1$ with $z_1 = 0$:</p>

<h2>$\sigma(z) = \frac{1}{1 + e^{-z}}$</h2>

<p>For softmax, this is the function for the first element from a 2-dimensional output:</p>

<h2>$softmax(z)_1 = \frac{e^{z_1}}{\sum_{i=1}^{2}e^{z_i}} = \frac{e^{z_1}}{e^{z_1} + e^{z_2}}$</h2>

<p>But we have set $z_1 = 0$, and $e^0 = 1$ so:</p>

<h2>$softmax(z)_1 = \frac{1}{1 + e^{z_2}}$</h2>

<p>This is the same function as $\sigma(-z_2)$, so the two functions have identical form (if you plot them, they overlap), although the parameters drive the specific value differently. </p>

<p>Note that as per step 1, we have not lost anything about the general behaviour by setting $z_1 = 0$, we just did it to simplify the equation. In fact it is possible to make a <a href=""https://stats.stackexchange.com/a/254071/43775"">very similar argument without fixing any $z$ values</a>, and you end up with a more generic linear combination of $z_1$ and $z_2$ in the softmax function being equivalent to just $z$ in $\sigma(z)$. </p>
","1","2","836","25573"
"19883","<p>I have replicated your results using Keras, and got very similar numbers so I don't think you are doing anything wrong.</p>

<p>Out of interest, I ran for many more epochs to see what would happen. The accuracy of test and train results remained pretty stable. However, the loss values drifted further apart over time. After 10 epochs or so, I was getting 100% train accuracy, 94.3% test accuracy - with loss values around 0.01 and 0.22 respectively. After 20,000 epochs, the accuracies had barely changed, but I had training loss 0.000005 and test loss 0.36. The losses were also still diverging, albeit very slowly. In my opinion, the network is clearly over-fitting.</p>

<p>So the question could be re-phrased: Why, despite over-fitting, does a neural network trained to the MNIST data set still generalise apparently reasonably well in terms of accuracy? </p>

<p>It is worth comparing this 94.3% accuracy with what is possible using more naive approaches.</p>

<p>For instance, a simple linear softmax regression (essentially the same neural network without the hidden layers), gives a quick stable accuracy of 95.1% train, and 90.7% test. This shows that a lot of the data separates linearly - you can draw hyperplanes in the 784 dimensions and 90% of the digit images will sit inside the correct ""box"" with no further refinement required. From this, you might expect an overfit non-linear solution to get a worse result than 90%, but maybe no worse than 80% because intuitively forming an over-complex boundary around e.g. a ""5"" found inside the box for ""3"" will only incorrectly assign a small amount of this naive 3 manifold. But we're better than this 80% lower bound guesstimate from the linear model.</p>

<p>Another possible naive model is template matching, or nearest-neighbour. This is a reasonable analogy to what the over-fitting is doing - it creates a local area close to each training example where it will predict the same class. Problems with over-fitting occur in the space in-between where the values of activation will follow whatever the network ""naturally"" does. Note the worst case, and what you often see in explanatory diagrams, would be some highly curved almost-chaotic surface which travels through other classifications. But actually it may be more natural for the neural network to more smoothly interpolate between points - what it actually does depends on the nature of the higher order curves that the network combines into approximations, and how well those already fit to the data.</p>

<p>I borrowed the code for a KNN solution from <a href=""https://medium.com/towards-data-science/mnist-with-k-nearest-neighbors-8f6e7003fab7"" rel=""nofollow noreferrer"">this blog on MNIST with K Nearest Neighbours</a>. Using k=1 - i.e. choosing the label of the nearest from the 6000 training examples just by matching pixel values, gives an accuracy of 91%. The 3% extra that the over-trained neural network achieves does not seem quite so impressive given the simplicity of pixel-match counting that KNN with k=1 is doing.</p>

<p>I tried a few variations of network architecture, different activation functions, different number and sizes of layers - none using regularisation. However, with 6000 training examples, I could not get any of them to overfit in a way where test accuracy dropped dramatically. Even reducing to just 600 training examples just made the plateau lower, at ~86% accuracy. </p>

<p>My basic conclusion is that MNIST examples have relatively smooth transitions between classes in feature space, and that neural networks can fit to these and interpolate between the classes in a ""natural"" manner given NN building blocks for function approximation - without adding high frequency components to the approximation that could cause issues in an overfit scenario.</p>

<p>It might be an interesting experiment to try with a ""noisy MNIST"" set where an amount of random noise or distortion is added to both training and test examples. Regularized models would be expected to perform OK on this dataset, but perhaps in that scenario the over-fitting would cause more obvious problems with accuracy.</p>

<hr>

<p><em>This is from before the update with further tests by OP.</em></p>

<p>From your comments, you say that your test results are all taken after running a single epoch. You have essentially used early stopping, despite writing that you have not, because you have stopped the training at the earliest possible point given your training data.</p>

<p>I would suggest running for many more epochs if you want to see how the network is truly converging. Start with 10 epochs, consider going up to 100. One epoch is not many for this problem, especially on 6000 samples. </p>

<p>Although increasing number of iterations is not guaranteed to make your network overfit worse than it already has, you haven't really given it much of a chance, and your experimental results so far are not conclusive.</p>

<p>In fact I would half expect your test data results to <em>improve</em> following a 2nd, 3rd epoch, before starting to fall away from the training metrics as the epoch numbers increase. I would also expect your training error to approach 0% as the network approached convergence.</p>
","7","2","836","25573"
"19973","<p>The issue with linear regression in your case is not whether it can be used - it can nearly always be used to build a predictive regression model - it is whether the resulting model is useful, usually whether it is accurate enough for the intended use.</p>

<p>The accuracy and utility of your model does not depend only on the nature of $\mathbf{x}$. Instead it depends on the true nature of the $\mathbf{x} \rightarrow y$ relationship. Your measurements and model are an approximation of that relationship. Linear models are fast and stable to compute, but can be limited if the true relationship being approximated is non-linear.</p>

<p>Here are some basic thoughts/feedback on your questions:</p>

<blockquote>
  <ol>
  <li>Can Linear Regression work for such datasets? </li>
  </ol>
</blockquote>

<p>Yes it can work. Will it be good enough? You will know after testing.</p>

<blockquote>
  <p>What are other models I can use?</p>
</blockquote>

<p>Almost any regression model class could be applied to this problem. For a quick comparison, to see if a non-linear model will make more accurate predictions for you, then you could try an easy-to-apply model from an existing library, <a href=""https://github.com/dmlc/xgboost/tree/master/demo/regression"" rel=""noreferrer"">such as XGBoost</a> (which has a stand-alone command-line version).</p>

<blockquote>
  <ol start=""2"">
  <li>Should I drop one feature (some $x_{i,d}$) from each data record as I can easily calculate the value of dropped feature by using 1 - summing prob(remainingfeatures).</li>
  </ol>
</blockquote>

<p>For a linear model, your intuition is correct that one feature is redundant, as it is itself a linear combination of the other features, so there is no impact to removing it. You could remove any single column. You should not need to though, and some model classes might work better if left in. As always, experiment if you are not sure, and use hold-out test data to verify your ideas (or even better a separate cross-validation set before final test stage)</p>

<blockquote>
  <ol start=""3"">
  <li>As I use Linear Regression all independent variables ($x_{i,0}$, $x_{i,d}$)'s are &lt; 1 (because of the probability), the dependent variable is not. Should I use $log(y_i)$ instead (because weights $W$ for $\hat{y}_i = W\mathbf{x}_i$ calculated would be very large)? </li>
  </ol>
</blockquote>

<p>For linear regression this scaling is not very important. However, there are a few caveats:</p>

<ul>
<li><p>If you measure success by an error metric (such as mean square error), then to compare different ideas fairly, you should report the test error in the same way - e.g. you should scale back the $log(y)$-based predictions and calculate error on the same $(y - \hat{y})^2$ each time.</p></li>
<li><p>For some model classes, they could work better if the target variable is within a certain range. You should check documentation for this.</p></li>
<li><p>How useful the mapping is depends on the true nature of the relationship between $\mathbf{x}$ and $y$. Specifically for a linear model, you have completely changed the line you are trying to fit - it could as a result be a lot better or lot worse, but that is not dependent just on $\mathbf{x}$. So again you will have to give it a try and test the results.</p></li>
</ul>
","6","2","836","25573"
"19989","<p>There is unlikely to be any useful pattern analysis for this problem. </p>

<p>I cannot prove it, but I think it highly likely that the raindrops are being generated using a pseudo-random process. Even if they are not, you have been given more than enough information in what your controller can ""sense"" in this simulation, so that predicting future raindrops is not necessary.</p>

<p>Instead, this is a problem of optimal control and planning. In machine learning, often reinforcement learning can be used to solve problems like this - potentially <a href=""https://en.wikipedia.org/wiki/Q-learning"" rel=""nofollow noreferrer"">Q-Learning</a> would work here, but it would be quite hard to implement it so that it learned quickly due to the large number of states. You would need to use a function approximator like a neural network, and although that can be made to work nicely here, it does not seem necessary.</p>

<p>I don't think you need any Data Science technique here, this is closer to more traditional <a href=""https://en.wikipedia.org/wiki/Automated_planning_and_scheduling"" rel=""nofollow noreferrer"">AI planning</a>. However, there are strong links between planning and learning models, so what I am going to suggest is also something you might see in game-playing systems that use a combination of machine learning (reinforcement learning) and planning.</p>

<p>A quick analysis shows that you have a large number of visible states ($2^{42}$) which would take a long time to produce optimal rules for using the simplest reinforcement learning algorithms - although it is feasible. However, you also have perfect knowledge of the dynamics and a fully deterministic system within what you can see. In addition, the branching factor of your action decisions is not high, just 3 (and sometimes 2) per step - so for instance looking 8 steps ahead will involve checking under 10,000 scenarios. That seems possible to do on every time step, which immediately suggests a simple search-based planning algorithm - similar to <a href=""https://en.wikipedia.org/wiki/Monte_Carlo_tree_search"" rel=""nofollow noreferrer"">Monte Carlo Tree Search</a>, except in your case you can just brute-force all combinations up to an arbitrary horizon. As long as the horizon is far enough ahead, you can achieve optimal results this way, because it only takes 3 steps to fully traverse the different positions the collector can be in.</p>

<ul>
<li><p>On each time step:</p>

<ul>
<li><p>Observe the current state and position of incoming raindrops</p></li>
<li><p>One by one, generate feasible chains of actions, working through all possibilities, some number of steps ahead (I'm suggesting 8 steps, but wonder if you can get away with less, e.g. just 5).</p>

<ul>
<li><p>If the sequence of actions would take the controller outside of the allowed area, discard it.</p></li>
<li><p>Score the sequence of actions by counting how many raindrops you predict it will collect, based on the information about raindrops at each timestep.</p></li>
<li><p>Keep a record of the best sequence of actions so far.</p></li>
</ul></li>
<li><p>Using the best sequence of actions so far, choose the first action in that sequence, and take that action ""for real"" in the simulation, advancing by one time step.</p></li>
<li><p>Discard the rest of the sequence, even though your controller really might take that path, ready for next phase of planning. In the simplest cases you don't revise the plan using clever tree pruning techniques, you just brute force another search after each update. A more advanced algorithm may be able to preserve the most likely sequences to go forward to next time step, and save computation (at the cost of complexity and memory used to store candidates).</p></li>
</ul></li>
</ul>
","2","2","836","25573"
"20046","<blockquote>
  <p>Is it possible to use data generated by a huge number of simulations to train a classification algorithm to perform this detection online? </p>
</blockquote>

<p>Yes, it is always possible to train a classification algorithm when you have labeled <a href=""https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables"" rel=""noreferrer"">i.i.d.</a> training data, and there is no hard reason why you cannot use a simulator to generate that.</p>

<p>Whether or not such a trained model is fit for purpose is hard to say in advance of trying it.</p>

<p>Using a simulation as your data source has some benefits:</p>

<ul>
<li><p>Generating more training and test data is straightforward. </p></li>
<li><p>You will automatically have high quality ground truth labels (assuming your goal is to match the simulation).</p></li>
<li><p>If you find a problem with certain parameter values, you can target them when collecting more training data.</p></li>
</ul>

<p>Just as with data taken from real world measurements, you will need to test your results to get a sense of how accurate your model is. </p>

<blockquote>
  <p>What are the considerations when using simulated data to train an algorithm that will then be used online with real data (expect from the obvious that the simulation needs to be very very accurate)? </p>
</blockquote>

<ul>
<li><p>Your model is a function approximator. At best it will match the output of the simulator. In practice it will usually fall short of it by some amount. You will have to measure this difference by testing the model, and decide whether the cost of occasional false negative or false positive is outweighed by the performance improvement. </p></li>
<li><p>Statistical machine learning models perform best when interpolating between data points, and often perform badly at extrapolating. So when you say that inputs can vary infinitely, hopefully that is within some constrained parameter space of real values, as opposed to getting inputs that are completely different from anything you have considered before - the simulation would cope with such inputs, but a statistics-based function approximator most likely would not.</p></li>
<li><p>If your simulation has areas where the class switches rapidly with small changes in parameter values, then you will need to sample densely in those areas. </p></li>
<li><p>If your simulation produces near chaotic behaviour in any region (class value varies a lot and is highly sensitive to small changes in value of one or more parameters), then this is something that is very hard to approximate.</p></li>
<li><p>If you have some natural scale factor, dimensionless number or other easy to compute summary of behaviour in your physical system, it may be worth using it as an engineered feature instead of getting the machine learning code to figure that out statistically. For instance, in fluid dynamics, the <a href=""https://en.wikipedia.org/wiki/Reynolds_number"" rel=""noreferrer"">Reynolds number</a> can characterise flow, and could be useful feature for neural network predicting vortex shedding.</p></li>
</ul>

<blockquote>
  <p>Any references to such examples?</p>
</blockquote>

<p>The examples I have found here are about are in renderings of fluid simulations and other complex physical systems where a full simulation can be approximated and they all use neural networks to achieve a speed improvement over full simulation.</p>

<ul>
<li><p><a href=""http://cims.nyu.edu/~schlacht/CNNFluids.htm"" rel=""noreferrer"">Accelerating Eulerian Fluid Simulation With Convolutional Networks</a> - I saw a video about this on YouTube's <a href=""https://www.youtube.com/user/keeroyz/featured"" rel=""noreferrer"">Two Minute Papers</a> channel.</p></li>
<li><p><a href=""http://polar.ncep.noaa.gov/mmab/papers/tn265/MMAB265.pdf"" rel=""noreferrer"">Using neural networks in weather prediction ensembles to improve performance</a></p></li>
<li><p><a href=""https://papers.nips.cc/paper/1562-fast-neural-network-emulation-of-dynamical-systems-for-computer-animation.pdf"" rel=""noreferrer"">Fast Neural Network Emulation of Dynamical Systems for Computer Animation</a> </p></li>
</ul>

<p>However, I don't think any of these are classifiers.</p>
","5","2","836","25573"
"20091","<p>Logits are the pre-transform values in a layer, and are not compared directly to the labels when calculating the cost function. In fact, with softmax layer, setting the matching logit to 1, and the rest to zero isn't even that good, because (using <span class=""math-container"">$z_i$</span> as logit <span class=""math-container"">$i$</span> and <span class=""math-container"">$a_i$</span> as activation <span class=""math-container"">$i$</span>):</p>

<p><span class=""math-container"">$$a_i = \frac{e^{z_i}}{\sum_{\forall j} e^{z_j}}$$</span></p>

<p>Substitute in your 0 and 1 values for <span class=""math-container"">$z_i$</span> and you will see the <span class=""math-container"">$a$</span> values are not that close to 0 and 1 (they would be 0.0853 and 0.2319 respectively for 10 classes). It is these activation values that feed into the loss calculation. So with typical multiclass loss function <span class=""math-container"">$l = -\sum_{\forall i} y_i log(a_i)$</span>, you would score a loss of 1.4611 for such an example.</p>

<p>To get a very low cost, instead set the logit matching the true label (or 1) to a higher value, e.g. 10, and the logits matching the false value (or zero) to a low value e.g. -10. This would lead to </p>

<p><span class=""math-container"">$$a_i = \frac{e^{z_i}}{\sum_{\forall j} e^{z_j}}$$</span></p>

<p>Where </p>

<p><span class=""math-container"">$$z_i = \{10\; if\;1\;\; |\;\;-10\;if\;0\}$$</span></p>

<p>And varies between 10 and -10 for values between 1 and 0.</p>

<p>That should significantly reduce the calculated loss (1.855e-08 for those values in a 10 class prediction). The logit allows us to 'stretch' the values for correct and incorrect predictions. </p>

<p>Getting an actual 0 loss when using softmax or sigmoid outputs and logloss is not technically possible. You might see it due to rounding in computation. However, neither sigmoid nor softmax equals exactly 0 or 1, except in the limit as input values tend to <span class=""math-container"">$\pm \infty$</span></p>
","3","2","836","25573"
"20142","<p>The bias term is very simple, which is why you often don't see it calculated. In fact</p>

<p><code>db2 = dz2</code></p>

<p>So your update rules for bias on a <em>single item</em> are:</p>

<p><code>b2 += -alpha * dz2</code></p>

<p>and</p>

<p><code>b1 += -alpha * dz1</code></p>

<p>In terms of the maths, if your loss is $J$, and you know $\frac{\partial J}{\partial z_i}$ for a given neuron $i$ which has bias term $b_i$ . . .</p>

<p>$$\frac{\partial J}{\partial b_i} = \frac{\partial J}{\partial z_i} \frac{\partial z_i}{\partial b_i}$$</p>

<p>and </p>

<p>$$\frac{\partial z_i}{\partial b_i} = 1$$</p>

<p>because $z_i = (\text{something unaffected by } b_i) + b_i$</p>

<hr>

<p>It looks like the code you copied uses the form</p>

<pre><code>db2=np.sum(dz2,axis=0,keepdims=True)
</code></pre>

<p>because the network is designed to process examples in (mini-)batches, and you therefore have gradients calculated for more than one example at a time. The sum is squashing the results down to a single update. This would be easier to confirm if you also showed update code for weights.</p>
","10","2","836","25573"
"20175","<p>There are a few mistakes in the code, so I am going to present a revised version here with comments.</p>

<h2>Setup</h2>

<pre><code>import numpy as np
import pandas as pd
x=np.array([[0,0],[0,1],[1,0],[1,1]])
y=np.array([[0],[1],[1],[0]])
np.random.seed(0)

# Optional, but a good idea to have +ve and -ve weights
theta1=np.random.rand(2,8)-0.5
theta2=np.random.rand(8,1)-0.5

# Necessary - the bias terms should have same number of dimensions
# as the layer. For some reason you had one bias vector per example.
# (You could still use np.random.rand(8) and np.random.rand(1))
b1=np.zeros(8)
b2=np.zeros(1)

alpha=0.01
# Regularisation not necessary for XOR, because you have a complete training set.
# You could have lamda=0.0, but I have left a value here just to show it works.
lamda=0.001
</code></pre>

<h2>Training - Forward propagation</h2>

<pre><code># More iterations than you might think! This is because we have
# so little training data, we need to repeat it a lot.
for i in range(1,40000):
    z1=x.dot(theta1)+b1
    h1=1/(1+np.exp(-z1))
    z2=h1.dot(theta2)+b2
    h2=1/(1+np.exp(-z2))
</code></pre>

<h2>Training - Back propagation</h2>

<pre><code>    # This dz term assumes binary cross-entropy loss
    dz2 = h2-y 
    # You could also have stuck with squared error loss, the extra h2 terms
    # are the derivative of the sigmoid transfer function. 
    # It converges slower though:
    # dz2 = (h2-y) * h2 * (1-h2)

    # This is just the same as you had before, but with less temp variables
    dw2 = np.dot(h1.T, dz2)
    db2 = np.sum(dz2, axis=0)

    # The derivative of sigmoid is h1 * (1-h1), NOT dh1*(1-dh1)
    dz1 = np.dot(dz2, theta2.T) * h1 * (1-h1)
    dw1 = np.dot(x.T, dz1)
    db1 = np.sum(dz1, axis=0)

    # The L2 regularisation terms ADD to the gradients of the weights
    dw2 += lamda * theta2
    dw1 += lamda * theta1

    theta1 += -alpha * dw1
    theta2 += -alpha * dw2

    b1 += -alpha * db1
    b2 += -alpha * db2
</code></pre>

<h2>Prediction</h2>

<p>This is where you can kick yourself, you forgot to use the biases!</p>

<pre><code>input1=np.array([[0,0],[1,1],[0,1],[1,0]])
z1=np.dot(input1,theta1)+b1
h1=1/(1+np.exp(-z1))
z2=np.dot(h1,theta2)+b2
h2=1/(1+np.exp(-z2))

print(h2)
</code></pre>

<p>When I run the above code I get a correct-looking output</p>

<pre><code>[[ 0.01031446]
 [ 0.0201576 ]
 [ 0.9824826 ]
 [ 0.98584079]]
</code></pre>

<hr>

<p>In summary your three big errors were the wrong dimension for the bias vectors in setup, incorrect derivatives for sigmoid function (using correct form, but with wrong variable) and forgetting to use bias at all when predicting at the end. Other details are still worth noting, but would not have prevented you getting something working.</p>
","0","2","836","25573"
"20180","<p>It is not possible to have general rotationally-invariant neural network architecture for a CNN*. In fact CNNs are not strongly translation invariant, except due to pooling - instead they combine a little bit of translation invariance with <a href=""https://datascience.stackexchange.com/questions/16060/difference-between-equivariant-to-translation-and-invariant-to-translation"">translation equivariance</a>. There is no equivalent to pooling layers that would reduce the effect of rotation this way (although for very small rotations the translation invariance will still help).</p>

<p>You can however construct features and create a pipeline that reduces incoming rotation differences in your inputs. For example, <a href=""https://dsp.stackexchange.com/questions/4893/scale-and-rotation-invariant-feature-descriptors"">this answer on Signal Processing Stack Exchange</a> suggests calculating dominant gradient of an image, then rotating the image so that this is always oriented the same way before further processing. If your image has strong straight edges, you could do similar by detecting those (e.g. by using a <a href=""https://en.wikipedia.org/wiki/Hough_transform"" rel=""nofollow noreferrer"">Hough transform</a>) and rotating the input so that these are always oriented the same way. These approaches work only within certain image tasks, but can save on processing time and potentially increase accuracy if they are possible. Effectively they are a form of input normalisation.</p>

<p>A more radical idea might be to perform a map to polar co-ordinates in your image before processing. This would effectively convert rotational (and radial) variance into translation variance in your image. A CNN processing this mapped image would effectively convert its translation invariance into rotational invariance on the original unmapped image. But the cost would be losing all translation invariance, so only worth considering if your inputs have high variance in rotation but low variance in translation.</p>

<hr>

<p>* Never say never. This is caveated by special cases, for example the paper <a href=""https://arxiv.org/abs/1602.07576"" rel=""nofollow noreferrer"">Group Equivariant Convolutional Networks</a> explains an architecture that adds support for multiples of 90 degree rotation (taking advantage of grid structure in computer images and the weight matrices that construct neural network layers). However, if you want to support free rotation of values other than 90, 180, 270 degrees, then as far as I know, there is no way to do that architecturally <em>within</em> the network.</p>
","3","2","836","25573"
"20230","<p>Sadly there is no generic way to determine <em>a priori</em> the best number of neurons and number of layers for a neural network, given just a problem description. There isn't even much guidance to be had determining good values to try as a starting point.</p>

<p>The most common approach seems to be to start with a rough guess based on prior experience about networks used on similar problems. This could be your own experience, or second/third-hand experience you have picked up from a training course, blog or research paper. Then try some variations, and check the performance carefully before picking a best one.</p>

<p>The size and depth of neural networks interact with other <em>hyper-paramaters</em> too, so that changing one thing elsewhere can affect where the best values are. So it is not possible to isolate a ""best"" size and depth for a network then continue to tune other parameters in isolation. For instance, if you have a very deep network, it may work efficiently with the ReLU activation function, but not so well with sigmoid - if you found the best size/shape of network and then tried an experiment with varying activation functions you may come to the wrong conclusion about what works best.</p>

<p>You may sometimes read about ""rules of thumb"" that researchers use when starting a neural network design from scratch. These things might work for your problems or not, but they at least have the advantage of making a start on the problem. The variations I have seen are:</p>

<ul>
<li><p>Create a network with hidden layers similar size order to the input, and all the same size, on the grounds that there is no particular reason to vary the size (unless you are creating an autoencoder perhaps).</p></li>
<li><p>Start simple and build up complexity to see what improves a simple network.</p></li>
<li><p>Try varying depths of network if you expect the output to be explained well by the input data, but with a complex relationship (as opposed to just inherently noisy).</p></li>
<li><p>Try adding some dropout, it's the closest thing neural networks have to magic fairy dust that makes everything better (caveat: adding dropout may improve generalisation, but may also increase required layer sizes and training times). </p></li>
</ul>

<p>If you read these or anything like them in any text, then take them with a pinch of salt. However, at worst they help you get past the blank page effect, and write some kind of network, and get you to start the testing and refinement process.</p>

<hr>

<p>As an aside, try not to get too lost in tuning a neural network when some other approach might be better and save you lots of time. Do consider and use other machine learning and data science approaches. Explore the data, maybe make some plots. Try some simple linear approaches first to get benchmarks to beat, linear regression, logistic regression or softmax regression depending on your problem. Consider using a different ML algorithm to NNs - decision tree based approaches such as XGBoost can be faster and more effective than deep learning on many problems.</p>
","27","2","836","25573"
"20253","<h3>A note on gradient direction</h3>

<p>As an aside, </p>

<pre><code>layer_2_error = Y - layer_2
</code></pre>

<p>Should be</p>

<pre><code>layer_2_error = layer_2 - Y
</code></pre>

<p>And all your update functions should be gradient <em>descent</em> e.g. </p>

<pre><code>weights_2 += -alpha * layer_1.T.dot(layer_2_delta)
</code></pre>

<p>This makes no difference to the performance of your network, but I assume you have done this in the rest of the answer.</p>

<h2>Scaling Input to Normalised Values</h2>

<p>One clue to performance problems was in your first version which included the code:</p>

<pre><code>X = preprocessing.scale(X)
</code></pre>

<p>With this included before training, then the inputs were scaled nicely for working with neural networks and the network converged quickly. Without this, then the network will still operate, but converges much more slowly. Increasing the max iterations to 1,000,000, then I get the result:</p>

<blockquote>
  <p>Goal reached after 209237 iterations: [ 0.00099997] is smaller than the goal of 0.001</p>
</blockquote>

<p>You mention that you don't want to do scale the input, but really in general for NNs you should. It is worth looking at other differences though, because the lack of scaling does not prevent the MLPClassifier from converging.</p>

<h2>Bias Gradients</h2>

<p>This is one you spotted and mentioned in the comments. Your bias update is the same for each bias value. To correct this, you want something like:</p>

<pre><code># Update the bias    
bias_2 += -alpha * numpy.sum(layer_2_delta, axis=0)
bias_1 += -alpha * numpy.sum(layer_1_delta, axis=0)
</code></pre>

<p>NB - I have assumed you have fixed the gradient direction here.</p>

<h2>Classification Loss Function</h2>

<p>MLPClassifier is running a classifier using logloss, which is more efficient loss function that the mean squared error you are using, if your targets are class probabilities. You can use this too, simply by changing:</p>

<pre><code>layer_2_delta = layer_2_error*sigmoid_derivative(layer_2)
</code></pre>

<p>To</p>

<pre><code>layer_2_delta = layer_2_error
</code></pre>

<p>This is the correct delta value to match the loss function  </p>

<pre><code>def calculateError(Y, Y_predicted):
    return -numpy.mean( (Y * numpy.log(Y_predicted) + 
       (1-Y)* numpy.log(1 - Y_predicted)))
</code></pre>

<ul>
<li><p>but only when your output layer is sigmoid (the sigmoid derivative cancels out the derivatives of this log function).</p></li>
<li><p>if you use this, you will want to reduce the learning rate for numerical stability. I suggest e.g. <code>alpha = 0.01</code></p></li>
<li><p>note you can still <em>report</em> your other loss function, for comparison with previous results. Just be aware that you are optimising the log loss</p></li>
</ul>

<h2>Difference in Optimisers</h2>

<p>You are running batched Stochastic Gradient Descent, which is the most basic optimiser type. The MLPClassifier is using <a href=""https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam"" rel=""nofollow noreferrer"">Adam</a>, which is faster and more robust. If you want to compete with it on even terms you need to implement a better optimiser. The simplest improvement is probably to add some <a href=""https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum"" rel=""nofollow noreferrer"">momentum</a>.</p>
","2","2","836","25573"
"20259","<p><strong>Limits of numerical accuracy and stability are causing the optimisation routines to struggle.</strong></p>

<p>You can see this most easily by changing the regularisation term to 0.0 - there is no reason why this should not work in principle, and you are not using any feature engineering that particularly needs it. With regularisation set to 0.0, then you will see limits of precision reached and attempts to take log of 0 when calculating the cost function. The two different optimisation routines are affected differently, due to taking different sample points on route to the minimum.</p>

<p>I think that with regularisation term set high, you remove the numerical instability, but at the expense of not seeing what is really going on with the calculations - in effect the regularisation terms become dominant for the difficult training examples.</p>

<p>You can offset some of the accuracy problems by modifying the cost function:</p>

<pre><code>def compute_cost_regularized(theta, X, y, lda):
    reg =lda/(2*len(y)) * np.sum(theta[1:]**2) 
    return reg - 1/len(y) * np.sum(
      y @ np.log( np.maximum(sigmoid(X@theta), 1e-10) ) 
      + (1-y) @ np.log( np.maximum(1-sigmoid(X@theta), 1e-10) ) )
</code></pre>

<p>Also to get some feedback during the training, you can add</p>

<pre><code>                       options = {
                           'disp': True
                       }
</code></pre>

<p>To the call to <code>minimize</code>.</p>

<p>With this change, you can try with regularisation term set to zero. When I do this, I get:</p>

<pre><code>predict_one_vs_all(X_bias, theta_all_optimized_cg)
Out[156]:
94.760000000000005
In [157]:

predict_one_vs_all(X_bias, theta_all_optimized_bfgs)
/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:2: RuntimeWarning: overflow encountered in exp
  from ipykernel import kernelapp as app
Out[157]:
98.839999999999989
</code></pre>

<p>The CG value of 94.76 seems to match the expected result nicely - so I wonder if this was done without regularisation. The BFGS value is still ""better"" although I am not sure how much I trust it given the warning messages during training and evaluation. To tell if this apparently better training result really translates into better digit detection, you would need to measure results on a hold-out test set.</p>
","3","2","836","25573"
"20301","<p>The <a href=""https://en.wikipedia.org/wiki/Cross_entropy"" rel=""noreferrer"">cross entropy</a> formula takes in two distributions, <span class=""math-container"">$p(x)$</span>, the true distribution, and <span class=""math-container"">$q(x)$</span>, the estimated distribution, defined over the discrete variable <span class=""math-container"">$x$</span> and is given by </p>

<h2><span class=""math-container"">$$H(p,q) = -\sum_{\forall x} p(x) \log(q(x))$$</span></h2>

<p>For a neural network, the calculation is independent of the following:</p>

<ul>
<li><p>What kind of layer was used.</p></li>
<li><p>What kind of activation was used - although many activations will not be compatible with the calculation because their outputs are not interpretable as probabilities (i.e., their outputs are negative, greater than 1, or do not sum to 1). Softmax is often used for multiclass classification because it guarantees a well-behaved probability distribution function.</p></li>
</ul>

<p>For a neural network, you will usually see the equation written in a form where <span class=""math-container"">$\mathbf{y}$</span> is the ground truth vector and <span class=""math-container"">$\mathbf{\hat{y}}$</span> (or some other value taken direct from the last layer output) is the estimate. For a single example, it would look like this:</p>

<h2><span class=""math-container"">$$L = - \mathbf{y} \cdot \log(\mathbf{\hat{y}})$$</span></h2>

<p>where <span class=""math-container"">$\cdot$</span> is the inner product.</p>

<p>Your example ground truth <span class=""math-container"">$\mathbf{y}$</span> gives all probability to the first value, and the other values are zero, so we can ignore them, and just use the matching term from your estimates <span class=""math-container"">$\mathbf{\hat{y}}$</span></p>

<p><span class=""math-container"">$L = -(1\times log(0.1) + 0 \times \log(0.5) + ...)$</span></p>

<p><span class=""math-container"">$L = - log(0.1) \approx 2.303$</span></p>

<p>An important point from comments</p>

<blockquote>
  <p>That means, the loss would be same no matter if the predictions are <span class=""math-container"">$[0.1, 0.5, 0.1, 0.1, 0.2]$</span> or <span class=""math-container"">$[0.1, 0.6, 0.1, 0.1, 0.1]$</span>? </p>
</blockquote>

<p>Yes, this is a key feature of multiclass logloss, it rewards/penalises probabilities of correct classes only. The value is independent of how the remaining probability is split between incorrect classes.</p>

<p>You will often see this equation averaged over all examples as a <em>cost</em> function. It is not always strictly adhered to in descriptions, but usually a <em>loss</em> function is lower level and describes how a single instance or component determines an error value, whilst a <em>cost</em> function is higher level and describes how a complete system is evaluated for optimisation. A cost function based on multiclass log loss for data set of size <span class=""math-container"">$N$</span> might look like this:</p>

<h2><span class=""math-container"">$$J = - \frac{1}{N}\left(\sum_{i=1}^{N} \mathbf{y_i} \cdot \log(\mathbf{\hat{y}_i})\right)$$</span></h2>

<p>Many implementations will require your ground truth values to be one-hot encoded (with a single true class), because that allows for some extra optimisation. However, in principle the cross entropy loss can be calculated - and optimised - when this is not the case.</p>
","86","2","836","25573"
"20424","<p>Although you can generate text in this way - sampling from a RNN trained to predict next character or word - it will not be meaningful. At best it will be grammatically accurate (in terms of nouns, verbs, adjectives etc) but semantic nonsense. It would not be a summary, except by lucky accident.</p>

<p>To generate summaries using LSTM directly, you would need to train a network with real examples of summary inputs and outputs. This would be similar task to machine translation, but much harder due to variability in size of the inputs. It is unlikely that you will find enough training data to test the idea fully, and it is not clear that such a direct approach <em>can</em> yield acceptable results even with large amounts of training data.</p>

<p>In fact text summarisation is not a solved problem by any means. There are deep learning approaches, such as <a href=""https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html"" rel=""nofollow noreferrer"">Google Brain team's effort using TensorFlow</a>, which you could study to get some sample code and a sense of state-of-the-art. This approach uses an <em>attention model</em> to extract apparently informational content (i.e. content that would have low probability of appearing in some assumed <em>generic</em> document, thus is assumed to be interesting due to standing out). It is possible to use a trained LSTM to build such an attention-based model - the intuition is that the parts of the document that the already-trained LSTM is least able to predict are likely to contain noteworthy information.</p>
","1","2","836","25573"
"20426","<blockquote>
  <p>However, I don't see anybody doing this so I suspect I'm missing something. What is it?</p>
</blockquote>

<p>You are concentrating on how this extra loss term affects the absolute value of the loss function. This is not really relevant. Many forms of regularisation are not expressed as changes to loss function at all - framing L2 regularisation as part of the loss function is convenient because it allows us to re-use the existing weight adjustment logic. However, functionally it is almost identical to a weight decay term applied per batch independently of loss function, e.g. multiply all weights by 0.99 every batch.</p>

<p>If you use L2 regularisation loss, then you will almost certainly want to track a separate metric such as mean square error (i.e. your original loss function before considering regularisation), when comparing test results between different levels of regularisation.</p>

<p>That said, there is no reason that you cannot have a different L2 weight param per layer, and many frameworks will support this. Sometimes this could result in better generalisation. It is not done much in practice because it adds yet more dimensions to search when optimising hyper-parameters to a problem.</p>

<p>There is no reason expect better performance by aiming for roughly equal loss values from regularisation per layer. However, if that is your goal, then a naive scaling by inverse layer size $\frac{1}{N_{in} \times N_{out}}$ would not do it. That is because the weights in a trained network will already tend to scale due to the training targets so that the mean squared value is proportional to $\frac{1}{N_{in} + N_{out}}$. So your actual scaling factor to make the L2 regularisation loss per layer roughly the same would be $\frac{N_{in} + N_{out}}{N_{in} \times N_{out}}$. In code this might look like:</p>

<pre><code>L2 = lambda * (
  sum((w1) ^2)*(n_in+n_1)/(n_in*n_1) + sum((w2) ^2)*(n_1+n_2)/(n_1*n_2)
)
</code></pre>

<p>(and of course matching changes to loss gradients). Although I will stress again that I do not expect this change to result in better generalisation.</p>
","2","2","836","25573"
"20437","<p>In your link, the author Jean states:</p>

<blockquote>
  <p>Additionally, I believe that if a CNN is trained showing faces only at one corner, during the learning process, the fully-connected layer may become insensitive to faces in other corners.</p>
</blockquote>

<p>I also believe this to be correct. The FCN does not in any way add or improve translation invariance. Instead it will treat all outputs - each individual pixel of the ""feature maps"" - of the last convolutional layer as entirely different features. It must be trained with enough examples in order to generalise well.</p>

<p>However, the feature maps are not themselves simple, clean detectors of objects as the simplified explanation of CNNs might imply. In a deep network they can be very complex and respond to a wide range of stimuli. They will also respond somewhat fuzzily, so that e.g. an eye or the side of a head showing an ear can trigger multiple feature map pixels (to the feature map's kernels, the same object slightly translated will look like distorted version of the same feature, and will still match enough sub-components of the object to trigger a positive response). The last layer will not necessarily detect full objects, but significant chunks of objects, areas of important texture etc. So position can still be quite fluid, and the ""head detector one pixel off in last convolutional layer"" scenario is not particularly realistic - although it may affect relative strength/confidence of predictions.</p>

<p>This can still be a problem if you need your network to generalise. If you suspect that your training data might not be covering enough variations in position, orientation etc of images, then a common approach is data augmentation. As a reflected, rotated or cropped image of an object should usually be classified as the same object, then you can pre-process your training data using those transforms to make many random variations of the input images. Some deep learning frameworks will allow you to do this continuously, generating fresh images for each and every batch.</p>
","1","2","836","25573"
"20498","<p>The problem is using <code>predict_classes</code> in:</p>

<pre><code>model.predict_classes(X_valid)
</code></pre>

<p>this is designed to select the argmax (index of the maximum output) and choose it as the predicted class, for a classifier.</p>

<p>You have a regression problem, and just want the raw output from the network. So instead you should call:</p>

<pre><code>y_pred = model.predict(X_valid)
</code></pre>
","3","2","836","25573"
"20512","<p>The network doesn't store its training progress with respect to training data - this is not part of its state, because at any point you could decide to change what data set to feed it. You could maybe modify it so that it knew about the training data and progress, stored in some tensor somewhere, but that would be unusual. So, in order to do this, you will need to save and make use of additional data <em>outside</em> of the TensorFlow framework.</p>

<p>Probably the simplest thing to do is add the epoch number to the filename. You are already adding the current step within the epoch, so just add in the epoch multiplied:</p>

<pre><code>saver.save(sess, 'my-model', global_step=epoch*1000000+step)
</code></pre>

<p>When you load the file, you can parse the filename to discover what epoch and step you were on and use those as the start point for the <code>xrange</code> functions. To make this easier to re-start from any given checkpoint, you could use <code>argparse</code> to allow your script to take the name of the checkpoint file you want to use.</p>

<p>In brief, it might look like this:</p>

<pre><code># Near top of script
import argparse
import re

# Before main logic
parser = argparse.ArgumentParser()
parser.add_argument('checkpoint')
args = parser.parse_args()

start_epoch = 0
start_step = 0
if args.checkpoint:
    saver.restore(sess, tf.train.latest_checkpoint(args.checkpoint))
    found_num = re.search(r'\d+', args.checkpoint)
    if found_num:
        checkpoint_id = int(found_num.group(0))
        start_epoch = checkpoint_id // 1000000
        start_step = checkpoint_id % 1000000

# Change to xrange:
for epoch in xrange(start_epoch, 25):
    for step in xrange(start_step, 1000000):
        sess.run(..training_op..) # etc

    # At end of epoch loop, you need to re-set steps:
    start_step = 0
</code></pre>

<p>You may want to reduce the number of checkpoints you are creating - as it stands you would have 25,000 checkpoint files generated by your code.</p>

<p>Another option would be use a single checkpoint file, and  to save and restore a Python pickle of a simple <code>dict</code> containing the state at the time you made the checkpoint, with a similar name.</p>
","4","2","836","25573"
"20542","<p>The key part of the quoted text is:</p>

<blockquote>
  <p>To perform experience replay we store the agent's experiences $e_t = (s_t,a_t,r_t,s_{t+1})$</p>
</blockquote>

<p>This means instead of running Q-learning on state/action pairs as they occur during simulation or actual experience, the system stores the data discovered for [state, action, reward, next_state] - typically in a large table. Note this does not store associated values - this is the raw data to feed into action-value calculations later.</p>

<p>The learning phase is then logically separate from gaining experience, and based on taking random samples from this table. You still want to interleave the two processes - acting and learning - because improving the policy will lead to different behaviour that should explore actions closer to optimal ones, and you want to learn from those. However, you can split this how you like - e.g. take one step, learn from three random prior steps etc. The Q-Learning targets when using experience replay use the same targets as the online version, so there is no new formula for that. The loss formula given is also the one you would use for DQN without experience replay. The difference is only <em>which</em> s, a, r, s', a' you feed into it.</p>

<p>In DQN, the DeepMind team also maintained two networks and switched which one was learning and which one feeding in current action-value estimates as ""bootstraps"". This helped with stability of the algorithm when using a non-linear function approximator. That's what the bar stands for in ${\theta}^{\overline{\space}}_i$ - it denotes the alternate <em>frozen</em> version of the weights.</p>

<p>Advantages of experience replay:</p>

<ul>
<li><p>More efficient use of previous experience, by learning with it multiple times. This is key when gaining real-world experience is costly, you can get full use of it. The Q-learning updates are incremental and do not converge quickly, so multiple passes with the same data is beneficial, especially when there is low variance in immediate outcomes (reward, next state) given the same state, action pair.</p></li>
<li><p>Better convergence behaviour when training a function approximator. Partly this is because the data is more like <a href=""https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables"" rel=""noreferrer"">i.i.d.</a> data assumed in most supervised learning convergence proofs.</p></li>
</ul>

<p>Disadvantage of experience replay:</p>

<ul>
<li>It is harder to use multi-step learning algorithms, such as Q($\lambda$), which can be tuned to give better learning curves by balancing between bias (due to bootstrapping) and variance (due to delays and randomness in long-term outcomes). Multi-step DQN with experience-replay DQN is one of the extensions explored in the paper <a href=""https://arxiv.org/abs/1710.02298"" rel=""noreferrer"">Rainbow: Combining Improvements in Deep Reinforcement Learning</a>. </li>
</ul>

<p>The approach used in DQN is briefly <a href=""https://www.youtube.com/watch?v=UoPei5o4fps&amp;index=6&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT"" rel=""noreferrer"">outlined by David Silver in parts of this video lecture</a> (around 01:17:00, but worth seeing sections before it). I recommend watching the whole series, which is a graduate level course on reinforcement learning, if you have time.</p>
","45","2","836","25573"
"21833","<blockquote>
  <p>My understanding is that $\eta$ is set before the training starts to a large value but then, as the training progresses and the function gets closer and closer to a local minimum, the learning rate is decreased. In that case, doesn't the learning parameter satisfy both the definitions of a parameter and of a hyper-parameter?</p>
</blockquote>

<p>No it does not, because you are at all times controlling the learning rate without reference to the data. Typically both the learning rate <em>and</em> the learning rate schedule are considered hyper-parameters, so by adding a learning rate schedule you have not transformed learning rate into a parameter, but instead <em>added</em> a new hyper-parameter that may need tuning!</p>

<p>The learning rate is also is not <em>part of</em> the model as it would be used in production, so it is a <em>training</em> hyper-parameter. Although that is not a strict separation, this also applies to things that are neither hyper-parameters nor parameters, such as the current <em><a href=""https://www.willamette.edu/~gorr/classes/cs449/momrate.html"" rel=""nofollow noreferrer"">momentum</a></em> values, which are used to improve training rate, and are usually stored in a some data structures alongside the model being trained. Likewise most adaptive learning rate structures (used in e.g. RMSProp, Adagrad etc) are in this category - neither model parameter nor hyper-parameter.</p>

<p>It might be possible to argue that for a continuous online model, the learning rate could be a model parameter, as it is part of the production model being used to make predictions - provided it was still being used, but adaptive, depending on data seen to date, and/or current learning progress. Then if you had to feed in an <em>initial</em> learning rate you could maybe say that the learning rate was both a parameter and hyper-parameter of the ANN model. Although you would likely have to explain the scenario in full to be understood, it would be more usual to just say you were using an adaptive learning rate.</p>
","1","2","836","25573"
"21882","<p>A minimum operation <em>is</em> differentiable, or at least you can easily express the partial derivatives w.r.t. its inputs:</p>

<h2>$f = min(x_1, x_2, x_3 ... x_n)$</h2>

<h2>$ \frac{\partial f}{\partial x_i} = 
\begin{cases}
    1,&amp; \text{if } argmin_i(x_i) = i\\
    0,              &amp; \text{otherwise}
\end{cases}$</h2>

<p>This does not hold strictly when multiple values share the same minimum value. However, that is not a problem in practice for gradient-based optimisers in TensorFlow, which can simply set all tied indices to have partial derivative of 1 (or a fraction $\frac{1}{n_{min}}$), with little impact to the eventual result, because ties for values will happen rarely. Ties may happen frequently enough in a ReLU-based network that the TensorFlow developers have considered a best response for them - I don't know specifically what TensorFlow does for that situation.</p>
","2","2","836","25573"
"21956","<p>Your network design/logic is basically correct, but you are seeing some very common problems with neural network numerical stability. This results in your weights diverging and not training accurately.</p>

<p>Here are the fixes, any one of them might help a little, but the first two should be used for nearly all neural network projects.</p>

<h2>1. Inputs need to be scaled to work with neural networks.</h2>

<p>This is called input normalisation. Usually you would do this in data preprocessing, but for your simple network we can include the scaling at the input:</p>

<pre><code>x_normalised = x * 0.2 - 0.5             # Arbitrary scaling I just made up
Out1 = feedForward(x_normalised,w1,b1)   # output of first layer
</code></pre>

<p>The most common scaling operation is to take all the training data and convert it so that it has mean $0$ and standard deviation $1$ (typically <em>per</em> feature, as opposed to global scaling for all data as I have done here) - store the values used to achieve that and apply them to all following data for training, tests etc.</p>

<h2>2. Adjust learning rate until you get a working value.</h2>

<pre><code>train_step = tf.train.GradientDescentOptimizer(0.001).minimize(J) 
</code></pre>

<p>A value that is too high will cause your training to fail. A value that is to low will take ages to learn anything.</p>

<h2>3. For small training sets, use more iterations than you might think</h2>

<p>This is not a general thing, but specific to demos with tiny amounts of training data like your example, or the commonly-used ""learning XOR function"".</p>

<pre><code>for _ in range(10000):                      # performing learning process

   sess.run(train_step, feed_dict = {x:xs, M:Ms})
</code></pre>

<p>With your very simple network actually this may cause over-fitting to the training data, so you will have to play with a value that gives you ""sensible"" results. However, in general how to spot and minimise over-fitting is a whole broad subject in itself, based on how you test and measure generalisation. This will need other questions if you are not sure when you learn it. It should be high on your list of things to learn though . . . it is a critical skill in producing useful neural networks that solve real problems.</p>
","8","2","836","25573"
"21992","<blockquote>
  <p>My wild guess is to weight each layer a ""wrongness"" factor (e.g if instead of wanting to move the first pawn , it tries to move the queen , it is labelled as really ""wrong"" and then apply some kind of spacial locality (e.g if it doesn't move the first pawn but the second and on the very right case, it is not very wrong). But is it correct ?</p>
</blockquote>

<p>You can relatively simply teach a policy network to predict human-like moves in your scheme using a database of moves. Actually your ""wrongness"" is probably well-enough represented by classification (your positive class might be ""This is a good move"") and the usual log loss that goes with it.</p>

<p>After you have trained a policy network, you will want to look in depth at the literature for game-playing bots. Your policy network might work quite well alongside <a href=""https://en.wikipedia.org/wiki/Monte_Carlo_tree_search"" rel=""nofollow noreferrer"">Monte Carlo Tree Search</a>, provided you have some kind of evaluation heuristic for the resulting position.</p>

<p>A <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow noreferrer"">reinforcement learning</a> approach to learn from self-play would take you further, enabling the bot to teach itself about good and bad moves/positions, but is too complex to explain in an answer here. I suggest look into the subject after training your network and seeing how good a player you can create using just the policy network and a move search algorithm.</p>

<blockquote>
  <p>And in general , how to compute the loss of a non classifying convolutional neural network ?</p>
</blockquote>

<p>There are a few common options available for regression, such as mean square error (MSE) i.e. $\frac{1}{2N}\sum_{i=1}^{N}(\hat{y}_i - y_i)^2$ where $\hat{y}_i$ is your prediction and $y_i$ is the ground truth for each example. If you use this loss function, and want to predict values outside of range 0-1, remember to use a linear output layer (i.e. no activation function after the last layer), so that the network can actually output close to the values you need - that's about the only difference in network architecture you need to care about.</p>

<p>In the more general case of game-playing bots, it is usual (but not required) to calculate a predicted ""return"" or ""utility"" which is the sum of all rewards that will be gained by continuing to act in a certain way. MSE loss is a good choice for that. Although for zero-sum two player games where the reward is simply win/lose, you can use a sigmoid output layer (predicting chance of a win) and cross-entropy loss, much like a classifier.</p>

<p>For your specific case, you <em>can</em> treat your initial policy network as a classifier. This immediately gives you some probability weightings for the predicted move, which you can use to pick the predicted best play, or maybe to guide <a href=""https://en.wikipedia.org/wiki/Monte_Carlo_tree_search"" rel=""nofollow noreferrer"">Monte Carlo Tree Search</a>.  </p>

<p>The kind of network that predicts <em>utility</em> or <em>return</em> from a position (or combination of position and action) is called a value network (or an action-value network) in reinforcement learning . If you have <em>both</em> a policy network and a value network, then you are on the way to creating an <a href=""https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"" rel=""nofollow noreferrer"">actor-critic</a> algorithm.</p>
","1","2","836","25573"
"22083","<p>Yes <em>in theory</em> the polynomial extension to logistic regression can approximate any arbitrary classification boundary. That is because a polynomial can approximate any function (at least of the types useful to classification problems), and this is proven by the <a href=""https://en.wikipedia.org/wiki/Stone%E2%80%93Weierstrass_theorem"" rel=""noreferrer"">Stone-Weierstrass theorem</a>.</p>

<p>Whether this approximation is practical for all boundary shapes is another matter. You may be better looking for other basis functions (e.g. Fourier series, or radial distance from example points), or other approaches entirely (e.g. SVM) when you suspect a complex boundary shape in feature space. The problem with using high order polynomials is that the number of polynomial features you need to use grows exponentially with degree of the polynomial and number of original features.</p>

<p>You could make a polynomial to classify XOR. $5 - 10 xy$ might be a start if you use $-1$ and $1$ as the binary inputs, this maps input $(x,y)$ to output as follows: </p>

<p>$$(-1,-1): -5 \qquad (-1,1): 5 \qquad (1,-1): 5 \qquad(1, 1): -5$$</p>

<p>Passing that into the logistic function should give you values close enough to 0 and 1.</p>

<p>Similar to your two circular areas is a simple figure-of-eight curve:</p>

<p>$$a(x^2 - y^2 - bx^4 + c)$$</p>

<p>where $a, b$ and $c$ are constants. You can get two disjoint closed areas defined in your classifier - on opposite sides of the $y$ axis, by choosing $a, b$ and $c$ appropriately. For example try $a=1,b=0.05,c=-1$ to get a function that clearly separates into two peaks around $x=-3$ and $x=3$:</p>

<p><a href=""https://i.stack.imgur.com/dNysS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/dNysS.png"" alt=""two separate classes""></a></p>

<p>The plot shown is from <a href=""https://academo.org/demos/3d-surface-plotter/?expression=x%5E2-y%5E2-0.05*x%5E4-1.0%3E0&amp;xRange=-5%2C%2B5&amp;yRange=-5%2C%2B5&amp;resolution=95"" rel=""noreferrer"">an online tool at academo.org</a>, and is for $x^2 - y^2 - 0.05x^4 -1&gt;0$ - the positive class shown as value 1 in the plot above, and is typically where $\frac{1}{1+e^{-z}} &gt; 0.5$ in logistic regression or just $z&gt;0$</p>

<p>An optimiser will find best values, you would just need to use $1, x^2, y^2, x^4$ as your expansion terms (although note these specific terms are limited to matching the same basic shape reflected around the $y$ axis - in practice you would want to have multiple terms up to fourth degree polynomial to find more arbitrary disjoint groups in a classifier).</p>

<p>In fact any problem you can solve with a deep neural network - of any depth - you can solve with a flat structure using linear regression (for regression problems) or logistic regression (for classification problems). It is ""just"" a matter of finding the right feature expansion. The difference is that neural networks will attempt to discover a working feature expansion directly, whilst feature engineering using polynomials or any other scheme is hard work and not always obvious how to even start: Consider for example how you might create polynomial approximations to what convolutional neural networks do for images? It seems impossible. It is likely to be extremely impractical, too. But it does exist.</p>
","8","2","836","25573"
"22178","<p>Empirically, the network performance does not increase much for a fully-connected network on MNIST when you add layers, but you can <em>probably</em> find ways to improve it on networks with 3+ hidden layers, such as data augmentation (e.g. variations of all inputs translated +-0..2 pixels in x and y, roughly 25 times the original data size, as a start).</p>

<p>I don't think this idea is pursued very far in practice, because CNNs offer a much better performance increase for the effort required. You hit the point of diminishing returns earlier with a basic MLP (around 96-97% accuracy) than you can reach easily with a CNN (around 99% accuracy).</p>

<p>The theory basis for this difference is not obvious to me, but very likely yes this is related to over-fitting. The weight sharing and feature pooling in a CNN is very effective way of processing image data for classification tasks, and avoids over-fitting by reducing the number of parameters, whilst re-using the parameters for the task in a way that makes very good sense given the nature of the inputs.</p>
","1","2","836","25573"
"22207","<p>The first variant <em>is</em> the second variant, or more accurately there is only one type of backpropagation, and that works with the gradients of a loss function with respect to parameters of the network.</p>

<p>This is not an uncommon point to have questions about though, the main issue that I see causing confusion is when the loss function has been cleverly constructed so that it works with the output layer activation function, and the derivative term is numerically $\hat{y} - y$, which looks the same as taking the linear error directly. People studying the code implementing a network like this can easily come to the conclusion that the initial gradient is in fact an initial error (and whilst these are numerically equal, they are different concepts, and in a generic neural network they don't have to be equal)</p>

<p>This situation applies for the following network architectures:</p>

<ul>
<li><p>Mean squared error $\frac{1}{2N}\sum_{i=1}^N(\hat{y}_i - y_i)^2$ and linear output layer - note the multiplier $\frac{1}{2}$ is there to deliberately simplify the derivative.</p></li>
<li><p>Binary cross-entropy $\frac{-1}{N}\sum_{i=1}^Ny_i\text{log}(\hat{y}_i) + (1-y_i)\text{log}(1-\hat{y}_i)$ and sigmoid output layer. The derivative of the loss neatly cancels out the derivative of the sigmoid, leaving you with gradient at the pre-transform stage of $\hat{y} - y$.</p></li>
<li><p>Multi-class logloss with one-hot encoding of true classes $\frac{-1}{N}\sum_{i=1}^N\mathbf{y}_i\cdot\text{log}(\hat{\mathbf{y}}_i)$ and softmax output layer. Again the derivative of the loss neatly cancels out, leaving you with gradient at the pre-transform stage of $\hat{y} - y$ for the true class.</p></li>
</ul>

<p>So when you are told that backpropagation processes an ""error signal"" or ""the error"" backwards through the network, just mentally add ""the gradient of"" to the start of the phrase. Some people will say it knowingly as shorthand, others might be honestly confused. </p>

<p>The same applies to deeper layers, although then there is no other source for the confused ""this is the error being distributed"" other than as shorthand for ""this is the [gradient of the] error being distributed"".</p>
","4","2","836","25573"
"22221","<p>A web search for <code>""policy collapse"" ""reinforcement learning""</code> finds this question, a related one in stats.stackexchange.com <em>and</em> the comments section where you found the phrase. There are two other results on unrelated subjects where the words happen to appear next to each other. Then that's it - 5 results total from Google.</p>

<p>A google books ngrams search for <code>policy collapse</code> finds no references at all.</p>

<p>It is hard to prove a negative, but I think this is not a widely used term.</p>

<p>However, the comment does appear to be referring to a real phenomenon. That is where a reinforcement agent, instead of converging on the value functions for an optimal policy as it gains experience, actually diverges (and the parameters of the approximator will diverge too).</p>

<p>This can happen when using non-linear function approximators to estimate action-values. More generally, it tends to happen when you have the following traits in your problem:</p>

<ul>
<li><p>Using a function approximator, especially a non-linear one (although even linear function approximators can diverge)</p></li>
<li><p>Using a bootstrap method, e.g. Temporal Difference (TD) Learning (including SARSA and Q-learning), where values are updated from the same value estimator applied to successive steps. </p></li>
<li><p>Off-policy training. Attempting to learn the optimal policy whilst not behaving optimally (as in Q-Learning).</p></li>
</ul>

<p>In <a href=""http://incompleteideas.net/sutton/book/the-book-2nd.html"" rel=""noreferrer"">Sutton and Barto's book</a> this is called the ""deadly triad"". If you do a web search for <code>""deadly triad"" ""reinforcement learning""</code> you will find many more results. It is an ongoing area of research how best to combat the effect. In the <a href=""http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html"" rel=""noreferrer"">paper that introduced the DQN model learning to play Atari games</a>, the researchers applied two things that help stabilise against the effect:</p>

<ul>
<li><p>Experience replay, where transitions are not learned from immediately, but put into a pool from which mini-batches are sampled to train the approximator.</p></li>
<li><p>Bootstrap estimates are made from a ""frozen"" copy of the learning network, updated every N training steps - i.e. when calculating the TD target $R + \gamma \hat{q}(S', A', \theta)$, use this old copy of the network.</p></li>
</ul>

<p>From the comment section you linked, it appears even applying these things is not a guaranteed fix and takes some judgement. In that case it was increasing the mini-batch size for experience replay that helped to stabilise an agent playing a variant of the video game <a href=""https://en.wikipedia.org/wiki/Pong"" rel=""noreferrer"">Pong</a>.</p>
","7","2","836","25573"
"22285","<p>There is usually only one learning rate active when using a neural network as function approximator in reinforcement learning. The different names $\eta$ and $\alpha$ are just different conventions for the same basic concept.</p>

<p>When you use a function approximator, other than a linear one, in reinforcement learning, then typically you would not use TD error based update like this:</p>

<p>$$\mathbf{w} \leftarrow \mathbf{w} + \alpha[R+\gamma\hat{q}(S', A',\mathbf{w}) - \hat{q}(S, A,\mathbf{w})]\nabla \hat{q}(S, A,\mathbf{w})$$</p>

<p>But you would train your estimator in a supervised learning manner on sampled TD target (which would use $\eta$ param in a neural network):</p>

<p>$$\mathbf{x} = \phi(S, A), y = R+\gamma\hat{q}(S', A',\mathbf{w})$$</p>

<p>You can actually do either if your library supports it - it is certainly possible to calculate $\nabla \hat{q}(S, A,\mathbf{w})$ for a neural network for instance, instead of using an explicit training loss function. However, the two approaches are equivalent ways of expressing the same thing, there is no reason to use both.</p>

<p>There are other parameters used in reinforcement learning that may affect rates of convergence and other properties of learning agents. For example with differential semi-gradient TD learning - which might be an algorithm you would look at for a continuous task - Sutton and Barto present $\beta$ as a separate learning rate for the average reward, distinct from the learning rate of the estimator.</p>

<blockquote>
  <p>So are these two parameters redundant? Do I need to worry about even having $\alpha$ as anything other than 1 if I'm already tuning $eta$, or do they have ultimately different effects?</p>
</blockquote>

<p>They are essentially the <em>same</em> parameter with a different name.</p>

<p>If you are trying to pass an error value like $R+\gamma\hat{q}(S', A',\mathbf{w}) - \hat{q}(S, A,\mathbf{w})$ (whether multiplied by $\alpha$ or not) into the neural network as a target, then you have got the wrong idea. Instead you want your neural network to learn the target $R+\gamma\hat{q}(S', A',\mathbf{w})$. That's because the subtraction of current prediction and multiplication by a learning rate is built into the neural network training.</p>
","1","2","836","25573"
"22328","<blockquote>
  <p>so what steps should I take when creating a network based around strings for the first time?</p>
</blockquote>

<p>Neural networks work with numerical data. They also work best with relatively small floating point numbers, centred around zero. You can be less strict about that part, but you will often see the approach in neural networks of calculating the mean and standard deviation from the training data, for each feature, then converting all the features by doing <code>x = (x - means) / stds</code> (you want to store these scaling factors you used along with the network data, because you will want to re-used the same values when you use the network to make predictions later).</p>

<p>So what do you do if the input data is not already in this form? You prepare it in your code, just before using it to train or predict. It is a very common structure to see in machine learning scripts:</p>

<pre><code>raw_features, raw_labels = load_from_disk( some_data_source )
all_features = convert_features( raw_features )
all_labels = convert_labels( raw_labels )
train_X, test_X, train_y, test_y = split_data( all_features, all_labels )
model = build_model( .... various model params ....)
model.fit( train_X, train_y )
test_predictions = model.predict( test_X )
report_accuracy( test_predictions, test_Y )
</code></pre>

<p>The above is rough pseudo code, so typically all the functions above have different names, or are multiple lines that do the same thing that you might not bother to encapsulate into a re-usable method if you are writing a quick script. The part I have shown that splits the features might be built in to the training function, and it is also common that the training process can use the test data to help monitor progress.</p>

<p>If the loading and conversion takes a long time, you might do it in a separate script and save the resulting NumPy array in a separate file to load it quicker next time.</p>

<p>So the part you are concerned about is how you might build a <code>convert_features</code> section of your code from the starting strings. The answer is to use whatever values you can extract from your strings that might be relevant. The string length might be a simple start i.e. <code>len( text )</code> - but you can also look into any other measure you can figure out (e.g. number of vowels, which uncommon bi-grams are in the word). Deciding which features to try and testing between them is <a href=""https://en.wikipedia.org/wiki/Feature_engineering"" rel=""nofollow noreferrer"">feature engineering</a>, and this often involves some creativity. The important thing is that the features must all be numeric. For a neural network, you should also try and make them relatively small and/or convert them to have mean 0, standard deviation 1 before going to next stage.</p>

<p>When you use the network to make predictions later, you have to repeat most of the pipeline:</p>

<pre><code>model = load_model( model_file_or_identifier )
raw_features, raw_labels = fetch_data( some_data_source )
X = convert_features( raw_features )
y = convert_labels( raw_labels )
predictions = model.predict( X )
report_predictions( X, y )
</code></pre>
","0","2","836","25573"
"22334","<p>I expect someone somewhere has used a RF estimator inside RL to approximate action values, if only to assess it as a comparison to other function approximators. However, it does look like from a web search that this is not used widely, and I could not find an example either.</p>

<p>The main problem with a RL/RF hybrid with RF as the value estimator is that the random forest base algorithm is not an online one - it works across a finalised data set and processes the whole batch in order to do things like bagging. Even when used as an estimator using experience replay, support for online learning is a desirable feature, and values are typically fed into supervised learning part in small or medium batches. That is because the action values that are learned by the internal estimation function in reinforcement learning are non-<a href=""https://en.wikipedia.org/wiki/Stationary_process"" rel=""noreferrer"">stationary</a>. </p>

<p>Once you have learned the action values for the current policy, and in most RL well before estimates for the values have converged, you change the policy. This changes the expected action values, so your estimator must be able to forget older data and bias towards most recent values. Algorithms that can be made to work online can do this, purely offline ones cannot.</p>

<p>However, there are <a href=""http://www.vision.cs.chubu.ac.jp/CV-R/pdf/AmirICCVW2009.pdf"" rel=""noreferrer"">online Random Forests</a> which have the necessary alterations to work with non-stationary data. I don't see any reason why they could not work. Decision trees in general have been used successfully as estimators in RL - see <a href=""https://pdfs.semanticscholar.org/f9b3/0e1f6d85cb77e95ff1d580ee67d7406f1dd6.pdf"" rel=""noreferrer"">Reinforcement Learning with Decision Trees</a>. It looks like that in principle it could be done.</p>
","7","2","836","25573"
"22413","<blockquote>
  <p>Why do we need n in the cost function update rule. Aren't we visiting each state exactly once?</p>
</blockquote>

<p>The update is assuming a static distribution and estimating the average value. As each estimate is made available, it is weighted less of the total each time. The formula means that the first sample is weighted $1$, second $\frac{1}{2}$, third $\frac{1}{3}$ which is what you need to get the mean value when you apply the changes due to the samples serially whilst maintaining the best estimate of the mean at each step.</p>

<p>This is a little odd in my experience of RL, because it assumes the bootstrap values (the max over next step) come from a final distribution to weight everything equally like this. But I think it is OK due to working back from final step, hence each bootstrap value should be fully estimated before going backwards to previous time step.</p>

<blockquote>
  <p>If I understand correctly, we should run this algorithm on every episode (in the experiment in the paper they had 45000 episodes)</p>
</blockquote>

<p>This looks like an algorithm that you run on the whole data set, where each episode is the same length $T$. So you run each timestep (starting with the end time step and working backwards since the ultimate reward is established at the end of the episode, so this is more efficient), and sample from <em>every</em> episode at that timestep in the <code>While (not end of data)</code> loop. The values are therefore combined inside the loop at that stage, and there is no need to add anything to the algorithm to combine episodes.</p>
","1","2","836","25573"
"22472","<p>In this case, the two math formulae show you the correct type of multiplication:</p>
<ul>
<li><p><span class=""math-container"">$y_i$</span> and <span class=""math-container"">$\text{log}(a_i)$</span> in the cost function are scalar values. Composing the scalar values into a given sum over each example does not change this, and you never combine one example's values with another in this sum. So each element of <span class=""math-container"">$y$</span> only interacts with its matching element in <span class=""math-container"">$a$</span>, which is basically the definition of element-wise.</p>
</li>
<li><p>The terms in the gradient calculation are matrices, and if you see two matrices <span class=""math-container"">$A$</span> and <span class=""math-container"">$B$</span> multiplied using notation like <span class=""math-container"">$C = AB$</span>, then you can write this out as a more complex sum: <span class=""math-container"">$C_{ik} = \sum_j A_{ij}B_{jk}$</span>. It is this inner sum across multiple terms that <code>np.dot</code> is performing.</p>
</li>
</ul>
<p>In part your confusion stems from the <em>vectorisation</em> that has been applied to equations in the course materials, which are looking forward to more complex scenarios. You could in fact use</p>
<pre><code>cost = -1/m * np.sum( np.multiply(np.log(A), Y) + np.multiply(np.log(1-A), (1-Y)))
</code></pre>
<p>or</p>
<pre><code>cost = -1/m * np.sum( np.dot(np.log(A), Y.T) + np.dot(np.log(1-A), (1-Y.T)))
</code></pre>
<p>whilst <code>Y</code> and <code>A</code> have shape <code>(m,1)</code> and it should give the same result. NB the <code>np.sum</code> is just flattening a single value in that, so you could drop it and instead have <code>[0,0]</code> on the end. However, this does not generalize to other output shapes <code>(m,n_outputs)</code> so the course does not use it.</p>
","13","2","836","25573"
"22486","<p>Batch normalization was introduced in the 2015 paper <a href=""https://arxiv.org/abs/1502.03167"" rel=""nofollow noreferrer"">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>

<p>It is conceptually an extension of the general advice to scale input values to neural networks (typically to mean 0, standard deviation 1). Batch normalization applies the same idea to re-scale neural network activations on a per-layer basis, and this can result in improved training times.</p>
","0","5","836","25573"
"22487","For questions about Batch Normalization of layer activations in theory and practice, as used in (typically deep) neural networks.","0","4","836","25573"
"22704","<p>In general, if you want to generate data in X, or X, Y pairs, then you should start by training a generative model - as opposed to a discriminative model, which is what most NN classifiers are. There are many types of generative model. </p>

<p><a href=""http://kvfrans.com/variational-autoencoders-explained/"" rel=""nofollow noreferrer"">Variational Autoencoders</a> (VAE)and <a href=""http://blog.aylien.com/introduction-generative-adversarial-networks-code-tensorflow/"" rel=""nofollow noreferrer"">Generative Adversarial Networks</a> (GAN) have been demonstrated recently with interesting results on images - although both take a lot of training data and time, and are limited to relatively small image dimensions (e.g. 128x128). There are many sub-types of these two designs, including a <a href=""https://github.com/anitan0925/vaegan"" rel=""nofollow noreferrer"">combined VAEGAN</a> which attempts to combine strengths of both.</p>

<p>With a trained CNN, you <em>can</em> generate data - sort of. What you can do is start with some arbitrary value of X and Y, then use back propagation to calculate gradients of a cost function. But instead of using the gradients to update weights, you back propagate all the way to the input, and use the gradients at the input to alter X, repeating the process multiple times. </p>

<p>This is essentially how Deep Dream and Style Transfer work (although in general these don't use a Y value, but selected activation values within layers). There is a major caveat to this approach - your generated X will not be sampled evenly from any distribution of X that the network has been trained with. Instead you will generate a ""super stimulus"" X for the given Y.</p>

<p>You mention RNNs. One way these can be used to generate X is by <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow noreferrer"">sampling from their output and feeding this back into the input</a>. For text sequences this tends to generate grammatically correct nonsense. I am not sure if this would be considered a strictly generative model, since it is not clear to me whether the input X is being sampled evenly. It is likely that you could use the approach to generate images too, although you would have to take care defining what the sequence is (just a sequence of pixels line-by-line will probably not produce any recognisable image).</p>
","1","2","836","25573"
"22734","<p>If you have to predict $y$ only given $a,b,c$, then as a first look just discard $d,e$ etc from other data sets*. Any kind of regression might be useful, you will have to explore that - you might as well start with linear regression as it is simplest. </p>

<p>In general, if you are not sure about which algorithm to apply for best results, you should set up an experiment to try multiple models:</p>

<ul>
<li>Decide on what ""best"" means by choosing a test metric (mean square error is a common choice for regression problems). </li>
<li>Split your data into training, cross-validation and test sets.</li>
<li>Train different types of model on the training set:

<ul>
<li>Tune any hyper-parameters (of more complex models) by taking the best results from the cross-validation set. </li>
<li>Pick your best cross-validation result as your guess at the best model. </li>
</ul></li>
<li>Finally get an unbiased estimate of its performance by using your test set. </li>
</ul>

<hr>

<p>* There might be ways to use $d,e$ values to refine some models, but if you are at the stage of asking whether or not to use linear regression, this is not worth exploring initially.</p>
","1","2","836","25573"
"22778","<blockquote>
  <p>As far as I know, the usp of CNN is the fact that location of a pattern doesnt matter.</p>
</blockquote>

<p>This is not true. Some options used in CNN architecture, such as max pooling, or strided convolutions, can add a moderate amount of translation invariance. However this will not cover larger translations - anything that is a significant percentage of the image width/height.</p>

<p>CNNs also support translation <em>equivariance</em>, which is where image textures and motifs that appear repeatedly but in different locations (lines, corners, curves) are learned efficiently and appear in the feature maps. This is probably closer to the USP of CNNs, that they learn the ""representation language"" of a signal, such as photograph, where it is consistent across the dimensions that are being convolved.</p>

<p>The cause of your problem is therefore very likely that your training set does not include enough images with the more central translation, or enough variations with the separate images.</p>

<p>To cover large translations you could look at one or more of:</p>

<ul>
<li><p>Image pre-processing. In your supplied examples it looks very feasible to centre the digits.</p></li>
<li><p>Data augmentation. Generate translated versions of your training data to cover expected range of input translations in test.</p></li>
<li><p>Network architecture. A RNN or RNN/CNN hybrid could probably consume the images using smaller overlapped tiles of the images in sequence, and be trained to output the captcha string at the end. You might also be able to do this without a RNN, but that would require labelling each tile for the separate digits.</p></li>
</ul>
","3","2","836","25573"
"22779","<p>The centroid is likely correct, you have a display error</p>

<p>The line</p>

<pre><code>plt.scatter(centroid2[0][0], centroid1[0][1], color = ""green"")
</code></pre>

<p>should be</p>

<pre><code>plt.scatter(centroid2[0][0], centroid2[0][1], color = ""green"")
</code></pre>

<p>That's just one of those things that happens when you implement an algorithm from scratch in order to learn it . . . I bet you've spent hours looking at the top part of your script :-)</p>
","3","2","836","25573"
"22793","<p>The problem is the <code>relu</code> unit. It is not a very good choice in such a simple network. There is a good chance that the ReLU starts off ""dead"" -
- if the weight for the neuron in the first layer is negative (a 50/50 chance), then both 0 and 1 inputs will produce a 0 output and no gradient, so the network cannot learn to separate them. </p>

<p>Change to <code>tanh</code> instead will completely fix the problem, and the network will learn the relationship trivially. This as will also work with ""leaky"" ReLU or any other unit without the simple cutoff of ReLU.</p>

<p>A leaky ReLU version of your model would look like this:</p>

<pre><code>model.add(keras.layers.Dense(1, input_dim=1, init='uniform' ))
model.add(keras.layers.advanced_activations.LeakyReLU(alpha=0.01))
model.add(keras.layers.Dense(1, init='uniform', activation='sigmoid'))
</code></pre>

<p>In larger/deeper networks with more complex input data, this disadvantage of ReLU units generally has lower impact and can be worked around more easily.</p>
","1","2","836","25573"
"22850","<p>Your network is actually working, it just takes a lot of epochs to learn the spiral. In fact you can see from your learning curves that learning is still occurring, just not much per epoch.</p>

<p>Try 60,000 epochs . . . when I try your model (in Python, but still same data and model) using 60,000 epochs I get loss under 0.0001 and accuracy of 100% reliably.</p>

<p>There are a few factors involved in why you need this amount of iteration:</p>

<ul>
<li><p>The data set size is small, which means you get less updates to weights per epoch. You need to compensate by increasing the number of iterations.</p></li>
<li><p>You have a ""starved"" network topology that can just about learn the spiral, but needs to be quite precisely optimised before it starts performing well. You could increase the number of neurons per hidden layer slightly. Or maybe adding another hidden layer:</p>

<ul>
<li>If I add one more hidden layer, size 6, <code>tanh</code> activation, the network learns 100% accuracy in under 20,000 epochs.</li>
<li>If instead, I increase the original four hidden layers to size 16, the network learns 100% accuracy in under 10,000 epochs.</li>
</ul></li>
<li><p><code>tanh</code> is not optimal for a deep network, because the gradient diminishes in the deeper parts of the model. RMSProp will compensate for that, but still changing to <code>relu</code> will improve convergence speed.</p>

<ul>
<li>If I use the four-hidden-layer model, with layer size 16 and <code>relu</code> activation, the network converges to 100% accuracy on the training data set in around 2000 epochs.</li>
</ul></li>
</ul>



<pre><code>import pandas as pd
import numpy as np
np.random.seed(4375689)

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import RMSprop

train_data = pd.read_csv('spirals.csv').values
train_X = train_data[:,0:2]
train_y = train_data[:,2]

model = Sequential()
model.add(Dense(16, activation='relu', input_shape=(2,)))
model.add(Dense(16, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer=RMSprop(),
              metrics=['accuracy'])

history = model.fit(train_X, train_y, batch_size=32, epochs=2000, verbose=0)

score = model.evaluate(train_X, train_y, verbose=0)

print(score)
</code></pre>
","5","2","836","25573"
"22946","<p>Very briefly, homomorphic encryption as used by the linked paper in the question works as follows (excuse ad-hoc notation). Define an encryption method $E(x,k)$ that takes some data $x$ and secret key $k$ that has the following properties:</p>

<ul>
<li>There is a decryption method $D( E(x,k), k ) = x$ (optionally the two $k$ here can be different)</li>
<li>You can define operations that work on the encrypted data in a self-consistent manner, e.g. $\text{Add}_E(E(x,k),E(y,k)) = E(x+y,k)$ - note that the result is still encrypted, and you need to know $k$ in order to use method $D()$ and find out what $x+y$ is.</li>
<li>You may also need to support operations like $\text{Add}_E(E(x,k),y) = E(x+y,k)$ e.g. where one of the values is normal numeric variable - this might be useful for initialising a model - or using an existing trained model - but the catch is that when training you cannot keep these values unencrypted, once they use updated due to training results, they will also be encrypted.</li>
<li>Note that no operation can output a decrypted value without using the key, otherwise the whole scheme is insecure, it implies some backdoor to get decrypted data.</li>
<li>For ML use, you need an encryption scheme E that supports multiple basic operations, e.g. $\text{Add}_E$, $\text{Subtract}_E$, $\text{Multiply}_E$, $\text{GreaterThan}_E$ etc. You need some minimal set that is enough to build a model.</li>
</ul>

<p>You can then build variant models that instead of performing arithmetic to sum/multiply etc numeric values, work with the abstract operations on encrypted data. The models would otherwise look just like their unencrypted counterparts - e.g. you could quickly build a linear regression using only a few types of operation.</p>

<p>Given that you are replacing simple fast floating point add, multiply operations that are built into the processor with a more complex custom operation, this significantly affects performance compared to the unencrypted model. How much so depends on the encryption scheme, and how it is implemented. The paper shows in section 5, timings on unencrypted vs encrypted data and the difference is several orders of magnitude slower when encrypted.</p>

<p>The paper was published in 2012, so it is possible that some improvements have been made here. However, on a deeper read of the subject I think that this is still at the stage of research proof-of-concept. I <em>might</em> be wrong, and there are nice workable implementations available that could be used in something as complex as a CNN, but I have not found anything.</p>

<p>You also linked numer.ai from comments. Initially that looked interesting because surely they would have solved efficiency problems. But in fact their main competitions are using data obfuscation techniques, not encryption - a homomorphic encryption paper is linked from their main site, but it does not seem to be what they are using. End users are writing very familiar-looking scripts that perform logistic regression etc using regular Python (no special operators imported)</p>

<p>I think you should take a second look at your wider problem and analyse your threat model. What precisely are you trying to protect against? If it is about keeping your own operations separate from client data, then you might be looking into company process and auditing solutions, rather than purely technical (you may still want to add technical solutions such as disk-level encryption to protect customer's data at rest in you data centre, in case someone gets into the centre physically and just grabs a disk containing all the cat images) </p>

<p>For instance, look at <a href=""https://cloudsecurityalliance.org/"" rel=""nofollow noreferrer"">Cloud Security Alliance</a> which as well as having a certification scheme, has analysis showing how their recommendations map to other schemes such as UK's ISO27001. Note that this is typically significant capital investment including 1+ year project to implement, and usually undertaken by mid-sized or larger companies, when they want to work with government or large corporate's data. However, it is probably a more reliable and maybe still cheaper route than trying to research and build a technical solution involving models that process encrypted data at this time.</p>
","2","2","836","25573"
"22995","<blockquote>
  <p>My question is, why are we multiplying by 1/(i+1)? Is this supposed to be an implementation of epsilon annealing?</p>
</blockquote>

<p>The code looks like a relatively ad-hoc* adjustment to ensure early exploration, and an alternative to $\epsilon$-greedy action choice. The <code>1/(i+1)</code> factor is <em>similar</em> to decaying $\epsilon$, but not identical.</p>

<p>$\epsilon$-greedy with the same decay factor might look like this:</p>

<pre><code>a = np.argmax(Q[s,:])
if epsilon/(1+math.sqrt(i)) &gt; random.random():
    a = random.randrange(0, env.action_space.n)
</code></pre>

<p>The <code>math.sqrt(i)</code> is just a suggestion, but I feel that <code>epsilon/(1+i)</code> is probably too aggressive and would cut off exploration too quickly.</p>

<p>It is not something I have seen before when studying Q-Learning (e.g. in David Silver's lectures or Sutton &amp; Barto's book). However, Q-Learning is not predicated on using <em>any</em> specific action choice, it just needs enough exploration in the behaviour policy. For the given problem adding some noise to a greedy selection obviously works well enough. Technically for guaranteed convergence tabular Q-Learning needs infinite exploration over infinite time steps. The code as supplied <em>does indeed do that</em> because the noise is unbound from the Normal distribution. So there is always some small finite chance of selecting an action with a relatively low action-value estimate and refining that estimate later.</p>

<p>However, the fast decay (1/episode number) and initial scaling factor for the noise are both hyperparameters that need tuning to the problem. You might prefer something more standard from the literature such as $\epsilon$-greedy, Gibbs sampling or upper-confidence-bound action selection (the example is quite similar to UCB, in that it adds to the Q-values before taking the max). </p>

<hr>

<p>* Perhaps the approach used in the example has a name (some variation of ""Noisy Action Selection"") but I don't know it, and could not find it on a quick search.</p>
","6","2","836","25573"
"23106","<blockquote>
  <p>Is it updated before weight update or after weight update?</p>
</blockquote>

<p>It doesn't usually matter, provided your NN framework has separate gradient calculation stage from parameter update stage.</p>

<p>The gradients should be collected with all weights and parameters at their current values (during back-propagation). Then the parameters are updated, using those gradients. There should be no interaction between the parameters during the update step.</p>

<p>However, if you have implemented the training code yourself from scratch, and have interleaved the gradient calculations and weight updates, then this only works in a pure online situation with weights updated as the gradients are calculated. In that scenario, you should probably update the weights for a layer <em>before</em> updating the PReLU parameter for the same layer, to prevent changes in the activation function altering the gradients during your calculations (this is not a concern with usual weight updates). </p>

<p>Alternatively, separate your gradient calculations from the weight update steps. This is more flexible and allows you to use other more advanced optimisations and layer designs.</p>
","1","2","836","25573"
"23138","<blockquote>
  <p>Why isn't it very very simple?</p>
</blockquote>

<p>The problem description is simple, but not in a way that is accessible to the standard RL algorithms. First of all, the analogy of switches and ""corresponding"" has no meaning to the RL. Secondly, the low complexity is not detectable and is outside of the assumptions of RL agents in general.</p>

<p>Reinforcement Learning is based on an internal model of the problem as a generic Markov Decision Process (MDP). The RL agent has to discover, by trial and error, what the MDP does, typically starting with no knowledge or assumptions other than the set of states and allowed actions. The agent needs to discover <em>enough</em> about the MDP to make optimal actions, whilst the only assumption is that the environment is a MDP.</p>

<p>The simple switch problem can definitely be modelled as a MDP. The agent then has to discover the simple configuration in the toy problem. What makes it harder than the problem description:</p>

<ul>
<li><p>State/action space. There are 1024 states and 10 actions. There is no a priori reason to link any action with any outcome, so to be thorough the agent has to try them all. A minimal search would be across 10,240 state/action pairs.</p></li>
<li><p>State transitions. There are 1024 states where an action can be taken, and a terminal state. So there are 1024 * 1025 = 1,049,600 possible state transitions for every action. In your case, each state/action pair has one outcome, and there are roughly 5,000 different transitions (because half of them are to the terminal state).</p></li>
<li><p>The agent does not know that the state transitions are deterministic and sparse. It can only discover that by trial and error. For instance, perhaps in a particular configuration, taking one particular action would cause some bits to re-set. Maybe that only happens one time in four attempts. Once you add the possibility of arbitrary probability distributions here, then the number of potential MDPs is infinite. RL algorithms are designed to cope with this, still learning a good approximation of an optimal policy eventually. There is no simple way to make them dumber and faster against a simpler environment without encoding some of your knowledge about that environment.</p></li>
<li><p>The agent does not know that the rewards are deterministic and simple. Again it can only discover this by trial and error. To the agent the different state vectors and actions are neither similar to each other (all switches and bits) nor have any association (action 0 is not associated in any way with bit 0). Even after the associations are observed, they are not encoded in a way that the agent can reason by analogy. Again, once you add the possibility of arbitrary probability distributions for reward, then the number of potential MDPs is infinite. </p></li>
</ul>

<blockquote>
  <p>Suggest a reinforcement learning agent that will learn to efficiently switch on a bit vector</p>
</blockquote>

<p>You have made things a lot harder for yourself by trying a policy gradient method with a neural network function approximator. Although this probably can be made to solve your problem, it is an advanced technique, and also known to be unstable and hard to train. It might be a good choice when the relationship between the state, transitions and rewards was very complex, and there were more states possible with <em>meaningful similarity</em> between them (in a numerical/statistical sense, not in terms of analogy, the neural network still won't find the analogy with switches and numbered bits). Even then, it will learn slowly and cautiously, needing to repeat state/action pairs multiple times in order to learn that the transitions and reward are reliable.</p>

<p>Unless there is some good reason for you to stick with a policy gradient method, I suggest using a tabular algorithm (i.e. no function approximation, just a table of action value estimates) and something like single step Q-Learning. That has the advantage that because you know the algorithm is deterministic, you can set a high learning rate and it will remain stable. In fact Q-learning will probably learn an optimal policy in much less than 10,000 episodes. However, initially it will learn just one of the many possible optimal policies and tend to stick with it  - i.e. some permutation of taking each action 0 to 9, once each, but always in the same order because it has learned that is safe.</p>
","1","2","836","25573"
"23139","<blockquote>
  <ol>
  <li>Is the resulting $\hat{L}(h_*,V)$ an unbiased estimate of the true loss?</li>
  </ol>
</blockquote>

<p>No. You have taken multiple measurements, each with some uncertainty, and chosen the maximum or minimum value.</p>

<blockquote>
  <ol start=""2"">
  <li>How can I bound the true loss using $\hat{L}(h_*,V)$ ?</li>
  </ol>
</blockquote>

<p>In the general case you cannot. It will depend on how much over-fitting is occurring within the model on the training set, size of cv set, amount of times it has been used, and how similar the model's performance was on each use. There is also sampling bias in the cv set, and that interacts with the selection process.</p>

<p>What is generally done if you need unbiased estimate at the end of production is a train/cv/test split. The cv set is used for model selection, and once you have a single model selected, you estimate its loss - or other key metric - on the test set. It is important to use the test set minimally and not in order to select models, if you want it to be an unbiased measure. Otherwise you repeat the problem.</p>

<p>Another approach which maintains confidence in cv-based metrics is to use k-fold cross validation. Taking taking the mean of a metric in k-fold cross validation is still biased once you have used it a few times, but the bias is reduced somewhat. You can take that idea further with <em>nested</em> cross-validation, which allows you to get an unbiased estimate of model performance in a general fashion (i.e. using the same hyper-parameters) from more of your data.</p>
","3","2","836","25573"
"23162","<p>Although RL algorithms can be run online, in practice this is not stable when learning off policy (as in Q-learning) and with a function approximator. To avoid this, new experience can be added to history and the agent learn from the history (called experience replay). You could think of this as a semi-online approach, since new data is immediately available to learn from, but depending on batch sizes and history size, it might not be used to alter parameters for a few time steps.</p>

<p>Typically in RL systems like DQN, you would train for some randomly sampled batch between each action, out of some window of historical data (maybe all historical data, maybe last N steps). The amount of training you perform between actions is a hyper-parameter of your model, as is any sampling bias towards newer data. </p>

<p>For example in the <a href=""https://arxiv.org/pdf/1312.5602.pdf"" rel=""nofollow noreferrer"">Atari game-playing paper by Deep Mind team</a>, the agent sampled a mini-batch with 32 observations (state, action, reward, next state) to train the neural network on, in between each action, whilst playing the game online.</p>

<p>The concept of an epoch does not occur in online learning. If you are using each epoch to report performance metrics, and want to continue using comparable numbers, then you can pick some number of training iterations instead. There are no fixed rules for this, but you might want to consider reporting the same statistics on similar number of iterations as you trained on historical data - e.g. if you had 10,000 training samples in your history and are now training online with a mini-batch size of 50 per time step, then report progress every 10,000/50 = 200 time steps. </p>
","1","2","836","25573"
"23218","<p>The <code>[64, 64, 3]</code> shape you have found is a common convention to represent a colour image in (x, y, colour_channel) dimensions. </p>

<p>The key word here is <em>convention</em> - there is no inherently preferred way to represent a colour image in terms of fundamental maths or computing needs, and even within Python you will find multiple conventions, varying in the ordering within the dimension - e.g. OpenCV uses (x, y, channel) convention for the shape, but has channels in order BGR - so channel 0 is blue - whilst most other libraries will use RGB ordering (ignoring for now the alternative colour spaces).</p>

<blockquote>
  <p>My question is that in numpy if you have a three dimensional array, for accessing rows and columns you have to change the second and third entries in the indexing operator</p>
</blockquote>

<p>When you have a 3-dimensional array, what you decide to call ""rows"" and ""columns"" is also a convention. It depends partly on what that array represents, and there is no single way to visualise the contents.</p>
","3","2","836","25573"
"23243","<p>There are a few different data science and ML techniques you could throw at the numbers output by this game. You could try to analyse human players' style. You could generate a table of expected gains/losses by ignoring actual amount gained or lost and estimating the probability distribution of $x$ even though the player did not see it, just from know whether $x \gt w$ or $x \le w$ from enough examples.</p>

<p>If your end goal is to find out how a computer might <em>play</em> such a game, then one clear machine learning choice would be Reinforcement Learning (RL). It is not the only way for a computer to play a game - there are very many optimisation techniques for this. However, it is a machine learning approach - it learns from data observations - and in many variations it includes numerical analysis that you may find interesting, such as chance of winning from a certain position, or predicted future rewards.</p>

<p>RL is not a single algorithm, or even a single approach. Instead it is a way of framing a problem, and all ways of solving that kind of problem are considered to be RL. For RL to work, a problem needs to be framed as a <a href=""https://en.wikipedia.org/wiki/Markov_decision_process"" rel=""nofollow noreferrer"">Markov Decision Process (MDP)</a>. The good news is that your example game, and similar ones, are good examples of MDPs already.</p>

<p>In RL, the way that an agent plays a game is called its <em>policy</em>. The usual goals of RL include measuring the performance of a specific policy, or discovering the best policy.</p>

<p>So, where to start? First, if you want to try an RL process on your game you should probably do a few things:</p>

<ul>
<li><p>Understand what in the game is the <em>State</em>, what are the <em>Actions</em> and what are the <em>Rewards</em>.</p>

<ul>
<li><p>The state is anything that the agent knows about and can affect the outcome. Probably the only thing to worry about here is the current funds $f$, although this may not have a huge influence unless $w$ can always go up to $f$. Note each value is treated as a <em>different</em> state.</p></li>
<li><p>The action is the agent's choice of both $w$ and $g$ - note each specific combination is considered a <em>different</em> action. If there are a large number of actions, this can make learning harder, requiring more advanced RL algorithms, so I recommend initially you try variations of your game with limited number of choices.</p></li>
<li><p>The most obvious choice of reward is the change to $f$ at the end of each turn. It doesn't have to be though, it depends on what you consider a ""winning condition"" to be for the game. See below.</p></li>
</ul></li>
<li><p>Simplify the game options a little. Set a small limit to possible ranges of $x, w, g$. This will make it easier to try things out initially.</p></li>
<li><p>Set a clear goal that you want to achieve, and make sure that the game structure works for that. You may need to adjust the game representation for even simple change such as ""get the most reward in 10 turns"" - because that might encourage large wagers at certain steps, and the agent will need to know how many turns it has left (so your state becomes combination of $(f, t)$. Another variation that may work is to set the goal of getting to a certain amount of funds, such as 1000. In which case, the reward would not be the increase in $f$, but actually be +1 for getting to that target, and 0 for any other result. That might radically change the behaviour of the agent, which could be interesting to experiment with.</p></li>
<li><p>Look up simple Reinforcement Learning algorithms. I suggest you start with tabular methods - perhaps Monte Carlo Control, or Q-Learning. They can typically be implemented in a few tens of lines in Python/Numpy.</p></li>
</ul>

<p>A useful resource for RL is <a href=""http://incompleteideas.net/sutton/book/the-book-2nd.html"" rel=""nofollow noreferrer"">Sutton &amp; Barto's <em>Reinforcement Learning: An Introduction</em></a>. The draft of the second version is free to download.</p>
","2","2","836","25573"
"23256","<p>Perceptrons <em>can indeed</em> be implemented using this kind of boolean logic. Perceptron training is about finding suitable values to go into the if/else expressions, using example data. In your first example <code>someValue</code> has to be discovered, from known practice time and test results. In your second example <code>squishiness &gt; 7 and colour &gt;= green</code> has to be discovered by measuring some papayas. </p>

<p>In the more general case, the perceptron finds a hyperplane (line in 2D, plane in 3D) that discriminates between two target classes based on the input feature vector $\mathbf{x}$. That hyperplane could be in any direction and is expressed mathematically in terms of the input features. It still can be expressed as an if/else statement though:</p>

<p>If $\mathbf{\theta}^T \mathbf{x} + b &gt; 0$:<br/>
$\qquad$class A<br/>
Else:<br/>
$\qquad$class B<br/></p>

<p>The perceptron training finds parameter vector $\mathbf{\theta}$ and scalar value of bias $b$.</p>
","0","2","836","25573"
"23258","<blockquote>
  <p>I see clearly that this works for $l(w) \in \mathbb{R}$, but am wondering how it generalizes to vector-valued loss functions, i.e. $l(w) \in \mathbb{R}^n$ for $n &gt; 1$.</p>
</blockquote>

<p>Generally in neural network optimisers it does not*, because it is not possible to define what optimising a multi-value function means whilst keeping the values separate. If you have a multi-valued loss function, you will need to reduce it to a single value in order to optimise.</p>

<p>When a neural network has multiple outputs, then typically the loss function that is optimised is a (possibly weighted) sum of the individual loss functions calculated from each prediction/ground truth pair in the output vector.</p>

<p>If your loss function is naturally a vector, then you must choose some reduction of it to scalar value e.g. you can minimise the magnitude or maximise some dot-product of a vector, but you cannot ""minimise a vector"".</p>

<hr>

<p>* There is a useful definition of <a href=""https://en.wikipedia.org/wiki/Multi-objective_optimization"" rel=""nofollow noreferrer"">multi-objective optimisation</a>, which effectively finds multiple <em>sets</em> of parameters that cannot be improved upon (for a very specific definition of optimality called Pareto optimality). I do not think it is commonly used in neural network frameworks such as TensorFlow. Instead I suspect that passing a vector loss function into TensorFlow optimiser will cause it to optimise a simple sum of vector components.</p>
","2","2","836","25573"
"23296","<blockquote>
  <p>Would that be a good idea? </p>
</blockquote>

<p>That is hard to tell from your description. It is not an immediately bad idea. If it results in a better classifier (according to cross-validation), then it has probably worked. </p>

<p>The main things that would concern me about splitting behaviour data by quarter and treating as independent are:</p>

<ul>
<li><p>Your data samples will very likely be correlated when they share a person. You can work around this by careful splitting between training and cross-validation / test sets. Do <em>not</em> make a fully random split, but split by person - any individuals records should appear only in one of the training, cross-validation or test sets (assuming your goal is to take similar data in production from users who are not in your current database, and predict their class).</p></li>
<li><p>There could be seasonal variation in the records that reduce the effectiveness of the split. So a ""type 1"" person's records in APR-JUN might look like a ""type 0"" person from JAN-MAR.</p></li>
<li><p>How will you receive data in production - when you want to classify new users? If you only want to work on single-quarter data, then your new classifier is fine. If you have more data, you have to deal with your classifier maybe predicting different target variable for the <em>same person</em> depending on the quarter. You could combine these in some way - but if you do so, you should also do this in test to see what the impact of doing this is, which may be counter-productive (you end up with the same number of test examples as if you had not done the split). It might also be OK, perhaps it will add some regularisation.</p></li>
</ul>

<blockquote>
  <p>Do I have to be cautious of a particular hypothesis that could impact my results?</p>
</blockquote>

<p>You have to be very cautious about testing your classifier, because you could get data leakage from the cross-validation and test sets to the training set, which would make you think the classifier is generalising well when in fact it is not. The fix for this described above - split by person when deciding train/cv/test split.</p>

<blockquote>
  <p>I have some data available for some quarters of the year but sometimes it is missing (for instance let's imagine I don't have ""Age"" available for my last quarter) ; do I have to drop the feature ? </p>
</blockquote>

<p>Handling missing data is a complicated topic in its own right, there are lots of options. You can start with:</p>

<ul>
<li><p>If data is missing at random (i.e. there is no reason to suspect it is related to the target variable, or only impacts certain types of record), you can substitute the mean value of that feature from the training set, or <em>impute</em> it based on a statistical model from the other features.</p></li>
<li><p>If data is missing for reasons that might impact the target variable, then you should give that information to the classifier, because it might be an important feature in its own right. You can take the mean or more complex imputed value as before for the original feature, but <em>also</em> you should add a new boolean feature ""feature X was missing"". </p></li>
</ul>

<p>Whether or not you should use the partial data or drop it is not possible to say in general. If you are not sure, then try both and pick the version with the best cross-validation result.</p>
","2","2","836","25573"
"23307","<p>Simple games such as tic-tac-toe (noughts and crosses) can make interesting and practical toy problems to apply a variety of data analysis. More complex games can challenge state of the art algorithms and are areas on ongoing research.</p>

<p>Artificial intelligence that seeks to optimise game play may or may not be related to data science. </p>

<p>Most likely to be on topic are questions about markov models, bandit algorithms and reinforcement learning. Reinforcement learning specifically is a machine learning approach where analysis or optimal play can be discovered through observing outcomes of actions taken in a game, without prior knowledge of how the game is supposed to behave or be played.</p>

<p>Probably off-topic are questions about designing games, implementing game rules in software, ""traditional AI"" search algorithms such as A*.</p>
","0","5","836","25573"
"23308","Questions related to analysing or optimising games, such as board games, card games or video games.","0","4","836","25573"
"23318","<p><em>Choosing</em> a variation of your model is a form of training. Just because you are not using gradient descent or whatever training process is core to a model class, does not mean your parameters are not influenced by this selection process. If you generated many thousands of models with random parameters and picked the best performing one on a data set, then this is also form of training. In fact, this is a  valid way of optimising, called <a href=""https://en.wikipedia.org/wiki/Random_search"" rel=""nofollow noreferrer"">Random Search</a> - it is somewhat inefficient for large models, but it still works.</p>

<p>You may generate hundreds of models using the training data and using gradient descent or boosting (depending on what the training algorithm uses in your model), then select the one that performs best on cross-validation. In that case, then as well as the selection process that you intend to use this for, you are also effectively using the cv data set to fine-tune the training from the first step, using something quite similar to random search.</p>

<p>The main benefit of having two stages to testing (cv and test sets), is that you will get an <em>unbiased</em> estimate of model performance from the test set. This is considered important enough that it has become standard practice.</p>
","2","2","836","25573"
"23335","<blockquote>
  <p>Is there a reason I shouldn't be doing it this way?</p>
</blockquote>

<p>Depends on the nature of the data. There might be an element of ""Scissor/Paper/Stone"" in the competition you are scoring, where different strengths and weaknesses of competitors can combine such that Player A beats Player B, Player B beats Player C, but Player C beats Player A. In that case, you cannot produce reliable ranking between players by considering each entrant separately, and a network that rates each player individually will perform less well than one that can compare players.</p>

<p>If players are in more of a race-to-finish or score max points separately in a competition, then separately rating each player in each competition should be more reliable. And it is definitely easier to build and train a neural network to predict that.</p>

<p>An alternative, if your events are more like tournaments where entrants oppose each other (even if within some larger free-for-all), is to predict relative rank between pairs of players. This may not be consistent, so you will need to use a pairwise ranking method to resolve that for the final winner. If it really is a knockout tournament, and you know how the initial draw and team combinations will work, then you could maybe make a prediction by simulating the possible games.</p>

<p>There is nothing preventing you from combining these approaches in some way either.</p>

<p>Whichever method you use, you will want to think a little about what your metric is going to be to select the best approach. If you only care about predicting the winner, then accuracy of that prediction might be enough. If you care about where the eventual winner is placed, perhaps <a href=""https://en.wikipedia.org/wiki/Mean_reciprocal_rank"" rel=""nofollow noreferrer"">mean reciprocal rank</a> would be better (score 1 for correct prediction, 1/2 for predicting winner as ranked second, 1/3 if third etc).</p>
","0","2","836","25573"
"23345","<blockquote>
  <p>Is there any better approach apart from writing custom web scraper/parser?</p>
</blockquote>

<p>Probably not. Although this problem might be tractable using machine learning and natural language processing techniques in future, it would be far more work, and a much larger challenge, than the typical 80/20 rule you get from a web scraper plus regular expressions. </p>

<p>E.g. you write a web scraper with your best guess at the rules for extracting names. It finds 800 names correctly out of 1000. You analyse the problem sites, and add custom CSS selectors and tweak some of the regular expressions. That finds 150 of the remaining 200. Repeat the analysis and fix, you are down to maybe 10 problem sites, each requiring an individual fix. It is dull, repetitive work.</p>

<p>The alternative of attempting this problem using machine learning and NLP: First get the true labels for 10,000+ sites . . . </p>
","1","2","836","25573"
"23405","<blockquote>
  <p>If I combine all the fields value in my second Example and then form a single vector from that and then pass those vectors for classification, would that be the ideal way of doing it?</p>
</blockquote>

<p>In short, yes. You can convert each input variable into a vectorised feature separately, then concatenate those vectors into one long vector of features per example. Most statistical machine learning models don't care (or ""understand"") that the feature data has been formed in different ways, it is just treated numerically.</p>

<p>You do need to pay attention to each input data type in order to decide how best to convert to a usable feature. There is no single ""best"" way to do this, and this is partly an art - you add the ""science"" part of Data Science by testing your ideas and taking measurements, e.g. accuracy.</p>

<p>These are some quick thoughts on how you might prepare the data into features from each column:</p>

<ul>
<li><p><em>Comments</em>. You have already converted this, you are using a variant of ""bag of words"" to convert text into a fixed length numerical vector. This is a common approach and hard to beat. If you have a <em>lot</em> of data you can look into more sophisticated models that take account of word order, such as Recurrent Neural Networks, but that's a whole new subject area for now.</p></li>
<li><p><em>Gender</em>. You can convert this into a simple vector, using <a href=""http://www.ritchieng.com/machinelearning-one-hot-encoding/"" rel=""nofollow noreferrer"">one hot encoding</a> - this will have two or more columns.</p></li>
<li><p><em>City</em>. You could one hot encode this also. It might also be worth grouping cities and having a smaller feature vector. E.g. group by state/country before one hot encoding.</p></li>
<li><p><em>Age</em>. Either group and one hot encode, or scale to a smaller number,
by dividing by e.g. 50 (this helps algorithms like SGD by keeping distance metrics similar between different feature types). I suspect grouping in typical demographic splits (e.g. 0-11, 12-17, 18-24, 25-34 etc) and one hot encoding would work well for sentiment analysis, because that would capture some generalised differences between uses of text expression.</p></li>
</ul>
","1","2","836","25573"
"23444","<blockquote>
  <p>I'm not sure if I have to use a TimeDistributed layer or not</p>
</blockquote>

<p>You definitely don't <em>have</em> to use TimeDistributed. You have other choices, that may be equally valid, depending on your data:</p>

<ul>
<li><p>Flatten your example data into 81920 features per example and use a simple Dense layer.</p></li>
<li><p>Use a Conv3D layer.</p></li>
<li><p>Use some form of RNN, such as LSTM.</p></li>
</ul>

<p>From your data description, I would expect <em>either</em> a TimeDistributed or 3D CNN based approach to be a good first bet. Intuition suggests that the CNN would work better if there was minor change between frames (because it has capability to directly find subtle frame differences), the TimeDistributed approach would work better processing larger changes (because it will ignore frame differences until the fully-connected layer).</p>

<blockquote>
  <p>Should I use a TimeDistributed layer?</p>
</blockquote>

<p>Only you can answer this, by trying it and measuring performance of your classifier. However, it should function correctly, and intuition suggests it would be a good choice if your frames are in sequence but visually disjoint.</p>
","1","2","836","25573"
"23466","<blockquote>
  <p>There seems to be an error in the screenshot. The weight, $W$ should be transposed, please correct me if I am wrong.</p>
</blockquote>

<p>You are wrong.</p>

<p>Matrix multiplication works so that if you multiply two matrices together, $C = AB$, where $A$ is an $i \times j$ matrix and $B$ is a $j \times k$ matrix, then C will be a $i \times k$ matrix. Note that $A$'s column count must equal $B$'s row count ($j$).</p>

<p>In the neural network, $a^{[1]}$ is a $n^{[1]} \times 1$ matrix (column vector), and $z^{[2]}$ needs to be a $n^{[2]} \times 1$ matrix, to match number of neurons.</p>

<p>Therefore $W^{[2]}$ has to have dimensions $n^{[2]} \times n^{[1]}$ in order to generate an $n^{[2]} \times 1$ matrix from $W^{[2]}a^{[1]}$</p>
","6","2","836","25573"
"23482","<p>You should save the scaler params used to fit the training set and use the <em>same</em> ones to transform all other data used with the model from then on - whether CV, test or new unseen data.</p>

<blockquote>
  <p>After training/testing my algo, I will make new predictions using new actual data which will not be scaled or normalized.</p>
</blockquote>

<p>No that won't work. Once you add scaling/normalisation to the training pipeline, the exact same scaling (as in same scaling params, not re-calculated) should be applied to all input features.</p>

<p>The scikit-learn scalers like e.g. <code>StandardScaler</code> have two key methods:</p>

<ul>
<li><p><code>fit</code> should be applied to your training data</p></li>
<li><p><code>transform</code> should be applied after fit, and should be used on every data set to normalise model inputs.</p></li>
</ul>

<p><code>fit_transform</code> can be used on the training data only to do both in a single step.</p>

<p>If you need to do the training and predictions in different processes (maybe live predictions are on different devices for instance), then you need to save and restore the scaling params. One basic, simple way to do this is using <code>pickle</code> e.g. <code>pickle.dump( min_max_scaler, open( ""scaler.p"", ""wb"" ) )</code> to save to a file and <code>min_max_scaler = pickle.load( open( ""scaler.p"", ""rb"" ) )</code> to load it back.</p>
","0","2","836","25573"
"23511","<p>The loss functions are not related to the noise image.</p>

<p>The network is run twice to determine parameters to the loss functions. Once for the content image, and once for the style image. The code you are asking about occurs afterwards.</p>

<p>First the network is run against the content image (note no noise mixing here):</p>

<pre><code>sess.run([net['input'].assign(content_img)])
</code></pre>

<p>Aside: Uses an assignment function - not a <code>feed_dict</code> - because in order for the style transfer to work, the input has become a tf variable, not a placeholder.</p>

<p>Then the first loss function is defined against some of the network parameters from this run (this extracts the activations of interest from the networks as fixed params to the loss function - because it uses output from <code>sess.run</code>, not references to tensor variables):</p>

<pre><code>cost_content = sum(map(lambda l,: l[1]*
  build_content_loss(sess.run(net[l[0]]),net[l[0]]) , CONTENT_LAYERS))
</code></pre>

<p>The same thing is done for the style image:</p>

<pre><code>sess.run([net['input'].assign(style_img)])
cost_style = sum(map(lambda l: l[1]*
      build_style_loss(sess.run(net[l[0]]), net[l[0]]), STYLE_LAYERS))
</code></pre>

<p>And a final cost function defined:</p>

<pre><code>cost_total = cost_content + STYLE_STRENGTH * cost_style
</code></pre>

<p>Now, after the cost function is defined, the code initialises a new input (which is also our style transfer output):</p>

<pre><code>sess.run(net['input'].assign( INI_NOISE_RATIO* noise_img + 
                   (1.-INI_NOISE_RATIO) * content_img))
</code></pre>

<p>This line has nothing to do with defining loss. It just initialises the network ready to generate gradients against a starting image and the loss functions that were already defined beforehand. You can start with pure noise or a copy of the content image to start the process, and each will give a slightly different result. </p>

<p>The original paper started from a random image. Quoting from the paper (on page 10):</p>

<blockquote>
  <p>Thus we can change the initially random image $\tilde{x}$ until it generates the same response in a certain layer of the CNN [. . .]</p>
</blockquote>

<p>. . . however, starting from the content image was tried and used very soon afterwards by many in practice, and it works well. </p>

<p>This mix of both approaches is what the author has chosen, presumably because it gets pleasing results. As there are no lower layers in the network than the input, in fact you can start from an arbitrary image (e.g. all zeros), and you should still get a result, although it may not look as nice.</p>

<p>To (sort of) answer your question as written:</p>

<blockquote>
  <p>Why use a mix of the noise image and content image as input for the white noise bit in the content loss?</p>
</blockquote>

<p>The code is not doing that. The mixing is occurring as you say. But the line you show is not part of the content loss. The mixed image is the initial solution, and iterations will turn it into the stylised image.</p>
","1","2","836","25573"
"23543","<blockquote>
  <p>For backpropagation algorithm, is it true to have weight, $w$ transposed in the expression of $dz^{[1]} = w^{[2]T}dz^{[2]} * g^{[1]'}(z^{[1]}) $ ?</p>
</blockquote>

<p>Yes it is correct. You can show it is with some different arguments:</p>

<h2>Checking correct dimensions</h2>

<p>Although this is not a robust theoretical argument, checking dimensions is actually what I do in practice when I am confused when implementing NNs.</p>

<p>Here's the rough logic of a dimension check on the equation in your question:</p>

<ul>
<li><p>$W^{[2]}$ is a $n^{[2]} \times n^{[1]}$ matrix for the forward propagation to work.</p></li>
<li><p>$dz^{[2]}$ is a $n^{[2]} \times 1$ column vector (because it is the gradient of cost function with respect to $z^{[2]}$, so it has to have the same dimension as it)</p></li>
<li><p>$dz^{[1]}$ is a $n^{[1]} \times 1$ column vector</p></li>
<li><p>It is $dz^{[2]}$ that is being multiplied - so the multiplying matrix must be $\text{something} \times n^{[2]}$.</p></li>
<li><p>The output needs to be same dimensions as $dz^{[1]}$ - so the matrix must also be $n^{[1]} \times \text{something}$.</p></li>
<li><p>That means the multiplying matrix must have dimensions $n^{[1]} \times n^{[2]}$ - which $W^{[2]T}$ has (and no other component of the network could match)</p></li>
</ul>

<h2>Theory from item-by-item calculation, converted to matrix math</h2>

<p>But <em>why</em> is it $W^{[2]T}$ and not some other matrix with the desired dimensions? For that we need to go back to the chain rule. Note that the expression you have written is actually <em>two</em> steps of the chain rule combined - first we derive $da^{[1]}$ from $dz^{[2]}$, then we derive $dz^{[1]}$ from $da^{[1]}$. It is that first step that introduces terms from $W^{[2]}$, so I will just show that. Also, <a href=""https://datascience.stackexchange.com/questions/18279/deriving-backpropagation-equations-natively-in-tensor-form"">deriving this directly using matrix notation is more complex</a>, so we'll just drop into using the index values, and use the ""official"" partial derivative notation.</p>

<p>To calculate $da^{[1]}$ for a specific $i^{th}$ neuron, you have to sum over the gradients $dz^{[2]}$ for all the neurons it links to:</p>

<p>$$ \frac{\partial J}{\partial a_i^{[1]}} = \sum_j \frac{\partial J}{\partial z_j^{[2]}} \frac{\partial z_j^{[2]}}{\partial a_i^{[1]}} = \sum_j \frac{\partial J}{\partial z_j^{[2]}} W_{ij}^{[2]} $$</p>

<p>Doing this for each $i$ value in turn gets your the full $da^{[1]}$ vector. </p>

<p>Compare this to how the forward propagation works in the same network using the same notation and indexing:</p>

<p>$$z^{[2]}_j = b^{[2]}_j + \sum_i W_{ij}^{[2]} a_i^{[1]}$$</p>

<p>And you can see that the index used to drive the sum is different between forward and backward calculations:</p>

<ul>
<li><p>When we write $W^{[2]}_{ij}$, then $i$ is index of matrix row, $j$ is index of matrix column from $W^{[2]}$. </p></li>
<li><p>When multiplying forward, we sum over $i$, i.e. each row of $W^{[2]}$ multiplied by each matching element of $a^{[1]}$, and this is summed up. </p></li>
<li><p>When you look at the gradient, you can see the equivalent sum is over $j$. </p></li>
<li><p>To turn that gradient calculation into a normal matrix multiply (just so the matrix notation works, and you don't have to write out the sum) you have to swap rows and columns - which is what a transpose is.</p></li>
</ul>

<p>This is partly a definitions and notation issue. We could define a different kind of matrix multiply that worked with sums column-by-column. But this is not usually done, because multiplying by the transpose does the same thing.</p>
","8","2","836","25573"
"23568","<p>It is usually good news when an approximating function you use to take an action to optimise something is unbiased compared to the thing being approximated. It means your actions <em>in aggregate</em> will head towards the same optimisation point. </p>

<p>If the gradient was biased in the long term, then (depending on nature of the bias) an optimisation routine will converge incorrectly by following them in gradient descent, and give an incorrectly optimised answer.</p>

<p>It is sometimes OK if a value is initially biased, but the bias reduces over time towards zero. Not that this is the case here, but you will see that in some machine learning algorithms. For instance, in a neural network using momentum, typically the initial momentum values are biased, but the bias decays exponentially (it is actually possible to make an unbiased momentum estimate, but most implementations of SGD with momentum or Nesterov momentum do not do that - Adam implementations typically do).</p>
","2","2","836","25573"
"23649","<blockquote>
  <p>Now when I am reporting the test result, I set the epoch number for training on my training data a fixed number of times, and when my network is doing the best on the test set, I stopped the training, saved that model for future use, and report that result. My question is about this last part. Am I doing something wrong on reporting this result?</p>
</blockquote>

<p>Technically yes this is incorrect process for reporting an unbiased test metric. This could be a bad over-estimation of performance if cv results are noisy and vary randomly epoch-to-epoch. You should in theory treat the early stopping epoch number same as your other hyper-parameters, discover a good value from the lower-level cross-validation and stick with it. </p>

<p>There is a problem with my suggestion though - the early stopping epoch number is sensitive to training data set size, and you just increased the size when you changed from lower level cross-validation to the higher level one. So you might get an unbiased measure at the expense of significantly worse results.</p>

<p>First, take a look at your learning curves. Just how sensitive are the cv results to epoch number? If they are not sensitive - no obvious over-fit over a reasonable range of epoch numbers - then just pick a mid-range fixed number of epochs that applies to all folds. That way you will have your unbiased estimate, and <em>probably</em> not compromised on model quality.</p>

<p>Alternatively, you may just have to be happy knowing that your test estimate is biased, but you have reduced the variance significantly by using 10-fold CV, and have only searched one hyper-parameter at the top level (not all three). It may only be slightly biased, and still a good estimate. The smoother and less jittery the learning curves are, the more you can get away with this - but sadly you won't get a measure of the bias, so you'll never be 100% sure.</p>
","4","2","836","25573"
"23695","<p>Cross-entropy loss still works with probabilities in $[0,1]$ as well as $\{0,1\}$. Most importantly, $\hat{y} = y$ is still a stationary point (although it will not equal $0$).</p>

<p>It is also the case that the possible improvement in loss (and immediate gradient) is larger for $\hat{y} = 0.99, y = 0.89$ than for $\hat{y} = 0.4, y = 0.5$. If you use a sigmoid output, then the gradient at the logit $\hat{y} - y$ still applies - the larger gradient of the loss function scales inversely to the lower gradient of the sigmoid at that point.</p>

<p>So, in short, yes use binary cross-entropy loss for single-class probabilities, even when they are not strictly in $\{0,1\}$.</p>
","3","2","836","25573"
"23822","<p>Your hypothesis about missing colours in your samples affecting results in production could be correct. However, it is trivial to convert images to greyscale as you load them from storage. So keep them in colour, and convert them as you load them if you need black and white.</p>

<p>That way you can try both with and without colour as input and you will have your answer.</p>

<p>This is very little effort to do in practice, and allows you to do the ""science"" part of data science by comparing two approaches and measuring the difference. This is standard practice, even if you are reasonably certain one approach or another is ""the best"", it is normal to explore a few variations. When you are not sure, then it is even more important to try it and see.</p>

<p>To test your hypothesis, you could put all the t-shirts of a particular colour in your test set. If that reduces the accuracy of your results with the colour model, it would back up your concern. One fix for that might be to remove colour information from the model, if it is not relevant to the task. An alternative is to collect more data so you have enough samples of different colour shirts. However, you might find if you are fine-tuning a neural network trained on many more images (e.g. Inception v5) that the impact of colour is less even though your samples do not cover all possible T-shirt colours.</p>
","1","2","836","25573"
"23838","<blockquote>
  <p>are there ""machine learning"" ways to evaluate a pseudorandom number generator?</p>
</blockquote>

<p>No it is the wrong tool for the job. Statistical tests, like those you mention <em>monobit, runs, poker test</em> etc, are a way to evaluate a PRNG and search for bias or unwanted patterns which would establish that it had flaws. </p>

<p>If you try to use ML on random data - e.g. map dependent x,y data to a label z where all are determined randomly - then one sign the data is random will be that the ML does poorly. However, this will not give any measure of quality of the randomness. Using a high variance model it will still be possible to fit the data, although cross validation and test sets will score badly.</p>

<blockquote>
  <p>It might be way over my head, but could a generative adversarial network learn a truly pseudorandom generator that way?</p>
</blockquote>

<p>A GAN learns how to map a low dimensional space into a manifold (sub-shape) of a larger dimensional space, allowing you to sample from a population in the higher dimension. It cannot inject ""noise"" into that space other than using something provided already by a PRNG. The GAN itself cannot be the PRNG. The GAN sampling routine is typically driven by an external PRNG (i.e. not driven by the weights or processing of the GAN)</p>

<p>Probably you could construct a PRNG from a RNN, although I suspect it would be hard to create one of any quality, and even if you could get something close to statistical randomness from it, the performance is likely to be poor compared to standards such as Mersenne Twister.</p>
","3","2","836","25573"
"23845","<p>This part is badly enough wrong that you will get poor results:</p>

<pre><code># Store layers weight &amp; bias
weights = {
    'h1': tf.get_variable('h1', shape=[n_input, n_hidden]),
    'h2': tf.get_variable('h2', shape=[n_hidden, n_hidden]),
    'h3': tf.get_variable('h3', shape=[n_hidden, n_hidden]),
    'h4': tf.get_variable('h4', shape=[n_hidden, n_hidden]),
    'h5': tf.get_variable('h5', shape=[n_hidden, n_hidden]),
    'h6': tf.get_variable('h6', shape=[n_hidden, n_hidden]),
    'h7': tf.get_variable('h7', shape=[n_hidden, n_hidden]),
    'out': tf.Variable(tf.random_normal([n_hidden, n_output]))
}
</code></pre>

<p>The problem is the initialization. Your hidden layers have no initialization at all. The output layer initializes with likely the wrong scale. To match Keras, your initialiser should be something like:</p>

<pre><code>tf.random_normal([n_in, n_out]) * (math.sqrt(2.0/(n_in + n_out))
</code></pre>

<p>or you can use the built-in Xavier initialiser:</p>

<pre><code>tf.contrib.layers.xavier_initializer()
</code></pre>

<p>In addition, you can probably drop the initializer for the bias values.</p>
","2","2","836","25573"
"23971","<p>There is no ""mismatch"" of accuracy. Your problem is that you have an image segmentation problem where 99% of the pixels should be zero. So getting 99% accuracy is trivially easy. A model that predicts just blank output images would score roughly the same as your network has so far. Your accuracy metric is not meaningful.</p>

<p>The low Dice coefficient score gives you a better idea of what is going on. That covers positive case matches only, and a good score would be close to 1.0. The low score shows that the network is not focussing on getting positive pixels correct. Instead the network has learned to predict close to zero everywhere, because that minimises the loss metric to a first approximation very well.</p>

<p>How to fix things?</p>

<p>First, stop reporting accuracy. This metric is misleading you, and you need to find another so that you can assess your model fairly. As you already have Dice, you could just drop accuracy and use Dice instead. Alternatively, you could use a weighted accuracy, inversely proportional to mean number of pixels in each class of your training data. Looking at <a href=""https://keras.io/metrics/"" rel=""noreferrer"">Keras metrics options</a>, you will probably want to add another custom metric for this. You have already done this with the <code>dice_coef</code> function, so that should not be a problem.</p>

<p>Second, train your network as it is for longer. Your example has only one epoch. Depending on how many training examples you have, this is probably too low for an image processing network. Try increasing the number in a geometric advancement - 3, 10, 30, 100, 300, 1000 - using the new metric to see if you get any improvement during training.</p>

<p>You could also try altering the cost function to weight it in favour of positive pixels. Keras' <a href=""https://keras.io/models/sequential/#fit"" rel=""noreferrer"">fit function has a class_weight parameter</a> for this purpose, you could set it e.g. to <code>class_weight = {0:1, 1:20}</code></p>
","7","2","836","25573"
"24074","<p>Essentially you are correct, there are a lot of calculations necessary to process inputs and train neural networks.</p>
<p>You have some terminology a bit wrong or vague. E.g.</p>
<blockquote>
<p>In a feedforward meural network each neuron of the first layer multiplied with all the neurons of the second layer.</p>
</blockquote>
<p>The neurons do not multiply together directly. A common way to write the equation for a neural network layer, calling input layer values <span class=""math-container"">$x_i$</span> and first hidden layer values <span class=""math-container"">$a_j$</span>, where there are N inputs might be</p>
<p><span class=""math-container"">$$a_j = f( b_j + \sum_{i=1}^{N} W_{ij}x_{i})$$</span></p>
<p>where <span class=""math-container"">$f()$</span> is the activation function <span class=""math-container"">$b_j$</span> is the bias term, <span class=""math-container"">$W_{ij}$</span> is the weight connecting <span class=""math-container"">$a_j$</span> to <span class=""math-container"">$x_i$</span>.</p>
<p>So if you have <span class=""math-container"">$M$</span> neurons in the hidden layer, you have <span class=""math-container"">$N\times M$</span> multiplications and <span class=""math-container"">$M$</span> separate sums/additions over <span class=""math-container"">$N+1$</span> terms, and <span class=""math-container"">$M$</span> applications of the transfer function <span class=""math-container"">$f()$</span></p>
<blockquote>
<p>And in addition to a forward pass in a typical Neural network they also have a backward pass that because of my calculations doing earlier they are 1,800 derivatives (gradient) for the entire backward pass.</p>
</blockquote>
<p>It doesn't work quite so directly, and there as a small factor of more calculations involved (you do not calculate each derivative with a single multiplication, often there are a few, some results are re-used, and other operations may be involved). However yes you do need to calculate a derivative for each weight and bias term, and there are roughly that number of weights in your network that require the calculations done.</p>
<p>Your suggested numbers are actually quite small compared to typical neural networks used for image problems. These typically perform millions of computations for a forward pass.</p>
<blockquote>
<p>That's why a CPU computer takes so long to train a model because it has to do about 3,600 (1,800 + 1,800 ) mathematical operations.</p>
</blockquote>
<p>Actually that is a trivial number of calculations for a modern CPU, and would be done in less than a millisecond. But multiply this out by a few factors:</p>
<ul>
<li><p>You must do this for <em>each and every example in the training data</em></p>
</li>
<li><p>Your example network is small, think bigger</p>
</li>
<li><p>This does not include the activation function calculations - typically slower than a multiply</p>
</li>
<li><p>Your rough estimate ignores some of the necessary operations, so as a guesstimate, multiply number of CPU-level operations by 3 or 4 from your analysis.</p>
<p>. . . and the number of operations does start to get to values where CPUs can take hours or days to perform training tasks in practice.</p>
</li>
</ul>
","2","2","836","25573"
"24394","<p>Applying any non-linear model in a <a href=""http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/"" rel=""nofollow noreferrer"">model stacking</a> approach should do what you want. In brief the approach is to take predictions from other models as new features, plus the original data and labels, then use them to train a meta-model. Read the link, it offers practical advice on how to do this within a k-fold validation framework, which will give it a much better chance of doing well.</p>

<p>Non-linear models that combine simpler units - e.g. neural networks and models that use multiple decision trees (e.g. xgboost) - already perform this kind of internal split during training. So if you are already using those, you might not gain such a big improvement over simpler ensemble techniques, such as taking a mean or weighted mean over models.</p>
","2","2","836","25573"
"24420","<p>Whether or not a Dutch Auction can be framed as a reinforcement learning problem depends on whether there is meaningful state that can be changed by the auctioneer's actions.</p>

<p>If the bidders are behaving in a simplified rational manner* - e.g. they are working to a simple mechanic of buying the item when it reaches a price they have decided to pay - then the behaviour of the auctioneer will not make much difference, it is very formulaic as it is already part of a price optimising strategy. There is no meaningful state change between actions - the bidder valuations and attention stay fixed no matter what the auctioneer does. In an extreme, automated case, the optimal action would be to set a very high initial price and then reduce it in many small increments so as to trigger the highest purchase possible from bidders. This is not done in real auctions because it is a waste of everyone's time (and it would be weird, socially). However, it shows you a problem with the framing of the problem. In a basic rational setting, with nothing to gain or lose apart from money and value of the item, there is little room for altering strategy.</p>

<p>The problem becomes far more interesting when bidders are not purely rational optimisers themselves (or have some other unknown values related to the auction such as personal time they are willing to invest in the process, or rivalry with other bidders), and when some information is available about their past behaviour, or at least in general about <em>human</em> behaviour. In that case the problem can modelled as a <a href=""https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process"" rel=""nofollow noreferrer"">Partially Observable Markov Decision Process (POMDP)</a>, and you can concern yourself with rewards that might vary due to your actions - e.g. risk of making bidders skittish with ""odd"" behaviour as the auctioneer. There are limits to this due to the anonymous bidders. If you kept bidder identity between items (if a single auction consisted if multiple item sales with the same bidders) then you might gain some information on each sale in order to optimise better your actions within that auction.</p>

<p>Another possible way to have stateful behaviour is if item ownership has value, such that market forces of supply/demand come into effect (once a bidder has one item maybe they value the next one less, or maybe more if multiple items have higher value to the bidder when kept together). Again this is only going to be relevant to the auctioneer if bidder identities remain constant for a multi-item auction, and even then only makes a difference if there is non-rational pricing behaviour in the bidders.</p>

<p>The main problem you will have with training RL auctioneer to work with non-rational bidders, is modelling those bidders. You could maybe bootstrap with data from real auctions with human auctioneers - this can still be done in a Q learning framework. Q-Learning can learn about optimal policy from observing non-optimal behaviour within limits, optimal actions still need to be observed, but don't have to all be in the same episodes. However, in general to improve from that model and apply reinforcement learning, you have to you put it in control. When it explored unusual actions in order to learn, that could make it a liability if real money was at stake - to defend against this you may need to keep action choice limited.</p>

<hr>

<p>* Staying with ""purely rational"" approaches, you could get more sophisticated. You could maybe model the bidders as trying to predict the auctioneer and other bidder's next action, and extract the best expected value, in which case you have framed the problem more like adversarial game theory. This could make the auctioneer's actions have more impact. The most interesting part of this is occurring in the bidder though, not the auctioneer, as the bidder would have to incorporate a meaningful value to owning the item and be smart enough to second-guess other bidder's valuations. </p>

<p>The auctioneer's goal would then be to find the best price that the highest bidder was willing to pay, and behave so that the highest bidder predicted that a rival would bid on the next lower increment. This would be very similar to the non-rational approach in practice, but more tractable to do in simulation - you would need a good simulated model of the bidders before you could train an auctioneer, and the bidders would probably need to be RL or similar too, with both auctioneer and bidder models evolving together.</p>

<p>Intuitively, and assuming the game theory model was stable here, I would expect the auctioneer, with no other information to go on than the number of bidders, would have an optimal strategy where selling price decreased in increments inversely sized to some fractional power of the number of bidders, as more rivals means that bidders will predict rivals willing to pay values closer to their own evaluation. A partially stochastic policy might also be favourable to the auctioneer, as being unpredictable means that bidders cannot rely on guessing the next price reduction accurately (although they might counter that by choosing whether to bid stochastically).</p>
","2","2","836","25573"
"24521","<blockquote>
  <p>It is my understanding that both methods should be achieving the optimal policy, can you confirm or deny my understanding?</p>
</blockquote>

<p>Yes, I would expect a neural network for Q Learning to find the optimal policy, provided it remains stable*. The value estimates might be slightly inaccurate, but the resulting policy should be completely optimal. That is because in tic tac toe, all the value estimates should be -1, 0 or +1, and the data is cleanly separated. </p>

<p>You should be able to get a neural network to learn the optimal Q table from the first experiment using supervised learning. In fact that would be a good test of whether your NN has capacity to learn that table.</p>

<p>* Neural networks added naively to Q-learning agents are often not stable. In fact that is so common a problem in scaling up RL agents that it has a name: ""<a href=""https://davidsanwald.github.io/2016/12/11/Double-DQN-interfacing-OpenAi-Gym.html"" rel=""nofollow noreferrer"">the deadly triad</a>"". This is generally not solved by elegant mathematical changes to the agent, but by some engineering tricks:</p>

<ul>
<li><p>Experience replay. Save observations (S, A, R, S') and sample from this memory table later to train in mini-batches.</p></li>
<li><p>Alternating networks. Use an old frozen copy of the neural network to calculate $\text{max}_{a'} Q(S',a')$ for the TD target $R + \gamma\text{max}_{a'} Q(S',a')$</p></li>
</ul>
","1","2","836","25573"
"24581","<h3>Precision</h3>

<p>Your step change in precision looks to be almost entirely explained by the change in positive class frequency. It is reasonable to expect the proportion of false positives to increase when increasing the proportion of negative examples. Even if you assume your cv results were perfect, then you would see some increase.</p>

<p>As an example, assume you have cv results representative of test results 
 - which means same distribution before random under-sampling, and no over-fit to the cv set. </p>

<p>Say you measured precision at 0.97 with a t:f ratio of 1:2, and for the sake of simplicity that this was due to the following confusion table:</p>

<pre><code>      Predicted:  T    F
Real T           97    3
Real F            3  197
</code></pre>

<p>What precision should you expect when going to the real distribution? That is the same as multiplying the bottom row of the confusion table by 50. Precision is $\frac{TP}{TP+FP}$, so your expected precision would be $\frac{97}{97+150} \approx 0.39$</p>

<h3>Recall</h3>

<p>The same effect does not impact recall, because it is about the ratio between true positive and false negative. So when you change the ratio of positive to negative classes, in theory recall should be unaffected.</p>

<p>In your case, recall has been affected, but a lot less than precision. that is promising. A drop from 0.95 to 0.85 between cv and test is not great perhaps, but it doesn't point to a really major problem, just room for improvement.</p>

<p>There are a few possible causes. The ones that I can think of are:</p>

<ul>
<li><p>Your test set might be too small, so estimates of precision and recall have large error. So in fact there is no problem . . .</p></li>
<li><p>Test distribution might be different to train and cv set. </p></li>
<li><p>Train/CV set split might allow some data leakage (e.g. they share some common features such as data about the same person, and should be split by that common feature). In which case CV estimates could be too high.</p></li>
<li><p>Your mechanism for under-sampling the negative class may be biased.</p></li>
</ul>

<h3>What to do?</h3>

<p>First of all, these results are unlikely to be anything directly do with faults in the model, and are not informed much by the training curves. They are also not that bad out of context (i.e. they are much better than simply guessing which items are in the positive class) - the question is more whether you could improve on them, and what the costs are to you for the different types of error. It might be worth you actually assigning real-world comparable costs to each type of error, to help decide whether your model is successful/useful and to pick the best model later on.</p>

<p>One thing from the training curves is that your cv and training loss look pretty close. It implies you are not over-fitting to the training data (or you should check to a train/cv data leak). You may have room to add more parameters and improve the model in general.</p>

<p>It is possible you could make the model even better with different hyper-parameter choices, feature engineering etc, and that would improve the scores. There is no general advice for that though, it depends on what you have available.</p>

<p>It might be worth experimenting with training on the unbalanced training set (take the raw data without undersampling) and instead weighting the loss function, so that costs are larger for inaccurate classification of positive class. This is not guaranteed to fix your problem, but will increase the amount of data you use for training.</p>

<p>Otherwise, you should investigate whether any of the possible causes listed above is likely and try to apply fixes.</p>

<p>Finally, in this situation, it is not unheard of to have a four-way data split:</p>

<ul>
<li><p>A ratio-adjusted set split two ways:</p>

<ul>
<li>Training data</li>
<li>CV or ""Dev"" set A</li>
</ul></li>
<li><p>A same as production set split two ways:</p>

<ul>
<li>CV or ""Dev"" set B</li>
<li>Test set</li>
</ul></li>
</ul>

<p>CV set A is used to perform early stopping and low-level model selection.</p>

<p>CV set B is used to perform high-level model selection against production metric.</p>

<p>Test set is used to assess the chosen ""best"" model without bias.</p>
","5","2","836","25573"
"24594","<blockquote>
  <p>Lets say you have a max pooling layer that gives 10 downsampled feature maps. Do you stack those feature maps, treat them as channels and convolve that 'single image' of depth 10 with a 3d kernel of depth 10? That is how I have generally thought about it. Is that correct?</p>
</blockquote>

<p>Yes. The usual convention in a CNN is that each kernel is always the same depth as the input, so you can also think of this as a ""stack"" of 2D kernels that are associated with the input channels and summed to make one output channel - because under the convention that $N_{in\_channels} = N_{kernel\_depth}$ this is mathematically the same. Expressing as a 3D convolution allows for simpler notation and code.</p>

<blockquote>
  <p>On the second convolution layer in the above visualization most of the feature maps only connect to 3 or 4 of the previous layers maps. Can anyone help me understand this better?</p>
</blockquote>

<p>The diagram is non-standard in that respect, although it seems to show pooling and fully-connected layers as normal. It might be a mistake in the diagram, or something unconventional about that specific CNN.</p>

<blockquote>
  <p>If our input is a color image our first convolution kernel will be 3D. This means we learn different weights for each color channel (I assume we aren't learning a single 2D kernel that is duplicated on each channel, correct)?</p>
</blockquote>

<p>Correct. You can see this in the <a href=""http://cs231n.github.io/understanding-cnn/"" rel=""nofollow noreferrer"">visualised filters for AlexNet</a> (do note that for computational reasons, AlexNet specialised one half of its filters to work in greyscale, and had other clever optimisations that we don't use nowadays because available GPU power is high enough to not need them). Most implementations will also treat a greyscale image as a 1-channel 3D shape for consistency.</p>
","4","2","836","25573"
"24600","<p>As MountainCar is often solved with $\gamma = 1$ and a negative reward per timestep, you would immediately hit a problem with your ability to calculate a maximum action value in that case. However, I don't think that is your problem here, the discounted return with positive reward at the end should still encourage desired behaviour for the problem.</p>

<p>It is likely that you are experiencing known problems with RL, and the ""deadly triad"":</p>

<ul>
<li><p>Function approximation (neural network)</p></li>
<li><p>on a bootstrap method (Q-Learning or any TD-learning approach)</p></li>
<li><p>off policy (learning optimal policy from non-optimal behaviour*, which is a feature of Q-learning)</p></li>
</ul>

<p>This combination is often unstable and difficult to train. Your Q value clamping is one way to help stabilise values. Some features of DQN approach are also designed to deal with this issue:</p>

<ul>
<li><p>Experience replay. Agent does not learn online, but instead puts each sample (S, A, R, S') into a memory table and trains the neural network on mini-batches sampled from this memory. Commonly this minibatch update is run on every step (after enough experience collected) and might be e.g. size 32. So learning updates happen faster than experience is collected.</p></li>
<li><p>Frozen bootstrap target network. The network used to calculate $\operatorname{max}_{a'} Q(s', a')$ when setting the target values to learn (in td target $R + \gamma \operatorname{max}_{a'} Q(s', a')$) is kept stable, and refreshed to use a copy of the current weights after a certain number of steps (e.g. 100 or 1000, or once every 10 episodes).</p></li>
</ul>

<hr>

<p>* Technically off-policy learning is learning the cost function for any policy $\pi$ from a different behaviour policy $b$, but the common case is for control systems attempting to find an optimal policy from an exploratory policy.</p>
","2","2","836","25573"
"24695","<p>Neural style transfer is not really machine learning, but an interesting side effect/output of machine learning on image tasks. When performing neural style transfer using a pre-trained model, then a significant amount of supervised machine learning has already occurred to enable it.</p>

<p>The style transfer algorithm <em>is</em> still an example of gradient-based cost function optimisation, which it shares with many supervised and unsupervised learning algorithms.</p>

<p>The output of style transfer is partially a probe into what a network has learned about different levels of structure in the problem domain it has been trained on. However, its main use has been to generate images with altered/mixed aesthetics for art and entertainment.</p>
","11","2","836","25573"
"24879","<p>This is not a ""vanishing gradient"" problem, it is just your network converging as designed.</p>

<p>It is normal for gradients to become low as you approach convergence. In a really simple problem, like your linear regression, it is relatively easy to get a gradient of zero (within combined rounding errors). That is because, near a stationary point, gradients do approach zero. This can also be a weakness of gradient descent in general - learning will slow or halt near any stationary point, hence concerns about finding local minima as opposed to global minima.</p>

<p>The way to check your gradients are correct is to test them against small weight deltas. Pick a set of weight parameters for the network. Calculate the gradients using your code. Then to test, take each weight in turn, change it by +/- $\epsilon$ and use both variants to generate cost values, use them to estimate the gradient for that weight $\frac{J_{+\epsilon} - J_{-\epsilon}}{2\epsilon}$. Use this alternative (and much slower) gradient calculation to build up a second opinion of what <code>dJdW</code> should be. Then compare the two values. This process is often called <a href=""http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization"" rel=""nofollow noreferrer"">gradient checking</a>.</p>
","0","2","836","25573"
"24897","<p>This is going to be a situation where there will be no fixed rule. One important factor is how meaningful colour differences are to the other parts of your problem. If colour has low correlation/impact in a supervised learning/prediction scenario for example, and the dataset is noisy, then you will want to merge more colours (at a higher fraction of total number) to reduce sampling bias effects that might otherwise assign importance to the colour and increase error rates in test and production.</p>

<p>The safest approach is to treat the colour combination threshold as a hyper-parameter to the model building process, and test to see what differences it makes. If there is little or no impact to model effectiveness, then a higher threshold could be useful purely to reduce number of parameters in the model - decreasing resources used to train and run it.</p>

<p>If that seems time-consuming, then picking something by feel (e.g. your idea of picking count less than 50 in the dataset) is not usually too bad, at least to start with. You can go back and re-evaluate your choice if you have problems with the model.</p>

<p>One other possibility for feature engineering is to use the rareness of the specific colour as an additional feature. So in addition to categories for the popular colours and an ""other colour"" category, add a real value ""colour frequency"" = the observed ratio of that colour in the training set. Whether or not this is useful will depend on the problem, but it may help address some of the lost information when merging categories with a wider range of rareness values, assuming that unusual colours indicate anything at all (they may not).</p>
","1","2","836","25573"
"24924","<p>The de-correlation effect is more important than following sequence of trajectories in this case.</p>

<p>Single step Q-learning does not rely on trajectories to learn. It is slightly less efficient to do this in TD learning - a Q($\lambda$) algorithm which averages over multiple trajectory lengths would maybe work better if it were not for the instability of using function approximators.</p>

<p>Instead, the DQN-based learning bootstraps across single steps (State, Action, Reward, Next State). It doesn't need longer trajectories. And in fact due to bias caused by correlation, the neural network might suffer for it if you tried. Even with experience replay, the bootstrapping - using one set of estimates to refine another - can be unstable. So other stabilising influences are beneficial too, such as using a frozen copy of the network to estimate the TD target $R + \text{max}_{a'} Q(S', a')$ - sometimes written $R + \text{max}_{a'} \hat{q}(S', a', \theta^{\bar{ }})$ where $\theta$ are the learnable parameters for $\hat{q}$ function.</p>

<p>It might still be possible to use longer trajectories, sampled randomly, to get a TD target estimate based on more steps. This can be beneficial for reducing bias from bootstrapping, at the expense of adding variance due to sampling from larger space of possible trajectories (and ""losing"" parts of trajectories or altering predicted reward because of exploratory actions). However, the single-step method presented by DQN has shown success, and it is not clear which problems would benefit from longer trajectories. You might like to experiment with options though . . . it is not an open-and-shut case, and since the DQN paper, various other refinements have been published.</p>
","6","2","836","25573"
"24930","<p>As far as I can tell, there is no specific rule. It will depend in part on how crowded your scene will become with items that you want to detect and locate separately. Creating a high granularity grid increases computational cost for training, and there is no reason to do so if it would only cover additional cases that are much rarer than the detection accuracy that the base algorithm achieves.</p>

<p>The choice can be driven by the ground truth data. The ground truth for YOLO needs to be expressed in the form of grid locations with classes and bounding rectangle sizes. If you don't find any training examples where you want to label two items with their centre inside the same grid square, then this is a good indication that your grid size is fine-grained enough. </p>

<p>Even if there are one or two examples with a clash like this, you may be able justify labelling just one item in the ground truth and be OK with a resulting model that is not able to cope with close overlap between two separate objects. And even with smaller grid squares, YOLO may not be able to learn to separate the objects, if such an overlap only occurs rarely. </p>

<p>I would expect a simple rule of diminishing returns applies. As datasets grow larger, and object detection can be trained on more powerful computers, we may see state of the art models still using YOLO but with more grid points.</p>
","1","2","836","25573"
"25027","<p>This small instability at the end of convergence is a feature of Adam (and RMSProp) due to how it estimates mean gradient magnitudes over recent steps and divides by them.</p>

<p>One thing Adam does is maintain a rolling geometric mean of recent gradients and squares of the gradients. The squares of the gradients are used to divide (another rolling mean of) the current gradient to decide the current step. However, when your gradient becomes and stays very close to zero, this will make the squares of the gradient become so low that they either have large rounding errors or are effectively zero, which can introduce instability (for instance a long-term stable gradient in one dimension makes a relatively small step from $10^{-10}$ to $10^{-5}$ due to changes in <em>other</em> params), and the step size will start to jump around, before settling again.</p>

<p>This actually makes Adam less stable and worse for your problem than more basic gradient descent, assuming you want to get as numerically close to zero loss as calculations allow for your problem.</p>

<p>In practice on deep learning problems, you don't get this close to convergence (and for some regularisation techniques such as early stopping, you don't want to anyway), so it is usually not a practical concern on the types of problem that Adam was designed for.</p>

<p>You can actually see this occurring for RMSProp in a comparison of different optimisers (RMSProp is the black line - watch the very last steps just as it reaches the target):</p>

<p><a href=""https://i.stack.imgur.com/qAx2i.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/qAx2i.gif"" alt=""enter image description here""></a></p>

<p>You can make Adam more stable and able to get closer to true convergence by reducing the learning rate. E.g. </p>

<pre><code>optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
</code></pre>

<p>It will take longer to optimise. Using <code>lr=1e-5</code> you need to train for 20,000+ iterations before you see the instability and the instability is less dramatic, values hover around $10^{-7}$. </p>
","23","2","836","25573"
"25064","<p>Face recognition from images is still an open area of research. Many different techniques have been tried.</p>

<p>One approach is not to directly learn the identities of the people as classes, but to learn a multi-dimensional embedding for images of faces (e.g. 128 dimension vector) and train the network to make this embedding value closer for two different images of the same person than for two images of different people*. </p>

<p>You can use the network to generate feature vectors for each person who is registered, and store those values. Later when you want to identify someone, find the closest one (according to norm of the difference) and if the distance is less than a certain cutoff you can consider it to be a match.</p>

<p>To perform the training for the embeddings with this goal in mind, you can use a loss function called triplet loss, which is trained on 3 forward passes at a time. The network calculates feature vectors for an anchor image, a positive match (i.e. another image of the same face, so you need at least 2 for each person, and they should be taken from different contexts) and a negative match. Then it measures whether the distance between features from same person is less than distance between different people. The loss function to express that looks like:</p>

<p>$$\text{max}(0, |A - P|^2 - |A - N|^2 + \alpha)$$</p>

<p>. . . where $A$ is feature vector for the anchor image, $P$ for positive match image, and $N$ for negative match image. The parameter $\alpha$ is a hyper-parameter which creates a ""margin"" that you reward the model for maintaining between different identities. The idea being that if an image of different person is clearly further away in the feature space than a cutoff, the loss on that training example is zero. The model does not get rewarded for very large differences, just having ""consistently enough distance"".</p>

<p>The tricky thing with triplet loss is picking which samples to use for training. Purely random samples could make the network's task too easy. You need it to focus a bit on the more difficult cases to prevent mistaken identity. One way of doing this is to pick preferentially from closer faces, e.g. using k-means clusters and picking negative cases within own cluster.</p>

<p>The model can be trained on large numbers of faces, not necessarily the ones you will later want to classify. The point is to learn a feature vector that emphasises important differences between faces (and ideally ignores other variations such as lighting, background, pose). It's possible to start with any image classifier and adapt it for this task, including your original face classifier (just remove the final classifier layer and work with one of the later fully-connected layers as your output).</p>

<blockquote>
  <p>How can we scale up the number of classes for deep learning after training a model?</p>
</blockquote>

<p>This approach may not work well in all cases covered by your question title. I have recommended it for face recognition based on a <a href=""https://www.coursera.org/learn/convolutional-neural-networks"" rel=""nofollow noreferrer"">Coursera course on CNNs</a> where a few lectures and coding assignment are about face recognition specifically.</p>

<p>In some cases you may be better off re-building and re-training the model with new training set data, augmented by examples of your new class - you might start by fine-tuning only the last few layers.</p>

<hr>

<p>* In fact this will be the case in many CNN classifiers, but is more likely to be reliably linked to a person's identity if you have trained the network explicitly for distinguishing between identities in general (as opposed to just classify specific identities). The triplet loss approach is one way to train for useful representations, and makes the goal of disambiguating identity explicit.</p>
","4","2","836","25573"
"25098","<p>Each layer has a limited amount that it can transform the layer below it. There is one linear component (weighted sum of output of layer beneath it), and one non-linear component (typically ReLU).</p>

<p>It is in theory possible to approximate <em>any</em> function with a large enough single layer in a fully-connected network. However, a stack of similar smaller layers is more expressive using less resources. That means for the same number of <em>parameters</em> you have access to a more flexible function approximator. At some level of complexity for your target function, the cost (in terms of CPU time, data required and effort in training) of making a single layer wider is higher than the cost of stacking more, similar layers.</p>

<p>In addition, for a CNN, you have to worry about receptive field. Any feature map can only express values that the filter can ""see"" due to width of the kernel. As you add more layers, each kernel applied extends the width and height of the base image that the features in the last layer effectively calculate over. If you also have a fully-connected layer after the convolutional layer, then you can in theory compensate for a poor receptive field with a very large fully-connected layer - but then you are back to the first problem of wide network with more parameters than strictly necessary to learn the function.</p>
","4","2","836","25573"
"25204","<blockquote>
  <p>But how he removed the expectation part when talking stochastic gradient decent ?</p>
</blockquote>

<p>A result that shows what happens in expectation can be estimated by sampling it, and that is precisely what stochastic gradient descent (or ascent in this case) methods do - they operate on individual samples on the assumption that this will <em>on average</em> produce reasonable direction for optimising parameters. </p>

<p>So there is no need to ""get rid of"" the expectation. In fact the sequence of equations is working deliberately towards it, because in most situations we do not know the full characteristics of the MDP and cannot calculate $d(s)$ (and maybe not even $\mathcal{R}_{s,a}$) - the expectation is used to remove the need for knowing those terms.</p>

<p>Importantly, $d(s)$ is the probability density of the state = the likelihood on a random sample of all states visited under a certain policy of finding the agent/environment in that state. This is often hard to calculate directly, even if you know the MDP and the policy, and it is intractable if you do not have a model of the MDP. However, when you take many samples then just by the act of sampling and using the value of $s$ that is observed, then you will approximate the true distribution of states in the long run.</p>
","1","2","836","25573"
"25212","<blockquote>
  <p>But in policy iteration also we are have to output a softmax vector related to each actions </p>
</blockquote>

<p>This is not strictly true. A softmax vector is one possible way to represent a policy, and works for discrete action spaces. The difference between policy gradient and value function approaches here is in how you use the output. For a value function you would find the maximum output, and choose that (perhaps $\epsilon$-greedily), and it should be an estimate of the value of taking that action. For a policy function, you would use the output as probability to choose each action, and you do not know the value of taking that action.</p>

<blockquote>
  <p>So I don't understand how this can use to work with continuous action space ?</p>
</blockquote>

<p>With policy gradient methods, the policy can be <em>any</em> function of your parameters $\theta$ which:</p>

<ul>
<li><p>Outputs a probability distribution</p></li>
<li><p>Can be differentiated with respect to $\theta$</p></li>
</ul>

<p>So for instance your policy function can be </p>

<p>$$\pi_{\theta}(s) = \mathcal{N}(\mu(s,\theta), \sigma(s,\theta))$$</p>

<p>where $\mu$ and $\sigma$ can be functions you implement with e.g. a neural network. The output of the network is a description of the Normal distribution for the action value $a$ given a state value $s$. The policy requires you to sample from the normal distribution defined by those values (the NN doesn't do that sampling, you typically have to add that in code).</p>

<blockquote>
  <p>Why are policy gradient methods preferred over value function approximation in continuous action domains?</p>
</blockquote>

<p>Whilst it is still possible to estimate the <em>value</em> of a state/action pair in a continuous action space, this does not help you choose an action. Consider how you might implement an $\epsilon$-greedy policy using action value approximation: It would require performing an optimisation over the action space for each and every action choice, in order to find the estimated optimal action. This is possible, but likely to be very slow/inefficient (also there is a risk of finding local maximum).</p>

<p>Working directly with policies that emit probability distributions can avoid this problem, provided those distributions are easy to sample from. Hence you will often see things like policies that control parameters of the Normal distribution or similar, because it is known how to easily sample from those distributions.</p>
","8","2","836","25573"
"25240","<p>In the linked tutorial, each kernel is 2-dimensional, and applied to a single channel/feature map from its input. To construct a feature map, the output from $N_{input\_channels}$ convolutions are added together - this is important as it allows to build feature maps that have interactions between feature maps in the previous layer.</p>

<p>From the diagram, the first input layer has 1 channel (a greyscale image), so each kernel in layer 1 will generate a feature map. However, once you have 64 channels in layer 2, then to produce each feature map in layer 3 will require 64 kernels added together. If you want 256 feature maps in layer 3, and you expect all 64 inputs to affect each one, then you usually need 64 * 256 = 16384 kernels. The value 4096 is coming from some other aspect of the architecture not shown in the diagram, such as dividing the feature map into groups so that each output layer only processes a fraction of the input layers.</p>

<p>There are a few notation and presentation differences between presentations and tutorials on convolutional networks, depending on the source. This is one of them. Other sources may arrange the kernels into a 4D structure: $N_{input\_channels} \times N_{output\_channels} \times K_{width} \times K_{height}$ to make the relationship between kernels and input/output more explicit. </p>

<p>Another way to show the same thing is to treat the kernels as 3D, and ensure that the kernel depth $K_{depth}$ is the same as the number of input channels $N_{input\_channels}$ - this is same thing mathematically as summing up multiple separate convolutions, and dimensions might be $N_{output\_channels} \times K_{width} \times K_{height} \times K_{depth}$ </p>
","2","2","836","25573"
"25295","<p>With dropout, the sum of activations feeding each neuron during training and testing needs to be roughly the same. The problem is that during training some neurons are masked out, and this does not happen during testing. Therefore there <em>has</em> to be a scaling difference between train and test phases using dropout, otherwise each neuron would be receiving larger magnitude inputs and the network as a whole could behave very differently.</p>

<p>This is <em>not</em> mathematically the same as averaging the weights - although the effect is compared intuitively to bagging many related neural networks and averaging their output. Instead, it is usually explained that dropout scales the neuron <em>activations</em> to have same expected total magnitude when dropout is active (during training) vs inactive (during testing/prediction). </p>

<p>Applying L2 or any other kind of parameter-controlling regularisation technique will not help with this, because these do not change any of the activations between train and test phases.</p>

<p>You either treat training activation values as canonical and adjust activations to be lower at test time (multiply by keep probability $p$), which is ""classic dropout"" or you treat test activation values as canonical and multiply activations by $\frac{1}{p}$ during training, which is called ""inverse dropout"". The latter approach is nowadays more common, as it is possible to completely ignore dropout layers during prediction that way.</p>
","1","2","836","25573"
"25624","<p>For concatenation, the gradient values during back propagation split to their respective source layers. There is no direct interaction between gradients in either of the source layers. </p>

<p>The layer immediately after the concatenated layer does interact with both networks, and it will have some weight parameters that multiply outputs from network A and some that multiply outputs from network B. There will not be any parameters that multiply outputs from both layers (unless you are forcing them to be the same through weight sharing, but that won't be the case if for example you are stacking features from both starting networks).</p>

<p>The only issue you might have is clearly identifying which parameters link to each original network. That is an implementation detail, so you would need to share your code so far in order to debug that if it goes wrong.</p>
","0","2","836","25573"
"25681","<p>The formula you have quoted is a bit unwieldy, precisely because the R function as defined needs to ""look ahead"" to all possible outcomes and their probabilities, but how to do that is not included explicitly.</p>

<p>There are actually a few variants of the Bellman equation that express more  or less detail. A good place to start for a truly generic version is Sutton &amp; Barto (2nd Edition):</p>

<p>$$v^*(s) = \text{max}_{a \in \mathcal{A}}[\sum_{r,s'}p(r,s'|s,a)(r + v^*(s')]$$</p>

<p>Where $\sum_{r,s'}$ is over all possible reward and next state pairs. </p>

<p>The above equation changes your transition function that only handles next state, to a similar function that handles successor state and reward:</p>

<p>$$p(r,s'|s,a) = Pr\{ R_{t+1} = r, S_{t+1} = s' | S_{t} = s, A_{t} = a \}$$</p>

<p>Usually this does not increase the number of items to sum over, or add much complexity, because reward will most often associated with the transition.</p>

<p>The benefit is that this approach removes the need for a reward function that works with <em>expected</em> reward, and just works with specific rewards. Other variants are possible too, such as an expected reward function based on $(s,a)$ or $(s,a,s')$ - the difference is just a little bit of juggling with the expression so that it remains effectively the same given the subtle differences in definition for $r$.</p>
","1","2","836","25573"
"25715","<blockquote>
  <p>Is the ""expected reward"" actually $\mathcal{R}^a_{ss'}$ instead of $V^\pi(s)$?</p>
</blockquote>

<p>In short, yes. </p>

<p>Although there is some context associated - $\mathcal{R}^a_{ss'}$ is in the context of specific action and state transition. You will also find $\mathcal{R}^a_{s}$ used for expected reward given only current state and action (which works fine, but moves around some terms in the Bellman equations). </p>

<p>""Return"" may also be called ""Utility"".</p>

<p>RL suffers a bit from naming differences, however the meaning of <em>reward</em> is not one of them. </p>

<p>Notation differences also abound, and in Sutton &amp; Barto <em>Reinforcement Learning: An Introduction (2nd edition)</em>, you will find:</p>

<ul>
<li><p>$R_t$ is a placeholder for reward received at time $t$, a random variable.</p></li>
<li><p>$G_t$ is a placeholder for return received after time $t$, and you can express the value equation as $v_{\pi}(s) = \mathbb{E}[G_t|S_t=s] = \mathbb{E}[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t=s]$</p></li>
<li><p>$r$ is a specific reward value</p></li>
<li><p>You won't see ""expected reward"" used directly in an equation from the book, as the notation in the revised book relies on summing over distribution of reward values.</p></li>
</ul>

<p>In some RL contexts, such as control in continuous problems with function approximation, it is more convenient to work with maximising average reward, than maximising expected return. But this is not quite the same as ""expected reward"", due to differences in context (average reward includes averaging over the expected state distribution when following the policy)</p>
","7","2","836","25573"
"25721","<p>The equations you have given <em>are</em> consistent with each other. The outer sum over $a$ provides the value of $a$. The ""link"" between $s, a$ and $s'$ that gives the correct associations is the transition function $\mathcal{P}^a_{ss'}$ which will be zero for any state/action/next-state triplet that does not make sense for the MDP. By ""not make sense"", I mean not allowed by the dynamics of the problem - such as an action that would successfully move too many spaces or through a wall for a maze problem. </p>

<p>It is normal for the reward to <em>not</em> actually depend on $a$, but only on the successor state. This might happen in e.g. a dynamic or adversarial environment where the desired goal is to achieve a specific end state. However, it is also possible for the reward to depend on action taken in a given state, such as activating some object in a particular location. It depends on how you have set up the MDP. Another example of where $a$ might be important is if there is a cost associated with each action that varies, for example an energy requirement, so that the total reward might be separate contribution from $s'$ (for reaching a goal) and $a$ (for spending energy to reach any next state).</p>

<p>Using an expected reward $\mathcal{R}_{ss'}^a$ that depends on all three terms $s, a, s'$ gives you a generic version of the Bellman equation that can be applied to the widest range of MDP designs. It doesn't matter if, in a specific case, the only thing you care about is $s'$ which is what the example reward function shows. Once you decide that the expected reward is dependent on $s'$, then the Bellman equation has to have that expected reward term inside the inner sum (the only place where $s'$ is defined), and there is no additional structure in the equation needed to have this term dependent on $s$ and $a$ as well.</p>

<p>You could legitimately use a variant $\mathcal{R}_{s'}$ in a Bellman equation that is otherwise identical to the equation you give in the question, to describe the value function for a MDP where reward only depends on the state transitioned to. However, you will not find that in the literature because it is less generic.</p>

<p>One common variant that you will see is $\mathcal{R}_{s}^{a}$ i.e. the expected reward for taking action $a$ in state $s$. You might use this for cost-based systems, where the goal is to complete a task efficiently and taking actions expends some resource such as time. Using your notation, the Bellman equation for that design looks like this:</p>

<p>\begin{align}
V^\pi(s) &amp;= \sum_{a \in \mathcal{A}(s)} \pi(s, a)( \mathcal{R}_{s}^a + \gamma \sum_{s' \in \mathcal{S^+}} \mathcal{P}^a_{ss'} V^\pi(s')) 
\end{align}</p>
","4","2","836","25573"
"25904","<blockquote>
<p>Does this mean the errors for <span class=""math-container"">$y_i=0$</span> do not contribute to the loss?</p>
</blockquote>
<p>That is correct.</p>
<p>However, the respective weights that connect to wrong neurons will still have gradients due to the error, and those gradients will be influenced by the size of each incorrect classification. That is due to how softmax works:</p>
<p><span class=""math-container"">$$\hat{y}_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$$</span></p>
<p>(where <span class=""math-container"">$z_i$</span> is the pre-softmax value of each neuron, a.k.a. the <em>logit</em>) . . . weights that affect one neuron's pre-transform value affect the post-transform value of all neurons. So those weights will still be adjusted to produce a lower <span class=""math-container"">$z_j$</span> value for the incorrect neurons during weight updates.</p>
<blockquote>
<p>Why isn't the formula</p>
<p><span class=""math-container"">$$J_{y'}(y) = - \sum_{i} ({y_i' \log(y_i) + (1-y_i') \log (1-y_i)})$$</span></p>
<p>used?</p>
</blockquote>
<p>It is not clear why when selecting a single class, that you would care how probability estimates were distributed amongst incorrect classes, or what the benefit would be to drive the incorrect values to be equal. For instance if <span class=""math-container"">$y' = [1, 0, 0, 0]$</span> then using the suggested formula for <span class=""math-container"">$J_{y'}(y)$</span> gives ~ 0.67 for <span class=""math-container"">$y = [0.7, 0.1, 0.1, 0.1]$</span> and ~0.72 for  <span class=""math-container"">$y = [0.73, 0.26, 0.05, 0.05]$</span>, yet arguably the second result is better.</p>
<p>However, you <em>would</em> use this loss when dealing with non-exclusive classes (where the outputs would use sigmoid as opposed to softmax activation).</p>
","7","2","836","25573"
"26029","<blockquote>
  <p>This would lead me to believe that the only appropriate activation functions would either be linear or tanh. However, I see any many RL papers the use of Relu.</p>
</blockquote>

<p>Generally you want a linear output, unless you can guarantee scaling total possible reward to within a limited range such as $[-1,1]$ for $\text{tanh}$. Reminder this is not for estimating individual rewards but for total expected reward when following the policy you want to predict (typically the optimal policy eventually, but you will want the function to be able to estimate returns for other policies visited during optimisation).</p>

<p>Check carefully in the papers you mention, whether the activation function is applied in the output. If all rewards are positive, there should be no problem using ReLU for regression, and it may in fact help stabilise the network in one direction if the output is capped at a realistic minimum. However, you should not find in the literature a network with ReLU on output layer that needs to predict a negative return.</p>

<blockquote>
  <p>If you do want to have both negative and positive outputs, are you limited to just tanh and linear?</p>
</blockquote>

<p>There are likely others, but linear will be far the most common.</p>

<blockquote>
  <p>Is it a better strategy (if possible) to scale rewards up so that they are all in the positive domain (i.e. instead of [-1,0,1], [0, 1, 2]) in order for the model to leverage alternative activation functions?</p>
</blockquote>

<p>It may sometimes be worth considering scaling rewards by a factor, or normalising them, to limit gradients, so that learning is stable. This was used in the Atari-games-playing DQN network to help the same algorithm tackle multiple games with different ranges of scoring.</p>

<p>In continuous problems, the absolute value of reward is usually flexible, you are generally interested in getting the best mean reward per time step. So in that case you could scale so that minimum reward is 0, and use ReLU or other range limited transform in output - as above that <em>might</em> help with numeric stability.</p>

<p>In episodic problems without a fixed length, you typically don't have a such a free choice, because the agent is encouraged to end the episode quickly when rewards are negative. This is something you might want for instance if the goal is to complete a task as quickly or energy-efficiently as possible. A good example of this is ""<a href=""https://en.wikipedia.org/wiki/Mountain_car_problem"" rel=""nofollow noreferrer"">Mountain Car</a>"" - granting only positive rewards in that scenario would be counter-productive, although you might still get acceptable results with positive reward only at the end and discounting.</p>

<p>The general case is that rewards can be arbitrarily scaled and centred for continuous problems without changing the agent's goal meaningfully, but only arbitrarily scaled for episodic problems.</p>
","3","2","836","25573"
"26092","<p>The <em>TD Target</em> (for learning update) for using $\hat{q}(s,a)$ neural network in Q-learning is:</p>

<p>$$r + \text{max}_{a'} \hat{q}(s',a')$$</p>

<p>In order to calculate this, you need a starting state $s$, the action taken form that state $a$, and the resulting reward $r$ and state $s'$.</p>

<p>You need the $s, a$ to generate the <em>input</em> to the neural network (what you might call <code>train_X</code> for supervised learning). You need $r, s'$ to generate the TD Target shown above as an <em>output</em> to learn for regression (in supervised learning, that would be <code>train_y</code>). And you need to work through all possible $a'$ based on $s'$ in order to find the maximum value for the TD Target equation used in Q learning - other RL algorithms may use variations of this for calculating the TD Target.</p>

<p>This means your suggestions are all <em>close</em> but not quite right.</p>

<blockquote>
  <p>Should one rely on the B2 state to iterate over possible actions from this state (next state) to get an approximation of highest reward (max Q)?</p>
</blockquote>

<p>Sort of. The B2 state is $s'$ from the equation, so is responsible for calculating the TD target for state-action value $q(s,a)$.</p>

<blockquote>
  <p>Then, why do we store the A1 and move-to-B2 information at all in the replay buffer?</p>
</blockquote>

<p>You still need to know $s$ is A1, because the representation of A1 (and whichever action is taken) will be the <em>input</em> to your network.</p>

<blockquote>
  <p>Or I am wrong and we just use the A1 and iterate over possible actions (including that to B2) to get the max Q?</p>
</blockquote>

<p>Iterate over actions from B2 using your neural network to pick the highest estimate.</p>

<blockquote>
  <p>I think I have found an answer ). We need to store previous state (A1) and action (move to B2) in order to create the state-action distribution, which will be met with the expected long-term reward distribution, that we get after the next state routine. Right?</p>
</blockquote>

<p>I'm not sure I fully understand this, but it does not seem quite right. One thing that might be confusing you is having your actions as ""move to state"". Whilst this is quite normal in many deterministic environments, especially board games, most RL formula and tutorials are written using separate state/action pairs, so whilst getting this straight in your mind, best to find another way to represent actions - e.g. move piece from X to Y, or place new piece at (I,J) . . . you can return to ""change state from A1 to B2"" as the representation later. This is actually more efficient and is called the <em>after-state</em> representation, but most of the literature will show <em>state-action</em> value functions.</p>
","1","2","836","25573"
"26215","<p>Predicting $0.5$ for all items in your case would also give you MSE of $0.25$. That is because independently of whether the true label is $0$ or $1$, the squared error for each example will be $0.5^2 = 0.25$</p>

<p>Your model is performing badly under an MSE measure, when such a simple model that does not take input data into account can get the same score.</p>

<p>However, it is debatable whether MSE gives you a useful metric here.</p>
","1","2","836","25573"
"26237","<p>The problem you have is that the users were originally shown A or B under a different policy to that which the optimiser is learning. You probably don't know the probabilities for A or B from that policy (if you do, that would allow for a bit more subtlety). </p>

<p>You will need to reject samples that do not match what the optimiser does at any point. This may leave you with far less historical data to mine, and the possibility of getting a biased result.</p>

<blockquote>
  <p>to do this we sample uniformly at random from the set of data where users were shown the output to obtain this reward.</p>
</blockquote>

<p>If I understand this correctly, you are planning to: </p>

<ol>
<li>Pick an event (including user data) for input context</li>
<li>Generate the choice of A or B using your learning agent</li>
<li>Find another user that was presented the same output in the historical data, by sampling randomly from ""all users shown A"" or ""all users shown B""</li>
<li>Count the reward generated in the sampled historical record</li>
</ol>

<p>Sadly this will not work, because in step 3 you divorce the context from the agent's decision. So you will not be measuring the agent's likely reward, but the hybrid reward of an agent that uses a different user population and gains rewards according to the historic policies (which you may not even know).</p>

<p>A less biased approach might be:</p>

<ol>
<li>Pick an event (including user data) randomly from historic data</li>
<li>Generate the choice of A or B using your learning agent</li>
<li>If the agent's choice and historic choice for that event match, then count the reward, else discard the event from the test</li>
</ol>

<p>You may get sampling bias issues here if the agent's policy is different to historic policy for large number of users. The problem is that you are systematically removing users where old policies said one thing and new policies another, due to lack of data about the reward. So unless the original policy was completely random, the samples you actually train/evaluate on will not be chosen with correct ratios across the user population as it varies. Importance sampling could help mitigate that, but only if you know the probabilities of making A|B selection in both historic and learning agent policies.</p>

<hr>

<p>Another possibility is to train a supervised model that predicts reward given user data and action from your historic data, and use that to drive a <em>simulator</em> of the online environment. If you are also <em>training</em> your agent during this test, you may need to simulate variance - e.g. turn probability of a click-through event into either a click or no click event by sampling $x &lt; p(click)$ - because that is the data you need your online algorithm to learn from. This is clearly limited by the accuracy of the supervised model from the training data, but could reduce variance, because you will get some kind of indication of reward for events that did not happen historically (with some bounds on accuracy that you can estimate). That is due to function approximation in the supervised model - it is pretty similar idea to bootstrapping the Critic evaluation in an Actor-Critic reinforcement learning model.</p>
","2","2","836","25573"
"26277","<p>Your problem is that neural networks work poorly when the input is not scaled to a simple range. A usual choice is to scale and offset each column so that it has mean 0 and standard deviation 1.</p>

<p>In your case, <code>x1</code> and <code>x2</code> vary from 0 to 49999 and roughly 10 to 50009. This range for inputs will causes lots of numeric issues. </p>

<p>With a balanced dataset as you have, 51% accuracy is basically just guessing (within experimental error), so the network has learned nothing. Try again with scaling - e.g.</p>

<pre><code>x1 = (x1 - 25000) / 14433
x2 = (x2 - 25000) / 14433
</code></pre>

<p>I have tested your code with this addition, and it gains 100% validation accuracy within the first epoch.</p>

<p>If you want to assess other values in testing later you will need to scale them in the same way.</p>

<p>Note your predictions may be off in testing when <code>x1</code> and <code>x2</code> are not close to 10 apart, because you have only trained with examples which are close to exactly 10 apart. How the network behaves when this is not the case - e.g. for inputs of x1 = 100 and x2 = 1000, or x1 = 90 and x2 = 15, may not generalise well compared to the original comparison function.</p>
","5","2","836","25573"
"26330","<p><em>Gradient exchange</em> occurs in distributed learning systems that perform gradient descent, when one part of the distributed system need to use the gradient values from another part in order to complete a task.</p>

<p>For example, you may distribute a large data set between multiple nodes, and want to calculate a gradient descent step as part of optimisation. One way to do so is calculate a subset of batch gradients on each node and collate them at a single node in order to alter parameters synchronously. This means it is necessary to fetch gradients from all nodes into a single node so that a combined gradient for some weight parameters can be calculated and the parameters updated consistently in the update step.</p>

<p><em>Gradient exchange</em> is just a term to describe that event - node A needs some gradients that node B has calculated, so they are requested (or pushed) and have to travel between the nodes. This is a relatively slow I/O process - it is necessary for distributed system to work, but for high performance you want to minimise time spent moving the data.</p>

<p>Other data (such as the parameters) also needs to be shared between nodes. This particular piece of data regarding the gradients is singled out for the paper as the authors have discovered a way to compress it significantly without losing performance of the learning algorithm. This is partly because gradients can be treated approximately in the first place. Many learning algorithms further adjust or normalise gradients after they have been calculated, so using super-precise values is not as important as you might think.</p>

<p>There may also be clever ways of splitting the update work so that each node only needs some of the gradients and only updates some of the parameters at each step. That will keep node CPU busy, possibly at the expense of more complicated communications. I do not know the details of any optimised distributed learning system in order to tell you the precise data exchanges and optimisations taking place. There are likely to be a few variations possible, depending on the framework and which algorithm is being implemented.</p>
","2","2","836","25573"
"26387","<p>By convention, in addition to the input feature map - which may be 1D for audio, 2D for a typical image, 3D for a sequence of video frames - for a convolutional network, there are two additional dimensions:</p>

<ol>
<li><p>The number of examples in a batch or mini-batch, counting even a single example as a mini-batch of size 1)</p></li>
<li><p>The number of feature maps or channels in the current layer, counting even a single grayscale image as an array of 1 channel, or RGB (or other colour space) image as 3 channels</p></li>
</ol>

<p>So in your case you will want a <code>conv2d</code> layer to process the image. The precise order of dimensions varies a lot - check your library docs, and also note most libraries allow you to alter the arrangement for cross-compatibility with different toolchains. But a typical layer input or output might be arranged as $\text{BatchSize} \times \text{Height} 
 \times \text{Width} \times \text{Channels}$</p>

<p>The input and output dimensions are the same (but usually with different sizes), so that layers can be chained together naturally.</p>

<p>The documentation you linked does in fact say this (sort of, it could explain the sameness in more detail, since data type and dimensionality are in fact the same and both could vary):</p>

<blockquote>
  <p>Returns:</p>
  
  <p>A Tensor with the same type as value.</p>
</blockquote>

<p>To answer your question in the title:</p>

<blockquote>
  <p>What is the shape of conv3d and conv3d_transpose?</p>
</blockquote>

<p>It is an order 5 tensor, and the dimensions are: $\text{BatchSize} \times \text{Depth} \times \text{Height} \times \text{Width} \times \text{Channels}$</p>

<p>You could in theory use this for your GAN, but you would need to add (a probably useless) <em>depth</em> dimension to the shape.</p>
","2","2","836","25573"
"26404","<p>Each grid predictor in YOLO should only have a high score that an object is within it, if it detects the <em>centre</em> of the bounding rectangle is inside itself. So a grid point that contains only the wing mirror of a car should decide it has a low probability of containing the centre of the car.</p>

<p>The predicted bounding rectangles are not constrained in the same way - YOLO can (and often does) predict bounding box dimensions from the centre that are larger than the grid cell dimensions. </p>

<p>Each grid point is independently able to predict whether or not it contains an object's centre, what the bounding box dimensions are for the object, and what the object class is.</p>

<p>If you trace the layer connectivity, you will see that the grid cells are effectively interconnected in lower output layers, so the network as a whole ""sees"" more of each object and can influence individual object predictions, suppressing some and encouraging others, when objects span multiple grid locations. The grid cells are not isolated into sections, or restricted to only using data from the area that they cover for the prediction. The concept of what part of each image a feature ""pixel"" can access from the base image in a CNN is called the ""receptive field"" of the network, and can be <a href=""https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807"" rel=""nofollow noreferrer"">calculated based on the architecture as explained in this blog on Medium</a>. </p>
","3","2","836","25573"
"26474","<blockquote>
  <p>1) With an on-policy algorithm we use the current policy (a regression model with weights W, and ε-greedy selection) to generate the next state's Q.</p>
</blockquote>

<p>Yes. To avoid confusion, it may be better to use the terms ""behaviour policy"" for the the policy that controls current actions and ""target policy"" for the policy being evaluated and/or learned.</p>

<blockquote>
  <p>2) With an off-policy algorithm we use a greedy version of the current policy to generate the next state's Q.</p>
</blockquote>

<p>Sort of. The only <em>requirement</em> for an algorithm to be <em>off-policy</em> is that the target policy is different to the behaviour policy. The usual target policy in Q-learning is not necessarily a greedy version of the behaviour policy, but is the maximising policy over Q. However, if the behaviour policy is ε-greedy over Q, and adapting to updates in Q, then yes your statement holds.</p>

<blockquote>
  <p>3) If an exploration constant ε is set to 0, then the off-policy method becomes on-policy, since Q is derived using the same greedy policy.</p>
</blockquote>

<p>This is true when comparing SARSA with Q learning, but may not hold when looking at other algorithms. This greedy-only action selection would not be a very efficient learner in all environments.</p>

<blockquote>
  <p>4) However, on-policy method uses one sample to update the policy, and this sample comes from on-line world exploration since we need to know exactly which actions the policy generates in current and next states. While off-policy method may use experience replay of past trajectories (generated by different policies) to use a distribution of inputs and outputs to the policy model.</p>
</blockquote>

<p>Experience replay is not <em>directly</em> related to on-policy vs off-policy learning. Technically though, yes when the experience is stored and used later, that makes it off-policy for SARSA if Q values have changed enough between the sample and current parameters of the learning agent.</p>

<p>However, you will see experience replay used more often with off-policy methods, since off-policy learners that boot-strap (i.e. use Q value of next state/action to help estimate current Q value) are less stable when used with function approximators. Experience replay helps to address that problem.</p>
","1","2","836","25573"
"26567","<blockquote>
  <p>One idea I have is that i have implemented the input neurons incorrectly.</p>
</blockquote>

<p>There is no single correct way to implement the solution to this problem. There are less efficient and more efficient ways for specific problems.</p>

<blockquote>
  <p>Use 18 input neurons and have the first 9 being the state before placing a move, and the next 9 being the state after placing a move.</p>
</blockquote>

<p>This should work. The first set of 9 neurons will receive a representation of the current game state. The second set of 9 neurons will receive a representation of the intended action.</p>

<p>For predicting Q values from a state, action pair, this is a reasonable approach. Staying with the same architecture of 18 inputs, you could also have the second set of neurons be one-hot encoded to where the agent will to place the 'X' or 'O'.</p>

<p>In the game Tic Tac Toe however, you can look for more compact representations if you like. You have already noticed that because the game is simple and deterministic, that you can represent the agent's action as simply ""desired next state"". And in fact, the current state does not actually matter to a player, other than to enforce the rules of what are valid next states.</p>

<p>You won't be implementing the rules of the game into the agent - they are part of the environment. Therefore, you can do away with the initial state altogether in your estimate, and work with the end state of each move - this is called the <em>afterstate</em> in the RL literature, and you will find it used a fair bit in deterministic games, or even in non-deterministic games where the randomness happens before the action choice (e.g. in Backgammon). An afterstate representation is more efficient because it encodes the fact that you don't care what route a player took to get to certain board position, you just care about the value of that position as the game continues.</p>

<p>Having said all that, if your goal is to learn basic RL, then you don't need to be looking for the most efficient solution, just one that works. Don't expect your NN-based learner with state-action logic to be the most efficient learner however.</p>

<blockquote>
  <p>Another question I have in terms of when you teach the neural network, would you feed it the old state and then the state after both you and the opponent have made a move?</p>
</blockquote>

<p>Not as inputs to the network at the same time, no. In your (state, action [=next_state]) representation, your action representation should be the board state for the current player's move and <em>before</em> the other player takes any action. The <em>resulting next state</em> however, will be after the other player takes their action.</p>

<p>If you want to train two separate bots against each other, then each would see the current state, then it would choose an action, then it would either get the reward for winning, or the opponent would take a turn. If the opponent won, then the first bot should receive the (negative) reward. If the opponent's move was not final, then the first bot should see the state <em>after</em> the opponent's move and get to choose its next action.</p>

<p>Again, this is inefficient. For a win/draw/lose game like Tic Tac Toe, you don't need two separate agents, each with their own learning algorithm, in order to train through self-play. You can instead alter Q-Learning slightly to work with the minimax algorithm. In brief this means alternating between the agent selecting actions that maximise the expected reward (for player 1) or minimise it (for player 2).</p>

<p>However, like before, your 2 networks set up should be able to work, and is quite interesting dynamic - you could try different learning parameters, different NNs etc, and see which learns to win quickly (but don't forget starting player has an advantage for early random play, so you'd want to switch which learning algorithm was used for which player to get a fair assessment). </p>

<p>The difference again is in terms of efficiency - a single network inside a modified RL with minimax will typically learn faster that two separate networks.</p>

<hr>

<p>I have implemented a simple <em>tabular</em> (i.e. no neural networks) <a href=""https://github.com/neilslater/game_playing_scripts/blob/master/tictactoe_q.py"" rel=""nofollow noreferrer"">Q-Learning based agent for Tic Tac Toe in Python</a>. It uses afterstate representation and modified Q learning with minimax, and learns purely online through repeated self-play.</p>
","0","2","836","25573"
"26602","<p>You are effectively implementing $\epsilon$-greedy action selection. </p>

<p>The usual way to represent this in RL, at least that I am familiar with, is not as a ""threshold"" for probability of choosing the best estimated action, but as a small probability, $\epsilon$, of not choosing the best estimated action. </p>

<p>For consistency with RL literature that I know, I will use the $\epsilon$-greedy form, so instead of considering what happens as your threshold rises from 0 to 1, I will consider what happens when $\epsilon$ drops from 1 to 0. It is the same thing. I hope you can either adjust to using $\epsilon$ or mentally convert the rest of this answer so it is about your threshold . . .</p>

<p>When monitoring Q-Learning, you have to be careful how you measure success. Monitoring the behaviour on the learning games will give you slightly off feedback. The agent will make exploratory moves (with probability $\epsilon$), and the results from a learning game might involve the agent losing even though it already has a policy good enough to not lose from the position where it started exploring. If you want to measure how well the agent has learned the game, you have to stop the training stage and play some games with $\epsilon$ set to $0$. I suspect this could be one problem - that you are measuring results from behaviour during training (note this would work with SARSA)</p>

<p>In addition, choosing values that are too high or low for your problem will reduce the speed of learning. High values interfere with Q-learning because it has to reject some of data from exploratory moves, and the agent will rarely see a full game played using its preferred policy. Low values stifle learning because the agent does not explore different options enough, just repeating the same game play when there might be better moves that it has not tried. For Tic Tac Toe and Q-learning I would suggest picking a value of $\epsilon$ between $0.01$ and $0.2$</p>

<p>In fact, with Q-learning there is no need to change the value of $\epsilon$. You should be able to pick a value, say $0.1$, and stick with it. The agent will still learn an optimal policy, because Q-learning is an <a href=""https://stats.stackexchange.com/questions/184657/what-is-the-difference-between-off-policy-and-on-policy-learning"">off-policy algorithm</a>. </p>
","1","2","836","25573"
"26609","<p>This update scheme:</p>

<pre><code>Q(s,a) += reward * gamma^(inverse position in game state)
</code></pre>

<p>has a couple of problems:</p>

<ul>
<li><p>You are - apparently - incrementing Q values rather than training them to a reference target. As a result, the estimate for Q will likely diverge, predicting total rewards that are impossibly high or low. Although in your case with a zero sum game and initial random moves, it may just random walk around zero for a long time first.</p></li>
<li><p>Ignoring the increment, the formula you are using is not from Q-learning, but effectively <a href=""http://www.incompleteideas.net/book/ebook/node54.html"" rel=""nofollow noreferrer"">on policy Monte Carlo control</a>, because you use the end-of-game sum of rewards as the Q value estimate. In theory, with a few tweaks this can be made to work, but it is a different algorithm than you say you want to learn.</p></li>
</ul>

<p>It is worth clarifying a few related terms (you clearly know these  already, but I want to make sure you have them separated in your  understanding of the rest of the answer):</p>

<ul>
<li><p><em>Reward</em>. In RL, a reward (a real number) can be returned on every increment, after taking an action. The set of rewards is part of the problem definition. Often noted as $R$ or $r$.</p></li>
<li><p><em>Return</em> (aka <em>Utility</em>). The sum of all - maybe discounted - rewards from a specific point. Often noted as $G$ or $U$.</p></li>
<li><p><em>Value</em>, as in state value or action value. This is usually the expected return from a specific state or state, action pair. $Q(S_t, A_t)$ is the expected return when in state $S_t$ and taking action $A_t$. Note that using $Q$ does not make your algorithm Q-learning. The $Q$ action value is the basis for several RL algorithms.</p></li>
</ul>

<p>Your formula <code>reward * gamma^(inverse position in game state)</code> gives you the <em>Return</em>, $G$ seen in a sampled training game, $G_t = \gamma^{T-t} R_T$ where $T$ is the last time step in the game. That's provided the game only has a single non-zero reward at the end - in your case that is true. So you could use it as a training example, and train your network with input $S_t, A_t$ and desired output of $G_t$ calculated in this way. That should work. However, this will only find the optimal policy if you decay the exploration parameter $\epsilon$ and also remove older history from your experience table (because the older history will estimate returns based on imperfect play).</p>

<p>Here is the usual way to use experience replay with Q learning:</p>

<ul>
<li><p>When saving experience, store $S_t, A_t, R_{t+1}, S_{t+1}$ - note that means storing immediate <em>Reward</em>, not the <em>Return</em> (yes you will store a lot of zeroes). Also note you need to store the next state.</p></li>
<li><p>When you have enough experience to sample from, typically you do not learn from just one sample, but pick a minibatch size (e.g. 32) and train with that many each time. This helps with convergence.</p></li>
<li><p>For Q-learning, your TD target is $R_{t+1} + \gamma \text{max}_{a'} Q(S_{t+1}, a')$, and you bootstrap from your current predictions for Q, which means:</p>

<ul>
<li><p>For each sample in the minibatch, you need to calculate the predicted Q value of all allowed actions from the <em>next</em> state $S_{t+1}$ - using the neutral network. Then use the maximum value from each state to calculate $\text{max}_{a'} Q(S_{t+1}, a')$.</p></li>
<li><p>Train your network on the minibatch for a single step of gradient descent, with NN inputs $[S_t, A_t]$ and training label of the TD target from each example.</p></li>
<li><p>Yes that means you use the same network to first predict and then learn from a formula based on those predictions. This can be a source of problems, so you may need to maintain two networks, one to predict and one that learns. Every few hundred updates, refresh the prediction network as a copy of the current learning network. This is quite common addition to experience replay (it is something that Deep Mind did for DQN), although may not be necessary in your case for a game as simple as Tic Tac Toe.</p></li>
<li><p>The TD target is a bootstrapped and biased estimate of expected $G$. The bias is a potential source of problems (you may read that using NNs with Q-learning is not stable, this is one of the reasons why). However, with the right precautions, such as experience replay, the bias will reduce as the system learns.</p></li>
</ul></li>
<li><p>In case you are wondering, it is the use of both $S_t$ (as NN input) and $S_{t+1}$ (to calculate TD target) in the Q-learning algorithm, which effectively distributes the end-of-game reward back to the start of the game's Q value.</p></li>
<li><p>In your case (and in many episodic games), it should be fine to use no discount, i.e. $\gamma = 1$</p></li>
</ul>

<hr>

<p>From your previous question, you noted that you were training two competing agents. That does in fact cause a problem for experience replay. The trouble is that the next state you need to train against will be the state <em>after</em> the opponent has made a move. So the opponent is technically viewed as being part of the environment for each agent. The agent learns to beat the current opponent. However, if the opponent is also learning an improved strategy, then its behaviour will change, meaning your stored experiences are no longer valid (in technical terms, the environment is non-stationary, meaning a policy that is optimal at one time may become suboptimal later). Therefore, you will want to discard older experience relatively frequently, even using Q-learning, if you have two self-modifying agents.</p>
","2","2","836","25573"
"26739","<blockquote>
  <p>Why don't we convolve our images against the last convolution layer and see how many of these complex feature filters get activated?</p>
</blockquote>

<p>The answer is that all the layers are fully dependent on the exact features of the previous layer. The last layer is simply not capable of taking a raw image as input and outputting meaningful values. Most likely it is not even going to be able to fit to the shape of the output, it will require dozens of channels, where a colour image only has 3. If you somehow forced it to fit, then the colour image data is meaningless to it. It might still be made to output <em>something</em>, but it would essentially be gibberish, and unrelated to the desired outcome.</p>

<p>Each layer is a function of the previous layer. Ignoring the details of convolution, a neural network is essentially a composition of multiple functions (let's call them $f, g, h, i, j$ for example) so that:</p>

<p>$$y = j(i(h(g(f(x))))$$</p>

<p>You are essentially asking here, can you just do $y = j(x)$ instead of running all those functions in sequence. And the answer is no.</p>
","5","2","836","25573"
"26823","<p>The image, and variants of it that are commonly used are for illustrative purposes only. They generally do not represent data that has been extracted from real CNNs.</p>

<p>The first ""Low-level features"" part of the diagram is possibly from a real network (I am not sure in this case, it looks more like a constructed filter, e.g. Sobel, to me). That is because it is feasible and relatively easy to interpret the first layer's filter weights directly as images, and the filters do indeed look like the components that they detect.</p>

<p>The ""Mid-level features"" and ""High-level features"" in your specific diagram have probably been constructed without using a neural network. They are likely to be an artists impression of what the high level features might be. They may have been sampled from real datasets, then just cropped and arranged into the image.</p>

<p>Caveat: I cannot find absolute evidence for the specific image being constructed for illustration only, just I suspect this to be the case.</p>

<p>It is possible to extract visualisations of features detected by deeper layers. The two common ways to do this are:</p>

<ul>
<li><p>Dataset matching. Finding examples in the dataset which trigger a specific neuron to output a high value. This can be isolated to a crop of the original image, because you know the combined sizes of all the filters and pools that occur before layer you are interested in.</p></li>
<li><p>Optimising the input image. Using gradient ascent, but instead of changing the weights, make a cost function that scores the neuron you want to visualise and keep adjusting the input until</p></li>
</ul>

<p>You can get more information from resources such as <a href=""https://distill.pub/2017/feature-visualization/"" rel=""nofollow noreferrer"">this article on feature visualisation</a>.</p>
","1","2","836","25573"
"26945","<p>Bootstrapping in RL can be read as ""using one or more estimated values in the update step <em>for the same kind</em> of estimated value"". </p>

<p>In most TD update rules, you will see something like this SARSA(0) update:</p>

<p><span class=""math-container"">$$Q(s,a) \leftarrow Q(s,a) + \alpha(R_{t+1} + \gamma Q(s',a') - Q(s,a))$$</span></p>

<p>The value <span class=""math-container"">$R_{t+1} + \gamma Q(s',a')$</span> is an estimate for the true value of <span class=""math-container"">$Q(s,a)$</span>, and also called the TD target. It is a bootstrap method because we are in part using a Q value to update another Q value. There is a small amount of real observed data in the form of <span class=""math-container"">$R_{t+1}$</span>, the immediate reward for the step, and also in the state transition <span class=""math-container"">$s \rightarrow s'$</span>.</p>

<p>Contrast with Monte Carlo where the equivalent update rule might be:</p>

<p><span class=""math-container"">$$Q(s,a) \leftarrow Q(s,a) + \alpha(G_{t} - Q(s,a))$$</span></p>

<p>Where <span class=""math-container"">$G_{t}$</span> was the total discounted reward  at time <span class=""math-container"">$t$</span>, assuming in this update, that it started in state <span class=""math-container"">$s$</span>, taking action <span class=""math-container"">$a$</span>, then followed the current policy until the end of the episode. Technically, <span class=""math-container"">$G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}$</span> where <span class=""math-container"">$T$</span> is the time step for the terminal reward and state. Notably, this target value does not use any existing estimates (from other Q values) at all, it only uses a set of observations (i.e., rewards) from the environment. As such, it is guaranteed to be <em>unbiased</em> estimate of the true value of <span class=""math-container"">$Q(s,a)$</span>, as it is technically a <em>sample</em> of <span class=""math-container"">$Q(s,a)$</span>.</p>

<p>The main disadvantage of bootstrapping is that it is biased towards whatever your starting values of <span class=""math-container"">$Q(s',a')$</span> (or <span class=""math-container"">$V(s')$</span>) are. Those are are most likely wrong, and the update system can be unstable as a whole because of too much self-reference and not enough real data - this is a problem with off-policy learning (e.g. Q-learning) using neural networks.</p>

<p>Without bootstrapping, using longer trajectories, there is often <em>high variance</em> instead, which, in practice, means you need more samples before the estimates converge. So, despite the problems with bootstrapping, if it can be made to work, it may learn significantly faster, and is often preferred over Monte Carlo approaches.</p>

<p>You can compromise between Monte Carlo sample based methods and single-step TD methods that bootstrap by using a mix of results from different length trajectories. This is called <a href=""https://en.wikipedia.org/wiki/Temporal_difference_learning#TD-Lambda"" rel=""noreferrer"">TD(<span class=""math-container"">$\lambda$</span>) learning</a>, and there are a variety of specific methods such as SARSA(<span class=""math-container"">$\lambda$</span>) or Q(<span class=""math-container"">$\lambda$</span>).</p>
","29","2","836","25573"
"27107","<p>The standard approach with policy gradients for continuous action spaces is to output a vector of parameters to a probability distribution. To resolve the policy into an action for the agent, you then sample from the distribution.</p>

<p>In policy gradients with discrete action spaces, this is actually already the case - the softmax layer is providing a discrete distribution for you, that you must sample from to choose the action.</p>

<p>The general rule is that your probability distribution function needs to be differentiable. A common choice is the Normal distribution, and for the output vector to be the mean and standard deviation. This adds an extra ""interpretation layer"" to the agent's model in addition to the NN, which needs to be included in the gradient calculation.</p>

<p>Your idea:</p>

<blockquote>
  <p>e.g. if I have values of 1.0/0.0 for left right, then make the hardest left turn possible, but make a much more gradual turn if my values are 0.6/0.4</p>
</blockquote>

<p>. . . is almost there. However, you need to interpret the output values <em>stochastically</em>, not deterministically, in order to use policy gradients. A deterministic output based on your parameters has no gradient wrt to improvements to the policy, so the policy cannot be adjusted*. Another way to think of this is that policy gradient methods must have exploration built into the policy function.</p>

<p>It would be quite difficult to turn the left/right outputs you have into a PDF that could be made progressively tighter around the optimal value as the agent homed in on the best actions, so I would instead suggest the common mean, standard deviation split for this, and have the environment cut off the actions at min/max steer if the sampled action ended up as e.g. hard left times 1.7</p>

<hr>

<p>* Actually this is incorrect as pointed out in Constantinos' answer. There are <a href=""https://deepmind.com/research/publications/deterministic-policy-gradient-algorithms/"" rel=""nofollow noreferrer"">deterministic policy gradient solvers</a>, and sometimes they are better. They work by learning off-policy. Your network could simply output a steering direction -1.0 to 1.0, but you would <em>also</em> need a behaviour policy which added some randomness to this output in order to learn. </p>

<p>I also <em>think</em> you would need to switch from A3C to A2C in order to take advantage of deterministic policy gradient solvers.</p>
","1","2","836","25573"
"27174","<p>The wikipedia formulation does indeed show you a better view of how the update rule for action values is constructed:</p>

<p>$$Q(s_t, a_t) \leftarrow (1-\alpha)\cdot Q(s_t, a_t) + \alpha\left[ r_t + \gamma \cdot \text{max}_{a'}(Q(s_{t+1}, a')) \right] $$</p>

<p>. . . here you can see that you are taking a weighted average between $Q(s_t, a_t)$ and $ r_t + \gamma \cdot \text{max}_{a'}(Q(s_{t+1}, a')$. An aside: In both cases you have written $\text{argmax}$, where the actual term is $\text{max}$.</p>

<p>The intuitive reason for the update is because the second term contains a new estimate of the true value of Q from the environment - the immediate reward $r_t$, and the actual state transition that occurred $s_{t+1}$ are observed data that are part of the new estimate. The term  $r_t + \gamma \cdot \text{max}_{a'}(Q(s_{t+1}, a'))$ is usually called the <em>Temporal Difference Target</em> or just <em>TD Target</em>.</p>

<blockquote>
  <p>In the original representation (the first equation), it magically seems that gamma would be enough - can someone clear it up for me?</p>
</blockquote>

<p>The $\gamma$ term is the discount rate, and nothing to do with convergence of Q-learning or SARSA. It is a term used to control preference for more immediate rewards (low values) or longer-term rewards (high values), and as such is part of the problem definition. It is not a learning parameter like $\alpha$, the learning rate. And in fact $\gamma = 0$ is possible (rarely used, but meaning that only next reward is important); $\gamma = 1$ is also possible and quite a common choice for episodic problems. Clearly when using $\gamma = 1$, there is no ""decay of future rewards"" happening.</p>

<p>What is actually happening is that the TD Target is a stochastic estimate for the correct Q action value (in fact with all TD learning such as SARSA and Q-learning, this is a biased estimate, although the bias should reduce as the system approaches convergence). If you rename $Q(s,a)$ as $Q_{so\_far}$ and $r_t + \gamma \cdot \text{max}_{a'}(Q(s_{t+1}, a'))$ as $Q_{latest\_estimate}$, you get the idea of the update:</p>

<p>$$Q_{updated} \leftarrow (1-\alpha)Q_{so\_far} + \alpha Q_{latest\_estimate}$$</p>

<p>which is the same as</p>

<p>$$Q_{updated} \leftarrow Q_{so\_far} + \alpha(Q_{latest\_estimate} - Q_{so\_far})$$</p>

<p>You might ask ""Why is the TD Target an estimate of $Q(s_t, a_t)$?""</p>

<p>The answer can be derived from the definition of Q, and is called the Bellman equation - in this case the Bellman equation for action value under the optimal policy $\pi^*$ (anther aside: what you have called the Bellman equation is not, although it is related). I won't give the full derivation here, but in essence the definition of Q is ""the expected total reward when starting with state $s_t$ and action $a_t$ then following the optimal policy form then on"", or</p>

<p>$$q_{*}(s, a) = \mathbb{E}_{\pi^*}[\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} | S_t = s, A_t = a]$$</p>

<p>and from this definition, you can extract the first reward term and write out $q_*(s_t, a_t)$ in terms of $q_*(s_{t+1}, a_{t+1})$.</p>

<p>$$q_{*}(s, a) = \sum_{r,s'} p(r,s'|s,a)(r + \gamma \text{max}_{a'} q_{*}(s', a'))$$</p>

<p>Where $p(r,s'|s,a)$ is the transition probability of getting reward $r$ and ending up in state $s'$ when starting in state $s$ and taking action $a$. </p>

<p>Note this is an equality, not an update rule. Q-learning and SARSA turn it into an update rule by sampling the right hand side, and using that sample to update the current best estimate of the left-hand term. Note that by taking a sample, you don't need to know $p(r,s'|s,a)$ . . . that is what is meant by ""model free"" learning, as if you knew $p(r,s'|s,a)$ then you would have a model of the underlying Markov Decision Process (MDP).</p>
","4","2","836","25573"
"27312","<p>Q-learning does not in fact need to be online or need an emulator, it can learn exclusively from <em><a href=""https://datascience.stackexchange.com/questions/20535/understanding-experience-replay-in-reinforcement-learning"">experience replay</a></em>. If you put all your history into a table or <em>state, action, reward, next state</em> and then sample from it, it should be possible to train your agent that way.</p>
<p>To do this, you will need to skip the algorithm steps that take actions and store results. The algorithm will then learn from the data you have. It will just not be possible to collect more. Depending on the problem you are trying to solve, this could be OK, or it may inhibit learning.</p>
<p>RL algorithms learning optimal control in complex environments benefit from sampling near to their current policy, so it is possible in your case that your agent will reach a limit on what it can learn from historic data. It may end up still quite far from optimal behaviour, although it should stand a reasonable chance of improving on the best that the historic data shows.</p>
<p>If you need to use function approximation (e.g. a neural network) due to size of the state, action space, then take extra care, because it will be hard to detect whether the action values have converged correctly. This is because you are learning the <em>optimal</em> Q values, and you will have no test data that demonstrates what those should be (to collect that data, you need to follow the optimal policy and measure the total reward).</p>
<p>Here is roughly what an experience-replay-only Q-learning algorithm would look like:</p>
<blockquote>
<p>Input: History <span class=""math-container"">$H$</span>, consisting of rows of <span class=""math-container"">$S,A,R,S'$</span></p>
<p>Initialise the NN for calculating <span class=""math-container"">$\hat{q}(s,a)$</span></p>
<p>Repeat until NN converges:</p>
<p><span class=""math-container"">$\qquad$</span> Sample <span class=""math-container"">$S,A,R,S'$</span> from <span class=""math-container"">$H$</span></p>
<p><span class=""math-container"">$\qquad$</span> <span class=""math-container"">$tdtarget \leftarrow R + \gamma \text{max}_{a'}[\hat{q}(S',a')]$</span></p>
<p><span class=""math-container"">$\qquad$</span> Train NN single step, <span class=""math-container"">$\hat{q}(S,A) \rightarrow tdtarget$</span></p>
</blockquote>
<p>You can make use of mini-batch processing to generate multiple <span class=""math-container"">$tdtarget$</span> values at once and train on them. A worthwhile improvement for stability is to use a frozen copy of the neural network when calculating the <span class=""math-container"">$tdtarget$</span> value, and update it only every N steps to be a copy of most recent network, with N maybe 1000 steps.</p>
","4","2","836","25573"
"27383","<blockquote>
<p>I want to understand if I must use exactly the same Q function (and policy) to get A and A'. If I update Q function in each iteration, it follows that the next action in a subsequent iteration will be derived using the latest Q updated, while the previous action was obtained using the former Q.\ On the other hand, I really can make A and A' with exactly the same Q, and only after that update the Q. So I will always consider A and A' derived using the same function.</p>
<p>Which is more orthodox / correct?</p>
</blockquote>
<p>The algorithm pseudocode given is more orthodox, since in order to revise the value of <span class=""math-container"">$A$</span> you would have to &quot;roll back&quot; the environment and see where the newly-sampled <span class=""math-container"">$A$</span> would take you from state <span class=""math-container"">$S$</span>. To make this clearer, you can see that:</p>
<pre><code>select action a' using a policy based on Q
</code></pre>
<p>could be re-phrased:</p>
<pre><code>select action a' by sampling epsilon-greedy function over Q(s',*)
</code></pre>
<p>. . . you cannot do that unless you have the value of <span class=""math-container"">$S'$</span>, and you may only have that value if you have already taken action <span class=""math-container"">$A$</span> when in state <span class=""math-container"">$S$</span>. Changing <span class=""math-container"">$A$</span> at that stage therefore means going back in time . . .</p>
<p>In practice it doesn't matter much, even if you have capability to roll back (in a simulator, or in a planning algorithm). If your policy is based on e.g. <span class=""math-container"">$\epsilon$</span>-greedy over the current Q values, then you are performing SARSA for optimal control (as opposed to prediction). In that case, changing Q means changing the policy. &quot;On-policy&quot; in SARSA for control must allow for the non-stationarity of the policy. Occasionally that means that the <span class=""math-container"">$A'$</span> value you just chose would have been chosen with a lower probability in a more optimal policy. But you chose it anyway this time, and the agent should choose it less often <em>in future</em>. The learning-rate based updates will remove estimation bias due to earlier poor/too-frequently-sampled choices over time.</p>
<p>Revising a single step &quot;mistake&quot; is possible, but not common practice in a purely online algorithm. I have not seen it in planning look-ahead or offline algorithms either that I have studied. I don't know for certain, but I suspect the occasional boost to learning you might get from revising the immediate part of the trajectory is too small to be worth the loss of generality of the algorithm. You may find it does help sometimes though, and worth an experiment to review whether it is helpful, provided you are working with a simulator/planner where rolling back state is relatively easy.</p>
<hr />
<p>Note that the way you are thinking does turn up again when using function approximators (e.g. linear functions or neural networks) in semi-gradient versus &quot;true gradient&quot; methods, where instead of this being an issue with which Q values to use, it is an issue with calculating gradients due to the TD error, when your TD target is based on the same parameters that you are are taking the gradient for. In semi-gradient methods, this issue is ignored, and the methods still work OK. However, the &quot;true gradient&quot; methods are more theoretically correct.</p>
","1","2","836","25573"
"27415","<blockquote>
  <p>Is it theoretically possible to correct the contents of the cell state, and what would it result in?</p>
</blockquote>

<p>Yes it is. Using back propagation, it is possible to get the gradient of any value that affects a well-defined output. For training data, that includes all current cell outputs - in fact these are necessary to calculate as an interim step in order to get the gradients of the weights. Once you have the gradients of a cost or error function, then you can perform a step of gradient descent in order to discover a value that would result in a lower error for given training data.</p>

<p>In usual training scenarios you do not alter neuron outputs after they have been calculated using feed-forward, because these are not parameters of the model. Typical reasons to alter neuron values (or even the input) are in order to view what the ideal state might be in a given scenario. If your state can be visualised through some decoder - maybe even another neural network - then this could allow you to see the difference between actual internal state, and a potentially better one. That could be useful in error analysis for example.</p>

<blockquote>
  <p>So why not to correct the value stored in the cell state? </p>
</blockquote>

<p>That's because in usual training scenarios, you are creating a network that predicts an output value. You can calculate the right corrections for training data, but not when predicting, because the whole point of predicting is to estimate a label that you do not already have. As such, you want to alter your function parameters, and not interim values.</p>

<blockquote>
  <p>It would be very useful if we carry the cell state forward, between minibatches.</p>
</blockquote>

<p>Only during training. In a prediction scenario you usually have no way of calculating the necessary gradients. What you don't want is to train a system that then requires using error values and gradients that you do not have in production.</p>

<p>In some scenarios, such as an online system predicting next item $x_{t+1}$ in a sequence, where you could immediately train based on error after you observed the next item and before you continued the prediction sequence for $x_{t+2}$, you could possibly use the approach. I am not sure whether it would help performance, but <em>in principle it could</em>. If it did help, you'd have to compare the improvement versus other simpler changes such as different hyper-parameters on a network that didn't correct internal state using gradients.</p>

<hr>

<p>In summary, it is possible your idea would work quite well in an online system with near-immediate feedback. In that case you could think of a set of weights as being ""rules to update a belief state from data"", and the output of hidden layer neurons as being ""a current belief state"". When errors occur, it does appear to make sense to update both the rules that led to the error and the current belief that resulted from earlier faulty rules. It is perhaps worth an experiment or two. The main caveat is that the two update processes (for weights and LSTM layer state) would interact and/or adapt to each other, so it may not lead to measurably different performance than just adding more LSTM cells to the layer.</p>
","1","2","836","25573"
"27512","<p>There were a few different techniques. One popular one was stacked autoencoders, where each layer was trained separately.</p>

<p>Essentially this was done by progressively growing the autoencoder, two layers at a time (one encode layer, plus equivalent decode layer), followed by complete training at each step of growth. </p>

<p>If learning from a fixed training set, you could store the encoded representation of the whole dataset so far as input into next stage of training, saving some computation when building up the layers.</p>

<p>After training each encoder layer separately you could use the weights of the encoder section of the autoencoder as the starting weights of the deep NN. Intuitively this made sense as you would have a representation of the input that you knew could be used to reconstruct it, and that typically was compressed, so should in theory have extracted salient details from the training data population. On top of these pre-trained layers, you may add one or two new layers that implemented whatever classification or regression task that you needed the final NN to perform. Then you would train with the labelled data - this is similar to fine-tuning networks and transfer learning that is still done nowadays.</p>

<p>The results from this pre-training stage could be worthwhile. It is still a valid technique if you have a lot of unlabelled data, and a relatively small amount of labelled data. However, the introduction of ReLU and careful controls on weight initialisation meant that deep networks could often be trained more directly. Recent additions such as skip connections and batch normalisation have further improved more direct training approaches.</p>

<p><a href=""http://cmgreen.io/2016/01/04/tensorflow_deep_autoencoder.html"" rel=""nofollow noreferrer"">Here is an example with code, using TensorFlow</a>.</p>
","3","2","836","25573"
"27616","<p>Yes you need to apply normalisation to test data, if your algorithm works with or needs normalised training data*. </p>

<p>That is because your model works on the representation given by its input vectors. The scale of those numbers is part of the representation. This is a bit like converting between feet and metres . . . a model or formula would work with just one type of unit normally.</p>

<p>Not only do you need normalisation, but you should apply the exact same scaling as for your training data. That means storing the scale and offset used with your training data, and using that again. A common beginner mistake is to separately normalise your train and test data.</p>

<p>In Python and SKLearn, you might normalise your input/X values using the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"" rel=""noreferrer"">Standard Scaler</a> like this:</p>

<pre><code>scaler = StandardScaler()
train_X = scaler.fit_transform( train_X )
test_X = scaler.transform( test_X )
</code></pre>

<p>Note how the conversion of <code>train_X</code> using a function which fits (figures out the params) then normalises. Whilst the <code>test_X</code> conversion just transforms, using the same params that it learned from the train data.</p>

<p>The tf-idf normalisation you are applying should work similarly, as it learns some parameters from the data set as a whole (frequency of words in all documents), as well as using ratios found in each document.</p>

<hr>

<p>* Some algorithms (such as those based on decision trees) do not need normalised inputs, and can cope with features that have different inherent scales.</p>
","54","2","836","25573"
"27659","<p>Yes. This is how a face recognition algorithm might work for example, where two pictures might be of the same person or different person.</p>

<p>To build such a system, just pair up your data in a training set, double the input vector space and run a binary classifier that outputs ""true"" if the two items are the same. Any ML classifier could be adapted to this problem. </p>

<p>In practice, you may want more control over classification, and be robust against new classes that your algorithm has not seen before. A big problem in face recognition is the large number of potential classes, including classes not seen in training data, coupled with an equally large variance of images in the wild. This means that a naive approach as above will have poor performance in practice.</p>

<p>There is a more sophisticated approach: <a href=""https://www.coursera.org/learn/convolutional-neural-networks/lecture/HuUtN/triplet-loss"" rel=""nofollow noreferrer"">Triplet Loss</a>. This requires that you train with three inputs for each example. Unlike the naive version:</p>

<ul>
<li><p>The input is one image at a time. You train in triplets - an ""anchor"", and a postive match, and a negative match, in order to calculate one loss value for backprop.</p></li>
<li><p>The output is not a same/different class, but a vector description of the object. Similarity of objects is the distance between vectors. You do not need a label for this vector, even though this is supervised learning, thanks to how the loss function works.</p></li>
<li><p>The loss function is based on difference of distance from anchor example to positive example compared with a desired higher distance to negative example. This encourages learning key features of the inputs by making distance between vectors low (ideally zero) when they represent the same class, and as high as possible when they represent different classes.</p></li>
</ul>

<p>In both the naive approach and triplet loss approach, you need to be careful about selecting training data. You want to make the learning algorithm work hard to learn key differences, otherwise it is too easy to get a good loss. So taking MNIST as an example, when looking at negative matches, you want to pair up more (1, 7), (3, 2), (3, 8), (4, 9) etc pairs than (0,1), (9, 5). There are ways to drive this selection based on feedback from previous training epoch. </p>
","2","2","836","25573"
"27924","<p>The usual way to use interaction terms in linear regression is to construct new $x_n$, e.g. $x_3 = x_1 x_2$, and treat those identically as any other $x_n$. The learned parameter $b$ does not ""know"" the difference in how you calculated $x$, and the problem is still considered linear regression even if you create really complex functions of $x_n$ to create an input.</p>

<p>Taking your example, but with slightly different notation:</p>

<ul>
<li><p>The model estimate for Y is $\hat{y} = a + bx_1x_2$</p></li>
<li><p>The value of Y you want to learn is $y$</p></li>
<li><p>Mean squared error for a single example is $L = \frac{1}{2}(y-\hat{y})^2$ The factor of 2 does not change this answer, and is commonly used to simplify the gradient. Typically $C$ is the mean of $L$ over all examples.</p></li>
</ul>

<p>In order to learn optimal value of $b$, for one example the gradient you need is $\frac{\partial L}{\partial b}$. We can get that by expanding the loss function:</p>

<p>$L = \frac{1}{2}(y-\hat{y})^2$ </p>

<p>$L = \frac{1}{2}(y^2 - 2y\hat{y} + \hat{y}^2)$ </p>

<p>We could expand further, but typically now we calculate $\frac{\partial L}{\partial \hat{y}}$ and use the chain rule, because that has a simpler, more intuitive-looking result. Terms without $\hat{y}$ are zero:</p>

<p>$\frac{\partial L}{\partial \hat{y}} = \hat{y} - y$</p>

<p>We want $\frac{\partial L}{\partial b}$ for gradient descent</p>

<p>$\frac{\partial L}{\partial b} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial b}$ (Chain rule)</p>

<p>$\frac{\partial L}{\partial b} = (\hat{y} - y)(\frac{\partial}{\partial b} a + bx_1x_2)$</p>

<p>Again, terms without $b$ in them are constants:</p>

<p>$\frac{\partial L}{\partial b} = (\hat{y} - y)(x_1x_2)$</p>

<p>Note that the $x_1x_2$ term is unchanged from the input. It could be any function $x_n = f(x_1, x_2, x_3 ....)$</p>
","2","2","836","25573"
"28696","<blockquote>
  <p>What I am trying to understand is, the reward calculation does not take an action as a parameter. How does it choose an action properly.</p>
</blockquote>

<p>The reward function does not choose the action. It gives the immediate <em>consequences</em> for your previous action - and in most cases is a consequence of all previous actions, states and random factors in the environment. Although it should only <em>depend</em> on most recent state, previous action plus a random factor at most. </p>

<p>In your case reward depends directly on the resulting state = how many cars at each junction after taking an action, and not <em>directly</em> on the previous action = which traffic flow to enable. It does still depend on the action indirectly, via however the environment works. But that does not need to be expressed mathematically (as a model of the system) for Q learning to work. Although not stated, the reward probably also depends on a random factor outside of your control = how many cars turn up at each junction.</p>

<p>It is the Q value which eventually ranks the different actions and allows you to select the best action. The value $Q^t(s, a)$ gives you the current best estimate for future rewards (for a continuous problem, such as yours, it is common to have a discount factor, $\gamma$, to give more weight to immediate rewards).</p>

<p>There are multiple ways to select the next action in reinforcement learning, depending on the problem, your learning algorithm etc. In the example pseudo-code you copied, the selection process is using <a href=""http://banditalgs.com/2016/09/18/the-upper-confidence-bound-algorithm/"" rel=""nofollow noreferrer"">Upper Confidence Bound</a> (or UCB), which is a relatively sophisticated way of balancing exploration of actions that <em>might</em> be better than your current best estimate, versus simply using the ""best"" action so far. If you always took the ""best"" action, in many cases this would cause a problem because you would never update your knowledge of what the other actions do.</p>

<p>One of the standard textbooks for all this is <a href=""http://incompleteideas.net/book/the-book-2nd.html"" rel=""nofollow noreferrer"">Sutton &amp; Barto, <em>Reinforcement Learning: An Introduction</em></a>. The notation in that book is slightly different to what you have used, but it explains the concepts involved in your problem in easy-to-understand detail.</p>
","1","2","836","25573"
"28789","<blockquote>
  <ol>
  <li>Is this correct?</li>
  </ol>
</blockquote>

<p>Yes the diagrams both look correct to me. The key thing to understanding both diagrams is that the inputs and outputs of an LSTM cell are vectors. </p>

<p>The circles in the first diagram do represent the concept that the layer contains multiple individual artificial neurons, and that might make you assume that the second diagram is a picture of <em>one</em> of those neurons. Arguably there are multiple ""neurons"" or sub-layers with different roles inside a cell, because there are multiple places in which calculations of the form $f(W\mathbf{x} +b)$ occur, performing slightly different roles. I think the term ""cell"" is used to refer to this architecture of neurons. As a short-hand when I say ""neuron"", I tend to think of the output stage of the hidden layer.</p>

<p>However, in the second diagram, all of the operations shown work with vectors. Most importantly the left-to-right arrows in the second diagram represent vectors of hidden state from timestep to timestep from the <em>whole layer</em>. So each neuron in the cell is recurrently connected to every other neuron in that cell - twice as it happens in an LSTM, because LSTMs have both an internal cell state and a layer output.</p>

<blockquote>
  <ol start=""2"">
  <li>Is each unit in the cell independent from the others? Or do they share information?</li>
  </ol>
</blockquote>

<p>To match your description of the diagrams, let's define a ""unit"" as a collection of one of each type of neuron/gate used to make up the cell, that in theory could be wired together to make a working LSTM cell layer with a single scalar cell state and output value.</p>

<p>These units <em>are</em> independent in that each has its own weight parameters. There are no shared parameters for the connections between input and the units, or for the recursive connections that forward state from one time step to the next. In that sense the units do not share information.</p>

<p>However, the connections do mean that on each time step, input data and hidden state plus output from last output from all other units in the cell are combined are used in calculations. Any cell unit can base its new internal state plus its output on the values of all other outputs and internal states from other units in the cell. In this sense, the units do share information. I guess from your question that it is probably this second issue that you are concerned about, as the second diagram makes you think of a wiring diagram for a single neuron, but as explained above that is not the case.</p>

<blockquote>
  <ol start=""3"">
  <li>Imagine I have the following configuration: Number of samples = 1000 Number of time-steps = 10 Number of features = 5
  In this case, each unit in a cell will take as input a vector of size 5 right?</li>
  </ol>
</blockquote>

<p>Almost. Each neuron inside the cell will take an input of 5 from $\mathbf{x}$, plus an input of the hidden layer output, $\mathbf{h}$. So if in your case the LSTM cell size was 10, then each neuron would take a combined vector of 15. In addition, a second cell state vector is maintained, not labelled in your diagram. That is not used directly as an input to any neurons (i.e. components of the form $f(W\mathbf{x}+b)$), but does interact with the other values and can get changed itself, through the various gates. In your second diagram, it is the uppermost arrow going left to right.</p>

<blockquote>
  <p>But what will be the size of the output for one unit? 1?</p>
</blockquote>

<p>The cell as a whole will have the output of whatever size you have made the layer. That is what the diagram number 2 tries to show. However, using our working definition of ""unit"", the output of each unit will be two scalar values - the hidden layer output, and the cell state - which will be part of their respective vectors shown in the diagrams.</p>
","2","2","836","25573"
"28864","<blockquote>
  <p>My question is how to shape the rewards for card rejection and for winning a round. Any ideas? Positive or negative?</p>
</blockquote>

<p>In reinforcement learning, you must set rewards so that they are maximised when the agent achieves the <em>goals</em> of the problem. You should avoid trying to ""help"" the agent by setting interim rewards for things that <em>might</em> help it achieve those goals.</p>

<p>For card rejection, if that is part of the game (i.e. it is valid to play a ""wrong"" card, and you lose your turn), then either no reward, or a negative one might suffice. Probably you should go with no reward, because the punishment would be in not winning that round anyway.</p>

<p>If an invalid card cannot actually be played according to the rules of the game, and there is no ""pass"" move or equivalent, then you should not allow the agent to select it. Simply remove the action from consideration when making action selection. It is OK for the agent/environment to enforce this in a hard-coded fashion: A common way to do that, if your agent outputs a discrete set of action probabilities or preferences, is to filter that set by the environment's set of allowed actions, and renormalise.</p>

<p><em>What if you want the agent to learn about correct card selection?</em> Once you have decided that, then it becomes a learning objective and you can use a reward scheme. The action stops being ""play a card"" and becomes ""propose a card to play"". If the proposal is valid, then the state change and reward for the round's play are processed as normal. If the proposal is not valid, then the state will not change, and the agent should receive some negative reward. Two things to note about this approach:</p>

<ul>
<li><p>Turns in the game and time steps for the agent are now separate. That's not a problem, just be aware of the difference. </p></li>
<li><p>This will probably not encourage the agent to play better (in fact for same number of time steps, it will probably have learned less well how to win, because it is busy learning how to filter cards based on the observed features), but it will enable it to learn to propose correct cards without that being forced on it in a hard-coded fashion.</p></li>
</ul>

<p>For winning a round, then you  <em>might</em> want to reward the agent according to the game score it accumulates. Assuming that the winner of the overall game is the player with the highest score, this should be OK. </p>

<p>However, there is a caveat to that: <em>If</em> by making certain plays the agent opens up <em>other</em> players to score even higher, then simply counting how many points the agent gets is not enough to make it competitive. Instead, you want very simple sparse rewards: e.g. +1 for winning the game, 0 for drawing, -1 for losing. The main advantage of using RL approach in the first place is that the algorithms can and should be able to figure out how to use this sparse information and turn it into an optimal strategy. This is entirely how AlphaGo Zero works for instance - it has absolutely no help evaluating interim positions, it is rewarded only for winning or losing.</p>

<p>If you go with +1 win, -1 lose rewards, then you could maybe make players' current scores part of the state observation. That may help in decision making if there is an element of risk/gambling where a player behind in the scores might be willing to risk everything on the last turns just for a small chance to win overall.</p>
","2","2","836","25573"
"28928","<p>Kudos for figuring out a working tic-tac-toe playing algorithm from scratch!</p>

<blockquote>
  <p>Question 1: Can I successfully argue that I am estimating the reward based on history, and still claim the algorithm is reinforced learning or even Q-learning?</p>
</blockquote>

<p>First things first, this is definitely <em>not</em> Q-learning.</p>

<p>However, I do think it classifies as Reinforcement Learning. You have implemented these key components of RL:</p>

<ul>
<li><p>A state (the current board), used as input on each step.</p></li>
<li><p>An action (desired next board arrangement), used as output. When the action is effectively to choose the next state directly, this is sometimes called the afterstate representation. It is commonly used in RL for deterministic games.</p></li>
<li><p>Rewards generated by the environment, where the agent's goal is to maximise expected reward.</p></li>
<li><p>An algorithm that can take data about states, actions and rewards, and learn to optimise expected reward through gaining experience within the environment.</p></li>
</ul>

<p>Your algorithm is closest IMO to <a href=""http://www.incompleteideas.net/book/ebook/node53.html"" rel=""noreferrer"">Monte Carlo Control</a>, which is a standard RL approach.</p>

<p>One of the big advantages of Q Learning is that it will learn an optimal policy even whilst exploring - this is known as <em>off-policy</em> learning, whilst your algorithm is on-policy, i.e. it learns about the values of how it is currently behaving. This is why you have to reduce the exploration rate over time - and that can be a problem because the exploration rate schedule is a hyper-parameter of your learning algorithm that may need careful tuning.</p>

<blockquote>
  <p>Question 2: If I replace the reward lookup which is based on the board layout, with a neural network, where the board layout is the input and the reward is the output, could the algorithm be regarded as deep reinforcement learning?</p>
</blockquote>

<p>Yes, I suppose it would be technically. However, it is unlikely to scale well to more complex problems just from adding a neural network to estimate action values, unless you add in some of the more sophisticated elements, such as using temporal-difference learning or policy gradients. </p>

<blockquote>
  <p>Question 3: I'd don't think that I have either a learning rate or a discount factor. Is that important?</p>
</blockquote>

<p>A discount factor is not important for episodic problems. It is only necessary for continuous problems, where you need to have some kind of time horizon otherwise the predicted reward would be infinite (although you could also replace the discount mechanism with an average reward approach in practice).</p>

<p>The learning rate is an important omission. You don't explain what you have in its stead. You have put <code>update reward for board layout based on game outcome</code> - that update step typically has the learning rate in it. However, for tic-tac-toe and Q-Learning, you can actually set the learning rate to 1.0, which I guess is the same as your approach, and it works. <a href=""https://github.com/neilslater/game_playing_scripts/blob/master/tictactoe_q.py"" rel=""noreferrer"">I have written example code that does exactly that</a> - see <a href=""https://github.com/neilslater/game_playing_scripts/blob/master/tictactoe_q.py#L233"" rel=""noreferrer"">this line which sets learning rate to 1.0</a>. However, more complex scenarios, especially in non-deterministic environments, would learn badly with such a high learning rate.</p>

<blockquote>
  <p>Question 4: Can tictactoe algorithms be classed as real learning rather than simply brute force?</p>
</blockquote>

<p>Your algorithm is definitely learning something from experience, albeit inefficiently compared to a human. A lot of the more basic RL algorithms have similar issues though, and often need to see each possible state of a system multiple times before they will converge on an answer. </p>

<p>I would say that an exhaustive tree search from the current position during play was ""brute force"". In a simple game like tictactoe, this is probably more efficient than RL. However, as games get more and more sophisticated, the machine learning approach gets competitive with search. Often both RL and some form of search are used together.</p>
","6","2","836","25573"
"29108","<p>GRU and LSTM are two popular RNN variants out of many possible similar architectures motivated by similar theoretical ideas of having a ""pass through"" channel where gradients do not degrade as much, and a system of sigmoid-based control gates to manage signals passing between time steps. </p>

<p>Even with LSTM, there are variations which may or may not get used, such as adding ""peephole"" connections between previous cell state and the gates.</p>

<p>LSTM and GRU are the two architectures explored so far that do well across a wide range of problems, as verified by experiment. I suspect, but cannot show conclusively, that there <em>is no strong theory</em> that explains this rough equivalence. Instead we are left with more intuition-based theories or conjectures:</p>

<ul>
<li><p>GRU has less parameters per ""cell"", allowing it in theory to generalise better from less examples, at the cost of less flexibility.</p></li>
<li><p>LSTM has a more sophisticated memory in the form of separating internal cell state from cell output, allowing it to output features useful for a task without needing to memorise those features. This comes at the cost of needing to learn extra gates which help map between state and features.</p></li>
</ul>

<p>When considering performance of these architectures in general, you have to allow that some problems will make use of these strengths better, or it may be a wash. For instance, in a problem where forwarding the layer output between time steps is already a good state representation <em>and</em> feature representation, then there is little need for the additional internal state of the LSTM.</p>

<p>In effect the choice between LSTM and GRU is yet another hyperparameter to consider when searching for a good solution, and like most other hyperparameters, there is no strong theory to guide an a priori selection.</p>
","3","2","836","25573"
"29425","<p>Going from step (4.3) to (4.4) is turning the expectation (dependent on following policy $\pi$) into a more concrete calculation. To do this, you must resolve your random variables ($R_{t+1}$ and $S_{t+1}$) into the specific values from the finite sets of $\mathcal{R}$ and $\mathcal{S}^+$ - where individual set members are noted $r$ and $s$.</p>

<p>Recall that $\pi(a|s)$ is the probability of taking action $a$ given state $s$, so by taking a sum of any function depending on $(s,a)$ e.g. $\sum_{a} \pi(a|s)F(s,a)$ you will get the <em>expected</em> value of $F(s,a)$ starting from state $s$ and following policy $\pi$.</p>

<p>So you could write (4.3b) as:</p>

<p>$$v(s) = \sum_{a} \pi(a|s)\mathbb{E}_{\pi}[R_{t+1}+\gamma v_{\pi}(S_{t+1})| S_{t}=s, A_t=a]\qquad\qquad(4.3b)\\$$</p>

<p>Similarly, now you have written the expectation when given $s$ and $a$ (so you have all the possible cases of $a$ available to work with), you can use the transition and reward probabilities to fully resolve the expectation and substitute the random variables ($R_{t+1}$ and $S_{t+1}$) for their MDP model distributions described in $p(s',r|s,a)$, leading to equation (4.4). E.g.</p>

<p>$$v(s) = \sum_{a} \pi(a|s)\sum_{s',r} p(s',r|s,a)\mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1})| S_{t+1}=s', R_{t+1}=r]\qquad\qquad(4.3c)\\$$</p>

<p>$$\qquad = \sum_{a} \pi(a|s)\sum_{s',r} p(s',r|s,a)(r + \gamma v_{\pi}(s'))\qquad\qquad(4.4)\\$$</p>

<p>Your last equation is a slightly different formulation of the same result, where:</p>

<ul>
<li><p>$R_s^a$ is the <em>expected</em> reward when taking action $a$ in state $s$. It is equal to $\sum_{s',r} p(s',r|s,a)r$. It is also equal to $\mathbb{E}[R_{t+1}|S_t=s, A_t=a]$, by definition and independently of $\pi$.</p></li>
<li><p>$P_{ss'}^a$ is the state transition probability - the probability of ending up in state $s'$ when taking action $a$ in state $s$. It is equal to $\sum_r p(s',r|s,a)$ given $s'$</p></li>
</ul>

<p>Technically using $R_s^a$ loses some information about the dynamics of the underlying MDP. Although not anything important to reinforcement learning, which deals with maximising expected reward, so in some ways it is more convenient to already characterise the MDP with expected rewards. </p>
","2","2","836","25573"
"29939","<blockquote>
  <p>As far as I understand, in each iteration, Q-learning algorithm predicts the future reward of next step (and next step only) using the machine learning technique in use (be it the CNN, DNN etc.). </p>
</blockquote>

<p>The Q values should eventually converge to the <em>expected sum</em>, <em>future</em>, <em>discounted</em> reward when taking action A in state S and following the optimal policy. Breaking it down:</p>

<ul>
<li><p><em>Expected sum</em> is not exactly the same as ""predicted"", but close enough for our purposes. And it really does mean sum of the rewards, not a single reward. To differentiate, this is often called the ""return"" or ""utility""</p></li>
<li><p><em>Future</em> -> from the step being evaluated onwards until end of episode, or the limit as time goes to infinity for continuous tasks with discounting.</p></li>
<li><p><em>Discounted</em> -> a discount factor is only necessary for continuous tasks.</p></li>
</ul>

<blockquote>
  <p>And we are multiplying the reward of next step (and that specific next step only) with discount rate, to make it less important than the immediate reward (with the ratio we specified).</p>
</blockquote>

<p>No, there is no multiplication of the reward. Let's take a look at the line:</p>

<pre><code>target = (reward + self.gamma *
                  np.amax(self.model.predict(next_state)[0]))
</code></pre>

<p>The <code>reward</code> is not being multiplied by anything.</p>

<p>What is being multiplied by $\gamma$ is the Q value of the next state. That value represents the total sum of all rewards following on from that point - not a single reward value at all.</p>

<blockquote>
  <p>So, my question is, how does the algorithm takes even further steps (say, 5 steps) ahead into account?</p>
</blockquote>

<p>It is in the Q values. The pseudocode for the code you are looking at is not:</p>

<pre><code>target_for_Q(s,a) = next_step_reward * gamma
</code></pre>

<p>It is:</p>

<pre><code>target_for_Q(s,a) = next_reward + gamma * current_value_of_Q(s',a')
</code></pre>

<p>Or:</p>

<pre><code>target_for_Q(s,a) = next_reward + gamma * estimate_all_future_return
</code></pre>

<p>This is closely related to the <a href=""https://en.wikipedia.org/wiki/Bellman_equation"" rel=""nofollow noreferrer"">Bellman function for policy evaluation</a>.</p>

<p>Intuitively what is happening is that you start with (really poor) estimates for expected <em>return</em> (not expected reward), and update them by inserting observed values of <code>next_reward</code>, <code>s'</code> and <code>a'</code> into the update rule above. The values always represent a learned estimate of total expected <em>return</em>.</p>
","2","2","836","25573"
"30308","<blockquote>
  <p>Let's say we're playing a game where the reward is always positive (eg. accumulating a score), and there are never any negative rewards, the gradient will always be positive, hence θ will keep increasing! So how do we deal with rewards that never change sign?</p>
</blockquote>

<p>This is true. However, in many policy functions and in most situations, the gradient part $\nabla_{\theta} log \pi_{\theta}(s_t,a_t)$ will tend to zero as you reach a deterministic policy. This happens for a softmax action selection based on ""preferences"" (a matrix of softmax weights per action for each state) or as the output layer of a neural network. And it will counteract the tendency for the final layer preferences (or the logits of the neural network last layer) to grow uncontrollably.</p>

<p>You have identified a true weakness of REINFORCE. For instance, using the softmax action selection as an example and your always-positive returns:</p>

<ul>
<li><p>When the agent selects a non-maximising action, this will result in a positive return, the agent will add to its preference for that action.</p></li>
<li><p>When the agent selects a maximising action, this will result in a <em>larger</em> positive return, the agent will add to its preference for that action <em>more</em></p></li>
<li><p>REINFORCE works by increasing the preferences of better actions <em>faster</em> than preferences of worse actions. </p></li>
<li><p>This leads to a feedback process where better actions are chosen more often, increasing their preference values even faster.</p></li>
<li><p>Ultimately the preference for best actions will be so much higher than the alternatives that the softmax function will saturate. The gradient  $\nabla_{\theta} log \pi_{\theta}(s_t,a_t)$ for all actions will be close to zero.</p></li>
</ul>

<p>This does not always happen quickly, and basic REINFORCE implementations can be numerically unstable for exactly the reason in your question. To improve stability (and often learning speed), you can use <a href=""https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/CliffWalk%20REINFORCE%20with%20Baseline%20Solution.ipynb"" rel=""noreferrer"">REINFORCE with baseline</a>, which starts to address your concern by using offsetting values ($v_t - \bar{v}_t$ or similar). You can also then take that idea further and use Actor-Critic.</p>
","5","2","836","25573"
"30478","<blockquote>
  <p>Since the convergence of QLearning is so slow I am wondering if it is possible with QLearning to interpolate the QValue of unexplored states since QLearning does not use a model?</p>
</blockquote>

<p>When Q learning is described as ""model free"", it means that the agent does not need access to (or use) a predictive model of the environment. It cannot refer to state transitions and rewards in advance, but has to experience them in order to learn. </p>

<p>This <em>does not</em> mean that you have to avoid using a learning data model (such as a neural network) in order to generalise to new unseen data. </p>

<p>So, yes, Q learning can interpolate from unseen states and predict their Q value. To do this, you replace the state/action table with a supervised learning method based on descriptions of state $s$ and action $a$ as inputs, that you train as a regression model to predict $Q(s,a)$ (as a variant you can also have just state as input and predict $Q(s,a)$ for all possible actions as a vector in one go).</p>

<p>However, Q learning with a neural network suffers from instability. See <a href=""https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"" rel=""nofollow noreferrer"">Deep Mind's DQN paper</a> for example of a system that solves that instability. In short:</p>

<ul>
<li><p>Use experience replay - store S, A, R, S' data for each step and run the Q learning update on random mini-batches of the stored data, instead of online.</p></li>
<li><p>Keep two copies of the Q estimator neural network. Train one continuously, and copy it to a ""frozen"" version every now and then (e.g. every 100 mini-batches). Use the ""frozen"" copy to calculate the new $Q(s,a)$ targets.</p></li>
</ul>

<p>This still might not match your learning scenario. If you want to solve mazes, think carefully about what data is truly available to the agent and how you might use it. For instance if you are using Q learning to solve a maze where you have a map, it is very inefficient approach. This is often shown as a toy problem, because it is possible to view the learning data very easily. But in that toy problem, the agent is not given the map, nor any knowledge of what a grid is.</p>

<p>Here are a couple of suggestions that may still help, separate to using a neural network value estimator:</p>

<ul>
<li><p>If you do have a model of your environment (but not a map or other data that could be directly analysed for a solution), joining Q learning with a planning algorithm might work better for you than Q learning, as <a href=""https://medium.com/@ranko.mosic/online-planning-agent-dyna-q-algorithm-and-dyna-maze-example-sutton-and-barto-2016-7ad84a6dc52b"" rel=""nofollow noreferrer"">in Dyna-Q</a>. This is relevant where you have an agent exploring in real time that would benefit from ""looking"" ahead before taking actions.</p></li>
<li><p>If your problem is very sparse rewards (due to larger maze, and only getting different reward at the end), then a worthwhile improvement is to look into multi-step TD learning, where the rewards are propagated back to previous steps more efficiently. <a href=""https://stackoverflow.com/questions/40862578/how-to-understand-watkinss-q%CE%BB-learning-algorithm-in-suttonbartos-rl-book"">Maybe look into $Q(\lambda)$</a></p></li>
</ul>
","1","2","836","25573"
"30668","<blockquote>
  <p>I would like to know, is there any procedures or rules, that needs to be considered before formulating an MDP for a problem</p>
</blockquote>

<p>For a simple problem like this, I would do the following:</p>

<ul>
<li>List the states.

<ul>
<li>You have missed a state - the terminal state of leaving the building</li>
</ul></li>
<li>Mark terminal states

<ul>
<li>In this case, only exiting the building is a terminal state. The hungry tiger doesn't actually do anything to Bunny, the problem states that the MDP should just continue.</li>
</ul></li>
<li>List the actions possible in each state.

<ul>
<li>In your starting diagram, you do not show actions, and this is already limiting your ability to express the MDP.  </li>
</ul></li>
<li><p>List the possible transitions and probabilities of them</p>

<ul>
<li>For most action choices in the example, there are two possible outcomes from each action. The problem doesn't actually describe what happens when there is only one door, for the room with the tiger, but it seems reasonably safe to say that it is not possible for Bunny to get confused when there is only one door, so exiting the room with the tiger is one exception.</li>
</ul></li>
<li><p>Assign rewards. Rewards should relate to the goals stated in the problem definition. Negative rewards for things to avoid happening (or in general to avoid time wasting). Positive rewards for reaching desired targets.</p>

<ul>
<li>In your case you have two goals - avoid tiger and exit building. There does not appear to be any time constraint.</li>
</ul></li>
</ul>

<p>Typically you would show action choices as labelled arrows going from states to an action node (usually just a small solid circle) where the state transitions then take effect, with further arrows labelled with probabilities going to states. So State A should have two arrows going out ""choose left door"" and ""choose right door"", then these choices will branch again - ""choose left door"" will go to state B with p=0.9 and state C with p=0.1.</p>

<p>You have some free choices over where to put reward labels, and there are a few different standard approaches. In this case, I would put the non-zero rewards on the state transitions that lead to goal-affecting states. E.g. maybe a +1 reward on the state transition from B taking the exit door and ending up outside. Note that for both the exit and the room with the tiger, that there are two possible ways to get to them due to the random transitions (thanks to Bunny getting confused).</p>

<p>Note there are no actions, and no possibility of different transitions, where Bunny starts in state C and ends up in state C on the next timestep. I think you are confusing state C as being a terminal state - although this is understandable as some encounters with hungry tigers in the real world could well be terminal, the problem clearly states that everything just carries on for Bunny if he encounters the tiger. It is just something he wants to avoid. If he ends up in state C, then he still has an action he can take to go through the door. When he takes this action, he cannot ""get confused"" and pick the wrong door, so should always end up in state B with p=1.0.</p>

<p>If you decide to draw a transition diagram, you have free choice of layout. There is no fixed approach for this, and for many MDPs it may not be possible to draw a nice 2D diagram. However, in your case, it is definitely possible to layout the states in a simple 2D arrangement. A good place to start would be to place the states in locations that correspond to the imaginary room map. Leave plenty of space initally, as the states do not link directly (as you have started), but link via action nodes which themselves branch to the next state.</p>

<p>I hope this enough guidance for you to complete the problem by yourself. Solving the remainder on your own will be much better for your learning than turning it into a worked example here.</p>
","2","2","836","25573"
"31565","<p>The book you are reading is being somewhat lax with terms. It uses the terms ""actor"" and ""critic"", but there is another algorithm called <a href=""https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"" rel=""nofollow noreferrer"">actor-critic</a> which is very popular recently and is quite different from Q learning. Actor-critic does have two function estimators with the roles suggested in the quote. Q-learning has one such estimator*.</p>

<p>I have looked at the chapter in more detail, and where it says:</p>

<blockquote>
  <p>one will be used to drive Ms. Pac-Man during training (the actor), and the other will watch the actor and learn from its trials and errors (the critic).</p>
</blockquote>

<p>I would substitute:</p>

<blockquote>
  <p>one will be used learn from current actions, and the other will remember results from some time steps ago in order to estimate the values for next actions.</p>
</blockquote>

<p>This is not something that's inherently part of Q-learning, but it is part of DQN's adjustments when combining Q-learning with neural networks. Both experience replay and having two copies of the learning network (one a temporarily ""frozen"" version of the other) are important for stabilising the learning algorithm. Without them it can become numerically unstable.</p>

<blockquote>
  <p>Is this a typical Q-learning implementation? </p>
</blockquote>

<p>It's a typical implementation of basic DQN, which is how many people nowadays would implement Q-learning with neural networks. </p>

<p>You can ignore the references to ""actor"" and ""critic"". Instead it is easier to consider that there is just one ""action value"" network, and you keep an old copy of it around to help with stability.</p>

<hr>

<p>* Generally in RL, the term ""model"" is reserved for a model of the environment - which neither Q-learning nor actor-critic provide. So you will also read that Q-learning is a ""model free"" algorithm. For the rest of the book, you will have seen ""model"" to refer to any statistical learning algorithm (or the architecture and learned parameters) . . . what you will see in RL texts is the careful use of ""function estimator"" or other terms for networks which learn something else other than how the environment behaves.</p>
","1","2","836","25573"
"31630","<p>Nearest neighbour algorithms (kNN and variants) do not have a training phase. They work by storing all the labelled examples, and using them directly for inference on new data.</p>

<p>There are some caveats:</p>

<ul>
<li><p>""Training"" is fast as it just stores each example once, but inference is slow, as typically it involves searching for relevant examples. This can be improved by indexing routines (which are not part of the algorithm, but might be part of a specific implementation).</p></li>
<li><p>Although there are no parameters to train, there may still by hyper-parameters to select, which will involve running a cross-validation exercise with hold-out data and varying the choices, e.g. of number of nearest neighbours to consider, or best distance metric to use to determine closest neighbours.</p></li>
<li><p>Data preparation (normalisation) and feature engineering are often required to get the best results out of kNN, and also require multiple attempts and tests.</p></li>
</ul>

<p>In general, approaches that store and query labelled data without a training phase are <a href=""https://en.wikipedia.org/wiki/Instance-based_learning"" rel=""nofollow noreferrer"">instance based learning</a></p>
","2","2","836","25573"
"31653","<p>Once converted to numerical form, models don't respond differently to columns of one-hot-encoded than they do to any other numerical data. So there is a clear precedent to normalise the {0,1} values if you are doing it for any reason to prepare other columns.</p>

<p>The effect of doing so will depend on the model class, and type of normalisation you apply, but I have noticed some (small) improvements when scaling to mean 0, std 1 for one-hot-encoded categorical data, when training neural networks.</p>

<p>It may make a difference too for model classes based on distance metrics.</p>

<p>Unfortunately, like most of these kind of choices, often you have to try both approaches and take the one with the best metric.</p>
","18","2","836","25573"
"31816","<blockquote>
  <p>I'd like to implement this symmetry in the network, and by my understanding of CNNs that means i'll generally need all the filters to be horizontally symmetric, and then the network would both be more robust and would take almost 40% less time to train assuming filter size = 5.</p>
</blockquote>

<p>If two mirrored images are in the same class - e.g. they both show a dog or a cat - that is <em>not</em> the same as having all the components in the image (lines, textures, shapes) responding well to symmetric filters. In general this is not the case. Even for symmetric looking shapes such as faces, it is only true at a certain scale and specific pose.</p>

<blockquote>
  <p>what disadvantages does this architecture have that I can't find a single mention of such idea?</p>
</blockquote>

<p>It will work poorly, because the lines, textures and shapes being detected as components of the image are rarely horizontally symmetrical.</p>

<p>CNNs cannot easily take advantage of mirroring or rotational invariance - where the class or detection does not vary under mirror or rotation transformations. One thing you can do to improve generalisation for transformations is data augmentation, i.e. performing the non-class-changing transformations of your training images, perhaps randomly on demand during the training process. This does work, but has the opposite impact that you were hoping for in terms of efficiency.</p>

<p>The recently-published <a href=""https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b"" rel=""nofollow noreferrer"">CapsNet by Geoffrey Hinton's group</a> may be able to take advantage of more types of transformation invariance. However, this is still in early stages of research, and not clear whether it offers a practical advantage in your case. There are implementations in various frameworks <a href=""https://github.com/XifengGuo/CapsNet-Keras"" rel=""nofollow noreferrer"">such as Keras</a>, that you could try if you are interested.</p>
","2","2","836","25573"
"31847","<blockquote>
  <p>But I would like to know if AlphaGo Zero can adapt to the way the oponent plays (oponent profile) or something like this. </p>
</blockquote>

<p>That is not included in the algorithm as written, where the ""profile"" of the opponent is effectively AlphaGo Zero itself (learned through self play).</p>

<p>It is not clear whether adapting play style to a given opponent would offer any advantage. It would be difficult to assess because AlphaGo Zero is such a strong player, that it will win a large percentage of games against human players as-is. Seeking and measuring any improvement, except versus earlier versions of itself, would be quite hard.</p>

<p>However, there are a likely a few places in the code where learned play style of an opponent <em>could</em> in theory allow AlphaGo Zero to be more efficient. The most obvious is in the ""rollout"" policy (I'm not 100% sure if they use the same term), where the algorithm simulates and samples different possible trajectories through the game in order to predict likely outcomes. </p>

<p>The current rollout policy in AlphaGo is learned through self play. But it is just a neural network that predicts probability of making plays given board state. It could easily be adjusted in a supervised learning fashion, based on sampled plays from an opponent. If it could be learned accurately, then it should make searches more efficient and accurate - the impossible but ideal situation being that it predicted opponents' move exactly and thus could quickly find the ultimate counter to their actions. In fact the original AlphaGo rollout policy did model human play in this way. It was based on large database of many human master level play moves, not a single player. The Deep Mind team did suggest in their paper that this gave better results <em>at the time</em> than a self-play policy - they tried both and the human database was better. Since then, AlphaGo Zero has surpassed the performance of original AlphaGo without the database of human moves.</p>
","1","2","836","25573"
"32021","<blockquote>
<blockquote>
<p>Having a bias term with the one-hot encoding prevents each state’s Q values from being independent</p>
</blockquote>
<p>Any ideas why this is the case?</p>
</blockquote>
<p>The usual point of using linear regression or other function approximation in Q-learning is to generalise, and thus prevent the Q values from being independent deliberately. So this is not a general statement about Q-learning and linear function approximation. In general yes you can add bias terms - also, in general when using linear regression or neural networks to estimate action values, then you <em>should</em> have bias terms.</p>
<p>Having said that, I would expect the bias should make little difference here, since the one hot coding of state makes it redundant - you can already express any mapping of state to Q value of each possible action using just the weights. Although you are using what looks like a linear regression model, essentially this is just the tabular form of Q learning.</p>
<p>Q learning with function approximation can be unstable. The combination of off-policy, bootstrap updates and function approximation is known as the &quot;deadly triad&quot;, and you need to add tricks like experience replay to keep it stable. Although in theory the linear regression should learn a correct bias, probably adding bias puts the algorithm into that unstable zone.</p>
","2","2","836","25573"
"32218","<p>The equation and value of <span class=""math-container"">$f_t$</span> by itself does not fully explain the gate. You need to look at first term of the next step:</p>

<p><span class=""math-container"">$C_t = f_t \odot C_{t-1} + i_t \odot \bar{C}_{t}$</span> </p>

<p>The vector <span class=""math-container"">$f_t$</span> that is the output from the forget gate, is used as element-wise multiply against the previous cell state <span class=""math-container"">$C_{t-1}$</span>. It is this stage where individual elements of <span class=""math-container"">$C$</span> are ""remembered"" or ""forgotten"". </p>

<p>Due to the sigmoid function, the vector <span class=""math-container"">$f_t$</span> behaves like a binary classifier for each element, with saturated values tending to settle on either not modifying <span class=""math-container"">$C$</span> at all (a value of <span class=""math-container"">$1$</span>) or ""forgetting"" what the previous value was (a value of <span class=""math-container"">$0$</span>). Of course intermediate values are also possible, and the analogy there between simply remembering and forgetting values is less direct.</p>

<p>The analogy with forgetting or remembering helps towards understanding the improvements regarding preserving gradients over multiple timesteps. For any step where an element of <span class=""math-container"">$f_t$</span> is close to <span class=""math-container"">$1$</span> (and thus the effect of previous timesteps is being ""remembered""), then the corresponding gradient elements backpropagating from <span class=""math-container"">$\nabla_{\theta} C_{t}$</span> to <span class=""math-container"">$\nabla_{\theta} C_{t-1}$</span> remain the same, avoiding the gradient loss seen in more simple RNN architectures (especially related to saturated values). Once a previous element in <span class=""math-container"">$C$</span> is ""forgotten"", then the error gradient connection is cut off between time steps.</p>
","3","2","836","25573"
"32597","<p>It is just a type of namespacing, because $a$ is already assigned the chosen action. There are two contexts of action being considered in the equation, so there needs to be a symbol for each context. Using $a'$ is an obvious choice as the letter $a$ is implicitly linked to representing an action already.</p>

<p>The sum over $a'$ is a sum over all possible actions in state $s$, irrespective of the chosen action $a$.</p>

<p>So both $a$ and $a'$ represent actions. $a$ is the current action, supplied on the LHS of the equation. $a'$ represents the iterator of a sum over all actions $[\forall a' \in \mathcal{A}(s)]$, only used in the calculation on the RHS. Sometimes you will see a completely different letter chosen, or some subscripting or other way to show these represent <em>different</em> actions.</p>

<p>It is also quite common to see $a$ representing current action, and $a'$ representing the <em>next</em> action (taken when in state $s'$). But that is not what is happening here.</p>
","2","2","836","25573"
"32611","<blockquote>
  <p>Given the significant advancements in reinforcement learning</p>
</blockquote>

<p>Worth noting that many of the recent advancements are due to improvements in neural networks used as function approximators, and understanding how to integrate them with reinforcement learning (RL) to help solve RL challenges involving vision or other complex non-linear mapping from state to best action. </p>

<p>So at least some of current improvements in RL were due to researchers asking the opposite question ""Given the significant advancements in neural networks . . .""</p>

<blockquote>
  <p>I wanted to know whether it is possible to recast problems such as action recogniton, object tracking, or image classification into reinforcement learning problems.</p>
</blockquote>

<p>Generally, at the top level, the answer is <em>Yes, but it offers no benefit, and could perform a lot worse</em>. </p>

<p>That is because in typical supervised learning scenarios there is nothing to match the concept of actions that modify state. In the example classifiers you can have the equivalent of a state (the input to be classified), an action (the choice of category) and the reward (whether of not the choice matches the label). But taking an action does not lead to another state, rewards are not sparse or cumulative across multiple actions within a specific environment or ""episode"". There are no time steps. </p>

<p>RL algorithms are generic <a href=""https://en.wikipedia.org/wiki/Markov_decision_process"" rel=""nofollow noreferrer"">MDP solvers</a> - they can learn about relationships between state, action and likely next state, and optimise long-term goals over may time steps when given a current state. That also makes them less efficient learners when those relationships are not valid or important in the problem you want to solve. If you trained e.g. Q-learning on a typical image classification dataset, and added time steps, it would spend a lot of time/resources establishing that its choice of action had no influence on which images it was subsequently presented with, or how easy it was to get a reward from later images as opposed to earlier ones depending on how the state varied. If you really did allow the choice of action to determine the next image, then you would be training the RL to do something else other than classification.</p>

<p>You could frame the classifiers as <a href=""https://en.wikipedia.org/wiki/Multi-armed_bandit#Contextual_bandit"" rel=""nofollow noreferrer"">contextual bandits</a>, which is perhaps closer match. However, that still throws away knowledge that you have about the classification problem, replacing it with a generic reward system. For instance, a contextual bandit solver would deliberately guess wrong classes to check whether sometimes there was a small chance of a high reward for doing that.</p>

<p>If you were very careful about how you represented actions and rewards, and set other hyper-parameters, then you might be able to re-create a similar gradient setup to normal supervised learning, and only lose a little bit of efficiency through using RL or contextual bandit framing of your problem. However, you would still have added some non-necessary complexity.</p>

<p>If you search, you may find some ways to combine RL with supervised learning, for instance in <a href=""https://arxiv.org/pdf/1611.02796v3.pdf]"" rel=""nofollow noreferrer"">this paper, the authors propose using RL to refine a generative RNN</a>. However, these currently seem niche and are not aimed to improve upon or replace supervised learning.</p>

<p>Finally, in theory you could allow RL to control a video camera pan/zoom as part of activity recognition (or any other video or multi-image classification task). That <em>would</em> be a full RL problem, because the agent's actions really would be influencing the later states and hopefully improving the accuracy of recognition. For learning efficiency, you would likely want to combine this initially with a network that had already been trained to recognise actions on a supervised data set. You would need to experiment with how much the recognition part was trained compared to the RL part (as it will start to collect data outside of your normal datasets). And of course setup and training of the combined system could be a <em>major</em> project. You may be able to simulate it in a game engine perhaps in the early stages.</p>
","1","2","836","25573"
"32780","<p>Policy Iteration is essentially a two step process:</p>

<ol>
<li>Evaluate the current policy by calculating $v(s)$ for every non-terminal state.

<ul>
<li>Internally this requires multiple loops over all the states until the value calculations are accurate enough</li>
</ul></li>
<li>Improve the policy by choosing the best action $\pi(s)$ for every non-terminal state</li>
</ol>

<p>The overall process terminates when it is not possible to improve the policy in the second part. When that happens, then - from the Bellman equations for the optimal value function - the value function and policy must be optimal.</p>

<p>In detail for your example, it might work like this:</p>

<ul>
<li><p>Initialise our value function for $[S_0, S_1, S_2]$ as $[0,0,0]$ - note that by definition for the terminal state $v(S_2)=0$ so we never need to recalculate that (this is also consistent with the diagram, which shows the ""absorbing"" actions) so we <em>could</em> actually calculate it if we want, but it's a waste of time.</p></li>
<li><p>Choose a value for accuracy limit $\theta$. To avoid going on forever in the example, let's pick a high value, say $0.01$. However, if this was computer code, I might set it lower e.g. $10^{-6}$</p></li>
<li><p>Set our discount factor, $\gamma = 0.1$. Note this is quite low, heavily emphasising immediate rewards over longer-term ones.</p></li>
<li><p>Set our current policy $\pi = [B, R]$. We're ready to start.</p></li>
<li><p>First we have to do policy evaluation, which is multiple passes through all states, updating $v(s) = \sum_{s',r} p(s',r|s,\pi(s))(r + \gamma v(s'))$. There are other ways of writing this formula, all basically equivalent, the one here is from Sutton &amp; Barto 2nd edition. We also can choose between a ""pure"" iteration calculating $v_{k+1}(s)$ from $v_{k}(s)$, but I will do the update in place to the same $v$ array, because that is easier and usually faster too.</p>

<ul>
<li><p>Note each sum here is over the two possible $(s', r)$ results from each action.</p></li>
<li><p>Pass 1 in detail, iterating though states $[S_0, S_1]$:</p>

<ul>
<li>Set max delta ($\Delta = 0$) to track our accuracy</li>
<li>$v(S_0) = \sum_{s',r} p(s',r|S_0,B)(r + \gamma v(s')) = 0.5(-5 + 0.1 v(S_1)) + 0.5 (-2 + 0.1 v(S_0)) = -3.5$</li>
<li>This sets max delta to 3.5</li>
<li>$v(S_1) = \sum_{s',r} p(s',r|S_1,R)(r + \gamma v(s')) = 0.6(-5 + 0.1 v(S_1)) + 0.4 (0 + 0.1 v(S_2)) = -3$</li>
<li>Max delta is still 3.5</li>
<li>The $v$ table is now $[-3.5, -3, 0]$</li>
</ul></li>
<li>Pass 2, because $\Delta \gt 0.01$:

<ul>
<li>Reset $\Delta = 0$</li>
<li>$v(S_0) = 0.5(-5 + 0.1 v(S_1)) + 0.5 (-2 + 0.1 v(S_0)) = -3.825$</li>
<li>$\Delta = 0.325$</li>
<li>$v(S_1) = 0.6(-5 + 0.1 v(S_1)) + 0.4 (0 + 0.1 v(S_2)) = -3.180$</li>
<li>$\Delta = 0.325 \gt 0.01$</li>
<li>The $v$ table is now $[-3.825, -3.180, 0]$</li>
</ul></li>
<li>Pass 3, $v = [-3.850, -3.191, 0]$</li>
<li>Pass 4, $v = [-3.852, -3.191, 0]$, this is small enough change that we'll say evaluation has converged.</li>
</ul></li>
<li>Now we need to check whether the policy should be changed. For each non-terminal state we need to work through all possible actions and find the action with the highest expected return, $\pi(s) = \text{argmax}_a \sum_{s',r} p(s',r|s,a)(r + \gamma v(s'))$

<ul>
<li>For $S_0$:

<ul>
<li>Action B scores $0.5(-5 + 0.1 v(S_1)) + 0.5 (-2 + 0.1 v(S_0)) = -3.852$ (technically, we already know that because it is the current policy's action)</li>
<li>Action R scores $0.9(-2 + 0.1 v(S_0)) + 0.1 (0 + 0.1 v(S_2)) = -2.147$ </li>
<li>R is the best action here, so change $\pi(S_0) = R$</li>
</ul></li>
<li>For $S_1$:

<ul>
<li>Action B scores $0.8(-2 + 0.1 v(S_0)) + 0.2 (-5 + 0.1 v(S_1)) = -2.971$</li>
<li>Action R scores -3.191 </li>
<li>B is the best action here, so change $\pi(S_1) = B$</li>
</ul></li>
<li>Our policy has changed, so we go around both major stages again, starting with policy evaluation and the current $v = [-3.852, -3.191, 0]$, using the new policy $[R, B]$</li>
</ul></li>
</ul>

<p>I leave it up to you to do the second pass through evaluation and improvement. I suspect it will show that the new policy is optimal, and the policy will remain $[R, B]$ (although $[R,R]$ seems possible without doing the calculation). If the policy stays the same on a complete pass through evaluation and improvement, then you are done.</p>
","1","2","836","25573"
"33508","<blockquote>
  <p><strong>Each training sample ends up in a distant, completely separate location on the error-surface</strong></p>
</blockquote>

<p>That is not a correct visualisation of what is going on. The error surface plot is tied to the value of the network <em>parameters</em>, not to the values of the data inputs. During back-propagation of an individual item in a mini-batch or full batch, each example gives an <em>estimate</em> of the gradient in the same location in parameter space. The more examples you use, the better the estimate will be (more on that below).</p>

<p>A more accurate representation of what is going on would be this:</p>

<p><a href=""https://i.stack.imgur.com/cZJQN.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/cZJQN.jpg"" alt=""enter image description here""></a></p>

<p>Your question here is still valid though:</p>

<blockquote>
  <p>But why does averaging the gathered gradient work? </p>
</blockquote>

<p>In other words, why do you expect that taking all these individual gradients from separate examples should combine into a better approximation of the average gradient over the error surface? </p>

<p>This is entirely to do with how the error surface is itself constructed as the average of individual loss functions. If we note cost function for the error surface as $C$, then </p>

<p>$$C(X, \theta) = \frac{1}{|X|}\sum_{x \in X} L(x, \theta)$$</p>

<p>Where $X$ represents the whole dataset, $\theta$ are your model's trainable parameters, $L$ is an individual loss function for $x$. Note I have rolled labels into $X$ here, it doesn't matter for this argument whether loss is due to comparison of model output with some part of the training data - all we care about is finding a gradient to the error surface.</p>

<p>The error gradient that you want to calculate for gradient descent is $\nabla_{\theta} C(X, \theta)$, which you can therefore write as:</p>

<p>$$\nabla_{\theta} C(X, \theta) = \nabla_{\theta}(\frac{1}{|X|}\sum_{x \in X} L(x, \theta))$$</p>

<p>The derivative of the sum of any two functions is the sum of the derivatives, i.e.</p>

<p>$$\frac{d}{dx}(y+z) = \frac{dy}{dx} + \frac{dz}{dx}$$</p>

<p>In addition, any fixed multiplier that doesn't depend on the parameters you are taking the gradient with (in this case, the size of the dataset) can just be treated as an external factor:</p>

<p>$$\nabla_{\theta} C(X, \theta) = \frac{1}{|X|}\sum_{x \in X} \nabla_{\theta} L(x, \theta)$$</p>

<p>So . . . the gradient of an average of many functions, is equal to the average of the gradients of those functions taken separately. Taking any completely random subset of $X$ will result in an unbiased estimate of the mean gradient, same as taking a random subset of any variable and taking its mean will give you an unbiased estimate of the population's mean.</p>

<p>This will not work if your samples are somehow correlated, hence why you will often see recommendations to shuffle the data prior to training, in order to <a href=""https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables"" rel=""noreferrer"">make it i.i.d</a>.</p>

<p>This will also not work if your cost function combines examples in any way other than addition. However, it would be an unusual cost function that combined separate training examples by multiplying their loss functions, or some other non-linear combination.</p>
","8","2","836","25573"
"33793","<blockquote>
<p>That is, maybe Err reduces when w1 goes towards ∂Err/∂w1 alone, but it might well be the case that Err actually increases when w1 is updated along with w2, that is when we are taking steps together in direction of all these weights, we might actually not go down the Err.</p>
<p>Isn't this the case?</p>
</blockquote>
<p>Not exactly. Using back propagation, the weight gradients may be calculated precisely (for the given training data). It doesn't matter that there are many of them updating at the same time. The gradients don't &quot;conflict&quot; as such, they are 100% compatible. But they are only valid locally to the current values of <span class=""math-container"">$w_i$</span></p>
<p>An update to the all the weights at once in the opposite direction to the gradient is <em>guaranteed</em> to reduce the error value for that training data, with an important caveat that it is only fully guaranteed to make an infinitesimal small improvement, when the step size is also infinitesimal.</p>
<p>If you make an update step that is too large (for some value of &quot;too large&quot; which varies a lot depending on context), then the curve may not remain consistent over the step and your error value could increase.</p>
<p>This problem is <strong>not</strong> directly related to updating multiple weights at once. It can occur even when you have a single weight. However, when there is a more complex function, with more weights all changing at once, there can be more places where the function does this.</p>
<p>In practice, <em>infinitesimal</em> updates would take too long to train, so you need to find a larger value - but not so large it causes problems - as a compromise.</p>
<p>In addition:</p>
<ul>
<li><p>The training data you have will usually allow you at best to create a rough approximation to the &quot;true&quot; function you are searching for. So you don't really want to find an actual global minimum in the error function for your training data set, as it would overfit.</p>
</li>
<li><p>Typically, using mini-batches, the gradient is also only a rough approximation, so updates only go roughly in the right direction (this can sometimes be good as it can help escape from local minima and saddle points)</p>
</li>
</ul>
","1","2","836","25573"
"33923","<blockquote>
  <p>I know it will give me a probability score of y being 1 but that will be too low to make use of.</p>
</blockquote>

<p>Given the description you have of input data, which as you explain it is just the specs of a product (and implicitly that you are classifying for a user who has reached the product page somehow), then that's the best you can do. You simply don't have the data about the motivations and history of the user that would enable you to refine further than the mean probability of purchase given the product stats.</p>

<p>If you can access data about the visitor, features such as what other pages they have visited, previous purchases, whether they have clicked through from a particular advert or search result etc, then you may be able to refine the value further. However, even with that kind of backing data, you would still be quite far from predicting a clear ""will buy"" vs ""won't buy"" - instead you may be able to refine a generic p=0.01 to a personalised range p=0.0001 to p=0.05 depending on the user details.</p>

<p>These kind of shifts in probabilities can still be used successfully in advertising and sales pipelines. The difference between a 0.01 and 0.05 probability when deciding what to advertise for example is a factor of 5 efficiency improvement on use of advertising sections of a web site. The trick is finding the right way to make use of the information. </p>

<p>In general, you won't be able to make a reliable prediction that a user will buy a product like a mobile phone on a website. Too many hidden variables. So don't try to design systems that rely on a direct yes/no prediction.</p>
","1","2","836","25573"
"34087","<blockquote>
  <p>A slightly unrelated thought, 'V' is the score of the current state only. 'A' is the total future expected Advantage, for a particular action, right?</p>
</blockquote>

<p>Not quite. $V$ is the total (discounted) future expected reward, assuming starting in state $s$, following current policy (in a control problem, usually best guess so far at the optimal policy) into the future. That includes selecting current action, $a$, according to the policy being assessed. The advantage function $A(s,a)$ (blog post has the arguments wrong for $A$) is the difference in value between selecting $a$ according to the current policy and selecting a specific action $a$. The value of $A$ also sums all future rewards assuming that the current policy is then followed into the future after this, maybe different, selection. Note that when the policy is the optimal one, and $V(s)$ is accurate, then $A(s,a)$ should always be zero or negative; optimal actions score zero, non-optimal ones will be negative.</p>

<blockquote>
  <p>Even if network somehow benefits from one or the other, how does it help if both streams still end up as Q?
  ... Can someone provide a different example to the sunset?</p>
</blockquote>

<p>The more prosaic explanation is that the function decomposition is always technically correct (for an MDP). Coding the network like this incorporates known structure of the <em>problem</em> into the network, which otherwise it may have to spend resources on learning. So it's a way of injecting the designer's knowledge of reinforcement learning problems into the architecture of the network. </p>

<p>Conceptually this is similar to designing CNNs for computer vision with local receptive fields because we know edges and textures can be detected this way in images. Although CNNs have more than just that benefit, one of the positive aspects of the design for vision tasks, is that they structurally match known traits of the problem being solved.</p>

<p>Value-based RL control methods (as opposed to policy-gradient methods) work due to ""<a href=""http://www.incompleteideas.net/book/ebook/node46.html"" rel=""nofollow noreferrer"">generalised policy iteration</a>"", where the agent is constantly assessing the current values of a policy, then using those value estimates in order to make improvements. The split between $V$ and $A$ functions fits very well with that conceptually. The $V$ function is generally being adjusted to assess the current policy as accurately as possible, whilst positive values in $A$ function identify likely changes to the policy.</p>
","3","2","836","25573"
"34133","<blockquote>
  <p>Assuming the implementation is as simple as possible, with no advanced concepts, is it likely for something like this to happen or is it definitely an error in the implementation? </p>
</blockquote>

<p>In my experience, using the simplest possible network, and simplest gradient descent algorithm, then yes this happens relatively frequently. It is an accident of the starting weight values, and technically a local minimum of the cost function, which is why it is so stable when it happens. In the basic implementation you have then there are only 6 starting weights. If they are selected randomly, the chances of a ""special"" pattern (such as the weights to hidden layer being all positive or all negative) are relatively high (1 in 8 for all positive or all negative weights between input and first hidden layer).</p>

<p>This is also why the values sum to 2 - given the the network is stuck on the wrong part of the error surface, it will still minimise the cost function as best it can given that constraint, and this will usually end up with compromise values that still meet statistical means overall in the predictions. If you doubled up some, but not all of the input/output pairs (e.g. a training set of 6 inputs $\{(0,0:0), (0,1:1), (1,0:1), (1,1:0), (0,1:1), (1,0:1)\}$, then the network may converge to different wrong mean value when it failed.</p>

<blockquote>
  <p>Is it because of the very symmetrical nature of an XOR, which makes it impossible to handle for a single neuron?</p>
</blockquote>

<p>You don't have a single neuron here. Unless you mean in the output layer? In which case, no, this is not to do with having a single neuron in the output layer.</p>

<p>Pretty much any more advanced NN feature, or simply more randomness, will stop this problem happening. E.g. make the middle layer have 4 neurons instead of 2, use momentum terms, a larger dataset with random ""mini-batches"" sampled from it.</p>

<p>In general this kind problem does not seem to happen on larger, more complex datasets and larger more complex networks. These can have other problems, but getting stuck in a local minimum <em>far away from the global minimum</em> tends not to happen. In addition for those scenarios, you typically don't want to converge fully into a global minimum for your dataset and error function, but are looking for some form of generalised model (that can predict from input values that you have not seen before).</p>

<hr>

<p>On a practical note, if you want to add an automated test showing your NN implementation can solve XOR, then either use fixed starting weights or a RNG seed that you know works. Then your test will be reliable, even if the NN is not in all cases.</p>
","2","2","836","25573"
"34150","<blockquote>
  <p>Does it generate the set of the same image classes in the same order on each iteration? </p>
</blockquote>

<p>No, a basic generator in a GAN is typically fed a small random vector as input; e.g. a column of 100 gaussian samples, with mean 0, standard deviation 1. It then must use this random ""embedding"" in the feed forward network to produce output that is in the target distribution of real data that the discriminator is trying to assess it against.</p>

<p>So a GAN typically learns a mapping from an arbitrary relatively low dimensional space onto a higher-dimensional target space. DCGANs do this with image data.</p>

<p>The class of an image can be used as conditioning data, if it is fed as additional input to the the generator and discriminator. The sequence of image classes can be anything you like, and the GAN is not required to work in a particular sequence (unless perhaps you are working with a RNN-based GAN and are trying to learn to produce sequences).</p>

<blockquote>
  <p>What exactly does generator produce in DCGANs?</p>
</blockquote>

<p>It produces an output image depending on the input. The output is drawn from a distribution based on the training data, that the discriminator would struggle to distinguish from real data. If fed a random input vector, generated in same way as training stages, then the idealised output should appear to have been drawn equally randomly from the imagined population of all possible training data.</p>
","1","2","836","25573"
"34440","<p>Definitely using experience replay can slow down the agent processing each time step, because typically on each time step, a result is stored (possibly requiring another result to be deleted), a mini-batch number of results are chosen randomly and fetched, then the function approximator (usually a NN) has to be run forward for the mini-batch to discover max Q values (potentially multiple times per item if the NN uses action as <em>input</em> instead of multiple action values as output). Then it has to run a forward pass to calculate error values and back propagation and weight updates over the mini-batch. All that can take considerable amount of CPU and memory bandwidth compared to the same system running without experience replay.</p>

<p>However, there are a lot of factors to consider whether this is important. Here are a few that I can think of:</p>

<ul>
<li><p>Typically the environments using DQN agents are not training in real time. Often they run multiple times the speed of real time in order to collect data quickly and solve the environment faster. </p></li>
<li><p>Obviously something without the overhead of experience replay could collect more data in the same real time if the environment is simulated, but actually the stabilising effect of experience replay is more important and the effective ""wall clock"" speed of learning is faster and more reliable, using less game play experience.</p></li>
<li><p>Due to the way mini-batch processing is parallelised for neural networks on GPUs, there is not such a high overhead for increasing mini-batch size as you may think. At least up to the limits of GPU. There's still a cost for memory transfers etc per item, so it's not completely free.</p></li>
</ul>

<blockquote>
  <p>when the program starts the fit function on the replay I guess it is not sending the action to the agent, </p>
</blockquote>

<p>That is correct, the agent is updating its internal estimates of values in order to learn the optimal control actions for later actions. The mini-batch actions are not taken, just used to refine the estimates.</p>

<blockquote>
  <p>but the environment is still running.</p>
</blockquote>

<p>That is only correct in real-time systems. The Gym environments are not real-time as far as I know. They all run at the speed that the agent wants them to - they are effectively a turn-based game for the agent to play, although many of them are <em>based</em> on systems that run at a specific real-time rate. However, real-time reinforcement learning and control systems are a thing in other domains. If learning must be done online in such a system, then typically you have some choices on how to design the system:</p>

<ul>
<li><p>Have fixed time steps, and allow for maximum computation time between each one (with perhaps a margin of error).</p></li>
<li><p>Asynchronous interactions with the environment - with fixed time steps for sampling observations and taking actions - and learning continuously sampling from recent history and updating value functions and/or policy.</p></li>
<li><p>Allow for variable length time steps. I don't know much about this approach, but <a href=""https://pdfs.semanticscholar.org/0594/882ceef9312e3523dc0dbe4c162443990b87.pdf"" rel=""nofollow noreferrer"">it is an option</a>.</p></li>
</ul>

<p>Which to use depends on the problem, and a bit of guesswork.</p>
","0","2","836","25573"
"35647","<blockquote>
  <p>I thought of Experience and Replay as a mechanism to handle this delayed effect.</p>
</blockquote>

<p>Experience Replay will not help directly here, it is a mechanism to make learning more stable and effective from limited data, but it will not address delayed returns.</p>

<p>Any approach which learns Q should cope fine with delayed rewards. So Monte Carlo Control, SARSA, Q-learning, DQN and all their variants are in theory capable of learning the delayed reward. The total reward is the ultimate effect of any action.</p>

<p>However, I'm guessing just because it is not mentioned and you are asking this question, that you may still have a problem . . .</p>

<p>You claim that the action has a ""delayed effect on the environment"". You need to change that, because it implies that your state representation is incorrect for your problem. Clearly making an order for something changes the environment. An environment with an order in progress is different, in a way that is critically important to your problem, than one without an order in progress.</p>

<p>Probably what you are missing is a state representation that captures what your action has actually done. Without that, there are hidden variables (orders currently being handled) that a Q function cannot learn about because it is not in the state, $s$ for $Q(s,a)$. For RL to be reliable, the state value $s$ has to capture <em>all</em> relevant information about how future state changes and rewards will progress. For instance, if you were writing an agent to control a swinging pendulum (a standard toy problem), then you don't just need the position of the pendulum, you also want its velocity in order to predict where the pendulum might end up before any action is taken. For your issue, the current stock is like the pendulum's position, and you need to track orders ""in progress"" as they are a bit like velocity in that they will cause further changes to state regardless of action.</p>

<p>To address this, you will need to add a representation of recent stock orders to your state. I would guess something that describes the contents of the order (similar to your current stock representation) and a countdown for how many days left until it is delivered (or if they are not that reliable, something similar that correlates with likely arrival time). That state should be changed <em>immediately</em> in response to the action that caused the order, otherwise the agent will not learn the association between action and its effects, and will treat orders arriving as some kind of random effect from the environment.</p>

<p>Once you have a representation like this, then I think that is enough, and the next steps are finding the right hyper-parameter values to learn effectively. This will be easier initially if you have a simulated environment, although you could also learn off-policy from historical data using something like DQN, provided you have a <em>lot</em> of historical data to work with.</p>

<blockquote>
  <p>I thought of just adding the quantity reordered to the shelf at a later time and let the agent learn it's effects. Will this suffice?</p>
</blockquote>

<p>I think this is similar, but probably not enough. The most important thing to do is associate change to state with the action that caused it. Adding the new stock as it arrives you should do anyway, but the agent needs to learn that the earlier action is what caused this, and that is only possible if that action actually changes the state in some way.</p>
","2","2","836","25573"
"35650","<blockquote>
  <p>Is the basic idea of DQN Policy Gradient that simple, or am I getting things wrong?</p>
</blockquote>

<p>This is not correct. DQN is a value-function approach, as you thought initially.</p>

<p>Your confusion seems to stem from the two options for action representation possible in DQN. You can either estimate a single $Q(s,a)$ function by having $a$ as an input to a neural network, or can estimate all possible $Q(s, *)$ with multiple outputs, one for each $a$. However, that is an implementation detail in how you set up the neural network. It might change the efficiency of the resulting system, but it doesn't change the nature of the RL algorithm at all. </p>

<p>Policy gradient methods are based around modifying a parametric policy function $\pi(a|,s,\theta)$ and learning $\theta$. The most basic policy gradient algorithm is <a href=""https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/CliffWalk%20REINFORCE%20with%20Baseline%20Solution.ipynb"" rel=""nofollow noreferrer"">REINFORCE</a>, which requires an episodic problem, and updates on-policy after each episode. </p>

<p>Importantly, you cannot use the relationship between $Q(s,a)$ and resulting policy in Q-learning to create a policy gradient approach. The main blocker to doing this is that the effective policy $\text{argmax}_a \hat{q}(s,a,\theta)$ is not differentiable with respect to $\theta$, so it is not possible to calculate a gradient and make an update to improve the policy.</p>

<p>In general, if an algorithm learns a state value $V(s)$ or action value $Q(s,a)$ and then uses that to decide the policy, then it is value-based. If it learns the policy directly then it <em>may</em> be a policy gradient method (in RL terms it is very likely to be policy gradient, but you can also do policy search using e.g. genetic algorithms).</p>

<p>Policy gradient methods also include Actor-Critic approaches which learn both the policy and an associated value function (usually state value $V(s)$). This is a more advanced algorithm than REINFORCE, in that it can be applied to continuous (non-episodic) problems, and updates estimates on every step. One popular Actor-Critic approach is <a href=""https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"" rel=""nofollow noreferrer"">A3C</a></p>
","1","2","836","25573"
"35663","<blockquote>
  <p>as both components need to be equally weighted in order for the feature to make sense</p>
</blockquote>

<p>That is not the case.</p>

<p>For instance if $\text{sin}(\theta)$ of the cyclical feature is weighted strongly, it means that the original feature has the strongest positive effect on output at $\theta = \frac{\pi}{2}$.</p>

<p>If the two features are weighted equally, then the focus is around $\theta = \frac{\pi}{4}$</p>

<p>In general, you should expect different weights to apply to the different components, depending on which hours of the day maximise the target variable and which minimise it.</p>

<blockquote>
  <p>For a gradient boosting model, is it better to force equal weights on the two correlated features, or is it better to let the model figure it out even if it results in different weights on the two components?</p>
</blockquote>

<p>Definitely better to let the model figure it out in this case. </p>

<p>Your main concern would be whether a one-hot-encoded representation might be better than a cyclical representation for your problem. One-hot allows for arbitrary relationships to each hour, but adds more dimensions - and so may require more examples of each time. Cyclical has less dimensions, but is more likely to include non-linear effects, if for instance min and max effect on output target variable are not exactly 12 hours apart - thus it may require a more complex model.</p>
","4","2","836","25573"
"35815","<p>Both architectures work. It is probably more common to use two separate networks for simpler problems, and a combined network for a more complex problem, such as one involving machine vision.</p>

<p>In general, you need the two functions - policy and value - to be separate. There is no reason to expect them to have too much in common, in terms of the overall mapping from state to their output. However, if the state requires a lot of non-linear interpretation to get meaningful features, such as image, audio or video input, then it could be an overall benefit if the two functions share the lower-level feature representations. Not only will this encourage better generic learning of the low-level features (because they are effectively being trained with twice the data per time step compared to if they were separate), but the calculations should be faster too. </p>

<p>For a similar reason, if the agent works from natural images, you <em>could</em> use either pre-processed features from upper layers of some model trained on ImageNet, or you could start the network initialised with the first layers of such a model. This might apply for other RL agents, such as DQN too - if this works, then clearly sharing that part of the network when you have more than one function to work on could work also.</p>

<p>If your state data is simpler, e.g. a few positions and velocities, or one/both policy and value have a simple relationship to state, then a joint network may be less useful.</p>

<p>When the policy and value functions are in a shared network, they may still both have more than one dedicated layer, as it is expected that there is not any simple linear relationship between the two functions (a NN could learn this by itself, but the assumption is good enough that it may as well be hard-coded by splitting the network into two branches with maybe a couple of hidden layers each before output).</p>
","2","2","836","25573"
"35905","<p>The neural network you need to implement for Q-learning must approximate the Q function $Q(s,a)$. There are two ways to do this:</p>

<ul>
<li><p>Using $(s,a)$ as input. A lot of the literature will assume this for simplicity, and you will see notations like $\hat{q}(s,a,\theta)$ to show you are approximating the function, and have parameters $\theta$ to learn for the neural network.</p></li>
<li><p>Using $(s)$ as input, and have the network output multiple $Q(s,a)$ values - one for each possible action. E.g. if you have four possible actions, then your output might be $[\hat{q}_0, \hat{q}_1, \hat{q}_2, \hat{q}_3]$. This can be more efficient, since in order to find the best, maximising, action, you need all the action values calculated.</p></li>
</ul>

<p>Once you have a neural network set up like this, and a table of history (that grows on each action actually taken), this is how you make use of it:</p>

<pre><code>For each sampled [state, action, reward, future_state]:
  Calculate td_target = reward + gamma * max_a'(Q(future_state, a')):
    Run NN(future_state, a') forward for each possible action a' and find max
  Train the NN using the inputs (state, action) and desired output td_target
</code></pre>

<p>You would use this variant if the network output multiple Q values at once:</p>

<pre><code>For each sampled [state, action, reward, future_state]:
  Calculate td_target = reward + gamma * max_a'(Q(future_state, a')):
    Run the NN(future_state) forward and take the max output
  Construct the desired output:
    Run the NN(state) forward to get array e.g. [q0, q1, q2, q3]
    Substitute td_target depending on action e.g. [q0, q1, td_target, q3]
  Train the NN using the inputs (state) to learn the desired output
</code></pre>

<p>Although this second approach looks more complex, it avoids looping (or mini-batching) over multiple <code>a'</code> values to find a max, so can be more efficient.</p>

<p>If you are using a frozen copy of NN to help with stability (a common feature in DQN), then you should use the frozen copy exclusively to calculate TD targets, and the learning copy to calculate current outputs.</p>

<p>Note it is important that you don't store, but instead re-calculate Q values  at all times. That is both because the initial values will be incorrect (depending on how the NN was initialised), and also that they should improve over time as the agent learns a better policy.</p>

<p>Another important detail is that you should not use the NN to calculate Q value for terminal states. If <code>future_state</code> is terminal, then instead of using the NN, treat <code>max_a'(Q(future_state, a'))</code> as zero. Typically just detect that this is a terminal state and hard-code a <code>0</code> somehow for it.</p>
","2","2","836","25573"
"35952","<blockquote>
  <p>This affine operation $y = W^{\top} x + b$ has already add nonlinearity to the system given that $b \neq 0$.</p>
</blockquote>

<p>This is not considered a non-linearity in the context of data science. Different disciplines define linearity sometimes in subtly different ways. Critically, the $+b$ performs identically in terms of fitting to data, as extending $x$ with a new dimension, always $1$, and moving the values of $b$ into the weights $W$. This simpler multiplication is clearly linear.</p>

<p>Also importantly, the affine transformations form a group such that any two affine transformations combined are just another affine transformation with different parameters. Without a non-linearity in a hidden layer, a 2-layer neural network would be the same as a single layer one, and not able to learn whole classes on non-linear relations.</p>

<p>No matter how many affine transformations you apply to inputs for instance, you will not be able to approximate the XOR function, or any significant portion of $y=\text{sin}(x)$</p>
","4","2","836","25573"
"36063","<blockquote>
  <p>If yes, how can I go about this without that much RAM?</p>
</blockquote>

<p>Your value of 56GB seems correct to me, assuming you include a multiplier of 4 for the ""4 frames per state representation"" used in the DQN/Atari paper. However you should note that in the original paper, the images were converted to greyscale and downsampled to 110×84 prior to use in state representation. This made them 16 times smaller than the frames in your problem, so the whole data set would fit into 4GB.</p>

<p>If you must use larger images, you could store them on disk - maybe in a database - and load on demand. That may unfortunately create an I/O bottleneck that slows learning, although you should still be able to work with it. You can parallelise fetching from the database for mini-batches with the learning process, and this is similar to the mini-batch generators used for things like ImageNet training. You can also work on improving disk performance using optimisations such as parallel disk arrays or SSDs.</p>

<p>You could also pre-process the frames using a hidden layer embedding from a generic computer vision network trained on e.g. ImageNet, and store that representation, not the raw pixel values. This may limit self-discovery of important low-level features by the learning agent, but then again it may still be worth a shot, as lower-level image features are often very similar across different problems, and transfer learning in e.g. image classification has been quite successful using that approach.</p>

<p>More likely practical answers used by RL researchers at least up to a certain scale is one of:</p>

<ul>
<li><p>Store less states in replay memory. Base the size of replay on the memory you have available. Yes this <em>may</em> compromise the learning, but there is no special magic about the number 50,000 and if you are optimising resource use you may have to decide between how efficiently a system learns with 10,000 fast replay memory size or 50,000 slower I/O-based replay memory size.</p></li>
<li><p>Buy more memory. The big name research labs working in deep RL are funded well enough that they can afford to throw money at this problem. <a href=""https://en.wikipedia.org/wiki/AlphaGo_Zero#Hardware_cost"" rel=""nofollow noreferrer"">One estimate for how much AlphaGo Zero hardware cost is $25million</a>, so you can imagine that loading a few machines with 128GB+ RAM if they thought it necessary for any reason on any other flagship project would not be a major blocker.</p></li>
</ul>

<p>If you <a href=""https://blog.openai.com/openai-five/"" rel=""nofollow noreferrer"">look at what OpenAI are doing at the cutting edge of video game playing</a>, you can see that their hardware setup is equally monstrous. It is not clear whether they have an issue with storing experience as they use a different algorithm, or needing RAM in general, but if they did, it is also clear they could quite happily finance maximum RAM on their training rigs.</p>

<p>Do note I am not a RL researcher myself (just a hobbyist) and have not asked anyone else what they would do when faced with this problem.</p>
","1","2","836","25573"
"36162","<p>For a RBM, you run the stochastic network - forward and back from the ""input"" and hidden layers - multiple times. After a few steps it will converge into sampling from the data population it has learned from. If you have learned from a whole picture and want to fill in a patch, then hold the input values that are not in the patch (don't allow them to change randomly).</p>

<p>Other generative models may have different approaches. GANs and VAEs typically have a simpler approach where you generate a random input vector and run the generator part of network forward from that input.</p>
","1","2","836","25573"
"36176","<blockquote>
  <p>What is the proof of this? Can someone point me to a reference?</p>
</blockquote>

<p>It is called ""The Policy Gradient Theorem"", and a good reference would be <a href=""http://incompleteideas.net/book/the-book-2nd.html"" rel=""nofollow noreferrer"">Sutton &amp; Barto <em>Reinforcement Learning: An Introduction</em></a> In the second edition, the theory behind REINFORCE and Actor-Critic is examined in Chapter 13.</p>

<p>In brief, the proof is focused on showing that a sample of a particular expression based on reward seen and the parameters of the policy function, is a <em>sample of the gradient</em> of the policy functions with respect to the parameters. It thus follows that a step in the direction of this sampled value, will <em>in expectation</em> be a step in the direction that increases expected sum of discounted reward (or the average expected reward) associated with that policy.</p>

<blockquote>
  <p>What is the best convergence rate out there that's known (if any are proven)?</p>
</blockquote>

<p>This is not something that can be proven analytically, except in situations where you already know the loss function expressed in terms of the policy parameters (and nothing else that depends on them). Even for simple toy environments this is not really possible. The usual approach is to compare different algorithms experimentally, plotting learning curves. There is a lot of variance in practice, so learning curves are often plotted as averages of multiple training scenarios.</p>

<p>As far as I know, there is no single clear winner in a general sense between different policy gradient methods. However, Actor-Critic and similar methods that combine TD learning with policy gradients are usually preferred over the basic approach of REINFORCE, as the agents that estimate a value function allow ""bootstrapped"" updates to be made on every step. Although this adds initial bias (as value function estimator parameters initially have no relation to the true value function), it reduces variance and allows for more updates, thus often faster convergence. </p>

<blockquote>
  <p>Is there any formulation that works for the mean reward of a rollout?</p>
</blockquote>

<p>The base theory of policy gradients applies to episodic, continuous discounted, and continuous average reward formulations with minor changes in the proof.</p>

<p>The algorithm REINFORCE cannot be applied to a continuous environment, as updates are made only at the end of episodes. Actor-Critic approaches <em>can</em> be adapted to average reward formulation - <a href=""http://busoniu.net/files/papers/ivo_smcc12_survey.pdf"" rel=""nofollow noreferrer"">for examples see this survey</a>.</p>
","2","2","836","25573"
"36589","<p>Yes this is usually part of the early stopping algorithm, where you supply a cross-validation data set, and a limit on number of epochs since best result so far.</p>

<p>In Keras, you can use an instance of the <code>EarlyStopping</code> class, choosing the metric that you want the best model for, and setting the <code>patience</code> parameter to limit the number of epochs to test after any best so far result. The instance is supplied to the <code>fit</code> method as a callback.</p>

<p>See <a href=""http://parneetk.github.io/blog/neural-networks-in-keras/"" rel=""nofollow noreferrer"">http://parneetk.github.io/blog/neural-networks-in-keras/</a> for an example (last section)</p>
","1","2","836","25573"
"36748","<blockquote>
  <p>I am curious about what would happen to hyperparameters when they would be set by a neural network itself</p>
</blockquote>

<p>In general this is not possible as many hyper-parameters are discrete, so they are not differentiable with respect to any objective. For example, this applies to layer sizes, number of layers, choices of transfer functions. This prevents using any form of gradient descent to tune them directly as learnable parameters.</p>

<p>In fact the separation between parameters and hyperparameters is exactly that hyperparameters are not learnable by the model type. This applies to other ML models, not just neural networks.</p>

<blockquote>
  <p>or by creating a neural network that encapsulates and influences the hyperparameters of the network it encapsulates.</p>
</blockquote>

<p>This is more feasible. You could use one neural network to try and predict the results from another. Then prefer to run tests on target networks that look like they will do well. However, using a ""meta"" neural network like this has some major drawbacks:</p>

<ul>
<li><p>Neural networks require a lot of training data. Getting enough samples to make good predictions would require that you train your primary neural network (a time-consuming process) many times</p></li>
<li><p>Neural networks are bad at extrapolating to data outside of areas already experienced, so not so great at making creative predictions of new parameters to try</p></li>
<li><p>Neural networks have a lot of hyper-parameters to tune. Would you need a ""meta meta"" neural network to predict the performance of your ""meta"" network?</p></li>
</ul>

<blockquote>
  <p>either it has never been done before or the idea is just really dumb</p>
</blockquote>

<p>This is a real issue that comes up repeatedly. In general the search for best hyper-parameters is a chore. It is an active area of research and experimentation to find efficient ways of automating it, or avoiding it by making some hyperparameters less important or not necessary.</p>

<p>The reason you are not finding neural networks that tune neural networks is due to the issues listed above. So the main areas of research focus on different approaches, that can work with limited data and don't have so many hyperparameters themselves. Or models that are robust to large ranges of hyperparameters, so precise tuning is not a big deal.</p>

<p>Here are a few pointers to help with automated searches:</p>

<ul>
<li><p>You could use <a href=""https://en.wikipedia.org/wiki/Hyperparameter_optimization"" rel=""nofollow noreferrer"">a variety of hyperparameter optimisation schemes</a>, including random search, grid search, genetic algorithms, simple gradient methods etc.</p></li>
<li><p>Random searches, perhaps constrained by previous experience or second-hand knowledge from similar problems, can be reasonably effective.</p></li>
<li><p>The quality of any search is limited by the quality and amount of cross-validation data. There is not much point tuning the cv loss value to the point that you care about changes that are much less than the standard error in its estimate.</p></li>
<li><p>Response to hyperparameters is typically non-linear over the search space, which makes things harder.</p></li>
</ul>

<p>Outside of automation, expert analysis is often a good starting point, especially if you want to assess success of regularisation. Typically you can look at learning curves for training and cross-validation data, and based on that you can make a reasonable guess as to whether to increase or reduce regularisation hyperparameters and/or learning rate, even from observing results from a single training run. </p>

<p>There have likely been attempts to automate some parts of reading learning curves, since sometimes it is relatively easy to detect over-fitting and under-fitting scenarios. However, I could not find any examples when searching just now.</p>
","3","2","836","25573"
"36916","<blockquote>
  <p>Isn't the third advantage just another case of breaking correlation?</p>
</blockquote>

<p>It could be viewed that way, but it is a different kind of correlation.</p>

<p>The <em>second</em> advantage is about breaking correlation due to samples being from adjacent time steps on the same trajectory. This is a more important problem when state vectors evolve slowly/incrementally per time step. </p>

<p>The <em>third</em> advantage is about breaking correlation due to samples being taken from the same policy. This is a more important problem when some actions have very similar reward and state progression independently of state, or may result in no change to state (e.g. an agent tries to move into a wall - agents getting stuck in corners due to runaway feedback of the direction being ""best"" is a thing you can sometimes observe whilst DQN is learning).</p>

<p>Both can also be a factor when an early event puts the remainder of a trajectory into a single part of the overall space, which can occur in environments where state history is important - in those cases, almost <em>all</em> of the time steps in a single episode could be correlated. Think of a resource management game, where expending or keeping a key resource early on in the game has a large influence on eventual success at a task. This is impacted by elements of second and third advantages - i.e. the states in the trajectory are correlated due to state of the resource throughout, and across multiple episodes the current policy might prefer expending or keeping that resource at a specific stage.</p>

<p>The correlations avoided by the second and third advantages typically occur over different scales of time, although that does depend strongly on the specific problem. The second advantage might be gained with a relatively small replay memory (depending on how the state evolves and lengths of episodes). The third advantage will usually require a larger memory, in order that it captures episodes with varying policies. In addition the replay memory should include exploratory actions which also help with this third issue.</p>
","3","2","836","25573"
"37071","<blockquote>
  <p>I'm not aware about for which condition I have to give the rewards. Is my reward logic is correct or I have to change it?</p>
</blockquote>

<p>The reward structure in reinforcement learning is a little bit flexible, <em>provided</em> it expresses the goal of your agent.</p>

<p>In your case, you want to avoid two bad scenarios in a <em>continuous</em> problem. There is also a good scenario - providing enough water - which overlaps with the negative consequences of not providing water on demand, but may be subtly different.</p>

<p>Your suggestion of granting a negative reward for spilled water, and a negative reward for unavailable water seems quite reasonable. You have a few choices within that that could make differences to the behaviour of an optimal agent. Given your problem description so far, I think these are a free choice for you:</p>

<ul>
<li><p>You can decide on a fixed cost for an error e.g. $-1$ reward, or base the cost on the amount of water spilled or missing when required.</p></li>
<li><p>You can weight rewards for water spilled differently to water missing when required. This is the sort of thing that might be a business decision, based on different costs for the different kind of errors. If spillage costs are low, demand is high and impact of failed distribution is high, then the optimal policy may involve over-filling the tank and spilling water deliberately.</p></li>
<li><p>You can have a positive reward for the amount of water supplied. This is similar to having a negative reward for failed supply, but again might be weighted differently in a business scenario. If you are supplying water to paying clients, then you would care about profits here and have 3 types of reward covering the profits from selling water, the loss from wasted water and clean-up costs, and (the maybe more nebulous, but still estimated) loss from failing to meet customer demand when supply is not available.</p></li>
</ul>

<p>You should bear in mind, that combined with the behaviour of the environment, these decisions may <em>change</em> what the optimal policy is. Until you define these values, there is no single optimal policy. As your problem is simple, there may be a lot of overlap between optimal policies, but choices you make in reward functions and how the environment behaves will result in differences perhaps in how willing the agent is to risk over-filling in some cases.</p>

<p>As the problem is continuous, the long-term reward that you want to optimise could become infinite. To avoid this, and make the problem tractable, you need to make one of the following choices:</p>

<ul>
<li><p>Use a <em>discount</em> factor, $\gamma$, which can be from $0$ to $1$ (but not actually 1 for a continuous problem)</p></li>
<li><p>Formulate your goal as maximising <em>average reward</em>.</p></li>
</ul>

<p>In your case I suggest using a highish discount factor, e.g. $\gamma = 0.99$, as most of the RL literature for value-based agents uses discount factors.</p>

<p>Finally, you may have an issue here:</p>

<blockquote>
  <p>The distribution of removal of water is not fixed. we can remove any amount of water from bottle, i.e. any continuous value between [0, 5]</p>
</blockquote>

<p>RL depends on the environment behaving as a Markov Decision Process. Which means the theory works best if there is a fixed distribution here. That could be a flat distribution in $[0,5]$, or it could be drawn from a complex real-world scenario, but the assumption from the theory is that the distribution of changes to state is a function of most recent state and action plus a stochastic element.</p>

<p>When you simulate your problem in order to train the agent, you will have to use an <em>actual</em> distribution. The agent will then optimise against the scenarios it experiences. Depending on what you simulate, it may do poorly in other situations. However, provided you don't take this to extremes, the agent should learn to optimise over a broad range of states, and many reasonable distributions of the removal of water.</p>
","0","2","836","25573"
"37089","<p>I can see two issues:</p>

<ol>
<li><p>Your environment is not tracking changes to state, just random success/fail based on <code>self.initBlevel</code> which is never modified to reflect changes. Although you calculate and return the new state (as variable <code>UpdatedWaterLevel_</code>), this is not fed back into the system. You store it as the next ""state"" in the DQN replay table, but don't actually store it in the environment as the current state. You should do that - without it the replay table will be filled with incorrect values. There should be a variable that the environment has access to which represents the current state.</p></li>
<li><p>You are running the system as an episodic problem, but do not re-set the environment for the start of a new episode. This is a ""hidden"" bug due to the issue above, but would immediately become a problem for you if you let the state go outside the bounds of the problem you have defined.</p></li>
</ol>

<p>Given the problem setup, I would expect the agent to learn to always fill the container to the maximum possible capacity (and it would then get drained by the amount of the random request). That would lead to infinitely long episodes, so you do still need discounting.</p>

<p>Possibly your NN is over-complex for this simple task, which could make learning slower. But that's harder to tell. The relationship to expected future discounted reward based on current state and action is complex, so you might need a moderate size of network to capture that.</p>
","1","2","836","25573"
"37343","<p>It is difficult to say without access to the original author. However, I expect this refers to the ability of using each set to realise its <em>purpose</em>.</p>

<p>A validation set's purpose is to select hyperparameters that perform the best according to some metric. The best measurement on the validation set should always have the highest expectation of being the best in reality. If you make very many measurements, then the absolute probability of the best measurement being the real best could be low, but the chances of a generally poorly performing set of hyperparameters winning overall do not increase as fast. You can be reasonably certain that you have picked ""one of the best"" plus ""the one with highest probability of being the best"" even though that might be e.g. just a 10% chance if you have run 100s of validations.</p>

<p>A test set's purpose is to measure a metric without bias. If you use this for model comparison or selection, then this can be affected by maximisation bias - because there is uncertainty in the measurement, focusing on the relative values and picking a ""best"" almost certainly over-estimates the true value. This effect happens very quickly. If you measure metrics for two sets of hyperparameters and pick the best one, you should already expect that the value you got for the metric is an over-estimate. Note you still expect on average that you have picked the better option, but you cannot trust the measurement as much.</p>
","3","2","836","25573"
"37415","<p>Validation loss is the same <em>metric</em> as training loss, but it is <em>not</em> used to update the weights. It is calculated in the same way - by running the network forward over inputs $\mathbf{x}_i$ and comparing the network outputs $\mathbf{\hat{y}}_i$ with the ground truth values $\mathbf{y}_i$ using a loss function e.g. $J = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}(\mathbf{\hat{y}}_i, \mathbf{y}_i)$ where $\mathcal{L}$ is the individual loss function based somehow on the difference between predicted value and target.</p>

<blockquote>
  <p>is validation loss used in updating weights?</p>
</blockquote>

<p>No. In fact that would be counter to the purpose of it.</p>

<blockquote>
  <p>Or is it simply a measurement of how far off your observations were at the current epoch?</p>
</blockquote>

<p>Yes. The point of using a data set that you did not train on, is to measure how well your model is <em>generalising</em> to unseen records. Very often when building a predictive model, that is the main goal, and is actually more important than fitting to your training data (the fitting to data part is necessary for learning, but is not the goal). This is the case whenever you are building a model to use to make decisions on its predictions against new, previously unseen and unlabelled, data.</p>

<p>It is not safe to use the training data in the same role to check for generalisation. It is possible for many model types to learn the training data perfectly, but be terrible at predicting from new values that are not from the training set. This is something you will want to avoid, is often caused by overfitting, and neural networks will often overfit. Using a validation set is a way to monitor and help control against overfitting. </p>

<p>Neural networks (and other model types) typically use a validation set on every epoch, because training too long can cause over-fitting, and models don't recover from that, they just get worse form that point on. So it can save a lot of wasted effort to monitor validation loss, and stop training when it has not improved for a long while, or starts to get worse.</p>
","8","2","836","25573"
"37439","<p>The main requirement of on-policy policy gradient methods is that they use a parametric policy $\pi(a|s, \theta)$ that is differentiable with respect to the parameters $\theta$. </p>

<p>This is not restricted to only describing discrete probability distribution functions (e.g. softmax output layer of neural network). A description of any probability distribution function that is differentiable and possible to sample from is all that is required. This is true for the Normal distribution for instance, so one relatively common solution in continuous spaces is for a neural network to output the mean and standard deviation for the distribution of each component of the action vector that accepts continuous values. </p>

<p>Typically the neural network does not perform the sampling to choose an action. This is also true for a softmax output - it is only additional code, outside of the NN, that interprets the values and selects the action. In addition, and unlike softmax, the NN does not need to <em>directly</em> represent the probability distribution function, just enough data to drive the sampling process. However, the nature of the distribution function <em>does</em> need to be taken into account when calculating the gradient in policy gradient methods.</p>

<blockquote>
  <p>Thus, it's not obvious how can my action be something continuous like: ""turn +19.2345 angles clockwise"". Such an action must have already been pre-defined to the ""19.2345"" value, right?</p>
</blockquote>

<p>What the policy might output here, is the two parameters of the distribution $\mathcal{N}(\mu, \sigma)$, which you then must sample to get an action like ""turn x degrees clockwise"". So for example, the neural network could output $(25, 7)$ and then additional code will interpret those values as describing the distribution and take a sample. If you got a mean 25, standard deviation 7, then at the point you select the action you could get ""turn +19.2345 degrees clockwise"" amongst a range of other values. The value 19.2345 does not need to be pre-defined or represented in the neural network in order to do that.</p>
","6","2","836","25573"
"37521","<blockquote>
  <p>In Q-learning, how to tell the agent that action $a_7$  is unavailable from within state $s_{t}$? </p>
</blockquote>

<p>Partially, it depends on what you mean by ""unavailable"". If the environment is such that it is possible to take the action, but that the consequences are very bad, then a negative reward due to the consequences is probably the best option. You might do this for a real-time system where the action can actually be taken, cannot be automatically blocked in any way, and has unwanted consequences.</p>

<p>Perhaps an action is unavailable, because it is not possible to even attempt it in the supplied environment. This could occur in a board game where certain moves are not allowed by the rules of the game. There you have a few options, depending on how you have constructed the agent.</p>

<p>A simple approach is to only present the agent with actions that it is allowed to take in the first place. The code for the environment should already know this list, so if you were considering penalising the choice, then you can also choose not to present it. This works well for value-based methods such as Q-learning, when you have implemented $Q(s,a)$. If the set of allowed actions is $\mathcal{A}(s)$, then the greedy action choice from state $s_t$ is $a_t = \text{argmax}_{a \in \mathcal{A}(s_t)} Q(s_t, a)$. All you need to do is implement $\mathcal{A}(s)$ in code and loop over different calls to Q(s,a) to find the best value.</p>

<p>If you have implemented the Q function in parallel over the whole action space (a good choice for efficiency with neural networks), then the network takes only the state $s$ as input, and outputs a vector e.g. $[Q(s, a_0), Q(s, a_1), Q(s, a_2), Q(s, a_4), Q(s, a_5) ... Q(s, a_N)]$. In that case you would use your implementation of $\mathcal{A}(s)$ to mask out unwanted actions before choosing the best action. It is a little bit wasteful to calculate the non-required actions, but if it was a rare enough exception, then probably still more efficient overall.</p>

<p>Beyond Q-learning, you could also use a similar action mask when you had a policy network that output softmax probabilities of actions. In that case you would also want to re-normalise the probabilities before generating the selected action.</p>

<p>Unless you have a specific goal or benefit in mind for the agent to learn about actions that are blocked by the environment, then it is simplest to query the environment to support the blocking, as described above. This is definitely the case for game-playing bots covering moves which are illegal according to the game rules. The justification is that the agent is learning to <em>play</em> the game, and the goal of it learning what the rules are for the game is a less interesting distraction.</p>

<p>Alternatively, it <em>should</em> work to allow the incorrect action selection, then give a negative reward (can be a small one, in same scale as other rewards in the system). Note this should also increment the time step and return the unaltered state. An agent will eventually learn not to take such a pointless, barred action. Maybe sometimes you are interested in whether the agent can learn the rules.</p>
","3","2","836","25573"
"37597","<p>Yes, but in general it is not a good tool for the task, unless there is significant feedback between predictions and ongoing behaviour of the system. </p>

<p>To construct a reinforcement learning (RL) problem where it is worth using an RL prediction or control algorithm, then you need to identify some components:</p>

<ul>
<li><p>An <em>environment</em> that be in one of many <em>states</em> that can be measured/observed in a sequence.</p></li>
<li><p>An <em>agent</em> that can observe current <em>state</em> and take <em>actions</em> in the same sequence.</p></li>
<li><p>The evolution of <em>state</em> in the sequence should depend on some combination of the current <em>state</em> and the <em>action</em> taken, and may also be stochastic.</p></li>
<li><p>There should be a <em>reward</em> signal that the RL agent can observe or  measure. The value of <em>reward</em> should depend on the same factors as the evolution of the state, but can depend on them in a different way.</p></li>
</ul>

<p>The general case of time series forecasting can be made to fit with this by treating the prediction as the action, having the state evolution depend on only the current state (plus randomness) and the reward based on state and action. This will allow RL to be applied, but causality only flows one way - from the environment into your predictive model. As such, the best you can do for rewards for instance is to use some metric about the correctness of the predictions. Consequences for good or bad predictions do not affect the original environment. Essentially you will end up wrapping some predictive model for the sequence (such as a neural network) in a RL layer which could easily be replaced by basic data set handling for a supervised learning problem.</p>

<p>One way you <em>could</em> meaningfully extend series forecasting problems into RL problems is to increase the scope of the environment to include the decisions made based on the predictions, and the state of the systems that are affected by those decisions. For instance, if you are predicting stock prices, then include your portfolio and funds in the state. Likewise the actions stop being the predictions, becoming buy and sell commands. This will not improve the price prediction component (and you are likely better off treating that as a separate problem, using more appropriate tools - e.g. LSTM), but it will frame the issue overall as a RL problem.</p>
","17","2","836","25573"
"37603","<blockquote>
  <p>it is possible that performing an action $a$ that takes us to state $s′$, could result in multiple rewards?</p>
</blockquote>

<p>Yes, that is true the general case that any $(s,a)$ pair can result in a range of results for both $s'$ and $r$. Also $s'$ and $r$ can vary independently, provided each ones distribution only depends on $(s,a)$. In practice $r$ often depends strongly on one or more of $s$, $a$ or $s'$ (if it depends on the latter, then it still depends in absolute terms on just $s$ and $a$ because $s'$ does - it is just that values of $s'$ and $r$ are allowed to correlate).</p>

<p>This does not affect the statement about $p(s',r|s,a)$ being deterministic. It is the <em>probability</em> of specific $s',r$ results occurring that should be deterministic and depend on $(s,a)$ in a Markov Decision Process.</p>
","1","2","836","25573"
"37657","<blockquote>
  <p>Is the neural network in DQN used to learn like a supervised model? </p>
</blockquote>

<p>Yes. </p>

<p>In DQN, the neural network is used as a function approximator to learn the action value function $Q(s,a)$ - the neural network approximation to it is sometimes noted $\hat{q}(s,a,\theta)$ to show that this is an ongoing, approximate estimate, and that the values that it outputs depend on the NN's learnable parameters (weights and biases).</p>

<p>Why is this ""supervised learning""? That is because in order to improve an estimate for Q, the network is presented with labelled data. In DQN this is specifically a mini-batch drawn from experience replay memory, but in all cases in RL, the format of each row of data is the same: A representation of the current state (or state+action sometimes), and a <em>""ground truth"" target value</em> to learn to associate with that input. This learning by correct example is exactly supervised learning.</p>

<p>Now, there are some complications: </p>

<ul>
<li><p>The ""ground truth"" is built by the RL algorithm on-demand, and equals the underlying real value $q_{\pi}(s,a)$ only in expectation over time. However, that is fine for any statistics-based learning algorithm that will learn a mean value.</p></li>
<li><p>The expected ""ground truth"" changes over time as RL improves the policy, so higher Q values will be expected, and the old lower ones need to be forgotten. This is why RL algorithms build the ""ground truth"" on demand - for single-step Q-learning, as used in basic DQN that is the $r + \gamma \text{max}_{a'}\hat{q}(s', a')$ formula that you may see in explanations, and is called the <strong>TD Target</strong>. That means that RL algorithms prefer to work with online algorithms that are biased towards most-recent data and forget older data. Neural networks and linear regression trained by gradient descent both fit this requirement.</p></li>
</ul>

<p>An aside: In RL, to avoid using the term <em>model</em> twice, the term ""function approximator"" is used, where supervised learning might call the same thing a ""model"". That is because RL already uses the term <em>model</em> to refer to models of the environment (e.g. the rules of a game, or all the known transition probabilities and rewards). </p>
","0","2","836","25573"
"37665","<p>You would need to train the RF-QN or SVR-QN on a very large batch/sample generated in the same way as a mini-batch in the DQN version. The input data would be the states and actions visited whilst simulating or running the environment for the batch, and the label would be the <strong>TD Target</strong> $r + \gamma \text{max}_{a'}\hat{q}(s', a')$ using an <em>old</em> copy of the RF or SVR model to calculate $\hat{q}$ with a loss function of MSE.</p>

<p>The reason you would need a very large batch (and why you generally don't see this done, because it will be far slower than NN) is because neither RF nor SVR in their basic form can be made to run online. However, if you manage to find online versions of the algorithm (I know there are some for RF), then you can use them almost identically to a DNN. </p>

<p>Wherever you see $\hat{q}$ in the pseudo-code, you know you need to run the model forward to estimate Q values. Typically you need to do this to make action selections (by finding best Q value for possible actions) and to generate TD Targets for further training.</p>

<p>Here's how you would need to change a DQN implementation in general terms:</p>

<ul>
<li><p>You need two copies of your SVR or RF model: a ""target model"" and a ""learning model"". You would start with a basic target model that can predict either random values or fixed values for $Q(s,a)$ - those will clearly be wrong, but it should not matter. Each model will train using output from the previous one, plus the real data about transitions and reward. On each training session, the model becomes more accurate, and allows for better selection of actions.</p></li>
<li><p>Wherever you see a call to predict or train the DNN, replace with same call to predict or train the SVR or RF. When you see the current network being cloned to the ""target network"" do the same with the SVR or RF model. Pay attention to when the code uses the learning network or the target network, and use the appropriate SVR or RF model. </p></li>
<li><p>If you are using the default full batch training of RF or SVR, then you will need to generate a large dataset to train at once, and you should clone the resulting newly trained SVR or RF model direct to the target model immediately. Also, as the learning model in SVR or RF does not work online, you will need to use the target network to select actions in your case - that should be fine, although it may slow down learning further. Depending on your RL problem, without online versions of the algorithms, your dataset could easily need to be 100,000 or more records per training and cloning step.</p></li>
</ul>
","0","2","836","25573"
"37679","<blockquote>
  <p>In my understanding, $V(s)$ is always larger than $Q(s,a)$, because the function $V$ includes the reward for the current state $s$, unlike $Q$</p>
</blockquote>

<p>This is incorrect. There is not really such a thing as ""the reward for current state"" in the general case of a MDP. If you mean the $V(S_t)$ should include the value of $R_t$, then this is still wrong, <em>given David Silver's use of the conventions for time step indexing</em>. It is possible to associate immediate reward with either the current time step, leading to sequence $S_0, A_0, R_0, S_1, A_1, R_1$ etc <em>or</em> you can use the convention of immediate reward being on next time step $S_0, A_0, R_1, S_1, A_1, R_2$ etc. David Silver (and Sutton &amp; Barto's book) uses the latter convention.</p>

<p>Under that convention:</p>

<p>$$V(s) = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} 
\gamma^{k}R_{t+k+1}|S_t=s]$$</p>

<p>$$Q(s,a) = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^{k}R_{t+k+1}|S_t=s, A_t=a]$$</p>

<p>You can see that the first term in the expansion of the sum for <em>both</em> Q(s,a) and V(s) is $R_{t+1}$. If you changed the convention, then both would include the equivalent value, but would be <em>labelled</em> $R_{t}$ in any formula.</p>

<p>Q and V do not differ in which time steps they sum reward over. They <em>may</em> differ in the value of $R_{t+1}$ because $V(s)$ assumes following the policy $\pi$ when selecting $A_t$ whilst $Q(s,a)$ uses the value $a$ supplied as a parameter for $A_t$, which can be different. </p>

<blockquote>
  <p>how can we be certain what to subtract from what, such that our Advantage is always positive?</p>
</blockquote>

<p>Advantage can be negative, that is fine. It means that the action $a$ in $A(s,a)$ is a worse choice than the current policy's.</p>

<blockquote>
  <p>I don't see why a value of $(Q−V)$ would be useful. On the other hand, $(V−Q)$ would be useful </p>
</blockquote>

<p>Both would be equally useful, it is mainly convention that we work with finding maximum Advantage representing the benefit of choosing a specific action instead of following the current policy, as opposed to finding the minimum ""Disadvantage"". However, the concept of Advantage in this context is arguably the more natural view.</p>

<blockquote>
  <p>because it would tell us the reward we would get on $s_{t+1}$ if we took the action $a$</p>
</blockquote>

<p>As explained above, this is wrong. The value in $A(s,a)$ expresses the potential benefit we would get for changing the policy $\pi(s)$. That might include changes to $R_{t+1}$, but is not limited to a single time step.</p>

<p>Some RL approaches do create a predictive function for expected immediate reward $\hat{r}(s,a)$ - typically this is a secondary component, used to help refine parameters for other function approximators.</p>
","11","2","836","25573"
"37747","<p>The current standard is essentially:</p>

<blockquote>
  <p>Given this input data, can any other system or approach classify it or estimate a quantity of interest? If so, then a machine learning approach may be able to achieve the same.</p>
</blockquote>

<p>This is basically how machine learning challenges in computer perception can be treated as tractable. We have humans and other animals as working models, and make the assumption that the process can be automated. A similar approach can be made on any machine learning system which attempts to re-create the behaviour of an expert - provided we use the exact same input data, and enough of it, the ML system can learn what the expert does through statistical approximation.</p>

<p>The ""expert"" can be a statistician/data scientist looking at the data, using any tool. Exploratory plots of features and measures of correlation are a good way to assess whether a data set might be amenable to training a ML model for prediction. If you can visually separate classes on a scatter plot using some combination of features, then it is likely that a suitable ML model will be able to separate those classes too.</p>

<p>There are hard cases, where it seems on the surface like there is no pattern. Perhaps a relationship could be teased out and shown to exist with statistical analysis, but you could eschew that and directly throw some non-linear ML model at the problem in the hope that it finds it for you with the correct hyper-parameters. Of course you don't know in advance whether that is a worthwhile approach, and this carries some risks. But it is not that expensive to do once you have some data - just throw a fairly robust non-linear model at the problem, like XGBoost, and see what happens.</p>

<p>Of course, ML is not magic. If there is nothing to find, it will tend to find nothing. Worse than that, it can find spurious correlations, or patterns due to prejudice inherent in the data collection or labelling. Those issues are a problem regardless of evidence on whether it was theoretically possible to achieve a result at all. However, the kind of thinking that drives ""let's throw some neural networks at this"" has led to some published works which are quite terrifying and wrong on many levels. An example of such a system was <a href=""https://www.newscientist.com/article/2114900-concerns-as-face-recognition-tech-used-to-identify-criminals/"" rel=""nofollow noreferrer"">a NN which classified a person as criminal or not according to a picture of their face</a> - luckily flaws were pointed out in data collection on that one, but the original story made headline news in many places, despite essentially being a modern re-birth of Phrenology.</p>
","2","2","836","25573"
"37794","<blockquote>
  <p>How should I interpret this? If a lower loss means more accurate predictions of value, naively I would have expected the agent to take more high-reward actions.</p>
</blockquote>

<p>A lower loss means more accurate predictions of value <strong>for the current policy</strong> (technically it is more complicated for Q-learning off-policy estimates, but the covergence will still be limited by experience reachable in the current policy). Unfortunately a loss metric in RL cannot capture how good that policy is.</p>

<p>So what it means is that your policy has settled into a pattern where values can be estimated well by the neural network that you are using for Q. For some reason it is not finding improvements to that policy - typically it should be doing that <em>before</em> the loss metric drops, as each improvement in value estimates should reveal better possible actions, and once those start being taken by a new policy, the value estimates become out of date, and the loss increases again.</p>

<blockquote>
  <p>Could this be a sign of the agent not having explored enough, of being stuck in a local minimum?</p>
</blockquote>

<p>Exploration could be an issue. The ""local minimum"" in that case is probably not an issue with the neural network, but that small variations in policy are all worse than the current policy. As you are learning off-policy, then increasing the exploration rate may help find the better states, at the expense of slower overall learning. Also, methods that explore more widely than randomly on each action could be better - e.g. action selection methods that consistently pick unexplored state/action pairs such as Upper Confidence Bound.</p>

<p>Also a possibility is that the structure of your network generalises well under the current policy, but is not able to cover better policies. In that case, whenever the exploration suggests a better policy, the network will also increase estimates of unrelated action choices - so it would try them, notice they are better, then back off as the new values also cause unwanted policy changes in other situations. </p>

<p>If you know a better policy than the one that is being found, then you could plot a learning curve with the policy fixed, see if the network can learn it. However, usually you will not know this, so you may be stuck with trying some variations of neural network architecture or other hyperparameters.</p>

<p>There are other methods than DQN (e.g. A3C, DDPG), as well as many add-ons and adjustments to DQN that you could try (e.g. eligibility traces, double learning). </p>
","7","2","836","25573"
"37821","<blockquote>
  <p>Is backpropagation just fancy term for weights being optimized on every iteration?</p>
</blockquote>

<p>Almost. Backpropagation is a fancy term for using the chain rule. </p>

<p>It becomes more useful to think of it as a separate thing when you have multiple layers, as unlike your example where you apply the chain rule once, you do need to apply it multiple times, and it is most convenient to apply it layer-by-layer in reverse order to the feed forward steps.</p>

<p>For instance, if you have two layers, $l$ and $l-1$ with weight matrix $W^{(l)}$ linking them, <em>non-activated sum</em> for a neuron in each layer $z_i^{(l)}$ and activation function $f()$, then you can link the gradients at the sums (often called <em>logits</em> as they may be passed to  logistic activation function) between layers with a general equation:</p>

<p>$$ \frac{\partial L}{\partial z^{(l-1)}_j} = f'(z^{(l-1)}_j) \sum_{i=1}^{N^{(l)}} W_{ij}^{(l)} \frac{\partial L}{\partial z^{(l)}_i}$$</p>

<p>This is just two steps of the chain rule applied to generic equations of the feed-forward network. It does not provide the gradients of the weights, which is what you eventually need - there is a separate step for that - but it does link together layers, and is a necessary step to eventually obtain the weights. This equation can be turned into an algorithm that progressively works back through layers - that is back propagation.</p>

<blockquote>
  <p>To be more precise, what's the point of automatic differentiation when we could simply plug in variables and calculate gradient on every step, correct?</p>
</blockquote>

<p>That is exactly what automatic differentiation is doing. Essentially ""automatic differentiation"" = ""the chain rule"", applied to function labels in a directed graph of functions.</p>
","4","2","836","25573"
"37890","<p>The equations are not showing quite what you think. For the equations you have copied, when you see $\mathbf{x}^{(i)}$, you should not think of it as </p>

<blockquote>
  <p>The i<sup>th</sup> feature of an example record $\mathbf{x}$ - <em>[INCORRECT]</em></p>
</blockquote>

<p>Instead it is </p>

<blockquote>
  <p>The i<sup>th</sup> example record from the training dataset - <em>[CORRECT]</em></p>
</blockquote>

<p>So, $\mathbf{x}^{(i)}$ in this case is actually a vector including all features of a single example, and the equations show vector arithmetic. The parameter $\theta$ is also a vector. </p>

<p>The individual feature indexing is shown with a subscript $j$ i.e. $x^{(i)}_j$ is the j<sup>th</sup> feature of the i<sup>th</sup> record. The equations are also using bold $\mathbf{x}$ for a vector quantity and non-bold $x$ for a scalar quantity to try and make it clearer.</p>

<p>Take care, when you have multiple dimensions of data to work through, different document sources will use different conventions and indexing schemes. After a while, when you know what to expect is going on, you will spot the differences quickly and figure out which convention is in use. This is only a minor problem, and can crop up if you switch learning sources - e.g. watch a video lecture by one researcher then read a book by another.</p>

<blockquote>
  <p>Lets suppose we have n sized input data (x1,x2,...,xn) , each input data has m attributes. </p>
</blockquote>

<p>There are $m$ examples in the dataset. The data is $(\mathbf{x}^{(1)}, \mathbf{x}^{(2)},...,\mathbf{x}^{(m)})$</p>

<p>There are <em>not</em> $m$ components of each vector. It is not stated how many components there are in the equations.</p>

<blockquote>
  <p>In the cost fucntion we basically have an xi from (x1,x2,...,xn), but which one? We just take one at random? And that remains standard throughout the algorithm? </p>
</blockquote>

<p>Both equations sum over the whole dataset to calculate cost function or its gradient.</p>
","1","2","836","25573"
"38038","<blockquote>
  <p>Has there been put any thought into it?</p>
</blockquote>

<p>Probably not at a formal research level, it is basically a dead-end, as you have pointed out there will essentially be one correct solution amongst a large number of incorrect ones. No useful heuristic exists to assess partial solutions, so logically RL will not work, as an agent needs to experience (or simulate accurately) success and failure in order to learn correctly. This is a hard limit to all learning algorithms, they can only learn from available data - any combinatorial problem with one special permutation, and zero clues that disambiguate it from a large number of similar permutations is inherently unlearnable.</p>

<p>Only approaches that would work against a generic hash algorithm are brute-force searches that try every possible combination of operations suspected of being in the algorithm, up to a certain size of program. For any real-world <em>secure</em> hashing algorithm, this would be a huge search space, and intractable. For smaller insecure hashing algorithms, it is more feasible, but there would likely be no value to even demonstrating it was possible.</p>

<p>Related to this (when trying to reverse a hash or find collisions, <em>not</em> figure out the algorithm from examples), cryptographic hashes, such as MD5, SHA-1, SHA-256 etc, are <em>designed</em> such that brute-forcing them is the only way to solve them. In the case of MD5 and SHA-1, that has since been broken mathematically, and they are weaker then brute-forcing, but this was done starting with knowledge of the algorithm. Current RL agents and neural networks cannot match a human in learning and understanding abstract mathematics, so that approach is ruled out at least for now.</p>
","1","2","836","25573"
"38067","<p>All <em>Reinforcement Learning</em> (RL) control algorithms have to deal with the <a href=""http://tomstafford.staff.shef.ac.uk/?p=48"" rel=""nofollow noreferrer"">exploration/exploitation trade-off</a>. Do you trust that you have learned enough about the environment that you can act optimally, or do you still need to check alternative actions.</p>

<p>Related to this, there are two basic approaches when trying to learn an optimal policy in RL. An agent can either learn:</p>

<ul>
<li><p><strong>on-policy</strong> where the agent assesses the actions that it is taking, and adjusts behaviour directly. To solve the exploration/exploration trade-off with on-policy learning, a common approach is to decay $\epsilon$ (or a similar parameter which controls the amount of exploration), based on some idea of how much learning is necessary to learn the task.</p></li>
<li><p><strong>off-policy</strong> where the agent has separate <em>behaviour</em> and <em>target</em> policies. The agent takes actions from an exploring behaviour policy, and adjusts the results it gets mathematically so that it calculates a different target policy, which is often optimal in some way.</p></li>
</ul>

<p>Q learning is an off-policy algorithm, which learns the action value function of following deterministic greedy action choices at all times. In the simplest single-step version, the adjustment required is really simple - instead of estimating the action value based on what the behaviour policy does, Q learning updates using whatever the best next action would be, regardless of what gets chosen.</p>

<p>Q learning is proven to be stable and to converge when using a table of q values (i.e. not a neural network). However it is actually not very stable when used with neural networks for estimating $\hat{q}(s,a, \theta)$ (where $\theta$ is the NN weights). This is why DQN needs to use experience replay and a separate target network - in order to combat the instability. The instability is not <em>directly</em> caused by random actions though, it is more complex than that.</p>

<blockquote>
  <p>If we set the final epsilon=0.01, we will select a random the action one time in a hundred, which means that we are not going to get the same answer in different runs and probably our answers are not stable. Why does this still work?</p>
</blockquote>

<p>The agent will not <em>behave</em> consistently on each run, as it is still exploring. However, if it has been set up correctly, it will still converge to and learn an optimal policy, and in fact the constant exploration should help it get values for non-optimal action choices correct, and maintain the correct maximising actions.</p>

<p>When you <em>use</em> a Q learning policy after it has finished learning, typically you would set epsilon to zero and have the agent act greedily.</p>

<p>It is worth noting that even with a deterministic policy, RL is designed around the concept of an environment where random events can happen. For instance it will learn the best way to move when navigating, even if sometimes the distance travelled is not the same due to random events. When you test an agent in a non-deterministic environment, then different things will happen each time, and this is not a problem for the learning process. It might be a problem for you to evaluate the agent - you would need to run a test multiple times and take an average to assess how well it is performing.</p>
","3","2","836","25573"
"38207","<blockquote>
<p>What does G, 'return from step t' mean here?</p>
<ol>
<li>Return from step t to step T-1, i.e. R_t + R_(t+1) + ... + R_(T-1)?</li>
<li>Return from step 0 to step t?, i.e. R_0 + R_1 + ... + R_(t)?</li>
</ol>
</blockquote>
<p>Neither, but (1) is closest.</p>
<p><span class=""math-container"">$$G_t = \sum_{i=t+1}^T R_i$$</span></p>
<p>i.e. the sum of all rewards from step <span class=""math-container"">$t+1$</span> to step <span class=""math-container"">$T$</span>.</p>
<p>You are possibly confused because the loop for REINFORCE goes from <span class=""math-container"">$0$</span> to <span class=""math-container"">$T-1$</span>. However, that makes sense due to the one step offset from return to the sum of rewards. So <span class=""math-container"">$G_{T-1} = R_T$</span> and <span class=""math-container"">$G_{T} = 0$</span> always (there is no future reward possible at the end of the episode).</p>
","3","2","836","25573"
"38212","<p>In the simplest implementations, you simply discard the old population and maintain a population of size $n$ in each generation.</p>

<p>There is <em>lots</em> of variation on this though, any of which could be useful in practice. As a start to show the range of possibilities, you can do some or all of the following:</p>

<ul>
<li><p>Keep a top number (say $m$) of the original population, and only generate $n-m$ new offspring in each generation.</p></li>
<li><p>Generate offspring in small groups and assess them. They must beat a member of the current population on the fitness measure in order to find a place (this can work nicely in round-robin tournaments, where the fitness measure is winner of a game).</p></li>
<li><p>Keep snapshots every so many generations (e.g. every 10 generations) of the whole population or just the best performing individuals. You may use these snapshots to assess long-term improvement or as a ""gene pool"" to keep good performers around in some form for a while longer than a single generation.</p></li>
</ul>

<p>Variations like these are hyperparameters for your GA. They <em>might</em> help in certain circumstances, or be a hindrance in others. So start with the simple ""everything is replaced"" is recommended initially, especially if you are learning genetic algorithms.</p>
","1","2","836","25573"
"38325","<p>Training Neural Networks (NNs) with Genetic Algorithms (GAs) is not only feasible, there are some niche areas where the performance is good enough to be used frequently. A good example of this is <a href=""https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies"" rel=""noreferrer"">Neuroevolution of augmenting topologies or NEAT</a>, which a successful approach to generating controllers in simple environments, such as games.</p>

<p>In the more general case though, the approach does not scale well to large, deep networks with many parameters to tune. </p>

<p>Genetic algorithms and other global searches for optimal parameters are robust in ways that gradient-based algorithms are not. For instance, you could train a NN with step function activations, or any other non-differentiable activation functions. They have weaknesses elsewhere. One thing relevant in the case of GAs used for NNs, is that weight parameters are interchangeable in some combinations but heavily co-dependent in other combinations. Merging two equally good neural networks with different parameters - which you would do in cross-over in a GA - will usually result in a third network with poor performance. NEAT's success is partially in finding a way to address that issue by ""growing"" the NN's connections and matching them up between similar neural networks.</p>

<p>Gradient-based approaches are much more efficient. In general, and not just in domain of NNs, if you can calculate gradient of a function with respect to parameters, then you can find optimal parameters faster than most other optimising techniques. An accurate gradient guarantees at least a small improvement from a single evaluation, and most other optimisers fall into a generate-and-retry paradigm which cannot make that kind of guarantee. The weakness of tending to find local optima has turned out not be a major hindrance for the loss functions in NNs, and has been tackled with some degree of success using extensions to basic gradient descent such as momentum, RPROP, Adam etc.</p>

<p>In practice on a large multi-layer network, gradient methods are likely orders of magnitude faster than GA searches such as NEAT for finding network parameters. You won't find any GA-trained CNNs that solve ImageNet, or even MNIST, where the GA has found the network weights unaided. However, GAs, or at least some variants of them, are not 100% ruled out. For instance <a href=""https://medium.com/@moocaholic/2017-the-year-of-neuroevolution-30e59ae8fe18"" rel=""noreferrer"">this 2017 blog reviews recent papers</a> including <a href=""https://arxiv.org/abs/1703.01041"" rel=""noreferrer"">Large-Scale Evolution of Image Classifiers</a> which explores using GAs to discover NN <em>hyperparameters</em> which is an important task in machine learning, and not very tractable using gradient-based methods.</p>
","5","2","836","25573"
"38445","<blockquote>
  <p>How does reinforcement learning algorithms work without the assumption of (PO)MDPs?</p>
</blockquote>

<p>It doesn't. The theory of reinforcement learning is tied very strongly to an underlying MDP framework. The RNN-based solutions that you are referring to are fully compatible with such an MDP model, and don't even require a POMPDP to be useful.</p>

<p>Without the core guarantees of a (PO)MDP model, or something closely equivalent, it is not clear that any learning could occur, with any kind of agent. The MDP model of an environment is about describing consistent behaviour, that is predictable to some degree, and random/stochastic otherwise, and the predictable parts make it amenable to at least some optimisation. The split into states, actions, time steps and rewards help organise the thinking around this. They are not necessary for other kinds of policy-search approaches, such as genetic algorithms. However, if you try to break away from something that would fit to a (PO)MDP, it would break any other kind of meaningful policy too:</p>

<ul>
<li><p>If actions had no consequence, then you could learn the value of being in a particular state, but you could not optimise an agent. This could be modelled as a <a href=""https://en.wikipedia.org/wiki/Markov_reward_model"" rel=""nofollow noreferrer"">Markov Reward Process</a>, provided state transitions were not completely random, otherwise just learning associations of state to reward using a supervised learning approach would be the best you could do.</p></li>
<li><p>If rewards were not consistently based on any data available to the agent, not even history, but not random, then there is no way to learn how to predict or optimise rewards.</p></li>
<li><p>Similarly for state transitions, if they bear no relation to any information known about the environment, current state or history, but are not random, then there is no way to learn about the non-randomness, and no kind of agent could generate a meaningful policy to take advantage of knowledge about the system, because the knowledge available is not relevant. However, if the current state still influenced what rewards were available to which actions, then a contextual bandit approach might work (plus a supervised learning approach could predict currently available rewards).</p></li>
</ul>

<p>When the information about rewards or state transitions is not directly available, but can be inferred or guessed at least partially from history or context, then you can model this as a POMDP.</p>

<p>One common scenario you can face is that you have available some <em>observations</em> about the environment, but are not sure how to construct a state description that has the <a href=""https://en.wikipedia.org/wiki/Markov_property"" rel=""nofollow noreferrer"">Markov property</a>. The velocity of an object might be such a detail, when your observations only give you positions. Technically a POMDP and this observation/state mismatch are the same basic issue, if you decided arbitrarily that your observation <em>was</em> the state.</p>

<p>When faced with this mismatch between easily-available observations and a state description based on history that would be more useful, you can either try to engineer useful features, or you can turn to learning models that will infer them. <em>That</em> is where using RNNs can come in useful as part of RL, and they can help with both observation to state mapping and also inferring more complex hidden state variables in POMDPs. Use of hidden markov models to model a ""belief state"" that augments the observed state is similar to the latter use of RNNs.</p>
","2","2","836","25573"
"38527","<blockquote>
  <p>Here's the question: is rewarding the agent for going into the right direction a ""correct approach"" in Reinforcement Learning?</p>
</blockquote>

<p>It depends on what you are hoping the agent is capable of learning by itself. This is an issue for you here, because you have a ""toy"" problem where you can control a lot more of the environment and alter the meaning of what it means to win.</p>

<p>In general, then yes this is ""cheating"", at least in terms of claiming to have written an RL agent that solves the game. The <em>academically</em> ideal basic RL agent is rewarded by the gain of something meaningful in the context of the problem being solved, and is not helped by interim rewards. In a game of snake, and any other arcade-style game, it should really be the official points scored in the game and nothing else. </p>

<blockquote>
  <p>Is passing the coordinates of the food as state an other way of ""cheating""? </p>
</blockquote>

<p>Again it depends on what you expect the agent to learn from. If, in your target production environment, this data was easy to obtain, and you intended to use it to write a game bot working from the trained policy, then this is fine. There is no <em>requirement</em> that you do one thing or another if you have a practical problem to solve.</p>

<p>However, learning from a pixel-only state, as in the DQN original papers, is of academic interest, because that is a <em>generic</em> state representation that applies to many problems, whilst the distance from the snake to food is a specific feature that you have engineered that makes learning easier in a smaller set of games.</p>

<p>The main issue here is again that your goal is not really to put a ""snake bot"" into a production system, but to learn how RL works. RL is tricky, and often doesn't work as well as you expect - or at all, for many combinations of algorithm and problem. </p>

<p>It is worth reading this article: <a href=""https://www.alexirpan.com/2018/02/14/rl-hard.html"" rel=""nofollow noreferrer"">Deep Reinforcement Learning Doesn't Work Yet</a> - it may put disappointing results from basic DQN into perspective.</p>

<p>I would encourage you to strip back your Snake problem to remove ""helpful"" rewards and state, and instead look into extensions to the core DQN algorithm, or different learning agents such as A3C.</p>
","1","2","836","25573"
"38606","<blockquote>
  <p>What am I missing?</p>
</blockquote>

<ul>
<li><p>Incorrect architecture for the classification task. You have a single binary output, trained using <code>binary_crossentropy</code>, so the NN can only classify something as in a class (label 1) or not (label 0). Instead, you most likely want 10 outputs using <code>softmax</code> activation instead of <code>sigmoid</code>, and <code>categorical_crossentropy</code> as the loss, so that you can classify <em>which</em> digit is most likely given an input.</p></li>
<li><p>Incomplete processing of MNIST raw data. The input pixel values in <code>X_train</code> range from 0 to 255, and this will cause numeric problems for a NN. The target labels in <code>y_train</code> are the digit value (0,1,2,3,4,5,6,7,8,9), whilst for classification you will need to turn that into binary classes - usually a one-hot coding e.g. the label 3 becomes a vector <code>[0,0,0,1,0,0,0,0,0,0]</code>.</p>

<ul>
<li>Scale the inputs - a quick fix might be <code>X_train = X_train/ 255</code> and <code>X_test = X_test/ 255</code></li>
<li>One-hot code the labels. A quick fix might be <code>y_train = keras.utils.to_categorical(y_train)</code></li>
</ul></li>
</ul>

<p>I made those changes to your code and got this after 10 epochs:</p>

<pre><code>val_loss: 0.1194 - val_acc: 0.9678
</code></pre>
","9","2","836","25573"
"38712","<blockquote>
  <p>What are ""features"" in the specific case of RL? </p>
</blockquote>

<p>In RL, the supervised learning component is used to learn a function approximation for either a value function or a policy. This is usually a function of the state, and sometimes of the state and action. The features are therefore explanatory variables relating to the state (or state and action) and need to be sufficient to explain the value or optimal action of that state.</p>

<p>In RL literature, there is often no difference between terms ""state representation"" or ""features"". Although you may need to process the state representation using feature engineering, into suitable features for whatever supervised learning you are using. With neural networks, this may involve normalising values that might otherwise cause problems.</p>

<blockquote>
  <p>What are examples of features in RL? </p>
</blockquote>

<p>In <a href=""https://gym.openai.com/envs/CartPole-v1/"" rel=""nofollow noreferrer"">OpenAI's gym, CartPole-v1</a> has the following features describing state:</p>

<ul>
<li>Cart Position</li>
<li>Cart Velocity</li>
<li>Pole Angle</li>
<li>Pole Velocity At Tip</li>
</ul>

<p>The positions and speeds are measured in arbitrary units in this case, but you might consider them to be in SI units - metres along the track, metres per second, radians, metres per second.</p>

<p>Typically in RL, you will want features that:</p>

<ul>
<li><p>Describe the state (and possibly the action, depending on implementation)</p></li>
<li><p>Are feasible to collect by observation of the environment</p></li>
<li><p>Together with the action choice, can adequately explain expected rewards and next state. In RL terms, they should in aggregate possess the <a href=""https://en.wikipedia.org/wiki/Markov_property"" rel=""nofollow noreferrer"">Markov property</a></p></li>
</ul>

<blockquote>
  <p>Which RL algorithms require the specification of features? </p>
</blockquote>

<p>None of them*. However, without descriptive features of state, you are limited to <em>enumerating</em> all possible states, and using <em>tabular</em> methods. Working with tabular methods constrains you in practice by the time it takes to fully explore all the states, and the memory space required to represent all the individual estimates of value.</p>

<p>If you are looking into solving your problem using DQN, A3C or other ""deep"" RL method that uses neural networks, then you need to be thinking in terms of a state representation that is composed of features, and to treat those features as if they were inputs to the same neural networks used in supervised learning. </p>

<blockquote>
  <p>Why do we need ""features"" in RL?</p>
</blockquote>

<p>To work with complex environments with large state spaces that cannot be solved without some form of approximation and (hopefully) generalisation.</p>

<hr>

<p>* This is slightly more complex with policy gradient methods, as they do require that you use a function approximator, such as a neural network. However, it is possible to make the ""features"" in that case a one-hot coded vector of the state enumeration, which is the same representation as tabular value-based methods. There is little reason to do that in practice, but it is possible in principle.</p>
","0","2","836","25573"
"38851","<blockquote>
  <p>What is the relationship between Markov Decision Processes and Reinforcement Learning?</p>
</blockquote>

<p>In Reinforcement Learning (RL), the problem to resolve is described as a <a href=""https://en.wikipedia.org/wiki/Markov_decision_process"" rel=""noreferrer"">Markov Decision Process (MDP)</a>. Theoretical results in RL rely on the MDP description being a correct match to the problem. If your problem is well described as a MDP, then RL may be a good framework to use to find solutions. That does not mean you need to fully describe the MDP (all the transition probabilities), just that you expect an MDP model could be made or discovered. </p>

<p>Conversely, if you cannot map your problem onto a MDP, then the theory behind RL makes no guarantees of any useful result.</p>

<p>One key factor that affects how well RL will work is that the states should have the <a href=""https://en.wikipedia.org/wiki/Markov_property"" rel=""noreferrer"">Markov property</a> - that the value of the current state is enough knowledge to fix immediate transition probabilities and immediate rewards following an action choice. Again you don't need to know in advance what those are, just that this relationship is expected to be reliable and stable. If it is not reliable, you may have a <a href=""https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process"" rel=""noreferrer"">POMDP</a>. If it is not stable, you may have a non-stationary problem. In either case, if the difference from a more strictly defined MDP is small enough, you may still get away with using RL techniques or need to adapt them slightly.</p>

<blockquote>
  <p>Could we say RL and DP are two types of MDP?</p>
</blockquote>

<p>I'm assuming by ""DP"" you mean Dynamic Programming, with two variants seen in Reinforcement Learning: Policy Iteration and Value Iteration.</p>

<p>In which case, the answer to your question is ""No"". I would say the following relationships are correct:</p>

<ul>
<li><p>DP is one type of RL. More specifically, it is a <em>value-based</em>, <em>model-based</em>, <em>bootstrapping</em> and <em>off-policy</em> algorithm. All of those traits can vary. </p>

<ul>
<li>Probably the ""opposite"" of DP is REINFORCE which is <em>policy-gradient</em>, <em>model-free</em>, does not bootstrap, and is <em>on-policy</em>. Both DP and REINFORCE methods are considered to be Reinforcement Learning methods.</li>
</ul></li>
<li><p>DP requires that you fully describe the MDP, with known transition probabilities and reward distributions, which the DP algorithm uses. That's what makes it model-based.</p></li>
<li><p>The general relationship between RL and MDP is that RL is a framework for solving problems that can be expressed as MDPs.</p></li>
</ul>
","5","2","836","25573"
"38902","<blockquote>
  <p>What are difference between Reinforcement Learning (RL) and Supervised Learning?</p>
</blockquote>

<p>The main difference is to do with how ""correct"" or optimal results are learned:</p>

<ul>
<li><p>In Supervised Learning, the learning model is presented with an input and desired output. It learns <em>by example</em>.</p></li>
<li><p>In Reinforcement Learning, the learning <em>agent</em> is presented with an environment and must guess correct output. Whilst it receives feedback on how good its guess was, it is never told the correct output (and in addition the feedback may be delayed). It learns <em>by exploration</em>, or trial and error.</p></li>
</ul>

<blockquote>
  <p>Could we say RL has more difficulty in [finding] a stable solution?</p>
</blockquote>

<p>No, because the types of problems it solves are usually different, you cannot compare like with like.</p>

<p>However:</p>

<ul>
<li><p>You could use RL as a framework to produce a predictive model for a dataset (using RL to solve a Supervised Learning problem). This would be inefficient, but it would work and there is no reason to expect it to be unstable.</p></li>
<li><p>In general, RL problems present additional challenges to supervised learning problems with the same degree of complexity in the relationship between input and output. They are ""harder"" in that sense, as more details need to be managed, there are more hyperparameters to tune. </p></li>
<li><p>RL can become unstable in ways that don't apply in supervised learning. For instance Q learning with neural network approximation tends to diverge, and needs special care (often experience replay is enough)</p></li>
</ul>

<blockquote>
  <p>Could we say that [getting stuck] in [a] local minimum is seen more in supervised learning?</p>
</blockquote>

<p>No. Internally, a RL agent will often use one kind of supervised learning algorithm to predict value functions. RL does not include any special features that can avoid or work around local minima. In addition, a RL agent can get stuck in ways that don't apply to supervised learning, for instance if an agent never discovers a high reward, it will create a policy that ignores entirely the possibility of getting to that reward.</p>

<blockquote>
  <p>Is this figure is correct saying that Supervised Learning is part of RL?</p>
</blockquote>

<p>No. The figure is at best an over-simplified view of one of the ways you could describe relationships between the Supervised Learning, Contextual Bandits and Reinforcement Learning. </p>

<p>The figure is broadly correct in that you could use a Contextual Bandit solver as a framework to solve a Supervised Learning problem, and a RL solver as a framework to solve the other two. However, this is not necessary, and does not capture the relationships or differences between the three types of algorithm.</p>

<p>Otherwise the figure contains so much over-simplification and things that I would consider as errors (although this might just be missing context that explains the diagram), that I recommend you simply forget about trying to interpret it.</p>
","3","2","836","25573"
"38947","<blockquote>
  <p>What is difference between running in final episode of training mode and running in test mode in DQN?</p>
</blockquote>

<p>As Q learning is an off-policy method, and DQN is based on Q learning, then the agent effectively has two different policies:</p>

<ul>
<li><p>A <em>behaviour</em> policy that explores actions regardless of whether data so far suggests that they are optimal. The simplest and perhaps most commonly implemented behaviour policy is <span class=""math-container"">$\epsilon$</span>-greedy with respect to action values <span class=""math-container"">$\hat{q}(s,a,\theta)$</span>.</p></li>
<li><p>A <em>target</em> policy that is being learned, which is the best guess at an optimal policy so far. In Q learning that policy is fully greedy with respect to action values <span class=""math-container"">$\hat{q}(s,a,\theta)$</span>.</p></li>
</ul>

<p>When assessing how well the agent has learned the environment after training, you are interested in the <em>target</em> policy. So with Q learning you need to run the environment with the agent choosing actions greedily. That could be achieved by setting <span class=""math-container"">$\epsilon = 0$</span>, or with a different action selection routine which doesn't even check <span class=""math-container"">$\epsilon$</span> (which to use is an implementation detail).</p>

<blockquote>
  <p>Is there any difference more than after training and tune the hyper-parameters, we test for one episode and without any exploration? </p>
</blockquote>

<p>Essentially yes to second part - without exploration. But . . .</p>

<blockquote>
  <p>Why in some test code of DQN, they test for multiple episodes?</p>
</blockquote>

<p>That is because one episode is often not enough to get reliable statistics. </p>

<p>If the environment is completely deterministic, including having only one start state, then one episode would be sufficient to assess an agent. </p>

<p>However, if the environment is stochastic, or you are interested in multiple possible start states, then multiple episodes are necessary in order to have low mean squared error bounds on the average total return, or any other metrics you might be interested in. Otherwise it would be hard to compare results between different hyperparameters. </p>

<p>How many episodes are enough? There is no fixed number, it depends on how much variance there is in the environment and how accurately you want to know the values. In some cases - e.g. OpenAI gym, there are suggested values. For instance, LunarLander suggests a target score over 100 episodes, so they are suggesting measurements are accurate enough over 100 episodes. However, the advice only applies to that problem. In general, you can measure the variance in the score, like any other statistical measure, and calculate error bounds on your metric.</p>
","1","2","836","25573"
"39148","<blockquote>
  <p>Does state representation generally affect how difficult a problem is? </p>
</blockquote>

<p>Yes, due to it being easier or harder for a neural network to learn the relationships between input features and the target policy or value function.</p>

<blockquote>
  <p>Is this a poor state representation? </p>
</blockquote>

<p>No, it should be fine, provided you are interested to see a RL learn to convert from cartesian coordinate space to solving this problem, you should be able to use the state directly as ML features. </p>

<p>It <em>could</em> be made into better features for a NN in two ways:</p>

<ul>
<li><p>Scaling to fit in ranges with zero mean and limited max absolute values.</p></li>
<li><p>It could be engineered so that features include domain knowledge of the problem to be solved. </p></li>
</ul>

<p>The first part is optional for you, but I think that the bearing values benefit a little from being scaled and centred. In a more general case, even if the state description was complete, you may still want a step that re-scales it.</p>

<p>The second part is tricky - yes you could find better features, but part of the challenge is to create an agent that learns to do this itself. In this case, you may want to gain experience training agents where it is less easy to engineer ""golden"" features, and focus on RL methods.</p>

<blockquote>
  <p>Are there rules of thumb for how to design states? </p>
</blockquote>

<p>Conceptually, the problem of state representation can break down into three parts:</p>

<ul>
<li><p><strong>Observations</strong>. Raw observations are not always direct candidates for a state representation. In a toy problem, often this is ignored and you feed data from a simulation that looks useful as a state. In the real world you can be limited by what is detectable.</p></li>
<li><p><strong>State description</strong>. Unless you want to explore POMDPs then you usually want the state description to possess the <a href=""https://en.wikipedia.org/wiki/Markov_property"" rel=""nofollow noreferrer"">Markov property</a>. This might already mean processing observations into something else - e.g. using history of last 3 observations, keeping running totals or calculating differences.</p></li>
<li><p><strong>Feature vector</strong>. Once you have decided to use function approximation for calculating action values or a policy function, then your inputs need to conform to how the function approximation works. For most function approximators, this means numerical values. For neural networks, it means scaling inputs to fit relatively small ranges. There is also the question of feature engineering using domain knowledge of the problem.</p></li>
</ul>

<p>There is some overlap between these design steps, the distinctions are somewhat artificial. When you are designing a test problem like yours, you may find it simple to combine all three steps into a single observation = state = feature. However, in real-world problems, each of the steps may require some consideration.</p>

<p>You should also consider the likely nature/shape of the function that you will be approximating. Action-value methods like Q learning need to approximate their value functions, whilst policy gradient methods like REINFORCE and PPO need to approximate policies. Sometimes the map between input features and the target function is simple, and if you are lucky some intuition can lead you to figuring that out. This is also a big driver when choosing between DQN or PPO for example - what seems easier to figure out, the correct action, or the value of a state/action pair?</p>

<blockquote>
  <p>Would it help to reformulating the state, eg. to [distance_from_agent_to_rabbit, angle_between_agent_and_rabbit]?</p>
</blockquote>

<p>Maybe. The angle feature that would seem to most help is the difference in angle between the wolf's bearing and the vector between the wolf and rabbit. Then the correct action would map very clearly from that to steer the wolf towards the rabbit - in most cases a negative value means steer right and a positive value means steer left.</p>

<p>However, if you do this, you will in some ways have changed the nature of the problem to be solved. You have to ask, are you interested in applying your domain knowledge of the problem to help the agent, or are you interested to see if the agent can figure out an internal representation that discovers this relationship? For a toy problem, you may want to deliberately make something harder to learn.</p>

<blockquote>
  <p>I am having trouble solving this environment. Agents trained in it get scores only slightly better than a random agent even after long training sessions. I have tried Deep Q-learning (with experience replay, target network) REINFORCE (with and without baseline) and PPO.</p>
</blockquote>

<p>As discussed in comments, the biggest factor towards this actually turned out to be a bug in your environment code, where the agent could end up with a bearing value out of range. The environment still worked, because you are using trig functions to calculate movement, but this causes even larger range of bearing values plus makes states that are identical appear different to the agent making things even harder to learn.</p>

<h3>An experiment</h3>

<p>I managed to solve this environment using a simple single-step DQN-based agent, and had time to experiment with some different input features to the NN to demonstrate my points above. In each case, I used exactly the same hyperparameters for RL and NN (expect in the last case I had to change the size of the NN's input layer). </p>

<p>I counted the number of training episodes required (including ~80 episodes of purely random behaviour to start experience replay) for 100 completed training runs, and tried some different state feature representations. I counted the environment as ""solved"" when 100 test runs using the agent acting greedily scored an average return of 20 or more. I did not count the test runs towards the number of training episodes.</p>

<p>I got these results:</p>

<ul>
<li><p><strong>Unaltered state</strong> 629.99 +-23.66 episodes, but <em>failed in 28 out of 100</em></p></li>
<li><p><strong>Scaled state</strong> 577.78 +-19.41 episodes, no failures</p></li>
<li><p><strong>Engineered state</strong> 153.84 +-3.26 episodes, no failures</p></li>
</ul>

<p>There is not quite a significant difference in 100 trials between number of episodes for unscaled and scaled state. However, the high number of failures (gave up after 1500 episodes of training) for unscaled features <em>is</em> a significant difference.</p>

<p>For the <em>scaled state</em> variation, The scaling for the cartesian coordinates was <span class=""math-container"">$f_i = 2*(s_i-0.5)$</span> and the scaling for the bearings was <span class=""math-container"">$f_i = 0.5 * (s_i-\pi)$</span></p>

<p>The <em>engineered state</em> used [distance between wolf and rabbit, angle between vector to rabbit and wolf's bearing, rabbit's bearing] scaled as above, and the performance was radically better.</p>

<h3>Another consideration</h3>

<p>As an aside, it is worth mentioning episode timeout and ""done"" flag. </p>

<p>You need to carefully consider what it means for the episode to time out. There is a difference between this being part of the challenge (to succeed within a time limit) and being for training convenience (to avoid wasting time learning from overlong or stuck episodes). Sometimes it is a big difference:</p>

<ul>
<li><p><em>For training convenience</em>. The ""done"" flag is an annoyance here, you want to avoid claiming the episode is really over to the agent, as it will falsely learn that some states, sometimes, end the episode - these states may then even be desirable to the agent as they seem like they stop the flow of negative reward. You don't want to store that white lie in the experience replay table - a simple work-around is to have your agent stop prematurely at least 1 step before the environment times out. </p></li>
<li><p><em>Part of the challenge</em>. If the environment can really time out, and not randomly stop at any point, then in order to preserve the <a href=""https://en.wikipedia.org/wiki/Markov_property"" rel=""nofollow noreferrer"">Markov property</a>, you must include a representation of the time in the state - it can be time so far or remaining time. Otherwise you have turned the problem into a POMDP, and added complications for calculating value functions.</p></li>
</ul>
","1","2","836","25573"
"39379","<blockquote>
  <p>what is <span class=""math-container"">$R_a(s,s')$</span> ?</p>
</blockquote>

<p>In this case, it appears to represent the <em>expected immediate reward</em> received when taking action <span class=""math-container"">$a$</span> and transitioning from state <span class=""math-container"">$s$</span> to state <span class=""math-container"">$s'$</span>. It is written this way so it <em>could</em> be implemented as a series of square matrices, one for each action. Those matrices might of course be very sparse if only a few transitions are possible, but it's a nice generic form for describing MDPs.</p>

<p>Notation varies between different RL tutorials, so do take care when looking at other sources.</p>

<blockquote>
  <p>The problem I'm having is that terminal states have, by default, <span class=""math-container"">$V(s_T) = R(s_T)$</span> (some terminal reward).</p>
</blockquote>

<p>No, <span class=""math-container"">$V(s_T) = 0$</span> by definition. The <em>value</em> of a state, is the expected discounted sum of future rewards. A terminal state has no future rewards, thus its value is <em>always</em> <span class=""math-container"">$0$</span>.</p>

<p>The ""terminal reward"" in your system occurs on the transition <span class=""math-container"">$s \rightarrow s_T$</span>, so its expected value should be represented by <span class=""math-container"">$R_a(s,s_T)$</span> </p>

<p>If the terminal state is some goal state, or a bad exit from an episode, and it doesn't matter how you arrive there, then in the formulation you have given, you still represent it as <span class=""math-container"">$R_a(s,s_T)$</span>, just that the values of <span class=""math-container"">$a$</span> and <span class=""math-container"">$s$</span> don't matter for your case. In general they <em>might</em> matter, for other MDPs.</p>

<blockquote>
  <p>So the only conclusion I seem to be able to get is that in my case, <span class=""math-container"">$R_a(s,s') = R(s)$</span>.. is this correct?</p>
</blockquote>

<p>Not in general. However, in some problems this might be a reasonable simplification. If you make that simplification, then you should take care when having a value associated with a single state to be consistent about whether the reward is for entering a particular state, or exiting it. There cannot be a reward for ""being in a state"", the closest you can get to that is granting a consistent reward when <em>entering</em> a state (ignoring the previous state and action that caused the transition).</p>

<p>It appears here that you have suggested here that you want a reward for exiting a particular state - ignoring which action is taken in it, or what the next state is. I don't know the details of your MDP, so cannot say whether this would work for you. Maybe not, given the rest of your question. </p>

<p>If you are asking in general, then you have the answer <span class=""math-container"">$R_a(s,s') \ne R(s)$</span> </p>

<p>If you are asking in order to work on a specific MDP, I suggest take another look at your original problem, with the extra information in this answer.</p>
","2","2","836","25573"
"39522","<p>In short your friend is correct, seq2seq is a reasonable match to the problem. </p>

<p>However, from your numbers this could be too complex to use current machine learning libraries on. </p>

<p>Despite you calling it ""not feasible"", you are far better off with the reverse engineering effort in my opinion.</p>

<p>If you really want to have a try using ML, you could start with some shorter files with the generated output and attempt to train a LSTM-based seq2seq model to match them. It may help, if the HTML is strongly templated and driven by data in the binary file, to reverse-engineer just the templating, and use seq2seq model to derive a more raw data sequence as opposed to HTML. </p>

<p>You should expect to spend several weeks learning how seq2seq works, followed by an unknown amount of time in the attempt to apply it to your problem. You will need a large number of sample files to train your model - hundreds of thousands, maybe millions. There might be exceptions if the generated HTML is very repetitive/formulaic, or if short segments of binary data can also generate short segments of HTML (so you could train on smaller segments of the larger files).</p>

<hr>

<p><em>If any of this would break the terms and conditions for use of the software, I advise you not to attempt either reverse engineering or machine learning.</em></p>
","1","2","836","25573"
"39550","<p>Taking your question title literally:</p>

<blockquote>
  <p>Is RL applicable to environments that are totally RANDOM?</p>
</blockquote>

<p>The answer would be no. In a totally random environment there is little to nothing that can be learned.</p>

<p>However, you don't actually have a totally random environment. You have some quantities that fluctuate a lot in a way that you don't understand. Otherwise your environment behaves very logically - if there is demand for Y items, and you have X in stock, then you will end up with X - Y in stock if X > Y, or 0 in stock and Y - X incomplete orders otherwise. This is a very structured rule that you can definitely associate rewards with and learn. Presumably you have costs for ordering and holding stock and opportunity costs for not supplying orders.</p>

<p>A simple variant of your situation is used as a toy example in <a href=""http://incompleteideas.net/book/the-book-2nd.html"" rel=""nofollow noreferrer"">Sutton &amp; Barto: Reinforcement Learning, an Introduction</a> in chapter 4, called ""Jack’s Car Rental"" where the goal is to optimise stock at two locations with random demand occurring at either location. This toy problem is made easier by defining the distribution, and using it in a model-based way. But that is not necessary in general.</p>

<blockquote>
  <p>Is RL really applicable to such situations?</p>
</blockquote>

<p>In your case, probably yes. Although you do have to assume that ""totally random"" is just a way of phrasing ""highly variable"" and not literally 0 one day, 30 million the next day and 7.5 the next day. There are going to be limits to the orders and they will follow <em>some</em> distribution. </p>

<p>With very high variance then you are likely to find it hard to reach a balance point of costs for holding stock vs costs for missing orders, but in principle this is solvable and RL is a reasonable tool to attempt the solution.</p>

<blockquote>
  <p>If it does - then what will improve the performance?</p>
</blockquote>

<p>Check whether any fluctuations in demand depend on any variables that you can collect and add those variables to the state. For instance, if demand has some weekly or seasonal variation, then those parts of the date should be part of your state representation.</p>

<p>Understanding and predicting the distribution of demand, even if not the exact values, would also help with simulations and planning algorithms. The RL algorithm <em>will</em> over time learn the likely distribution, but it can only base its own predictions on state variables that you have let it observe. </p>

<p>You might be able to go one better: If demand is mostly independent of the stock levels that you hold, then you can separate the problem of predicting it and use more robust supervised learning to add a ""predicted demand"" feature to the state. The RL will on top of this prediction learn the costs associated with just trusting this prediction vs allowing for some extra incoming orders just in case etc.</p>
","3","2","836","25573"
"39620","<p>You have a couple of mistakes around assigning reward, and the update mechanism.</p>

<p>You intend to grant 0 reward for a loss, 0.5 reward for a tie and 1 reward for a win. And you place those rewards as fixed state values for completed boards, that won't logically get updated. That is OK (provided you do take care to never update those values). </p>

<p>In the <code>genStates</code> function you also assign an interim value of 0.5 to incomplete board states. This is also OK as the initial value (any value would be in principle). However, it points to a problem - you don't seem to differentiate between the state value and the reward.</p>

<p>You also don't have a learning rate, just a discount factor (which for some reason you are calling the learning rate - it is not). As it happens, in such a simple game, both can be 1.</p>

<p>The update function for Q learning using afterstates should be:</p>

<p><span class=""math-container"">$$V(s) \leftarrow V(s) + \alpha(R + \gamma \text{max}_{s'}[ V(s')] - V(s))$$</span> </p>

<p>If you set <span class=""math-container"">$\alpha = 1$</span> (as you effectively have), then this becomes:</p>

<p><span class=""math-container"">$$V(s) \leftarrow R + \gamma \text{max}_{s'}[ V(s')]$$</span> </p>

<p>Your code is implementing:</p>

<p><span class=""math-container"">$$V(s) \leftarrow V(s) + \gamma V(s')$$</span></p>

<p>You need to separate out the value function and reward calculation, so that you use <span class=""math-container"">$R$</span> and not <span class=""math-container"">$V(s)$</span> in the update. Not only should this fix the problem, but it will make how the algorithm works clearer. The simplest reward for incomplete game should be <span class=""math-container"">$0$</span> - other fixed values may work, but if they are high enough to interfere with the win/loss/tie rewards, then one of the players may prefer to lose early as opposed to win or draw in a longer game. So stick with <span class=""math-container"">$0$</span>.</p>

<p>In addition, you are not updating using the max action, when taking exploratory moves. That makes your algorithm SARSA, not Q learning. At a low exploration rate, this probably won't make a big difference to the eventual state values. However, they won't actually, represent optimal play, but something close to it. If you really want to implement Q learning, you need to call <code>updateState</code> based on the maximising (or minimising) action, regardless of whether the player took it in the actual game.</p>

<p>Note that typically in zero-sum games, the rewards are -1 for loss, 0 for tie and +1 for a win. However, I don't <em>think</em> that makes any difference to you here, the important trick in a two-player game is to switch between actions that minimise and maximise the value function depending on whose turn it is, and it looks to me that you do that correctly.</p>

<p>Expect to play a few times 10,000 games in order to learn a perfect value function.</p>
","3","2","836","25573"
"39671","<p>There is a lot of information that might be useful for a more precise answer in your case, but is hard to extract from your question description.</p>

<p>I think I can understand the following:</p>

<ul>
<li>You are training to <em>minimise</em> a cost instead of maximise a reward (this is unusual in RL, but not a problem).</li>
<li>Random agents score variably, anything from 120 to 400 average cost. </li>
<li>After many training steps, your agent is scoring 6.1 average cost, measured over the most recent 100 time steps.</li>
<li>In testing, the agent scores better, an average cost of 5.2.</li>
</ul>

<p>The usual way of testing a DQN agent (or any off-policy agent, of which Q Learning or DQN are one type) is to stop exploring actions. I am assuming you do this too. During training, your agent was not always taking the actions that it determined were best. In tests, you are interested to see what the agent thinks is ""best"" and how good that is. So it should be expected that test scores better than recent training. If you were training using <span class=""math-container"">$\epsilon$</span>-greedy action selection, then your scores would be consistent with <span class=""math-container"">$\epsilon \approx 0.1$</span></p>

<p>However, your measurements have limited accuracy in any case. It might be worth calculating your standard error on your test data, to get a sense of how accurate your measure really is. The easiest way is to run your test multiple times, create an array of scores and calculate the standard deviation, mean and standard error as usual using the test results as samples of a random variable.</p>

<blockquote>
  <p>Could we say our DQN is OK?</p>
</blockquote>

<p>That depends entirely on the problem. It seems that it does far better than acting randomly, which shows at least it has learned something.</p>

<p>In order to get a sense of how well it has done, you need to compare it with something meaningful related to the problem:</p>

<ul>
<li><p>For simple problems, such as toy problems used to prove DQN works, it is possible to calculate an optimal answer and see how close the DQN can get.</p></li>
<li><p>For problems that humans can attempt too, you can compare the performance of an agent to a human. Ideally a human who is expert at the task.</p></li>
<li><p>For problems for which there are some publications, you can compare with state-of-the-art. This is true for many Atari 2600 games for instance.</p></li>
<li><p>For problems which have a practical purpose, you can compare with a goal, such as would the agent be good enough to make money or reduce loss compared to current practice.</p></li>
<li><p>For some problems you could compare to a simple rules-based agent, coded to perform the specific task.</p></li>
</ul>

<blockquote>
  <p>The more time steps should be more stable?? What is the effect of time steps?</p>
</blockquote>

<p>Up to a limit, the more time and samples that a DQN trains on, the better its performance will be. The best thing is to plot this - number of timesteps vs average cost. You may see sudden increases or even ""catastrophic forgetting"" (where the agent does no better than random, or even worse) at some times. But if the agent has trained successfully there should be a general improvement over time that reaches a limit as the DQN approaches the maximum capability of the agent.</p>
","1","2","836","25573"
"39746","<p>In short there are no rules, and no ""best way"". Any combined feature for statistical learners can be judged on two basic criteria:</p>

<ul>
<li><p>Does it make sense in the domain of the problem?</p></li>
<li><p>Does it improve the metrics that you care about in the model?</p></li>
</ul>

<p>If you want to <em>compare</em> two radically different features, then you will probably need to introduce some kind of scaling factor. In which case you can make comparisons based on e.g. number of standard deviations away from the mean, or whether or not both features are in the same quartile of their respective distributions. Such a comparison does not rely on the units of the features.</p>

<blockquote>
  <p>If the units are different, it doesn't make sense to subtract them (e.g. trade volume vs orderbook size).</p>
</blockquote>

<p>Features as fed into a statistical classifier or regression system are effectively unit-less. Given that, you can make any mathematical conversion that you like. Although some will make more sense in the domain of your problem than others.</p>

<p>Once inside your model, there is every chance that the numbers will be compared, multiplied, added, subtracted in all sorts of combination. This is definitely true of neural networks for example.</p>

<p>If it helps you can think of any normalisation, or even the weights of your model, as having units that also normalise the units of your inputs to all be compatible. Using number of standard deviations away from the mean is a simple way to achieve this, but not the only way.</p>

<p>The exceptions to this approach would be models that are more like scientific models, where units are carefully tracked and a fixed formula applied. Even those commonly have constants which do unit conversion.</p>

<blockquote>
  <p>If either features can be 0, what's the best way to compare them?</p>
</blockquote>

<p>Depends on the features, and on your classifier, but if possible, and working with linear or logistic regression, I would do the following:</p>

<ul>
<li><p>Optionally, transform each feature non-linearly so that its distribution follows a bell curve that looks roughly like a Normal distribution.</p></li>
<li><p>Normalise both features to mean 0, standard deviation 1.</p></li>
<li><p>Subtract one from the other.</p></li>
<li><p>Normalise the new feature. If the two units you have combined are correlated, then the new feature may have an interesting distribution - worth plotting it and taking a look.</p></li>
</ul>

<p>There other equally valid approaches too. In part it will depend on the ML model you intend to use. Normalising as above works well for linear or logistic regression, neural networks (I would let the NN find the derived feature here though, unless I thought it was really important due to domain knowledge), kNN or other models that might use simple distance metrics.</p>
","1","2","836","25573"
"39809","<p>If you need to re-train the model to classify new faces, this will not scale well to registering new people routinely. You may also suffer from glitches in accuracy during new registrations unless the training routines are carefully monitored.</p>

<p>Instead, recognition systems that need to register new items typically don't re-train. They are trained on the general task of <em>separating</em> objects - identifying whether they are different or what is different about them.</p>

<p>One common way to achieve this separation is to use the NN to map images of faces to a descriptive vector, and match each new image according to distance to stored vectors of profile pictures. The distance, even to the closest stored vector, should be small in order to consider it a successful match. Registering a new user is then a matter of saving a new vector embedding calculated from the neural network. Matches can be done with database lookups - they are still limited by how fast you can do the distance calculations looking for a match, but you only need run the NN forward once. The distance calculations can be fast and batched - it should be easy to use e.g. tensorflow or torch to calculate 1000 potential matches in a fraction of a second.</p>

<p>To ensure that faces are well categorised according to their differences, ignoring lighting, pose, hairstyle etc, the network needs to be trained with this objective in mind. One way to do this is to train with <a href=""https://towardsdatascience.com/lossless-triplet-loss-7e932f990b24"" rel=""nofollow noreferrer"">triplet loss</a> using two pairs of images, one that should match, another that should not.</p>

<p>Andrew Ng's course on CNNs covers this approach very well, <a href=""https://www.coursera.org/lecture/convolutional-neural-networks/what-is-face-recognition-lUBYU"" rel=""nofollow noreferrer"">starting from this lecture</a>.</p>
","2","2","836","25573"
"39922","<p>This looks very much like a variation of the <a href=""https://en.wikipedia.org/wiki/Bin_packing_problem"" rel=""nofollow noreferrer"">bin packing problem</a>. The bad news is that this is mathematically a hard problem, NP-hard in fact (which means that no polynomial-time solution is possible). The good news is that it is heavily studied, with multiple approaches to solving it, or at least optimising solutions to within reasonable bounds.</p>

<p>Which approach to optimisation will work best for you will depend on how physically accurate you want your model to be. Your voxel approach already makes some decisions in that regard, but you may also want to consider issues such as gravity, stability of any piles of structures you create, and access to items. These things add additional constraints that make modelling the problem and implementing an optimiser harder, so you might want to start with more unrealistic model initially.</p>

<p>One simple approach might be to use a simple heuristic e.g. pack largest objects first, fill close to edges first etc. Then attempt to swap or revise the order of packing of a few pieces and see if it makes an improvement. You can make those changes greedily, or look into making them more randomly but with a system that rewards better improvements. Approaches such as <a href=""https://en.wikipedia.org/wiki/Simulated_annealing"" rel=""nofollow noreferrer"">simulated annealing</a> or <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow noreferrer"">genetic algorithms</a> can be used as stochastic approaches, many others are possible. There are a very large number of global optimisers that might help with such a combinatorial problem.</p>

<p><a href=""https://pdfs.semanticscholar.org/4608/f125f34da502e7a4dbe679b87e38de5071fb.pdf"" rel=""nofollow noreferrer"">Here is an example of solving a bin packing problem using Simulated Annealing</a>. </p>

<p><a href=""http://thejjjunk.ucoz.com/papers/AGeneticAlgorithmForTheBinPackingProblem.pdf"" rel=""nofollow noreferrer"">Here is an example of solving a bin packing problem using a Genetic Algorithm</a></p>
","0","2","836","25573"
"40007","<p>No, you cannot write a Kaggle kernel that will read your local data directly.</p>

<p>The best you can do is upload your data to the kernel. See <a href=""https://www.kaggle.com/docs/kernels#adding-data-sources"" rel=""nofollow noreferrer"">https://www.kaggle.com/docs/kernels#adding-data-sources</a></p>
","1","2","836","25573"
"40156","<p>I am not convinced that framing a document retrieval success as reinforcement learning problem will be easy to get working. </p>

<p>However, your core question is answerable. This problem occurs in other problems that are too large to explore all possible states. For instance, in the game of Go, the AlphaZero system cannot possibly explore all game positions. Similarly, in Atari games, many of which have also had reinforcement learning applied successfully, it is very unlikely that the agent has seen all possible states of the system - typically it will have trained on a million video frames.</p>

<p>The answer to this issue is to use some form of function approximation that can generalise to new unseen data. That is precisely what supervised machine learning models do, so the RL agent will use one internally, typically a linear regression or neural network, so that values and or policy learned from states it has explored will be associated with similar unseen states (for some interpretation of similar).</p>
","1","2","836","25573"
"40167","<p>It is not possible to predict outcomes of ideas like this with certainty. Too much depends on the specifics of your problem and dataset. You will have to do the experiments yourself. With a decent auto-differentiation engine like TensorFlow or PyTorch, you could try all your different ideas really quickly, with a few variations.</p>

<p>Here is what I think will happen from an intuitive guess:</p>

<blockquote>
  <p>Should I try to predict y1, y2, y3, then calculate Y and pass it to a loss function</p>
</blockquote>

<p>I think this has potential to be the strongest model, as it incorporates knowledge that you have of the problem into the structure of your model, whilst not adding more hyperparameters. This <em>may</em> also allow you to use less parameters to achieve the same accuracy, plus may work as a form of regularisation. </p>

<blockquote>
  <p>Should I try to predict Y standalone and ignore y1, y2 and y3?</p>
</blockquote>

<p>This should be considered the default/benchmark model, and you will want to run that anyway. If you only make one model, make this one.</p>

<p>If you run comparisons, you will want to compare <em>best</em> model of this type with best model(s) of other types. Don't just try the same network with an altered top layer to change the prediction mechanism - that will not explore your idea well enough to give you an answer.</p>

<blockquote>
  <p>Should I try to predict the 4 of them and penalize my model when Y != y1 * y2 * y3 ?</p>
</blockquote>

<p>I expect this would be the hardest to train. Although it may still add some regularisation effect, your problem is that the ""best"" model in this category may rate consistency over accuracy - you will need to play with different weightings of the penalty metric to find a compromise loss function.</p>

<p>Remember to use cross-validation and test data in order to search parameters and get fair unbiased measures of performance of the best model from each of the approaches at the end.</p>
","1","2","836","25573"
"40607","<blockquote>
  <p>Can I drop these features to improve the accuracy of my classification problem?</p>
</blockquote>

<p>If you are using a simple linear classifier, such as logistic regression then yes. That is because your plots are giving you a direct visualisation of how the model could make use of the data.</p>

<p>As soon as you start to use a non-linear classifier, that can combine features inside the learning model, then it is not so straightforward. Your plots cannot exclude a complex relationship that such a model might be able to exploit. Generally the only way to proceed is to train and test the model (using some form of cross-validation) with and without the feature.</p>

<p>A plot <em>might</em> visually show a strong non-linear relationship with zero linear correlation - e.g. a complete bell curve of feature versus target would have close to zero linear correlation, but suggest that something interesting is going on that would be useful in a predictive model. If you see plots like this, you can either try to turn them into linear relationships with some feature engineering, or you can treat it as evidence that you should use a non-linear model.</p>

<p>In general, this advice applies whether or not the features are derived features. For a linear model, a derived feature which is completely uncorrelated with the target is still not useful. A derived feature may or may not be easier for a non-linear model to learn from, you cannot easily tell from a plot designed to help you find linear relationships.</p>
","8","2","836","25573"
"40760","<p>Distributed RL is very much a thing. Google have created a distributed setup called <a href=""https://arxiv.org/pdf/1802.01561.pdf"" rel=""nofollow noreferrer"">IMPALA</a> for this, and there are multiple instances of A3C, PPO etc available if you search. I don't know much about IMPALA, but the basic idea of the scalable policy gradient methods is to run multiple environments, collecting gradients on each server, then collating them together to create improved policy and value networks every few steps. </p>

<p>There are a couple of variations in strategy based on which stage of the data is shared - observations or gradients. Gradient calculation is CPU intensive, so above a certain scale it is worth having that occur on the distributed devices, depending on how intensive it is to collect experience in the first place.</p>

<blockquote>
  <p>Obs: The computers aren't in the same LAN.</p>
</blockquote>

<p>This may prevent you implementing anything with low-level observation or gradient sharing, unless the bandwidth between the machines is high. </p>

<p>The simplest way to use two computers in this case is to perform basic hyper-parameter searches by running different tests on each computer and tracking which computer has done which experiments.</p>

<blockquote>
  <p>The first one would execute the game and send requisitions to the second computer which would store and train the model itself</p>
</blockquote>

<p>This could work with an off-policy value-based method, such as DQN. You still need a reasonable bandwidth between the two machines, especially if the observation space is large. DQN is a reasonable choice since you don't need the environment-running machine to follow the current optimal policy - although you will still want to update the policy on the first computer some of the time. </p>

<p>The basic algorithms of DQN do not require much changing to support this kind of distribution. Just comment out or make conditional a few parts:</p>

<ul>
<li><p>On the first machine:</p>

<ul>
<li>Comment out or logically block sampling and learning from experience table</li>
<li>Maintain a ""behaviour-driving"" Q-value network instead of the learning network, in order to run <span class=""math-container"">$\epsilon$</span>-greedy policy</li>
<li>Send experience to second machine instead of storing it in local experience-replay table (this is the bandwidth-intensive part)</li>
<li>Asynchronously receive updates to behaviour-driving Q-value network</li>
</ul></li>
<li><p>On the second machine:</p>

<ul>
<li>Comment out or logically block interactions with the environment</li>
<li>Asynchronously receive experience from first machine and add to experience-replay table</li>
<li>Every so many mini-batches, send updated current network to second machine</li>
</ul></li>
</ul>

<p>You will need to have some experience handling multi-threading or multi-process code in order to cover the asynchronous nature of the updates. If that seems too hard, then you <em>could</em> have the two machines attempt to update each other synchronously, or via a queueing mechanism. The disadvantage of that is one or other machine could end up idle waiting for its partner to complete its part of the work.</p>
","1","2","836","25573"
"40867","<blockquote>
  <p><code>R(a) = V(s') - V(s)</code></p>
</blockquote>

<p>This is not correct. The value of a state is based on all its <em>future</em> rewards. Your formula would be correct for a value function <span class=""math-container"">$V$</span> that accumulated <em>past</em> rewards, but that is not directly useful for action selection in reinforcement learning. The agent needs to choose an action that makes it the most reward in future. It cannot make any action to change what happened in the past, so value functions are forward-looking only. For instance, the value of a state where an agent has completed its task successfully (or failed) is always <span class=""math-container"">$0$</span>, because the agent can no longer act, and has no chance of any future reward.</p>

<p>Without discounting, then:</p>

<p><span class=""math-container"">$$V(S_t) = R_{t+1} + V(S_{t+1})$$</span></p>

<p>Therefore: </p>

<p><span class=""math-container"">$$R_{t+1} = V(S_t) - V(S_{t+1})$$</span></p>

<p>i.e. the opposite sign than you thought, but compatible with the TD update rule.</p>
","2","2","836","25573"
"40975","<p>Provided each network implements the XOR function approximately, but within reasonable error bounds, then this is NEAT behaving as designed.</p>

<p>Neural networks allow for multiple equivalent solutions. Even using a fixed architecture and stochastic gradient descent to learn a simple function, there is a good chance of ending up with very different weights each time. With NEAT, it also explores alternative architectures, and the search includes many calls to random number generators in order to make decisions. </p>

<p>The main deterministic step in NEAT is the selection process - comparing population members with each other to rank them in terms of fitness. However, as multiple designs of neural network can all solve the XOR problem roughly equally well, it is unlikely this will find the same one each time.</p>

<p>If you need to exactly reproduce an experiment in NEAT, you can achieve that by setting the RNG seed. I think this could just be a call to <code>random.seed()</code> for the library you are using, but some libraries may have other RNG instances that need setting up too.</p>
","1","2","836","25573"
"41041","<p>As it is such a small MRP, it is possible to solve it quickly and analytically using simultaneous equations based on the Bellman equation:</p>

<p><span class=""math-container"">$$v(s) = \sum_{r,s'} p(r,s'|s)(r + v(s'))$$</span></p>

<p>and substituting in each state in turn. Using the variables <span class=""math-container"">$a,b,c,d,e$</span> to represent <span class=""math-container"">$v(A), v(B), v(C), v(D), v(E)$</span> makes this easier to read:</p>

<ul>
<li><p><span class=""math-container"">$a = \frac{1}{2}(0 + b)$</span></p></li>
<li><p><span class=""math-container"">$b = \frac{1}{2}(a+c)$</span></p></li>
<li><p><span class=""math-container"">$c = \frac{1}{2}(b+d)$</span></p></li>
<li><p><span class=""math-container"">$d = \frac{1}{2}(c+e)$</span></p></li>
<li><p><span class=""math-container"">$e = \frac{1}{2}(d+1)$</span></p></li>
</ul>

<p>It takes multiple substitutions to resolve any individual value, at least to start with. But it is possible to resolve these terms by repeated substitution of how many times <span class=""math-container"">$a$</span> they are, and then proceeding to the next equation getting each variable:</p>

<p><span class=""math-container"">$a = \frac{1}{2}(0 + b) \therefore b = 2a$</span></p>

<p><span class=""math-container"">$b = \frac{1}{2}(a+c) \rightarrow 2a = \frac{1}{2}(a + c) \therefore c = 3a$</span></p>

<p><span class=""math-container"">$c = \frac{1}{2}(b+d) \rightarrow 3a = \frac{1}{2}(2a + d) \therefore d = 4a$</span></p>

<p><span class=""math-container"">$d = \frac{1}{2}(c+e) \rightarrow 4a = \frac{1}{2}(3a + e) \therefore e = 5a$</span></p>

<p><span class=""math-container"">$e = \frac{1}{2}(d+1) \rightarrow 5a = \frac{1}{2}(4a+1) \therefore a = \frac{1}{6}$</span></p>

<p>That last one resolves everything, and gives the expected answer. You can also see a very strong pattern here, which should intuitively hold for any length of one dimensional random walk with equal <span class=""math-container"">$p=0.5$</span> - there is a proof for that more general case that would void the need to work the substitutions in detail for each length, but that is more complex.</p>

<p>One other simple intuition: Each state value in turn is the <em>mean</em> of the two state values either side of it, without exception. That means taking any three values next to each other, they should be co-linear; the two higher and lower values with the mean between them. As this holds on all values with overlap, plotting the values in order should place all points all on the same line. That in turn means that there is a simple linear relationship between position in the random walk and the state value.</p>
","2","2","836","25573"
"41112","<p>You might be able to fit your training into a Google Colab session. You can search for tutorials using different libraries, <a href=""https://www.kdnuggets.com/2018/02/google-colab-free-gpu-tutorial-tensorflow-keras-pytorch.html"" rel=""nofollow noreferrer"">here is one for PyTorch</a>. Google Colab is essentially a free online Jupyter notebook, and makes K80 GPU available for acceleration. </p>

<p>Sessions on Colab are limited to ~12 hours maximum, so you would need to start with some simple variants and build up to max allowed training time (this is good practice anyway). There is no guarantee that this will be enough for your game, but you should learn a lot even if the resulting agent is not perfect.</p>

<p>It is not clear whether you have already figured this out, but you most likely will not be able to host the Android App natively to run as the environment, and will need to code some kind of simulation or re-implement the game. I would also assume it is a card, board or turn-based strategy game with perfect information, if you are hoping to use an AlphaZero-based learning agent with it (AlphaZero could probably be adapted to video games, but I would not expect it be as efficient as simpler algorithms without a planning phase, such as A3C in that case).</p>
","0","2","836","25573"
"41181","<p>The image in your question looks to me like a loose hierarchy explaining how various Reinforcement Learning methods relate to each other. At the top are broad categories of algorithm based on whether they are value-based or policy-based, towards the bottom are more specific methods.</p>

<p>There is more than one way to categorise and split RL algorithms, and it would be messy to try and include all the ways that they relate to each other. Just bear in mind that it is a very rough guide.</p>

<p>The main difference between value-based and policy-based methods is:</p>

<ul>
<li><p><strong>Value-based</strong> methods learn a value function (by interacting with the environment or a model of it), and consider the optimal policy to be one that takes actions with maximum value.</p></li>
<li><p><strong>Policy-based</strong> methods learn a policy function directly, and many can do so without considering the estimated value of a state or action (although they may still need to calculate individual values or <em>returns</em> within episodes).</p></li>
</ul>

<p>Note Actor-Critic is linked to both headings, as it learns both a policy function (the ""actor"") and a value function (the ""critic"").</p>

<blockquote>
  <p>I would like to know is it correct to say: Policy Optimization learns policies to make better actions with higher probability?</p>
</blockquote>

<p>Yes that is broadly correct, although you don't define ""better"". A policy function typically returns some probability distribution over possible actions for any given state. When learning, it will tend to increase probabilities of actions that resulted in better returns (discounted sums of rewards) and reduce probabilities of actions that did not. This may be a very random, high variance process though, depending on the environment.</p>

<p>There are exceptions. Some policy-based methods learn a deterministic policy <span class=""math-container"">$a=\pi(s, \theta)$</span>, and adjust the value based on adding some noise to this function to explore and adjusting the function based on the results of the different action. These don't behave like your statement (because there is no probability to make higher).</p>

<blockquote>
  <p>Also, what is the location of Proximal Policy Optimization in the picture?</p>
</blockquote>

<p>Proximal Policy Optimization is definitely a policy-based method, and it does use an estimate of a value function too for updates (in this case a value called <em>advantage</em> which you also see used in Advantage Actor-Critic). </p>

<p>In the diagram I would probably place it <em>under</em> Actor-Critic Methods in a new row as a specific example of one. However, it does have some significant variations from ""vanilla"" Actor-Critic, based on how it restricts large changes to the policy function.</p>
","0","2","836","25573"
"41214","<p>Both courses are correct. </p>

<p>For the output layer only, and depending on which activation function and which loss function are in use, sometimes the effects of <span class=""math-container"">$f'(z^{(L)})$</span> are perfectly cancelled out by the calculation of <span class=""math-container"">$\frac{\partial J}{\partial \hat{y}}$</span></p>

<p>This is often arranged deliberately to simplify calculations. So:</p>

<ul>
<li><p>MSE loss <span class=""math-container"">$(\hat{y} - y)^2$</span> is paired to linear activation function <span class=""math-container"">$f(x) = x$</span></p></li>
<li><p>Binary cross entropy loss <span class=""math-container"">$-y\text{log}(\hat{y}) - (1-y)\text{log}(1-\hat{y})$</span> is paired to sigmoid activation function <span class=""math-container"">$f(x) = \frac{1}{1+e^{-x}}$</span></p></li>
<li><p>Multi-class cross entropy loss <span class=""math-container"">$-\mathbf{y}\text{log}(\hat{\mathbf{y}})$</span> is paired to softmax activation function <span class=""math-container"">$f(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$</span></p></li>
</ul>

<p>In all these cases, the end result is that, for the output layer logits only:</p>

<p><span class=""math-container"">$$\frac{\partial J}{\partial z^{(L)}} = \hat{y} - y$$</span></p>

<p>Each of these results can be verified with some differentiation. E.g. it is possible to show that <span class=""math-container"">$\frac{d}{dx}(-y\text{log}(\hat{y}) - (1-y)\text{log}(1-\hat{y})) = \frac{1}{\hat{y}(1-\hat{y})}$</span></p>

<p>For all the other layers, you absolutely need to include the factor of <span class=""math-container"">$f'(z^{(l)})$</span> for whatever activation function is in use on the layer you just back propagated through.</p>
","0","2","836","25573"
"43197","<p>Yes embedding works on unseen images. That is the point of the approach - instead of learning to classify each face in the training set directly, the model learns a vector that is intended to be:</p>

<ul>
<li><p>far apart when the face itself is different</p></li>
<li><p>similar when the face is the same, but other items in the image - e.g. background, hair style etc - are different</p></li>
</ul>

<p>In order to use embeddings to recognise someone, you need one or more sample images of that person, so that you can generate a target embedding. Then you can predict any new image that generates a new embedding close enough to the target embedding(s) to be an image of the same person.</p>

<p>This can still suffer usual problems of overfitting to training set. So it is important to understand what training data was used for the model. Embeddings generated from images that are very different from the training data will not be as reliable as embeddings of images that are similar to the training data.</p>
","1","2","836","25573"
"44318","<p>Both Q learning and Value Iteration (a DP technique) use similar update rules based on Bellman optimality equations:</p>

<p><span class=""math-container"">$$v_*(s) = \text{max}_{a}\sum_{s',r} p(s',r|s,a)(r + \gamma v_*(s'))$$</span></p>

<p><span class=""math-container"">$$q_*(s,a) = \sum_{s',r} p(s',r|s,a)(r + \gamma\text{max}_{a'}q_*(s',a'))$$</span></p>

<p>The main difference is that DP uses an explicit model. DP requires that you know <span class=""math-container"">$p(s',r|s,a)$</span>. The update rule for DP is literally the first equation turned into an update rule:</p>

<p><span class=""math-container"">$$v_{k+1}(s) = \text{max}_{a}\sum_{s',r} p(s',r|s,a)(r + \gamma v_{k}(s'))$$</span></p>

<p>In comparison, Q learning does not require knowing <span class=""math-container"">$p(s',r|s,a)$</span>, as it is based on <em>sampling</em> from experience. The update rule is modified to be based on samples of observed data, which have the same values in expectation, as if you had used <span class=""math-container"">$p(s',r|s,a)$</span>, but without knowing it:</p>

<p><span class=""math-container"">$$Q_{k+1}(S_t,A_t) = Q_{k}(S_t,A_t) + \alpha(R_{t+1} + \gamma\text{max}_{a'}Q_k(S_{t+1},a') - Q_{k}(S_t,A_t))$$</span></p>

<p>This is still an important difference even when both systems are run on an internal model/simulation. DP does not need to simulate anything, it iterates over the model directly. Whilst Q learning needs to work with sampled transitions - they might be simulated, but this is not the same as iterating over all states as in DP. It can often be the case that it is easier to simulate the environment than to calculate <span class=""math-container"">$p(s',r|s,a)$</span> for the full model. </p>

<p>Which should you choose:</p>

<ul>
<li><p>Choose Dynamic Programming when you have access to the full state transition and reward model in a simple form (i.e. you have <span class=""math-container"">$p(s',r|s,a)$</span> or equivalent), and the state space is not too large - ideally the number of states is small enough to fit in memory. However, there are ways to use DP when you have a larger state space, by modifying which states it processes. So you still can use DP on larger problems if you really want to.</p></li>
<li><p>Choose Q learning when you don't have a model, or when the state space is too large to iterate over in full.</p></li>
</ul>
","2","2","836","25573"
"45560","<p>Your second function is <em>not</em> an update rule, it is just a re-statement of the approximation between using finite differences to measure a gradient and the infinitesimal ones used in calculus. Most importantly, <span class=""math-container"">$s$</span> is not a learning rate or parameter that you are free to change. It is, by definition, the (negative of the) change observed in <span class=""math-container"">$C$</span> when you alter <span class=""math-container"">$w$</span> to <span class=""math-container"">$w'$</span>.</p>

<p>The second equation you have derived is the same as:</p>

<p><span class=""math-container"">$$w + \Delta w \approx w + \Delta C \frac{dw}{dC}$$</span></p>

<p>In addition, for some reason you decided to drop the approximation symbol partway through and treat your manipulations as if you had created a new weight update rule. Be careful when manipulating expressions on either side of an approximation, they do not always follow the same rules as for an equality, and treating them as such can lead you down some false paths.</p>
","0","2","836","25573"
"46260","<p>In the Rainbow approach, theoretical correctness of the off-policy return values is completely ignored, and it just uses:</p>

<p><span class=""math-container"">$$G_{t:t+n} = \gamma^n\text{max}_{a'}[Q(S_{t+n},a')] + \sum_{k=0}^{n-1} \gamma^{k}R_{t+k+1}$$</span></p>

<p>It still works and improves results over using single-step returns.</p>

<p>They rely on a few things for this to work:</p>

<ul>
<li><p><span class=""math-container"">$n$</span> is not large, compared to amount of variability in the environment that occurs over those steps. So the return is usually correct even if there was an exploratory action taken. Many game events (and also real-world physics) have some fuzziness - e.g. as long as in Pong you hit the ball with a part of the paddle, it will be a reasonable result, and only some (literal) edge cases will cause a major difference in rewards.</p></li>
<li><p>Exploration is low (<span class=""math-container"">$\epsilon$</span> set to <span class=""math-container"">$0.01$</span> or <span class=""math-container"">$0.001$</span>)</p></li>
<li><p>Rewards are sparse, so the n-step return very often only depends on state progression. As long at <span class=""math-container"">$S_{t+n}$</span> is as likely to be reached starting from <span class=""math-container"">$S_t$</span> under both the exploratory and target policies, then the simple returns are approximately correct.</p></li>
<li><p>The policy changes slowly, maybe taking a million frames to converge to optimal, which is far larger than the replay memory.</p></li>
</ul>

<p>From a theoretical basis in Q learning, it would be better to use ""up to n-step"" returns that truncate at the time step when the action taken diverges from the  current learned policy. However, that is more complex, as you have to assess the learned policy at each of the n steps in order to decide when to truncate - I suspect given the researchers know how <span class=""math-container"">$Q(\lambda)$</span> works, that they may have tried this and just found the extra correctness was not worth the extra complexity and CPU cost. In other environments it may be worth the effort to implement this.</p>
","5","2","836","25573"
"47066","<blockquote>
  <p>One argument is that we generally don't know, a priori, what will be the best solution to an RL problem. So by tweaking the reward function, we may bias the agent towards what we think is the best approach, while it is actually sub-optimal to solve the original problem.</p>
</blockquote>

<p>This is the main issue.</p>

<blockquote>
  <p>Am I missing anything?</p>
</blockquote>

<p>I think you are missing the main point of setting a reward function. It should be a value that is maximised by an agent achieving the <em>goals</em> it has been set. Each time you change the reward function you may be explicitly setting new and different goals.</p>

<p>Changing a reward function should not be compared to feature engineering in supervised learning. Instead a change to the reward function is more similar to changing the objective function (e.g. from cross-entropy to least squares, or perhaps by weighting records differently) or selection metric (e.g. from accuracy to f1 score). Those kinds of changes may be valid, but have different motivations to feature engineering.</p>

<p>Tweaking a reward function is also called <a href=""https://arxiv.org/abs/1903.02020"" rel=""nofollow noreferrer"">reward shaping</a>, and sometimes it can have good results. It does come with the same risks as above, but if done carefully it can improve learning rate. For example if to achieve the main goal A, it is absolutely necessary to achieve B and C as interim steps, then it should be OK to reward achieving B and C - typically the things you have to worry about is whether it is possible for the agent to repeatedly achieve B or C via some loop through states - so you may need to add to the state vector whether it has achieved B or C enough times and only grant reward for <em>first</em> visit to B and C. </p>

<p>Feature engineering does still exist in RL, and is about how you represent state and action spaces. For instance, in a chase scenario where an agent needs to get close to a moving target, you should find that representing the state as polar co-ordinates of the target from the current agent's position is far easier for RL to learn optimal policy than if you represent state as cartesian co-ordinates of agent and target.</p>
","3","2","836","25573"
"47942","<blockquote>
  <p>Where the game theory is applied when it comes to reinforcement learning?</p>
</blockquote>

<p>It is not used directly in this case, and AlphaStar makes no breakthroughs in game theory. The blog's wording here is not super precise.</p>

<p>The point of the quote was to explain the extra challenge, which occurs in many games where opponents can react to each other's choices and there is often a counter-strategy to any given policy. Rock-paper-scissors is the simplest game that has this challenge, but it is common in many strategy games, as the game designers typically don't want a single best strategy to dominate the game, often going to some lengths to balance options in the game so that more of their game content is used and to keep a level of uncertainty and excitement in the game-playing community.</p>

<p>The actual breakthroughs in regards to the quote in your question, are in finding ways to perform the kinds of long-term exploration that allow for different high-level strategies. Many RL algorithms perform relatively local exploration which would be too weak to keep track of entirely different strategies and decide when to use them.</p>

<p>The way that Deep Mind team approached this <a href=""https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/"" rel=""nofollow noreferrer"">is explained in their blog</a>:</p>

<blockquote>
  <p>To encourage diversity in the league, each agent has its own learning objective: for example, which competitors should this agent aim to beat, and any additional internal motivations that bias how the agent plays. One agent may have an objective to beat one specific competitor, while another agent may have to beat a whole distribution of competitors [ . . . ]</p>
</blockquote>

<p>So Deep Mind have <em>not</em> resolved any of that at a theoretical level, and have not used game theory in any direct sense. However, they have identified the kind of game theory scenario that applies, and have used that in the design, making steps in an engineering sense towards practical solutions.</p>

<p>Other solutions in RL might also apply, such as <a href=""https://arxiv.org/abs/1805.08296"" rel=""nofollow noreferrer"">hierarchical RL</a> for capturing high-level actions as strategies to inform lower-level decisions, or using slow changing noise functions to drive exploration (as opposed to something which changes faster such as epsilon-greedy).</p>

<p>In general, game theory <em>is</em> related to reinforcement learning, in that both construct a formal view of optimising utility:</p>

<ul>
<li><p>Game theory is useful for analysing multi-agent scenarios, but generally analyses optimal policies for relatively simple single-step or repeated games.</p></li>
<li><p>Reinforcement learning is well-described for single agents, and deals well with sequential decision making, but does not have much quite as much material for dealing with competitive and co-operative multi-agent environments - typically treating other agents as ""part of the environment"". </p></li>
</ul>

<p>There is enough cross-over between the two theories that they can be used to inform each other in an intuitive way, as Deep Mind have done here. </p>

<p>In more tractable game environments, game theory is able to determine stable and effective policies - for instance in rock-paper-scissors, the Nash equilibrium policy (one which players will be punished for moving away from) is randomly selecting each action with 1/3 probability. Note this is not necessarily the optimal policy - that depends on the opponent's behaviour - but it is an expected stable outcome for two rational and capable opponents to arrive at. </p>

<p>If you develop a rock-player-scissor learning bot using RL, and it learns this strategy through self play, then you can be relatively happy that your learning algorithm worked. That would be one way of using RL and game theory together. </p>

<p>Deep Mind don't know the Nash equilibrium of Star Craft strategies, and in fact the strategies are only loosely defined in terms of low-level actions, so it is not clear whether it is possible. The analysis of strategies supplied in the blog (e.g. a ""rushing"" strategy) are based on observations of the game and adding a human narrative to help understand what is going on. In practice, it is the sampling of opponents each preferring a different strategy or set a particular goal in the game, that trains a single neural-network based bot that has experience of countering multiple strategies and can express actions that optimally beat any strategy that matches patterns it has learned in self-play and observes an opponent using.</p>
","3","2","836","25573"
"48910","<p>Your description doesn't seem so much like a ""recurrent action"" as a ""failed to progress"" situation.</p>

<p>The simplest ways to deal with possibility of a blocked/failed action are:</p>

<ul>
<li>Leave state and time step unchanged and pick next best action, without any new policy evaluations. This is effectively the approach used by many turn-based game playing RL, such as AlphaGo, although the rejection of non-valid actions is done using the game engine and not technically tried repeatedly in the environment, that does not make a big difference in all cases.</li>
</ul>

<p>Or</p>

<ul>
<li>Update the time step, receive a reward (maybe zero) and try again on next time step. The state information <em>might</em> usefully include blocked actions (or they can be applied programmatically as in the first option). You would typically use this approach if there was a real cost to attempting the action (i.e. a negative reward for losing time) and/or it is possible from the state information to <em>predict</em> which actions could be blocked, and you want to include these factors in the agent's behaviour.</li>
</ul>

<p>The second option is the approach you might use in a maze solver, if it could only sense walls by bumping in to them.</p>

<p>If the blocked actions are completely unpredictable from the agent's observations, then it doesn't matter too much which approach you use. However, if there is a cost for failing to act, then the second approach would give you more accurate action values in case you are using a value-based approach or need to assess the performance of your agent numerically.</p>
","1","2","836","25573"
"49382","<blockquote>
  <p>I am wondering why only a single Q value is learned? </p>
</blockquote>

<p>It is not a factor here, but it matches the Q-learning theory as written better to model <span class=""math-container"">$\hat{q}(s,a,\theta) \approx Q(s,a)$</span>. </p>

<p>It is not numerically superior, and often less efficient than modelling a single network with multiple action outputs. However, it is clearly valid to approximate the action value function directly.</p>

<p>If the policy is separate to Q values, and you are using Q-learning to <em>predict</em> the values of that policy (as opposed to using Q-learning to <em>control</em> and find an optimal policy) then a single action value can be more efficient. </p>

<blockquote>
  <p>If this is the case, how do we determine the action for the next step?</p>
</blockquote>

<p>If you want to drive a best guess at an optimal policy from action values <span class=""math-container"">$Q(s,a)$</span>, then typically you calculate </p>

<p><span class=""math-container"">$$\pi(s) = \text{argmax}_a Q(s,a)$$</span> </p>

<p>This rule is not dependent on a neural network architecture, but the implementation does vary:</p>

<ul>
<li><p>If your neural network outputs all action values at once from the state, <span class=""math-container"">$f_{nn}(s) \rightarrow [\hat{q}(s,a_0), \hat{q}(s,a_1), \hat{q}(s,a_2)...]$</span> then you perform a single forward pass of the network and find the maximising action inside a single output vector.</p></li>
<li><p>If your neural network outputs a single action value from a state, action <span class=""math-container"">$f_{nn}(s,a) \rightarrow \hat{q}(s,a)$</span> then you must run it for each possible action (typically in a minibatch) and find the maximising action across all outputs.</p></li>
</ul>

<p>Unless you have some other way to generate the policies (e.g. you are using a policy network and policy gradients, the Q value is then for a baseline in something like an Actor-Critic agent), then you must collect all action values from each state in order to find the maximising action. There is no easy way around this, and the paper you link does not have a clever solution to it.</p>

<blockquote>
  <p>It seems that people think that make action as input is an effective way to ease the large action space problem. [From comments]</p>
</blockquote>

<p>It can be, but not in the way you suppose. </p>

<p>Instead, the nature of the action representation is critical. In your question, you said ""<em>you can ignore this</em> but each action is a combination of the vector <span class=""math-container"">$c_t^1$</span> to <span class=""math-container"">$c_t^3$</span>"" (emphasis mine). However, this is not an ignorable detail, and is an important part of the paper. </p>

<p>By compressing the action representation into a small number of scalar traits, the authors allow the neural network to generalise between similar actions. For example if one action is represented by <span class=""math-container"">$c_t^1 = 0.5, c_t^2 = -0.3, c_t^3 = 0.25$</span> and another action is represented by <span class=""math-container"">$c_t^1 = 0.7, c_t^2 = -0.3, c_t^3 = 0.25$</span>, then an 
accurate value for the first action may result in at least an approximately accurate value for the second action, even if it has never been taken before in the current state. </p>

<p><em>Caveat: whether or not this is true or useful depends on the specifics of the problem being solved. It will not always be true, or be useful to transform the action space in this way. Actions with similar vectors need to be similar in some way that affects the outcome.</em></p>

<p>However, assuming it is true, the network design gives you an advantage in learning rate. Things that the agent learns about action values will be generalised more efficiently than a simpler network that enumerates each possible action. That makes the agent more <em>sample efficient</em>, it will need less experience to reach near optimal behaviour. It is this sample efficiency that the paper you linked is referring as an improvement to original DQN.</p>

<p>None of this changes the need to iterate over all possible actions when you want to evaluate the policy using DQN in this case.</p>

<p><strong>TL;DR: The agent in the paper still has to <em>evaluate</em> all actions in each state in order to choose the best one. However, it does not have to <em>try</em> them all in order to discover which is best in the long term.</strong></p>
","7","2","836","25573"
"49521","<p>You can still define state value functions <span class=""math-container"">$v(s)$</span>, action value functions <span class=""math-container"">$q(s,a)$</span> and policy functions <span class=""math-container"">$\pi(s)$</span> or <span class=""math-container"">$\pi(a|s)$</span> when the state <span class=""math-container"">$s$</span> is from a very large or continuous space. Reinforcement Learning (RL) is still a well-defined problem in that space.</p>

<p>What becomes harder is iterating through the state space. That rules out two simple approaches:</p>

<ul>
<li><p>Tabular methods - that store lists of all states with the correct action or value.</p></li>
<li><p>Any method that needs to iterate through all states, e.g. the dynamic programming methods Policy Iteration or Value Iteration.</p></li>
</ul>

<p>These are important methods for RL. With tabulation and assuming you can iterate through all possibilities, then you can <em>prove</em> that you will find the optimal policy. </p>

<p>However, RL methods can still work with large state spaces. The main method to do so is to use some form of function approximation, which then <em>generalises</em> the space so that knowledge learned about a single state is used to assess <em>similar</em> states. </p>

<p>Function approximation can simply be discretising the space to make the numbers more manageable. Or you can use a parametrisable machine learning approach, such as neural networks. The combination of neural networks with reinforcement learning methods is behind the ""deep"" reinforcement learning approaches that have been subject of much recent research.</p>

<p>If you use any function approximation with RL, then you are not guaranteed to find the most optimal policy. Instead you will find an approximation of that policy. However, that is often good enough for purpose.</p>

<p>To answer the questions more directly:</p>

<blockquote>
  <p>What will be the policy if the state space is continuous in Reinforcement learning</p>
</blockquote>

<p>There is no change at the theoretical level. You can express the policy as <span class=""math-container"">$\pi(s)$</span> for a deterministic policy, or <span class=""math-container"">$\pi(a|s)$</span> for a stochastic policy, regardless of the space of <span class=""math-container"">$s$</span>.</p>

<p>At the implementation level, you will need to implement a parametric function that takes <span class=""math-container"">$s$</span> as one of its inputs. The function parameters <span class=""math-container"">$\theta$</span> are what is learned. For instance if you use an action value based method such as Q-learning, then you will create an approximation to <span class=""math-container"">$Q(s,a)$</span> - in the literature you may see this directly represented as <span class=""math-container"">$\hat{q}(s,a,\theta) \approx Q(s,a)$</span></p>

<p>Using a neural network for <span class=""math-container"">$\hat{q}(s,a,\theta)$</span> is one common way to achieve this, where the neural network's weight and bias values are in <span class=""math-container"">$\theta$</span>.</p>

<blockquote>
  <p>What if the state space is continuous, will the agent have information of every possible state in the state space? </p>
</blockquote>

<p>Depends what you mean by ""have information"". The agent cannot possibly store <em>separate</em> data about each state. However, it may have information about similar states, or store its knowledge about states in a more abstract fashion (such as in the parameters <span class=""math-container"">$\theta$</span>)</p>

<blockquote>
  <p>Also will an RL agent be able to take decision if its in a new state that it has not encountered during training ?</p>
</blockquote>

<p>Yes. For this to work well with function approximation, it relies on successful generalisation between similar states. So it is important that the state space representation works towards this. For instance, if two states are close together in the state space representation you use, it should be expected that value function and policy functions are often similar - not always, the function can have arbitrary shape, but trying to learn effectively random mapping would be impossible.</p>
","3","2","836","25573"
"51080","<p>You are mixing up two concepts from reinforcement learning, reward and return (aka utility)</p>

<ul>
<li><p>Rewards are used to identify or specify goals of the agent. Whilst you can change them to help an agent focus on useful heuristics of the problem, it is more usual, especially in test/toy problems to have them very simple. In the case of CartPole, there is a positive reward for ""not falling over"" which importantly ends when the episode ends.</p></li>
<li><p>Returns (or utility) are what the agent learns to maximise. A return is typically the sum of all rewards, and might be discounted to prevent infinite results. In the case of CartPole, this means that the longer the agent can balance the pole into the future, the larger the return is.</p></li>
</ul>

<p>With Q-learning, the action values predict the expected future <em>return</em>. So it doesn't matter that the rewards are dense. It matters how long the agent can keep balance going into the long term, the longer the better, because the return will be higher. A combination of state and actions that the agent associates with longer-lasting not failing will predict a larger return and be chosen in preference to shorter-term success. This is how the Q-learning agent handles a situation with dense positive rewards that may end on a mistake.</p>

<hr>

<p>In practice, the OpenAI Gym CartPoleV0 environment does take a small liberty. Episodes will end at a fixed step in future. This is not available in the state information, and technically makes the problem non-Markov. However, it is possible to get away with this provided the timespan for maintaining stability is shorter than the maximum possible episode.</p>
","1","2","836","25573"
"51385","<blockquote>
  <p>If we forget about health for a second and we look at position alone, we have 6 players, each of which could be in any of the 100 locations so our state space for position alone would be 100^6.</p>
</blockquote>

<p>Yes, that is correct, adding health in, and assuming it was an integer from 1 to 20, then you would have <span class=""math-container"">$20^6 \times 100^6$</span> discrete states.</p>

<blockquote>
  <p>My understanding is that you would need to enumerate all possible states for this game in order to successfully implement Q-learning. How would this be done?</p>
</blockquote>

<p>For the enumerated case with enough resources, there is a theoretical guarantee that Q learning (and other RL methods) will converge on the optimal result. However, in this case - and many others - there are clearly too many states to calculate or store values for in detail. So the answer is that you don't do that. Instead you need look into ways to calculate <em>approximate</em> calculating action values which you hope will also <em>generalise</em> to previously unseen state/action pairs, at the very least to unseen states and actions that are relevant in real games.</p>

<p>There are many ways to structure the state and action representations to help with approximation in RL. Sutton &amp; Barto's <a href=""http://incompleteideas.net/book/the-book-2nd.html"" rel=""nofollow noreferrer"">Reinforcement Learning: An Introduction</a> dedicates the whole of the second (of three) sections to approximation in general, and chapter 9 part 5 reviews some popular approaches to representations that work well with simpler linear function approximation.</p>

<p>This kind of approximation is also the main driver behind ""Deep"" Q-learning (DQN), which is essentially Q-learning with a neural network approximating the action value table. There are some details to add to get this to work well - but plenty of resources available to help with this on the internet: Tutorials, examples and fully coded agents ready to set up and solve problems.</p>

<p>So I would recommend moving to DQN when the state/action space gets too large for a simple tabular approach, and where it is not clear what other simpler approximation could get the job done more efficiently. If you do that, then the state and action representation becomes what will work as inputs to a neural network. There are some rules to follow for that. First, all inputs should be on a suitable scale. Neural networks like to work with inputs with mean 0, standard deviation 1 over the sample population. Often you don't know that for a RL problem, so you can go to the next best thing - scaling to fit in a range such as 0..1 or -1..1</p>

<p>I have also written an answer covering approaches to choices in state representation and feature engineering for another problem  <a href=""https://datascience.stackexchange.com/questions/39052/reinforcement-learning-easily-learnable-state-representation?rq=1"">Reinforcement learning: easily learnable state representation</a> which might help.</p>
","1","2","836","25573"
"51420","<blockquote>
  <p>if the length and height of the rectangle are random, as well as the starting position and the location of the Treasure, how can the bot apply the knowledge acquired to the new problem? </p>
</blockquote>

<p>You have two possible approaches here, depending on how the problems are being presented to you:</p>

<ul>
<li><p>If the agent has time to learn/plan on each environment separately, then you need an agent that has capability to learn each environment. A simple tabular Q-learning agent already has this capability, up to a certain size of problem (where the number of states and actions would fit in memory and can be iterated over in simulation enough times). Beyond that size, provided you can come up with a fixed feature set capable of representing any shape and size of problem that the agent could be presented with, and used e.g. DQN or other approximation technique, then you still have a generalised <em>learning</em> bot.</p></li>
<li><p>A bot that is <em>generalised</em> during training to attempt to solve new instances with variation needs to be trained with many variations and more state data. If shape, size and layout of the maze can change between episodes, then this data must become part of the state. This can expand the state space a lot, and requires different representations. A simple representation of grid spaces would be actual map of the grid as a rectangular ""image"", assuming the agent start, walls and goal position could be almost anywhere within the space. If the maze has lots of objects, then you can put each object type into a separate ""channel"" and use a Convolutional Neural Network as part of the Q function approximation. If the area is more sparse with just a few objects (e.g. just the agent, a single blocker and the treasure) then it would be easier to use a direct vector representation of positions of the objects and use a fully connected neural network.</p></li>
</ul>

<p>For a toy problem where key positions can change between episodes, yet this still can be solved easily by a tabular agent, see <a href=""https://gym.openai.com/envs/Taxi-v2/"" rel=""nofollow noreferrer"">Open AI's Taxi-V2</a> which is an implementation of a classic control problem where the locations of a sub-goal and goal are randomised on each episode.</p>
","2","2","836","25573"
"53099","<blockquote>
  <p>During the derivation of the Bellman Equations, when the expected cumulative rewards are calculated on an infinite horizon, meaning <span class=""math-container"">$t \to \infty$</span>, shouldn't they be equal for different time steps; like <span class=""math-container"">$V^{\pi}(s_i=s) = V^{\pi}(s_{i+\Delta i}=s), \forall s$</span>?</p>
</blockquote>

<p>Yes, provided the state <span class=""math-container"">$s$</span> has the <a href=""https://en.wikipedia.org/wiki/Markov_property"" rel=""nofollow noreferrer"">Markov property</a>, which means it contains all relevant data about immediate state progression and reward, then this is true. I would read your equations as over-specified in fact, and often see the equations written differently:</p>

<p><span class=""math-container"">$$v_{\pi}(s) = \mathbb{E}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t \sim \pi]$$</span></p>

<p>although there are several roughly equivalent ways to describe value functions in RL (for instance, and trivially, you could write <span class=""math-container"">$\mathbb{E}_{\pi}$</span> for the expectation which already implies the condition <span class=""math-container"">$A_t \sim \pi$</span>). The one in this answer makes it clear that the value of the state <span class=""math-container"">$s$</span> is independent of the time step on which it is measured.</p>

<p>It is worth noting that although the <em>expectation</em> is always the same and independent of <span class=""math-container"">$t$</span>, that actual returns seen can be different, and if you know from some other context that you are seeing the first, or last visit to a state - or from the time step whether this is more or less likely given <span class=""math-container"">$\pi$</span> - then that may change the expectation when loops are possible.</p>
","3","2","836","25573"
"53807","<p>Your biggest issue with the evaluation scheme you have - ""success"" means within tolerance, ""failure"" means outside tolerance, plus your constraint on model outputs needing to vary per time step - is that it will be hard to extract gradients in order to train the prediction model directly. This rules out many simple and direct regression models, at least if you want to use ""maximise number of scores within tolerance"" as your <em>objective function</em>. The constraints on sequential predictions and allowing re-tries are also non-differentiable if taken as-is.</p>

<p>I think you have two top level choices:</p>

<h2>1. Soften the loss function, and add the hard function as a metric</h2>

<p>Use a differentiable loss function that has best score when predictions are accurate and constraints are met. For example your loss function for a single predicted value could be </p>

<p><span class=""math-container"">$$L(\hat{x}_n, \hat{x}_{n+1}, x_{n+1}) = (\hat{x}_{n+1} - x_{n+1})^2 +  \frac{a}{1+e^{s(|\hat{x}_n - \hat{x}_{n+1}| - \epsilon)}}$$</span></p>

<p>the second constraint part is essentially sigmoid with <span class=""math-container"">$a$</span> controlling the relative weight of meeting constraints with accuracy of the prediction and <span class=""math-container"">$s$</span> controlling the steepness of cutoff around the constraint.</p>

<p>a. The weighting between prediction loss and constraint loss will be a hyper-parameter of the model. So you would need to include <span class=""math-container"">$a$</span> and <span class=""math-container"">$s$</span> amongst parameters to search if you used my suggested loss function.</p>

<p>b. You can use your scoring system, not as an objective function, but as a metric to select the best model on a hyper-parameter search.</p>

<p>c. With this approach you can use many standard sequence learning models, such as LSTM (if you have enough data). Or you could just use a single step prediction model that you feed current prediction plus any other features of the sequence that is allowed to know, and generate sequences from it by calling it repeatedly.</p>

<p>This system should encourage re-tries that get closer to the true value.</p>

<h2>2. Use your scoring system directly as a learning goal</h2>

<p>This will require some alternative optimising framework to gradient descent around the prediction model (although some frameworks can generate gradients internally). Genetic algorithms or other optimisers could be used to manage parameters of your model, and can attempt to change model parameters to improve results.</p>

<p>For this second case, assuming you have some good reason to want to avoid constructing a differentiable loss function at all, then this problem can be framed as <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow noreferrer"">Reinforcement Learning (RL)</a>:</p>

<ul>
<li><p>State: Current sequence item prediction (or a null entry), as well as any known information such as tolerance, length of sequence, current sequence item value (which may be different from current prediction) <span class=""math-container"">$\epsilon$</span>, <span class=""math-container"">$d$</span>, <span class=""math-container"">$\mu$</span> or <span class=""math-container"">$\sigma$</span> can be part of the current state.</p></li>
<li><p>The action is to select next sequence value prediction, or probably more usefully, the offset for the next sequence item value. Using offsets allows you easily add constraint for minimum <span class=""math-container"">$\epsilon$</span></p></li>
<li><p>The reward is +1 for being within tolerance or 0 otherwise.</p></li>
<li><p>Time steps match the time steps within a current sequence. </p></li>
</ul>

<p>You can use this to build a RL environment and train an agent that will include your prediction/generator model inside it. There are a lot of options within RL for how to manage that. But what RL gives you here is a way to define your goal formally using non-differentiable rewards, whilst internally the model can still be trained using gradient based methods.</p>

<p>The main reason to not use RL here is if the prediction model must be assessed at the end of generating the sequence. In which case the ""action"" might as well be the whole sequence, and becomes much harder to optimise. It is not 100% clear to me from the question whether this is the case. </p>

<p>Caveat: RL is a large and complex field of study. If you don't already know at least some RL, you can expect to spend several weeks getting to grips with it before starting to make progress on your original problem. </p>

<p>There are alternatives to RL that could equally apply, such as <a href=""https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies"" rel=""nofollow noreferrer"">NEAT</a> - deciding which could be best involves knowing far more about the project (e.g. the complexity of the sequences you wish to predict) and practical aspects such as how much time you have available to devote to learning, testing and implementing new techniques.</p>

<h2>Have you forgotten something?</h2>

<p>If you allow infinite re-tries, then an obvious strategy is to generate a very large sequence moving up and down using different step sizes (all greater than <span class=""math-container"">$\epsilon$</span>). This doesn't require any learning model, just a bit of smart coding to cover all integers eventually. Chances are this model is only a few lines of code in most languages.</p>

<p>If this is to be ruled out, then some other rule or constraint is required:</p>

<ul>
<li><p>Perhaps only positive increments are allowed in the predicted sequence (so we cannot re-try by subtracting and trying again)? This conflicts with your ""unlimited predictions"" statement.</p></li>
<li><p>Perhaps a sub-goal here is to make the guessing efficient? In which case RL could be useful, as you can add a a <em>discount factor</em> to reward processing in order make the model prefer to get predictions correct sooner.</p></li>
</ul>
","1","2","836","25573"
"54284","<p>There are many variations of reinforcement learning methods. Some are tweaks to existing algorithms, some are radically different. The problems that you are concerned about with your analysis and suggestion appear to be:</p>

<ul>
<li><p><em>Credit assignment</em>. Whether or not a particular state or state action pair earlier in a trajectory is important to receiving a reward later on, and thus should have any value update due to new experience. This is a core problem to reinforcement learning (RL). All RL algorithms solve it, the differences are in how they deal with levels of difficulty.</p></li>
<li><p><em>Computational efficiency</em>. Whether the algorithm makes best use of CPU cycles and doesn't waste its time calculating many updates of zero in an environment with sparse rewards.</p></li>
<li><p><em>Sample efficiency</em>. Whether the algorithm makes best use of available data to make the most robust predictions of values it can.</p></li>
</ul>

<p>Single-step Q learning does address all of these issues to at least some degree:</p>

<ul>
<li><p>For credit assignment, the single step bootstrap process in Q learning will backup estimates through connected time steps. It takes repetition so that the chains of events leading to rewards are updated only after multiple passes through similar trajectories.</p></li>
<li><p>For computational efficiency, it is not always clear what the best approach would be. Single step Q learning updates are efficient to calculate, so even though each one only updates a small segment of a trajectory, this can be made up, in some environments, by running more updates in the same CPU time.</p></li>
<li><p>For sample efficiency, single-step updates on their own are not very good - they can leave many values untouched even though logically they could be updated based on new information. However, there are other approaches to address this - for instance using an experience replay table or Dyna-Q planning can take advantage of previous states and actions to re-calculate Q values without needing to see more experience.</p></li>
</ul>

<p>As usual with ML problems, it is not always clear what adjustments and variations of an algorithm may work best. So you need to experiment with your environment.</p>

<p>You also mention this:</p>

<blockquote>
  <p>Until the agent finds the cell leading to a positive reward, the agent gets small or no rewards.</p>
</blockquote>

<p>This is somewhere that single-step updates can work well. In your grid world example, if you are training to minimise the number of time steps to complete an episode, then typically you will have a negative reward associated with each time step that does not complete an episode. In that case, updating your Q estimates immediately on each action can be a large benefit - it encourages exploration by progressively scoring repetition worse (as the initial agent blunders around randomly) - meaning that the agent tries to find new states and actions that are still scored at original default levels. You won't get that searching behaviour if you wait until the end of the first episode before updating the value tables.</p>

<p>Whether or not this is an issue will depend on the environment, but such time related goals are relatively common, so it is worth bearing in mind.</p>

<blockquote>
  <p>Alternative approach
  My question is: why can't we update all the Q-values related to the agent at the end of the episode? </p>
</blockquote>

<p>This is a well known common approach in RL. The simplest variant is called Monte Carlo Control, and it uses trajectories from whole episodes to update values of all state/action pairs seen in those trajectories. </p>

<p>More sophisticated approaches blend between single-step Temporal Difference learning (of which Q learning is one example) and whole episode Monte Carlo methods. These are n-step methods (n-step Q-learning is one example) and TD(<span class=""math-container"">$\lambda$</span>) methods (Q(<span class=""math-container"">$\lambda$</span>) is one example). These are relatively popular as they combine best features of, and avoid compromises of, the two extremes of whole episode vs single step.</p>

<blockquote>
  <p>Is this approach legit? </p>
</blockquote>

<p>Yes, in fact as described above, variations of it are used routinely.</p>

<blockquote>
  <p>Which problem do we face following this? </p>
</blockquote>

<p>When using whole episodes, you face the following problems:</p>

<ul>
<li><p>High variance. If either your policy or your environment has randomness, then the final trajectory and reward in each episode will vary a lot. Which means to get reliable estimates of value you may need more samples, not less as you hoped.</p></li>
<li><p>Off-policy learning is more complex. Q-learning in particular is an off-policy method, meaning it learns values of its best guess at an optimal policy (called the target policy) whilst still exploring using a non-optimal policy (called the behaviour policy). With single-step updates this difference can be ignored as you always calculate the updated value estimate for an action you've taken (you don't care what policy generated this) using the maximising action for the next step (so you don't use the action <em>actually</em> taken). When you extend to calculating value updates over multiple steps, you need to adjust for differences between behaviour and target policies. If you look at Q(<span class=""math-container"">$\lambda$</span>) algorithm you will see it copes with this by resetting something called the eligibility trace (which you can think of as a vector that explicitly tracks credit assignment) whenever the agent takes a non-optimal action according to the current value estimates. </p></li>
</ul>

<p>There is also Off-policy Monte Carlo Control, which uses <em>importance sampling</em> to decide whether and how much to use particular updates - for learning optimal control typically this will only update from the ends of episodes after the last exploratory action that the agent took.</p>

<blockquote>
  <p>The only drawback I can think of is lack of exploration, that can be solved choosing random actions according to a decreasing epsilon value.</p>
</blockquote>

<p>Usually you will want some initial high exploration that settles down closer to the predicted optimal policy later on. This or something similar should be standard practice in many environments. However, it would not solve the search problem you mention - for that you need an exploration system that is aware of when states and values are repeated themselves over multiple steps <em>within a single episode</em>. There's more than one way to solve that, too, but the single-step Q learning approach is quite nice when it works because it is a natural consequence of the algorithm.</p>
","0","2","836","25573"
"54713","<p>With expected values you have a fair bit of freedom to expand/resolve or not.</p>

<p>For instance, assuming the distributions <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span> are independently resolved (i.e. the values are not correlated):</p>

<p><span class=""math-container"">$$\mathbb{E}[X + Y] = (\sum_x xp(x))+ \mathbb{E}[Y]$$</span></p>

<p><span class=""math-container"">$$\mathbb{E}[XY] = \sum_x xp(x)\mathbb{E}[Y]$$</span></p>

<p>Each time step of a MDP is independent in this way, so you can use this when handling sums and products within expectations in the Bellman equations (provided you separate terms by time step).</p>

<p>For the Bellman equation, the goal is to relate <span class=""math-container"">$v_\pi(s_t)$</span> to <span class=""math-container"">$v_\pi(s_{t+1})$</span>, and the definition of value is given as an expectation, so it makes sense to preserve the second expectation rather than expand it.</p>

<p>Something has to change though, as within the second sum a time step is effectively taken from <span class=""math-container"">$s$</span> to <span class=""math-container"">$s'$</span>, so the new expectation has to include that. It has in some sense been expanded, just not fully broken down into the full product of every following policy decision and state transition.</p>

<p>You could try to write out the full expansion from expected to products of sums over distributions using some container like <span class=""math-container"">$\Pi_{n=t+1}^{T}$</span> - showing how to calculate the expected value over full tree of all possibilities - and the maths would still work. But it would be a very longhand way of showing the same relationship.</p>
","1","2","836","25573"
"54734","<p>Reinforcement learning (RL) is completely based around MDPs, to the point where its definition is essentially ""RL is a collection of algorithms that can learn about action choices within a MDP environment"".</p>

<p>Outside of RL, you can <a href=""https://www3.nd.edu/~pantsakl/Publications/348A-EEHandbook05.pdf"" rel=""nofollow noreferrer"">work with control systems using differential equations more or less directly</a>, and some are solvable analytically. In principle these direct solutions are more robust and require no learning, compared to RL. However, they usually rely on having simple to describe goals - typically static control of maintaining some important value (speed, position, temperature). This works fine for cruise control in cars, thermostats and industrial processes. It also works great for simple environments like <a href=""https://en.wikipedia.org/wiki/Inverted_pendulum"" rel=""nofollow noreferrer"">inverted pendulum</a> which has been solved without RL for decades. </p>

<p>The analytical non-RL approaches are far more trusted than trial-and-error statistical learners such as RL. But they are limited in terms of describing goals and complexity of environments. They start to fail at the level of the <a href=""https://en.wikipedia.org/wiki/Mountain_car_problem"" rel=""nofollow noreferrer"">mountain car environment</a> where the correct action may be to move further from a goal state before moving towards it. Mountain car can still of course be fully described by relatively simple differential equations.</p>

<p>If you have equations for a dynamic system in an environment with goals that are too complex to solve analytically, then you can easily convert to discrete MDP form: Use the equations to simulate the environment, choosing a discrete time step for action choice. There are also RL methods that can work with continuous  control and variable time steps, which would also benefit from such a model. You could use such a model to learn in a simulated environment, or use it to help with planning in a real environment (or have a learning + planning algorithm in a simulated environment).</p>

<p>If you are starting with differential equations, then typically you would convert them into some non-differential form in order to apply them. If you can do this fully analytically - e.g. change something in the form <span class=""math-container"">$a\frac{d^2x}{dt}  + b\frac{dx}{dt} +c = 0$</span> into some <span class=""math-container"">$x = \alpha e^{\beta t}\text{sin}(\gamma t)$</span> - then this would make the most accurate predictions and simulations. Otherwise you can use <a href=""https://lpsa.swarthmore.edu/NumInt/NumIntIntro.html"" rel=""nofollow noreferrer"">some method of approximation</a> in order to resolve the differential equation into something that predicts next state from current state. </p>
","1","2","836","25573"
"55203","<p>In general, the average reward setting replaces the discounted setting in continuous tasks. It relies on there being a long term stable distribution of states under any particular policy (this is called <a href=""https://en.wikipedia.org/wiki/Ergodicity"" rel=""nofollow noreferrer"">ergodicity</a>) - and in this will usually be true for continuous MDPs that don't have absorbing states.</p>

<p>If you see an update rule in the discounted setting that looks like this (for Q learning):</p>

<p><span class=""math-container"">$$Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma\text{max}_{a'}Q(s',a') - Q(s,a))$$</span></p>

<p>Then you replace the discounted TD error by the <em>differential</em> TD error:</p>

<p><span class=""math-container"">$$Q(s,a) \leftarrow Q(s,a) + \alpha(r -\bar{r} + \text{max}_{a'}Q(s',a') - Q(s,a))$$</span></p>

<p>where <span class=""math-container"">$\bar{r}$</span> is the mean reward per time step under the current policy. You can estimate this simply from the rewards seen so far. </p>

<blockquote>
  <p>I searched a few articles, but could not find any practical answer.</p>
</blockquote>

<p>See <a href=""http://incompleteideas.net/book/the-book.html"" rel=""nofollow noreferrer"">Reinforcement Learning: An Introduction (Second Edition)</a> chapter 10, sections 3 and 4 for a more thorough description and more examples.</p>
","1","2","836","25573"
"55863","<blockquote>
<p>It is a finite MDP with states represented as 6 dimensional vectors of integers. The number of discrete values in each index of the state vector varies from 24 to 90.</p>
<p>The action space varies from state to state and goes up to 300 possible actions in some states, and below 15 possible actions in some states.</p>
</blockquote>
<p>In combination, this could be over a billion state/action combinations. It is likely to be too much for running a tabular method over. The dynamic actions, changing depending on what the state is are not an issue, but the total size of such a table probably will be.</p>
<blockquote>
<p>If I could make some assumptions (just for the purpose of testing the model), I could reduce the states to about 400 and actions to less than 200.</p>
</blockquote>
<p>This is a much more tractable number for creating a table. Your total number of records will be at most 80,000.</p>
<p>There are a couple of things worth looking at in more detail:</p>
<h2>Number of training examples needed</h2>
<p>With a tabular method, as well as constructing the table, you need to collect data for each possible state/action combination. If there is some randomness in the environment, then you need to collect data from each possible state/action combination multiple times. For a table with size 80,000 this might be very fast and easy if you have a fast simulation of the environment. However, it might still not be a feasible size if you can only collect information from a single real environment and the time step represents a day . . .</p>
<p>You will need to check your own numbers to figure out whether this is an issue.</p>
<p>Perhaps even the billion plus items for your full description is not an issue if you have a fast computer and lots of memory to store the table.</p>
<h2>Dealing with ragged data in a table</h2>
<p>You don't have to do this. If you have enough memory spare and want to take the easy route, you could just use a tensor description (i.e. allow all values in each dimension, and enumerate them from 0 to whatever size). See next section for how you should limit action choice.</p>
<p>However, it may be more efficient to do something about the ragged data.</p>
<p>The simplest way to do this is to generate a unique ID for each allowed state vector, and use that to reference a value in a nested hash structure, e.g. a Python <code>dict</code> of <code>dict</code>s.</p>
<p>A very simple ID generator would be to concatenate your state vector and action labels into a list, then convert that into a string using some kind of join function. Example in Python:</p>
<pre><code># Empty Q table
q = dict()

# Define an example state, action and value
state = [20,12,56,9,76,30]
action = 176
value = -2.4

# Store an example state, action and value

# This state to state_id conversion should be a re-usable function
state_id = '/'.join([str(x) for x in state])
# Example state_id is '20/12/56/9/76/30'

if state_id in q:
  q[state_id][action] = value
else
  q[state_id] = {action: value}
</code></pre>
<p>The easiest way to manage the Q table with this approach is to add entries as and when you need them. Don't pre-populate with zeros, only write action values when they are needed by your agent. That will mean that you only create table entries for state, action pairs that exist.</p>
<h2>Filtering action values</h2>
<p>You will need code, that you consider to be part of the environment, that knows which actions are valid depending on the current state. This code could return the list of all possible actions. For instance you might define a function <code>allowed_actions(state)</code>. You then use that list when working with the Q table:</p>
<ul>
<li><p>When looking for a maximising action to drive a policy, look up the whole dictionary of action to value (in <code>q[state_id]</code> and <a href=""https://stackoverflow.com/questions/268272/getting-key-with-maximum-value-in-dictionary"">pick the max one</a>)</p>
</li>
<li><p>When you reach a new state that you have not seen before, it may be convenient to populate all possible actions from that state with zeros. Prototype Python code for doing that might look like this:</p>
</li>
</ul>
<pre><code>state_id = '/'.join([str(x) for x in state])
if not state_id in q:
  q[state_id] = { action: 0 for action in allowed_actions(state) }
</code></pre>
","2","2","836","25573"
"56055","<p>This is called ""catastrophic forgetting"" and can be a serious problem in many RL scenarios. </p>

<p>If you trained a neural network to recognise cats and dogs and did the following:</p>

<ul>
<li><p>Train it for many epochs on a full dataset until you got a high accuracy.</p></li>
<li><p>Continue to train it, but remove all the cat pictures.</p></li>
</ul>

<p>Then in a relatively short space of time, the NN would start to lose accuracy. It would forget what a cat looks like. It would learn that its task was to switch the dog prediction as high as possible, just because on average everything in the training population was a dog.</p>

<p>Something very similar happens in your DQN experience replay memory. Once it gets good at a task, it may only experience success. Eventually, only successful examples are in its memory. The NN forgets what failure looks like (what the states are, and what it should predict for their values), and predicts high values for everything. </p>

<p>When something bad happens and the NNs high predicted value is completely wrong, the error can be high, and the NN may have incorrectly ""linked"" its state representations so that it cannot distinguish which parts of the feature space are the cause of this. This creates odd effects in terms of what it learns about values of all states. Often the NN will behave incorrectly for a few episodes but then re-learn optimal behaviour. But it is also possible that it completely breaks and never recovers.</p>

<p>There is lots of active research into catastrophic forgetting and I suggest you search that term to find out some of the many types of mitigation you could use.</p>

<p>For Cartpole, I found a very simple hack made the learning very stable. Simply keep aside some percentage of replay memory stocked with the initial poor performing random exploration. Reserving say 10% to this long term memory is enough to make learning in Cartpole rock solid, as the NN always has a few examples of what not to do. The idea unfortunately does not scale well to more complex environments, but it is a nice demonstration. For a more sophisticated look at similar solutions you could see the paper ""<a href=""http://rll.berkeley.edu/deeprlworkshop/papers/database_composition.pdf"" rel=""nofollow noreferrer"">The importance of experience replay database composition in deep reinforcement learning</a>""</p>
","6","2","836","25573"
"56119","<p><em>Action value</em> and <em>target value</em> in DQN refer to the same thing in terms of what they measure, but are obtained and used in different ways.</p>

<p>An action value does not approximate the <em>reward</em> of a given action, but represents the related concept of <em>expected return</em> - the expected discounted sum of future rewards when taking action <span class=""math-container"">$a$</span> in state <span class=""math-container"">$s$</span>. You see it very often associated with the action value function <span class=""math-container"">$q(s,a)$</span>. </p>

<p>Action values are not inherently approximated, they are more conceptual than that. However, in practice in Reinforcement Learning (RL), the action value function must be learned, and some form of approximation used based on observed data. When an action value function it is approximated by a neural network during learning you may see it written <span class=""math-container"">$\hat{q}(s,a,\theta)$</span> which makes explicit that the NN is approximating some ""true"" function that you don't know, and that it is parametrised by <span class=""math-container"">$\theta$</span>, the neural network weights and biases.</p>

<p>During learning, the agent takes actions and observes resulting states and rewards. It must use these observations to improve its estimates of values. To do so in DQN, the agent constructs a temporal difference (TD) target - for single-step Q-learning this is <span class=""math-container"">$G_{t:t+1} = r_{t+1} + \gamma\text{max}_{a'}\hat{q}(s_{t+1},a',\theta)$</span>. This is the ""target value"" and there are multiple ways to construct it, giving rise to variants of RL algorithms.</p>

<p>The value <span class=""math-container"">$G_{t:t+1}$</span> is also an estimate of expected return. Technically it is a sampled estimate, which may vary and may be biased (due to starting conditions of the neural network). However, it includes some real experience, so may be used to improve the neural network through training. You can use it like ground truth associated with the state and action, to train the neural network. </p>

<p>The TD target or ""target value"" gets its name because by updating a Q table or training a NN with it as a ground truth, the estimator will output values in future closer to the supplied value. The estimator ""gets closer to the target"".</p>
","2","2","836","25573"
"56310","<p>If you are using a value-based method, like Q-learning in Deep Q Networks (DQN), then the ""degree"" concept has little meaning to the agent, and you are effectively training an agent to learn the best discrete action out of 200 actions. </p>

<p>Yes this could take a lot longer than learning a simple 2-action scenario, as the agent will not <em>easily</em> learn that the value of ""Right 49%"" is correlated to the value of ""Right 52%"" in any particular state. </p>

<p>This is a similar problem you might face in supervised learning if you wanted to move from classifying a dog vs cat image classifier to add bounding boxes for where the creature is. You don't add the bounding box as a large number of classifications - instead you add it as a regression, mapping form the image directly to some vector of real numbers.</p>

<p>There <em>might</em> be ways to address this using value-based methods, but the usual approach here would be to move to a continuous action space (maybe with -1 for 100% Right and +1 for 100% Left), and use a Policy Gradient method. There are a few different choices of Policy Gradient, but they all share the basic principle of modelling a policy function - which might be a probability distribution over action space which you need to sample from, or a specific action - converting from state to action.</p>

<p>The most basic Policy Gradient approach, which you could use as an introduction for your problem if it is simple enough, is REINFORCE. Algorithms like A3C, DDPG, PPO are based over this idea and can cope with more challenging environments.</p>
","1","2","836","25573"
"56312","<p>The PPO approach directly generates stochastic policies. Its output is some probability distribution function over the action space.</p>

<p>This is not the case for all RL algorithms, but is common for many Policy Gradient methods.</p>

<p>In some cases you may want this behaviour to continue. Paper-Scissors-Stone is a classic example of a game where a stochastic policy is optimal, and there are other examples from game theory.</p>

<p>Also, the agent many have learned on-policy (I'm not sure in case of PPO without looking it up), in which case the stochastic behaviour should match with expected value predictions. This is similar conceptually to using SARSA to generate an <span class=""math-container"">$\epsilon$</span>-greedy policy. The value functions match following that policy, although if <span class=""math-container"">$\epsilon$</span> is low enough you may take a reasonable guess that a fully greedy policy is optimal.</p>

<blockquote>
  <p>To what degree is the trained agent stochastic (will it follow its model predictions 90% of the time and guess the other 10%)?</p>
</blockquote>

<p>To the degree that the output of the policy is stochastic. It will always ""follow its model predictions"". </p>

<p>Switching deterministic on actually stops the agent from following the model, and typically will select the mode (highest probability density) of the action distribution.</p>

<p>Unlike SARSA, with Policy Gradient methods, there is not always access to a ""greedy"" policy that picks according to action values. So instead, your <code>deterministic</code> flag is likely picking greedily according to action probability. </p>

<p>In some cases (e.g. Actor-Critic) you may also have a value based estimator for V(s) or Q(s,a), and it <em>could</em> be used instead, but that is usually considered secondary to the policy function (in addition, using this on a large or continuous action space is very inefficient compared to a real-valued policy function)</p>

<p>Also note in some cases e.g. DDPG (which stands for Deep <em>Deterministic</em> Policy Gradients), a policy gradient method can use a deterministic policy with exploration added as a behaviour policy, making the algorithm off-policy. DDPG should not exhibit exploration during testing as PPO is for you. You may find DDPG behaves closer to your original expectations because of this.</p>
","2","2","836","25573"
"56434","<blockquote>
  <p>Why is taking the gradient of the average error in SGD not correct, </p>
</blockquote>

<p>It <strong>is</strong> correct.</p>

<blockquote>
  <p>but rather the average of the gradients of single errors?</p>
</blockquote>

<p>You are mis-quoting the original comments. This your original comment:</p>

<blockquote>
  <p>In an MLP first averaging the error of the entire batch and then calculating the gradient on that average error is identical to calculating the gradient per item and then adjusting the parameters by the average gradient*learning rate, right?</p>
</blockquote>

<p>Specifically this is about process. You are looking for a way to take one initial sum <em>before</em> back propagation, not having to back propagate individual gradient calculations, and somehow get the gradient <span class=""math-container"">$\nabla J(\theta)$</span> In other words, you are looking for some equation:</p>

<p><span class=""math-container"">$$\nabla J(\theta) = g(J(\theta))$$</span></p>

<p>where <span class=""math-container"">$g()$</span> is a function that does not include a sum over individual items. More specifically, it can include a sum over the data items as a constant, but any such sum should not vary with <span class=""math-container"">$\theta$</span>.</p>

<p>However, your own calculations show that you do indeed need to back propagate over individual gradients, because <span class=""math-container"">$2(y_i - f(x_i))x_i$</span> is the gradient of a single term of <span class=""math-container"">$J(\theta)$</span> w.r.t. the data set and includes the value of <span class=""math-container"">$\theta$</span> in <span class=""math-container"">$f(x_i) = mx_i+b$</span>, where your <span class=""math-container"">$m$</span> and <span class=""math-container"">$b$</span> are the two components of <span class=""math-container"">$\theta$</span> that you want to calculate the gradient over. </p>

<p>This is unavoidable - to calculate <span class=""math-container"">$\nabla J(\theta)$</span> you need to calculate and sum the individual terms of <span class=""math-container"">$\sum_i \nabla \mathcal{L}(y_i, x_i, \theta)$</span> where <span class=""math-container"">$\mathcal{L}()$</span> is your loss function, and you cannot move your sums <em>inside</em> that loss function because <span class=""math-container"">$\nabla \mathcal{L}(\frac{y_1 + y_2}{2}, \frac{x_1 + x_2}{2}, \theta) \neq \nabla \frac{1}{2}(\mathcal{L}(y_1, x_1, \theta) + \mathcal{L}(y_2, x_2, \theta))$</span> nor are there any similar relationships that hold true on aggregating parameters of <span class=""math-container"">$\mathcal{L}$</span> in general that would allow you to work with a pre-calculated sum of losses and a non-linear loss function, then somehow calculate the correct gradient.</p>

<p>If you could remove <span class=""math-container"">$\sum_i$</span> from the right hand side and re-write it in terms of <span class=""math-container"">$J(\theta)$</span> plus some <em>general</em> derivative of the cost function then you would have found a way to feed just the average error into a back propagation routine and obtain <span class=""math-container"">$\nabla J(\theta)$</span> from <span class=""math-container"">$J(\theta)$</span>. </p>

<p>If your cost function is simply linear you can resolve this and create something that works. Here to keep the example simple, <span class=""math-container"">$\theta$</span> is just a single real value, and the ""partial"" derivative just a plain derivative, but the main difference is not using squared error:</p>

<p><span class=""math-container"">$$J(\theta) = \frac{1}{N} \sum_i (y_i - \theta x_i)$$</span></p>

<p>Then </p>

<p><span class=""math-container"">$$\nabla J(\theta) = \frac{1}{N} \nabla \sum_i (y_i - \theta x_i)$$</span></p>

<p><span class=""math-container"">$$= \frac{1}{N} \sum_i \nabla (y_i - \theta x_i)$$</span></p>

<p><span class=""math-container"">$$= \frac{1}{N} \sum_i -x_i$$</span></p>

<p>whilst this is a <em>still</em> a sum over <span class=""math-container"">$i$</span>, it is independent of <span class=""math-container"">$\theta$</span>, so you can pre-calculate <span class=""math-container"">$\frac{1}{N} \sum_i -x_i$</span> on a first iteration and treat it like a constant on all further iterations. Technically this meets the requirement above that <span class=""math-container"">$\nabla J(\theta) = g(J(\theta))$</span> where <span class=""math-container"">$g(z) = 0z + K$</span>  (<span class=""math-container"">$z$</span> is just the parameter of <span class=""math-container"">$g()$</span> and <span class=""math-container"">$K$</span> is a constant).</p>

<p>This also tells you as an aside that:</p>

<ul>
<li><p>there is no global minimum for the given error function <span class=""math-container"">$J(\theta) = \frac{1}{N} \sum_i (y_i - \theta x_i)$</span>. Assuming this constant is non-zero, you can always reduce <span class=""math-container"">$J(\theta)$</span> by changing <span class=""math-container"">$\theta$</span></p></li>
<li><p>you <em>need</em> the derivative of the cost function to depend on its parameters in order to talk meaningfully about optimising those parameters.</p></li>
</ul>

<p>It is harder to construct an error function where you did get some non-trivial function of <span class=""math-container"">$J(\theta)$</span> in the right hand side, and no sums over <span class=""math-container"">$i$</span> involving individual gradient calculations. I could not think of a way to do it offhand, but it could be possible. The chances of this being a useful objective function for minimisation seem low though.</p>

<p>I have not mentioned neural network back propagation so far in the above argument, because I wanted to show that the flaw in the thinking applies whenever there is a non-linear function to back propagate over. This even happens using MSE with linear regression. However in a neural network, the same issue occurs at each and every layer where there is a non-linear function (including the error gradient). </p>

<p>It is common to set up a neural network with a simple error gradient for the first layer by combining output transfer function with an objective function so that the initial gradient looks simple. Often literally just the difference between prediction and ground truth <span class=""math-container"">$\hat{y}_i - y_i$</span>. You may be thinking that you can average this gradient, then perform the rest of back propagation with it. You cannot, for a similar reason as outlined above, but using the back propagation relations between layers instead of the loss functions. The argument is the same, there is no <span class=""math-container"">$\nabla_{W^l} J = g(\nabla_{W^{l+1}} J)$</span> where <span class=""math-container"">$g()$</span> does not involve a sum over all the individual gradients due to data items from <span class=""math-container"">$W^{l+1}$</span>.</p>
","3","2","836","25573"
"56852","<blockquote>
  <p>Admittedly I am only a beginner in RL, so I haven't seen much more than MDP's. I believe that this isn't a Markov problem since the states aren't independent (past actions having an impact on the outcome of the current action). </p>
</blockquote>

<p>If there are enough hidden variables, then this could be a real problem for you. A policy maps states to actions - and searching for an optimal policy will always map the same action to the same state. If this is some kind of automated scarecrow system, then the animals are likely to become habituated to the ""best"" action.</p>

<p>Two ways around that:</p>

<ul>
<li><p>If the habituation is slow enough, you might get away with treating the environment as having the simple state that you suggest and have an agent which constantly learns and adapts to changes in what will trigger animals to leave. This would be an environment with non-stationary dynamics (the same action in the same state will, over time, drift in terms of its expected reward and next state). For a RL agent, this could just be a matter of sticking with a relatively high exploration rate and learning rate.</p></li>
<li><p>If the habituation is fast or adaptive, then you have to include some memory of recent actions used in the state representation. This will make the state space <em>much</em> larger, but is unavoidable. You could try to keep that memory inside the agent - using methods that work with Partially Observable MDPs (POMDPs) - it doesn't really make the problem any easier, but you may prefer that representation.</p></li>
</ul>

<blockquote>
  <p>The reward is the difference of detections from <span class=""math-container"">$s_{-1}$</span> to <span class=""math-container"">$s$</span>.</p>
</blockquote>

<p>You need to re-think the reward system if habituation is fast/adaptive, as it will fail to differentiate between good and bad policies. All policies will end up with some distribution of detections, and all policies will have a mean total reward of 0 as a result. Your reward scheme will average zero in the long term for random behaviour, and also will average zero if the number of creatures remains at 200 the whole time, or is zero the whole time. Presumably having zero creatures would be ideal, whilst having 200 would be terrible - you need to be able to differentiate between those two scenarios.</p>

<p>You want to minimise the number of creatures in the area consistently, so a simple reward scheme is just negative of the number of visible animals, per time step. Maybe scale this - e.g. divide by 100, or by some assumed background rate where you would get -1 reward per time step if the number of animals is the same on average as if the agent took no action. You could measure this average using the camera over a few days when the agent is not present or simply not active. It doesn't need to be super accurate, teh scaling is just for convenience - you could think of an agent that gets a mean reward of -0.2 per time step is five times better than having no agent at all, whilst an agent that scores -1.5 per time step might be <em>attracting</em> creatures for all you know!</p>

<blockquote>
  <p>That being said, I'm wondering if there's a specific RL algorithm for this setup or if RL even is the right way to go ?</p>
</blockquote>

<p>The problem does seem like a good match to RL. There are other ways of searching for good policy functions, such as genetic algorithms, that might also apply. However, you will still need to equip the agent with either continuous learning so it can re-find the best action as it changes, or with a memory of recent actions, depending on the speed of habituation. You may even need both as smart animals like birds or mammals can adapt in long and short term in different ways.</p>
","3","2","836","25573"
"56854","<blockquote>
  <p>I have a decision problem where the results are measured as a cost that I want to minimise. It seems like a good fit to Q-learning, but I am not sure how to adjust it to deal with a cost instead of a reward.</p>
</blockquote>

<p>The simplest way to do this, without changing anything else about your learning algorithm, is to notice that</p>

<p><span class=""math-container"">$$Reward = -Cost$$</span></p>

<p>So literally just optimise against the expected return based on sums over your negative costs, using standard Q learning. Everything will work as normal. Your best agent may still end up with a negative expected return (and negative Q values), but maximising this should still restult in an optimal policy.</p>

<p>If you really <em>must</em> use minimising cost as your objective for some reason, then there are a few small changes you need to make for that to work with Q learning. </p>

<p>The definition of your Q function becomes expected discounted sum of future costs:</p>

<p><span class=""math-container"">$$Q(s,a) = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^k C_{t+k+1} | S_t=s, A_t=a]$$</span></p>

<p>(This literally just replaces <span class=""math-container"">$R_t$</span> with <span class=""math-container"">$C_t$</span>)</p>

<p>Then your best guess at the optimal policy is the one that minimises expected future costs:</p>

<p><span class=""math-container"">$$\pi(s) = \text{argmin}_a Q(s,a)$$</span></p>

<p>And your Q-learning update rule is also based on assumed optimisation of minimising the next step</p>

<p><span class=""math-container"">$$Q(s, a) \leftarrow Q(s, a) + \alpha(c + \gamma \text{min}_{a'}[Q(s',a')] - Q(s, a))$$</span></p>

<p>This doesn't match either of your suggestions. If I was to correct your code, then using rewards (with <code>r = -c</code>), it would look like this:</p>

<pre><code>q_dict[state1][act1] += alpha * (r + max(q_dict[state2].values()) - q_dict[state1][act1])
</code></pre>

<p>where <code>alpha</code> is the learning rate, and I assume there is no discounting (so it must be an episodic problem, not continuous).</p>

<p>If you wanted to use cost <code>c</code> directly, and find policies that minimise total cost, then it looks like this:</p>

<pre><code>q_dict[state1][act1] += alpha * (c + min(q_dict[state2].values()) - q_dict[state1][act1])
</code></pre>

<p>i.e. you substitute <code>c</code> for <code>r</code> and <code>min</code> for <code>max</code>.</p>

<p>You ideas about having different starting values might make some difference to convergence rates. However, this is not related to whether you use a cost or a reward.</p>

<p>I advise against using the cost directly like this. Although it is simple and will work, whenever you read any RL articles you will have to keep adjusting whether to switch <code>max</code> for <code>min</code>. The convention of maximising the sum (or average) reward is much more common in RL tutorials, and whilst you are learning it, you will save yourself just a little bit of effort to follow this convention.</p>
","2","2","836","25573"
"56914","<p>The description you quote explains how the true values will be set in the test when setting up a test run. This is necessary to fully state how the test works.</p>

<p>Initialisation of your estimates is a different issue. If you know something about the distributions of the true action values, then it would make sense to use that. For instance you could set all action values to the mean expected true value. Which is <span class=""math-container"">$0$</span>. However, you may also use <span class=""math-container"">$0$</span> if you have no idea about true values, as it is a simple arbitrary value.</p>

<p>Setting the estimates to something using the same distribution is not unreasonable, as it that itself is an unbiased estimate of the same mean. However, it does not really serve you well here, because it adds variance to the initial estimates (as well as possibilty of them being closer to true values, they are equally likely to be worse) and on average slowing down some agent types slightly. </p>
","1","2","836","25573"
"57822","<blockquote>
  <p>So why we do not use gram matrix in loss function calculation between content and generated?</p>
</blockquote>

<ul>
<li><p>""Same content"" means same objects in same locations in the image. If the original content shows a face at the top, we want to see a stylised face at the top.</p></li>
<li><p>""Same style"" means same combinations of textures and line style together <em>in any location</em> in the image. The gram matrix is a good proxy measure of this, because it is invariant to the locations at which the style correlations occur, and similar line shapes and textures will tend to trigger the same combinations of low-level feature map channels.</p></li>
</ul>

<p>If you used gram matrix for the higher order content channels, you would probably find that larger objects would be treated like a style and would be moved, duplicated and distorted in the final output. Depending on the trained network, which layer(s) you picked as representing content etc, the effect might still be interesting or artistic. But it would no longer be style transfer, but perhaps some hybrid of style transfer and deep dream.</p>
","1","2","836","25573"
"57999","<p>Although you might find a way to apply machine learning (ML) to this optimisation problem, it does not look necessary, and is probably a distraction. ML might help if the scoring system was complex or if you had incomplete data for most matches, and needed to compute matching score estimates from some more limited set of attributes.</p>

<p>Instead here you seem to have a <a href=""https://en.wikipedia.org/wiki/Combinatorial_optimization"" rel=""nofollow noreferrer"">combinatorial optimisation problem</a>. A well known example of this is the <a href=""https://en.wikipedia.org/wiki/Travelling_salesman_problem"" rel=""nofollow noreferrer"">Travelling Salesman Problem</a>. </p>

<p>There are many possible algorithms to attack these kinds of problem. Which to choose may depend on other traits of the data, such as how quickly you can calculate the scores - both for the whole set and for individual changes. If calculating for changes is fast enough, you can use optimisers that work from a complete (but not yet optimal) solution and make changes.</p>

<p>There is a free PDF/book called <a href=""http://blog.shuo1.com/zms/books/program/Clever%20Algorithms.pdf"" rel=""nofollow noreferrer"">Clever Algorithms (Nature-Inspired Programming Recipes)</a> covering selection choice amongst all the varied optimisers. This may allow you to find something optimal in terms of speed and reliability of algorithm.</p>

<p>Here's a simple thing you could try though</p>

<ul>
<li><p>Create a ""greedy"" solution</p>

<ul>
<li>Shuffle one set of items that need to be paired</li>
<li>For each item in turn, pair it with the best scoring partner</li>
<li>Calculate the score for this solution</li>
</ul></li>
<li><p>Refine the solution</p>

<ul>
<li>Sample some subset of pairs (e.g. 2, 3, 4, 5 pairs). You could do this deterministically for small enough dataset, or stochastically, or use some algorithm to filter to ""at least has some chance of improving"".</li>
<li>Find the best score amongst all permutations in this small subset (i.e. brute force all 24 pairings amongst 4 couples)</li>
<li>Put the best subset back into the solution</li>
<li>Repeat until no improvement found after some number of tests</li>
</ul></li>
</ul>

<p>This routine can be altered in various ways to take advantage of aspects of your problem. A nice thing in your case, making this easier that the Travelling Salesman Problem, is that you don't have any constraints on valid pairings. In TSP you care about making a single circuit, and not multiple separate loops which limits how you can make changes, whilst in your case each of your pairs is entirely separate.</p>

<p>One example of possible algorithm improvement: You could pre-calculate the scores for the top N matches for each person, and only search amongst those when considering local changes.</p>
","3","2","836","25573"
"58113","<blockquote>
  <p>Is there any kaggle competition out there doing EDA (Explotary data analysis) not prediction for finding the most significiant feature that affects the net_revenue or sales ? </p>
</blockquote>

<p>Although it is hard to prove a negative, I would say ""no"" to this. </p>

<p>Kaggle competitions are based on continuous metrics that can be ranked, such as getting best log loss, mean average precision etc.</p>

<p>Identifying the ""most significant feature"", although a potentially useful question to answer from a data set, is not something that can be ranked automatically and objectively in a competition.</p>

<p>It is possible you will find competitors performing that kind of analysis <em>within</em> a prediction challenge. It is fairly common to rank features by relevance, especially when using ML techniques that support doing so, such as random forests or xgboost. I am pretty sure I have seen forum posts in Kaggle discussing these things and searches for a ""golden feature"" which is a similar concept - usually some processed combination of one or more original features.</p>

<p>In addition, there have been a few competitions where the entrants were expected to produce supporting documentation. Discussion of feature relevance could be an important part of such entries. These are much rarer competitions at Kaggle than the more direct finding best predictive model.</p>

<p>So you may find competitions where identifying a ""best feature"" is part of the winning strategy, but the goal of the competion will likely be getting the best mean log loss on the test set.</p>
","3","2","836","25573"
"58726","<p>As always in ML modelling problems: <em>it depends</em>.</p>

<p>The critical factor here is that you are predicting based on properties of a sequence. The sequence does not need to be sampled at fixed time steps, or even be time based at all. E.g. in natural language processing the sequence of letters or words is only very loosely associated with the timing of the same items when spoken or read. The sequence does need to be drawn from a self-consistent source where moving from one item to next item in the sequence has the same meaning with respect to what you are trying to predict.</p>

<p>If the interruptions you see would cause large differences in the expected distribution of your input vector X or output Y, then you may not be able to use them as-is. You may still be able to rescue them as input data for a RNN, and make predictions based on them, if you add whether or not there has been an interruption - and if it might be relevant to your purpose, for how long - as additional features of input items in X.</p>

<p>Another option is to follow the time sequence strictly but add placeholder value of X (with a different flag set) for ""unkown"", maybe along with some rough guess for the missing features (or even a ML prediction from a different model), which you might do if the quantity being aggregated over is still active and important where there are missing records, but the aggregation service you are using as input has failed.</p>

<p>Depending on how much data you have available, and whether you need to make predictions at times when your aggregation service has failed, you could do anything from discarding incomplete data to ignoring the time differences, to interpolating missing records.</p>

<p>The first question you need to answer is ""what does it mean for a sequence item to be missing"":</p>

<ul>
<li><p>If the whole system that provided the aggregation data is switched off, and there is no concept of pending items that will enter the system the moment it is switched on, then you can probably safely ignore the missing records and ignore the precise timing.</p></li>
<li><p>If your prediction target is sensitive to elapsed time, and yo uneed to make predictions even when there have been interruptions to the aggregation system, then you will want to augment the sequence data somehow to account for the missing records, and train/predict with that added feature engineering.</p></li>
<li><p>If only the aggregation is switched off (or has a chance of failure causing gaps in the record), and you don't need to run any predictions when it is not available, then you may be able to make the rule that the prediction system will only function when there are 6 recent consecutive records available, and discard the data that is causing you to worry. This makes sense if interruptions are rare or the predictive model is not critical.</p></li>
</ul>

<p>If you are not sure which of these things to do, then the next approach is to do all of them that seem that they could be intuitively correct, tuning each model separately, then see how each different approach performs on your test set.</p>
","2","2","836","25573"
"58803","<blockquote>
  <p>Is it possible to model an environment that is stochastic in nature?</p>
</blockquote>

<p>Yes, a model of a stochastic environment can be one of:</p>

<ul>
<li><p>A distribution model, that outputs a probability distribution for next state and reward, given current state and action. If you are reading Sutton &amp; Barto, or similar work which uses the function <span class=""math-container"">$p(s',r|s,a)$</span>, then if you can implement this function for the whole environment, it means that you have access to a distribution model.</p></li>
<li><p>A sampling model, that outputs a single reward and next state, given current state and action, with the same probability of any outcome as the real environment. If you can implement an accurate <em>simulation</em> of an environment, then you have a sampling model.</p></li>
</ul>

<p>If you want to use approaches such as Dynamic Programming, which work with expected values, then this is much easier with a distribution model, and in that case you need an accurate model from the start (otherwise Dynamic Programming may converge to a non-optimal policy).</p>

<blockquote>
  <p>Is it because its difficult to model such environment we use model free methods?</p>
</blockquote>

<p>Not really, there is no special difficulty in modelling stochastic environments. For instance, if your environment is a dice game, it is just a matter of implementing the rules and a random number generator for the dice to create a sampling model. A distribution model is usually straightforward for basic dice rules, such as rolling a die to see how many steps you can take.</p>

<p>However, independently of how random the environment is, complex environments can become hard to model. Distribution models may require a lot of maths to calculate all possibilities, so sampling models (simulations) are easier. For instance, a card game is relatively easy to implement in simulation when you track the deck contents. But a distibution model for it is more complex, as you have to track what has already been played or can be figured out from other observations.</p>

<p>Many environments are too complex to model. For example, they may involve real world physics but don't include enough measurements to establish the full state. For instance, when an agent flies a drone it will be affected by air turbulence but cannot directly observe it. Chaotic effects such as turbulence are very hard to model, and gaining real world experience is likely going to be more accurate than any model based on a physics engine, no matter how hard you try to code one. Similarly, visualising the real world, or navigating human social environments can be very hard to model accurately.</p>

<p>When deciding whether or not to use a model-free method, there needs to be a cost analysis. Even in complex environments you may prefer to use a model-based method:</p>

<ul>
<li><p>The advantage of a model is that it allows you to safely explore without taking a real action, and also in some cases it may be a lot faster to query the model than to take an action and wait for a result in the real world. In the time it takes for a real action to resolve a computer may be able to check 10, 100, 1000 or more simulated actions from its model.</p></li>
<li><p>The disadvantages of a model are:</p>

<ul>
<li>It might not be as accurate as you like, meaning that basing decisions off it makes the agent's policy too far from optimal</li>
<li>Using a model adds complexity to the agent.</li>
</ul></li>
</ul>

<p>In some cases, the real environment is fast, safe and reliable enough that there's not much to gain by using a model. These cases include a lot of the toy problems used to study learning, where the environment is actually simulated (which is a form of model) but kept separate from the agent (so despite this, the agent is still technically model-free).</p>
","3","2","836","25573"
"59002","<blockquote>
  <p>Suppose during testing it suggested with accuracy 40% for an image containing person of say class A. I would like to add this image back to the models training dataset and train the model adaptively, so that it improves the accuracy for this person. That would mean transferring an image from the test set to the training set.</p>
</blockquote>

<p>Whilst adding data that appears important to your task to the training set will often improve performance, you have a major problem here. You must base your measure of performance on the test set. Altering the test set can invalidate your test results so far. And of course if you take a problem item out of the test set, your performance on the test set will go up even if you don't use the it to train with, because it was a result that was bringing down your average. So, you <em>might</em> improve your classifier, but at the expense of not knowing <em>whether</em> you have improved the classifier.</p>

<p>A good default position here is that the test set should not be altered. Once set aside, that data is for measuring performance of your tuned model, and should not be used for anything else, to avoid this measurement problem.</p>

<p>However, there are a couple of things you could do:</p>

<h3>Error analysis</h3>

<p>Look at the failing cases in the test set and try to figure out why they are being misclassified. For an image classification task, you might be able to figure out by viewing the image directly. Is there unusual lighting, pose, any different foreground or background objects? In the case of face recognition, has the subject got different hairstyle, makeup, hat, glasses or clothing?</p>

<p>If you spot something that's a one-off difference in your problem image, then that may be a sign that you need some more variety in your training data and should collect more. You should not collect it from the test data though, because then you will not be able to tell how well your classifier is doing at the task any more.</p>

<p>It is possible that investigating a failed item will show up something that means you want to alter or remove the test data item. This would happen for example if you decide it is not representative of how you want your model to be used. For instance, all the other images in the data set are from CCTV - which is how you intend it to be used - and the bad example is from a studio photo shoot. In that case, it may be worth the effort to clean up your test data and re-evaluate your best models so far. </p>

<h3>K-fold cross validation</h3>

<p>You can use <a href=""https://machinelearningmastery.com/k-fold-cross-validation/"" rel=""nofollow noreferrer"">k-fold cross validation</a> to gain more accurate assessments of perfomance when tuning hyperparamaters (number of neurons, layer architecture). This doesn't technically replace a test set for measuring final performance, but it's not too bad a fallback position. An extreme version of this would be <a href=""https://stats.stackexchange.com/questions/27454/how-does-leave-one-out-cross-validation-work-how-to-select-the-final-model-out"">leave-onme-out cross validation</a>. </p>

<p>In theory you could select your best hyperparameters using k-fold cross validation, have a good idea that this would be the best you could get, and a rough idea of what the generalisation performance was. Then train a final model using the same hyperparameters and all of your data. </p>

<p>You may be able to get away with this if you are happy to not know (or not need to report) an unbiased performance metric, but be reasonably certain you have close to the top performance out of the models that you have investigated.</p>

<h3>Keep the test set as-is, do both the above, maybe collect more training data</h3>

<p>I would recommend instead this as the best approach if you can afford the time and effort. For better or worse, you have set your goals for the first attempt at your project, when you selected the test set. </p>

<p>You can search harder for hyperparameters that generalise the best, using a small amount of data, by using k-fold cross validation. You can try to figure out why your accuracy is low by looking at the errors from the model and finding ways to fix them <em>without</em> using the test data in the training set. </p>

<p>Eventually you will have your best model (according to k-fold CV on the training data), and a measure of how it performs on the test set. The project is then done. Any further work to get even better accuracy would require some kind of re-thinking or start of a new project.</p>
","1","2","836","25573"
"60312","<p>In general it is not possible to simultaneously optimise two separate objective functions. Your approach of adding weights (your coefficients) to each objective, then summing the scaled objectives, is a standard way of resolving that. </p>

<p>As your penalties are on different scales and in different units, it is your task as the engineer setting the objective to provide the conversion to a single scale. That is what the coefficients represent - you can even think of them as <span class=""math-container"">$points/Joule$</span> for energy and <span class=""math-container"">$points/(\Delta K)$</span> for temperature difference.</p>

<p>Sometimes analysis will show you that there is a natural combined scale. For instance, in business settings it may be possible to frame compromises as financial costs, e.g. your coefficients might be <span class=""math-container"">$\text{GBP}/Joule$</span> for energy and <span class=""math-container"">$\text{GBP}/(\Delta K)$</span> for temperature difference. Then you have one clear objective to minimise cost or maximise profit.</p>

<p>If that is not possible - a financial cost for exceeding temperature bounds may be hard if this is about human comfort in a building - deeper analysis might lead to thinking about longer-term outcomes. Perhaps your initial rewards are too focused on immediate numerical issues (that appear easy to collect, but don't represent your true goals), and a re-framing of the problem could work. For instance, perhaps it is more reasonable that both the temperature and energy costs stay within strict bounds over a year of varying external temperatures and system workload, with a scaling penalty depending on how badly these are exceeded. </p>
","1","2","836","25573"
"60621","<blockquote>
<p><strong>Here is my current understanding: Q-value is just a value for a particular action taken, and the Value function of a state sums over the Q-value of every action possible in that state.</strong></p>
<p>Would this description be accurate?</p>
</blockquote>
<p>Not quite.</p>
<p>First the Q value <em>is</em> a type of value function, it is often called the <em>action value</em>. Both <span class=""math-container"">$Q(s,a)$</span> and the state value function <span class=""math-container"">$V(s)$</span> calculate the <em>expected future return</em> given their parameters, a known environment and a known policy <span class=""math-container"">$\pi$</span> that describes how an agent will select actions in that environment.</p>
<p>In the case of action value <span class=""math-container"">$Q(s,a)$</span>, the expected future return is based on the agent taking the action <span class=""math-container"">$a$</span> in state <span class=""math-container"">$s$</span> first, and <em>afterwards</em> following <span class=""math-container"">$\pi$</span>. Whilst the state value <span class=""math-container"">$V(s)$</span>, the expected future return will depend on what action <span class=""math-container"">$\pi$</span> chooses in state <span class=""math-container"">$s$</span>.</p>
<p>There a few ways to write the relationship between <span class=""math-container"">$V(s)$</span> and <span class=""math-container"">$Q(s,a)$</span>. If the policy function is deterministic, action in state <span class=""math-container"">$s$</span> given by <span class=""math-container"">$\pi(s)$</span> then:</p>
<p><span class=""math-container"">$$V(s) = Q(s, \pi(s))$$</span></p>
<p>If the policy function is stochastic, with probability of selecting action <span class=""math-container"">$a$</span> in state <span class=""math-container"">$s$</span> given by <span class=""math-container"">$\pi(a|s)$</span>:</p>
<p><span class=""math-container"">$$V(s) = \sum_a \pi(a|s)Q(s, a)$$</span></p>
<p>This is a sum, as you suggest, but weighted by the probability of taking each action.</p>
","2","2","836","25573"
"60869","<p>The most likely issue here is to do with </p>

<blockquote>
  <p>fifty features, but those features were extracted from thousands</p>
</blockquote>

<p>If those features were selected according to a pre-data-analysis theory, and other selections were not considered, then a linear model that fit the data might be strong proof that the theory was plausible.</p>

<p>However, a linear model that fits well due to selection from a large feature set <em>in order to make it fit</em> is very likely to be overfit. You absolutely need a hold-out test data set in this case, as you have used your initial data to form a hypothesis, and have no proof of validity at all.</p>

<p>I cannot advise you whether to submit the paper or not. There may be ways you can word it to make it clear that the work establishes a hypothesis and does not validate it (but without making a song and dance about the lack of rigour in validation, as then you are undermining your own submission). </p>

<p>I think that as long as you do not try to obfuscate the lack of follow up work, and present results so far accurately, then it is a fair submission - it may then get rejected if a reviewer wants to see some validation, or it may get accepted and there will need to be follow up work that either validates or refutes the model in a second paper. That might be your work, it might be another team's. </p>

<p>How good/bad those scenarios are depends on how your field works in general. Perhaps ask with some relevant details on <a href=""https://academia.stackexchange.com/"">https://academia.stackexchange.com/</a> to gauge your response, as in some ways this is a people problem - how to please your mentor whilst retaining pride in your work and progressing your career (which in turn depends on a mix of pleasing your supervisor and performing objectively good work). </p>

<p>Your mentor may still be open to discussing the technical merits of the work. Perhaps they have not fully understood the implications that you are seeing for how the model was constructed. However, they might fully understand this, and may be able to explain from their view the merits of publishing at an early pre-validation stage for this project.</p>
","0","2","836","25573"
"61365","<blockquote>
  <p>Can't we use ReLU instead of tanh in RNNs for solving vanishing gradients too rather than opting for a more complex architecture?</p>
</blockquote>

<p>Yes you can. I'd expect it to help a little with vanishing gradients, but not be as effective generally as a LSTM is. Also, without some kind of bounded function in the feedback loop, a basic RNN using ReLU will suffer very easily from <em>exploding</em> values, resulting in very large errors as the feedback is run over multiple time steps - so you may need some adjustment like gradient clipping to account for that.</p>

<p>If you already have a problem to analyse, have prepared your data and decided on a top level architecture (e.g. how many recurrent layers, whether this is a seq2seq type model etc), then it should be relatively easy to compare LSTM with a basic RNN using ReLU. It is likely different hyperparameters would suit each architecture so you may need to do a search, and you could compare best possible results for each model, or look at things such as number of parameters or training time.</p>

<p>I expect many researchers have already tried this comparison, although the examples I found on a quick search appeared to use tanh for the simple RNN. </p>

<p>Even using ReLU in simple RNN architecture, I <em>suspect</em> that by most measures and in most problems, the LSTM or GRU architectures will perform better. There will be some edge cases where RNN performs better, such as if your goal is to get to a certain accuracy quickly during training and not necessarily to get the best accuracy in the end.</p>
","3","2","836","25573"
"61543","<p>Although this problem can happen quite frequently in Reinforcement Learning (RL) projects, it is not a RL issue. The issue is due to neural network requirements, and also applies to many other parametrised machine learning solutions.</p>

<p>You can vary the number of parameters of a neural network in a limited fashion. However, it can only usually be done before training (or re-training), and is not a suitable approach to deal with changes happening over a small number of time steps, or during post-training prediction (e.g. when playing the game with the trained agent).</p>

<p>There are a few different approaches you can take. Which is suitable will depend on details of the game:</p>

<h3>Pad the inputs</h3>

<p>This is the simplest approach, and you can do this without needing much additional research. It relies on there being a workable maximum number of inputs you could need at any time step.</p>

<p>Practical considerations:</p>

<ul>
<li><p>Have a simple mechanism to identify whether a particular set of inputs are present. E.g. add a <span class=""math-container"">${0,1}$</span> flag to show present of a unit, leaving rest of inputs that represent the unit at some neutral value such as <span class=""math-container"">$0$</span>.</p></li>
<li><p>If there are only rare occasions when the last few inputs are used, you should expect the neural network to learn about those situations very slowly. It will not generalise between units in different input ""slots"", unless you alter the architecture to link them.</p></li>
</ul>

<h3>Invert the representation from list of units to ""world map""</h3>

<p>This works nicely for a game where movement is discrete and there are rules that limit the amount of stacking possible in any single space. It also allows you to use a CNN architecture. </p>

<p>For example, this is how AlphaZero represents Go and Chess, which both allow varied number of pieces on the board at any one time, but where the board itself is strictly defined. In the case of Chess, this is also how AlphaZero identifies which piece to move, and how, by output on a matching grid to select the piece and a movement type (each selection/movement type combination in that case is a separate output channel, the output reprsents 1000s of potential moves all at once).</p>

<p>Practical considerations:</p>

<ul>
<li><p>If your units have statistics that you need to track, then each of these can go in a separate instance of the grid (sometimes called a ""plane"" or a ""channel""). So you might have one grid showing presence of a piece, another identifying its type, another its attach strength etc, all stacked into a 3D structure assuming the playing board is 2D.</p></li>
<li><p>If the map is very large or continuous, then it may be possible to discretise it into a grid.</p></li>
<li><p>Using a CNN allows for good generalisation based on similarity between pieces and their stats. However, the approach becomes limited if the grid is very large or stacks of pieces at any one grid position can become large, in addition to requiring a much larger memory footprint to represent the game state.</p></li>
</ul>

<h3>Feature engineering</h3>

<p>Technically, representing the game as a map/board is just one common form of feature engineering in RL. Whether or not it will work, you know the game and may have knowledge of what factors are important to playing it well. You can use this knowledge to simplify the rerpesentation and summarise key details that are independent of the number of units.</p>

<p>For instance, it may be important to prioritise play based on proximity of strongest units in play. So you could sort the available units based on this heuristic and present a smaller fixed number of key units.</p>

<p>Practical considerations:</p>

<ul>
<li><p>This can be done in combination with any other approach.</p></li>
<li><p>This relies heavily on your creativity and a good technical understanding of both the game and what works well in features used for neural networks.</p></li>
</ul>

<p>If you want advice on possible feature engineering approaches, I suggest you open a new question with more details of the game being played.</p>

<h3>Recurrent Neural Networks (RNNs)</h3>

<p>A recurrent neural network architecture, such as LSTM or GRU, can work directly with mixed length sequences. This can work really well when each item in the sequence is the same kind of object but with different feature values.</p>

<p>Practical considerations:</p>

<ul>
<li><p>RNNs are harder to understand and train than fully connected feed forward networks or CNNs.</p></li>
<li><p>If the two lists of units are just one part of the game representation, you will need a hybrid system that combines a RNN architecture with other neural network designs.</p></li>
</ul>
","4","2","836","25573"
"61622","<p>In general, if the agent is simply not able to take non-valid actions in a given environment (e.g. due to strict rules of a game, like chess), then it is standard practice to have the environment support that by providing some kind of function or filter for <span class=""math-container"">$\mathcal{A}(s)$</span>, the set of actions available in state <span class=""math-container"">$s$</span>.</p>

<p>However, it does seem like the basic Gym interface does not support it, and <a href=""https://github.com/openai/gym/issues/413"" rel=""nofollow noreferrer"">has no plans to support it</a>. </p>

<p>It is still possible for you to write an environment that does provide this information within the Gym API using the <code>env.step</code> method, by returning it as part of the <code>info</code> dictionary:</p>

<pre><code>next_state, reward, done, info = env.step(action)
</code></pre>

<p>The <code>info</code> return value can contain custom environment-specific data, so if you are writing an environment where the valid action set changes depending upon state, you can use this to communicate with your agent. The caveat is that both your environment and agent would be working to a convention you had invented in order to do this, and the same approach would not extend to other environments or be used by developers of other agents.</p>

<p>Alternatively, you can support calculating what the valid actions are in any other custom way that is not covered by the Gym API - a new method on the environment, a separate ""rules engine"" for a game etc. Both the agent and the environment could call that in order to perform their roles correctly.</p>

<p>The situation without an official API to do this may be subject to change, but from the linked Github issue it seems that Open AI developers consider a generic interface for this, that accounted for all the different kinds of action space, was more effort than it was worth.</p>
","3","2","836","25573"
"62172","<p>The value function you want to learn should be relatively simple, but I would expect optimal action values to be quite similar between moving left or right, at least when that is away from a wall. Recall that action values measure the expected reward for taking the selected action, then following the policy from that point on.</p>

<p>So in state <code>[0 0 8 5 0 0]</code>, the action value under an optimal policy for ""move right"" should be +1, but the action value for ""move left"" is not much different, at 0.9801 . . . because under an optimal policy the agent will move right twice afterwards when following the policy.</p>

<p>That means the neural network will have to learn fine detail differences between estimates. You have made this harder than it needs to be if you are using the representation as ""raw"" input to the neural network. Neural networks learn badly when the input number range is high. In addition your learning rates seem quite high.</p>

<p>My suggestions:</p>

<ul>
<li><p>Scale the inputs. You should have code that turns the state representation into neural network features. I'd suggest the neural network will function nicely with +1 for goal and -1 for agent, if you must work with this state representation.</p></li>
<li><p>Try lower learning rates. A rate of <span class=""math-container"">$0.01$</span> is high for the Adam optimiser.</p></li>
<li><p>Reduce discount factor <span class=""math-container"">$\gamma$</span> - that will make a clearer difference in this short episode-length environment between returns for faster and slower routes.</p></li>
</ul>
","1","2","836","25573"
"62422","<p>If your goal is optimal control, then you will want to measure the agent by how well it does at the task. You should use some aggregate measure of reward, such as total reward per episode (aka ""return""), or mean reward per time step.</p>

<p>If you are working with a toy problem, or one designed so that it is easy to identify a maximum bound on the reward-based measure, then you can compare your agent against this known value. It is reasonable to expect a good agent to approach the maximum value.</p>

<p>In practice, many interesting problems do not have a known upper bound on reward totals or averages. For those problems, typically the best you can do is compare between agents. You can compare with:</p>

<ul>
<li><p>A randomly acting agent. This would normally be just as a baseline, to show that the agent had learned <em>something</em>.</p></li>
<li><p>An automated agent using a simple action choice heuristic, which might be something natural or obvious in the given problem.</p></li>
<li><p>One or more humans on the same task</p></li>
<li><p>Other ML-trained agents including previous instances of the same agent</p></li>
</ul>

<p>You will probably want to run multiple tests and average the results, if either the policy or environment are stochastic, in order to assess an agent with <em>expected</em> values as much as possible. It is also important, if you are using any off-policy technique such as DQN, to switch off any exploration during tests, to get a fair measure of how well the trained agent behaves (as opposed to how it behaves during training, which will be different).</p>

<p>If your agent is designed to continually learn and explore, and/or uses an on-policy approach, you can use results during training to assess it. For instance you can take a rolling average of total reward over the last N episodes or something similar. This is not a bad metric to monitor training, even for off-policy approaches, although for off-policy you will likely get an underestimate of performance compared to separate test runs.</p>

<p>There are other approaches and other metrics to assess an agent - e.g. how much experience, or how much computation the agent needs to learn to a certain level is often of interest. </p>

<p>If you want to ""conclude that the agent is trained well or bad"" for an optimal control task, then this assessment of total reward might be all you need. However, you can <em>also</em> look at loss metrics inside any neural networks - you would not do this in order to rank agents as ""better"" or ""worse"" but might do so in order to identify problems. These loss metrics are generally the same as supervised learning equivalents. So for instance in DQN, or for the ""critic"" part of PPO, you would be interested in whether the predicted value of any state matched the eventual value, and use MSE loss. For stochastic environments there is a caveat to this, that any collected data will be noisy so it is hard to tell the difference between a high loss due to variance in the data and high loss due to poor training or incorrect hyperparameters.</p>
","2","2","836","25573"
"62458","<blockquote>
  <p>Is each gate simply a feed forward neural network who’s output is either squished through sigmoid or tanh depending on which gate it is?</p>
</blockquote>

<p>Close. Each gate is an activation function (typically sigmoid for actual gates, but tanh is used for other functions within the cell) over a weighted sum of all inputs to the cell's <em>layer</em>. Although spome reading of the diagrams may imply LSTM cells are processing a few single inputs in isolation, recombined later, in fact the inputs to each cell are the whole previous layer and the whole previous timestep cell states for the whole layer. This is similar to a view of a single neuron in a fully-connected feed-forward network.</p>

<p>Each gate in each cell has its own set of weights - one weight for each input from previous layer plus one weight for each cell state in the same layer, plus a single bias. When you combine these cells into a layer of multiple cells (e.g. when you choose to have an LSTM layer with 64 cells), this results in a separate matrix plus bias for each gate. In descriptions of LSTMs, these different matrices are often named for the type of gate they parametrise, so e.g. there will be a matrix plus bias for the layer's ""forget"" gate, which might be noted as <span class=""math-container"">$\mathbf{W}_f$</span>. </p>

<p>If you have <span class=""math-container"">$N_i$</span> inputs, and <span class=""math-container"">$N_c$</span> LSTM cells in a single LSTM layer, then <span class=""math-container"">$\mathbf{W}_f$</span> will be a <span class=""math-container"">$N_c \times (N_i + N_c)$</span> matrix of weights, and so will all the other parameter matrices describing other gates and value calculations for the combined cells.</p>

<p>In practice the calculations don't need to be handled per cell, but calculations over a layer of cells can be vectorised, and it is more like having a few parallel fully connected layers that combine in various ways to generate output plus next cell states.</p>

<p>There would be nothing stopping you extending this architecture and making any single gate or value calculation deeper by giving its own hidden layers. I suspect this has been tried by researchers, but cannot find any references. But without this customisation, the gates in standard LSTM are more like logistic regression over concatenated input and cell state.</p>
","1","2","836","25573"
"63327","<blockquote>
  <ol>
  <li>If I train an agent for taking actions for 15 mins during the training process, is it ok I make my agent take actions at every 5 min interval during deployment?</li>
  </ol>
</blockquote>

<p>It is impossible to say in general. Depending on the nature of the environment and controller, this may work just fine, or may completely destroy the ability of the agent to function at all.</p>

<p>I would <em>suspect</em> that you could get away with this change, if your controller is adjusting things that manage <em>rates</em> of change - e.g. the power levels of heating or cooling. That is because the amount of change forced per time step (by both controller and environment) is likely to scale with the size of time step. The agent is also likely to choose to continue the same action at <span class=""math-container"">$t' = 1, 2, 3$</span> that the original agent chose at <span class=""math-container"">$t = 1$</span>, because the state will not have changed as much.</p>

<p>Much better though would be to alter your training simulation to be more accurate in this regard. There is a trade-off here. The more faithful your simulation is to real world use, the more likely it is that you will train an agent which works well in the real world. However, creating precise and accurate simulations can be hard work. I cannot tell how much work going from 15 minute time steps to 5 minute ones would be for you - just saying the change, it doesn't seem like much effort, but perhaps it is not under your control.</p>

<blockquote>
  <ol start=""2"">
  <li>If I train an agent on a particular ambient temperature profile, will the agent by heart / remember the profile used for training and expect the same temperature profile during deployment for it to work well ?</li>
  </ol>
</blockquote>

<p>Over-fitting and failure to generalise are issues with neural networks wherever they are used. Yes, this is possible.</p>

<p>You can control for one or both of these changes between training and production systems - time step differences and temperature profile. Check to see whether your controller has generalised enough to deploy it, by <em>testing</em> your agent in a simulation with characteristics closer to production. For instance, you can have the simulation work with newly generated, and previously unseen (to the agent) external temperature profiles. </p>

<p>You can also, at least in theory, decrease the time step size in the simulation environment, although I recommend that you make that change during training instead, unless there is some strong reason why it is not possible. It is better to fix parameters that you can, rather than expect the agent to generalise over them - especially in your case where you only have one target value.</p>

<p>If you are not able to adjust your simulation to use it for testing, then you have to test your trained controller in a real-world environment. At that point you will want to monitor it closely to see how well it performs. That is good practice regardless of an additional test phase in simulation. Having a test phase in simulation is simply a good idea to mitigate risk and reduce costs.</p>
","2","2","836","25573"
"63756","<p>There is no theory or general case that sets the size of dataset required to reach any target accuracy. Everything is dependent on the underlying, and usually unknown, statistics of your problem.</p>

<p>Here are some trivial examples to illustrate this. Say want to predict the sex of a species of frog:</p>

<ul>
<li><p>It turns out the skin colour is a strong predictor for the species <em>Rana determistica</em>, where all males are yellow and all females blue. The minimal dataset to get 100% accuracy on the prediction task is data for two frogs, one of each sex.</p></li>
<li><p>It turns out the skin colour is uncorrelated for the species <em>Rana stochastica</em>, where 50% of each sex are yellow and the other 50% are blue. There is no size of dataset of frog colour labelled with sex that will get you better than 50% accuracy on the task.</p></li>
<li><p>However, <em>Rana stochastica</em> does have eye colouration with almost determistic relationship to the sex of the creature. It turns out that 95% of males have orange eyes and 95% of females have green eyes (with only those two eye colours possible). Those are predictive variables that are strong enough that you can get 95% accuracy if you can discover the relationship.</p></li>
</ul>

<p>Some related theory worth reading to do with limitations of statistical models is <a href=""https://en.wikipedia.org/wiki/Bayes_error_rate"" rel=""nofollow noreferrer"">Bayes error rate</a>.</p>

<p>In the last case, simply predicting ""male"" for orange eyes and ""female"" for green eyes will give you 95% accuracy. So the question is what size of dataset would guarantee a model would both make those predictions, plus give you the confidence that you had beaten your 90% accuracy goal? It can be figured out, assuming you collect labelled sample data at random - note that there is a good chance that models trained on very little data would get 95% accuracy, but that it could take a lot more data for a test set in order for you to be confident that you really had a good enough result. </p>

<p>The maths to demonstrate even this simple case is long-winded and complex (if I were to outline the theory being used), and does not actually help you, so I am not going to try and produce it here. Plus of course I chose 95%, but if the eye colour relationship was only 85% predictive of sex, then you would never achieve 90% accuracy. With a real project you have many more variables and at best only a rough idea on how they might correlate with the target variable or each other in advance, so you cannot do the calculation.</p>

<p>I think instead it is more productive to look at your reason for wanting a theory to choose your dataset size:</p>

<blockquote>
  <p>Such a theory would also help me to gather data until I reach that point and then do some productive work.</p>
</blockquote>

<p>Sadly you cannot do this theoretically. However, you <em>can</em> do a few useful things:</p>

<h2>Plot a learning curve against data set size</h2>

<p>I'd recommend this as your approach here. The driving question behind your question is: Will collecting more data improve my existing model?</p>

<p>Using the same cross-validation set each time, train your model with increasing amounts of data from your training set. Plot the cross-validated accuracy against number of training samples, up to the whole training set that you have so far.</p>

<ul>
<li><p>If the graph has an upward slope all the way to end, then this implies that collecting more data will improve accuracy for your current model.</p></li>
<li><p>If the graph is nearly flat, with accuracy not improving towards the end, then it is unlikely that collecting more data will help you.</p></li>
</ul>

<p>This does <em>not</em> tell you how much more data you need. An optimistic interpretation could take the trend line over the last section of the graph and project it to where it crosses your target accuracy. However, normally the returns for more data will become less and less. The training curve will asymptotocally approach some maximum possible accuracy for the given dataset and model. What plotting the curve using the data you have does is allow you to see where you are on this curve - perhaps you are still in the early parts of it, and then adding more data will be a good investment of your time.</p>

<h2>Reassess your features and model</h2>

<p>If your learning curve is not promising, then you need to look in more detail. Here are some questions you can ask yourself, and maybe test, to try and progress.</p>

<ul>
<li><p>Your features:</p>

<ul>
<li>Are there more or different features that you could collect, instead of focussing on collecting more of the same?</li>
<li>Would some feature engineering help - e.g. is there any theory or domain expert knowledge from the problem that you can turn into a formula and express as a new feature?</li>
</ul></li>
<li><p>Your model:</p>

<ul>
<li>Are there any hyper-parameters you can tune to either get more out of the existing data, or improve the learning curve so that is worth going back to get more data?</li>
<li>Would an entirely different model help? Deep learning models are often top performers only when there is a lot of data, so you might consider switching to a deep neural network and plotting a learning curve for it. Even if the accuracy on your current dataset is worse, if the learning curve shows a different model type might have the capacity to go further, it <em>might</em> be worth it. </li>
</ul></li>
</ul>

<p>Do note however, that you could just end up with the same maximum accuracy as before, after a lot of hope and effort. Unfortunately, this is hard to predict, and you will need to make careful decisions about how much of your time is worth sinking into solving the original problem.</p>

<h2>Check confidence limits to choose a minimum test dataset size</h2>

<p><em>Caveat: This is a guide to thinking about data set sizes, especially test set sizes. I have never known anyone use this to actually select some ideal data set size. Usually it happens the other way around, you have some size of test data set made available to you, and you want to understand what that tells you about your accuracy measurements.</em></p>

<p>You could determine a test set size that gives you reasonable confidence bounds on accuracy. That will mean, when you measure your 90% accuracy (or better) that you can be reasonably certain that the true accuracy is close to it. You can do this <a href=""https://machinelearningmastery.com/confidence-intervals-for-machine-learning/"" rel=""nofollow noreferrer"">using confidence intervals on the accuracy measure</a>.</p>

<p>As an example from the above link, you could measure a 92% accuracy on your test set, and want to know whether you are confident in that result. let's say you want to be 95% certain that you really do have accuracy > 0.9 . . . how should you choose N, the size of your test set?</p>

<p>You know that you are 0.02 over the desired accuracy by measurement, and you want to know if this enough that you can claim to be certain that you have 90% accuracy:</p>

<p><span class=""math-container"">$$0.02 &gt; 2 \sqrt{\frac{0.92 \times 0.08}{N}}$$</span></p>

<p>Therefore you need</p>

<p><span class=""math-container"">$$N &gt; \frac{0.92 \times 0.08}{0.0001}$$</span></p>

<p><span class=""math-container"">$$N &gt; 736$$</span></p>

<p>This is the minimum test data set size that would give you confidence that you have met your target of 90% accuracy, provided that </p>

<ul>
<li>you have actually measured 92% or higher accuracy</li>
<li>you have selecting test data at random from the target population</li>
<li>that you have <strong>not</strong> used the test data set to select a model (by e.g. doing this test multiple times until you got a good result)</li>
</ul>

<p>Typically you don't work backwards like this to figure N for a specific accuracy, but it is useful to understand the limits of your testing. You should generally consider how the size of the test dataset limits the accuracy by which you can confirm your model. </p>

<p>The formula above also has limitations when measuring close to 100% accuracy, and this is because the assumptions behind it fail - you would need to switch to more complex methods, perhaps a Bayesian approach, to get a better feel for what such a result was telling you, especially if the test sample size was small.</p>

<p>After you have established a minimum test dataset size, you could use that to guide data collection. For instance, the typical train/cv/test dataset might be 60/20/20, so with your result above you could choose an overall dataset size of 5 times 736, let's round up and call it 4000. In general this sets a lower bound on the size of dataset, as it says nothing about how hard it would be to <em>learn</em> a specific accuracy.</p>
","2","2","836","25573"
"63795","<p><strong>TL;DR:</strong> Relative scale of multiple different rewards can be important. However, granting +10 for a win and -1 for a loss in a game will <em>not</em> improve speed of learning how to win any better than tuning the learning rate.</p>

<blockquote>
  <p>from a given state if a agent takes a good action i give a positive reward, and if the action is bad, i give a negative reward. </p>
</blockquote>

<p>Usually you do not know what a ""good action"" or ""bad action"" are, and the reward system is based on the immediate outcome of the action from a certain state. </p>

<p>You may already know that, and I will phrase the rest of this answer as if that is what you meant by ""good action"". However, that is an important detail, so if you are not sure what the difference is, you could ask another question about it.</p>

<blockquote>
  <p>So if i give the agent very high positive rewards when it takes a good action, like 100 times positive value as compared to negative rewards, will it help agent during the training?</p>
</blockquote>

<p>The ideal reward scheme is based on easy to measure outcomes that you care about. E.g. reaching a destination, solving a puzzle, how many items are collected, the score in a game, winning a game against an opponent.</p>

<p>If you have both negative and positive rewards, they usually need to be scaled relative each other in a way that makes sense given the problem. It is quite common to have a low negative reward for every time step for instance, if some resource such as time, fuel or money is being used up just from the agent acting and not solving the task. In that case it may make sense to have e.g have a small -0.1 reward for ""bad actions"" and a larger +10 reward for ""good action = completing the task"".</p>

<p>Scaling up rewards is like increasing the learning rate. There is usually some optimal learning rate where the agent learns fastest. Too high and the learning is not stable. Too low and the learning is too slow. Given that is the case, usually you just care about getting relative sizes of rewards correct, then you can scale the learning rate to get the best learning speed.</p>

<p>There is no specific benefit to scale up positive rewards only. You should generally do so only when the problem definition allows you to. It <em>might</em> help with speed that the agent learns when you test it, but most of that effect will be the same as scaling the learning rate.</p>

<p>This is different to training animals, teaching small children or rewards for humans in general, where you may be advised to use positive rewards and positive signals more often than negative. However, this is likely to be related to problem domain of general survival and generalist learning in living creatures, which most RL does not replicate. It definitely is <em>not</em> advice that extends to the simpler statistical agents built using RL such as Q-learning.</p>
","1","2","836","25573"
"63838","<blockquote>
  <p>Is my understanding correct and is it recommended to have such a reward structure for my use case ?</p>
</blockquote>

<p>Your understanding is not correct, and setting extremely high rewards for the goal state in this case can backfire.</p>

<p>Probably the most important way it could backfire in your case, is that your scaling of bad results becomes irrelevant. The difference between 0 and -50 is not significant compared to the +1000 result. In turn that means the agent will not really care by how much it fails when it does, except as a matter of fine tuning once it is already close to an optimal solution. </p>

<p>If the environment is stochastic, then the agent will prioritise a small chance of being at the target temperatures, over a large chance of ending up at an extreme bad temperature.</p>

<p>If you are using a discount factor, <span class=""math-container"">$\gamma$</span>, then the agent will prioritise being at the target temperatures immediately, maybe overshooting and ending up with an unwanted temperature within a few timesteps.</p>

<p>Working in your favour, your environment is one where the goal is some managed stability, like the ""cartpole"" environment, with a negative feedback loop (the correction to the measured quantities is always to force in the opposite direction). Agents for these are often quite robust to changes in hyperparameters, so you may still find your agent learns successfully.</p>

<p>However, I would advise sticking with a simple and relatively small scale for the reward function. Experimenting with it, after you are certain that it expresses your goals for the agent, is unlikely to lead to better solutions. Instead you should focus your efforts on how the agent is performing, and what changes you can make to the learning algorithm.</p>

<p>What I would do (without knowing more about your environment):</p>

<ul>
<li><p>Reward +1 per time step when temperature is in acceptable range</p></li>
<li><p>Reward between -0.1 * temperature difference per time step when temperature is outside acceptable range. It doesn't really matter if you measure that in Fahrenheit or Celsius.</p></li>
<li><p>No discounting (set discount factor <span class=""math-container"">$\gamma =1$</span> if you are using a formula that includes discounting)</p></li>
</ul>

<p>The maximum total reward possible is then +36, and you probably don't expect a worse episode than around -100 or so. This will plot neatly on a graph and be easy to interpret (every unit below 36 is roughly equivalent to performance of an agent spending 15 mins per day just outside acceptable temperatures). More importantly, these lower numbers should not cause massive error values whilst the agent is learning, which will help when training a neural network to predict future reward.</p>

<hr>

<p>As an aside (as you didn't ask), if you are using a value-based method, like DQN, then you will need to include the current timestep (or timesteps remaining) in the state features. That is because the total remaining reward - as represented by action value Q - depends on the remaining time that the agent has to act. It also doesn't matter to the agent what happens after the last time step, so it is OK for it to choose actions just before then that would make the system go outside acceptable temperatures at that point.</p>
","1","2","836","25573"
"64655","<p>The Medium post has brackets in the wrong place . . . the second expectation <strong>must</strong> be inside the sum to make sense, otherwise <span class=""math-container"">$t$</span> is not defined. You can see a couple of steps later that <span class=""math-container"">$Q(s_t, a_t)$</span> gets magically moved back inside the sum. </p>

<p>I cannot see a way to fix the first expectation though that uses <span class=""math-container"">$t$</span> <em>before</em> it is defined!</p>

<p>The expectations are trying to show which parts of the distribution of trajectories each calculation <em>explicitly</em> depends upon. I don't think you need to read into it anything more than that.</p>

<p>However, I would suggest you find another article which does not have these errors. There are similar derivations of Policy Gradient in <a href=""http://incompleteideas.net/book/the-book.html"" rel=""nofollow noreferrer"">Reinforcement Learning: An Introduction</a></p>
","1","2","836","25573"
"65515","<p>DQN solves optimal control problems for maximising average reward. Although it typically <em>uses</em> discounted reward, the discount factor is not part of the setting - instead it is part of the solution hyperparameters, and usually set quite high e.g. 0.99 - when using function approximators.</p>

<p>The TD target used in DQN is a problem for you:</p>

<p><span class=""math-container"">$$G_{t:t+1} = R_{t+1} + \gamma \text{max}_{a'}Q(S_{t+1},a')$$</span></p>

<p>as it relies on a Bellman equation that no longer holds for the given value functions. In your case, there does not seem to be any way to express the TD target for time <span class=""math-container"">$t$</span> by referencing other Q values, as you would need to then <em>subtract</em> future rewards, which is very ungainly. Instead, you could use a simple truncated Monte Carlo return</p>

<p><span class=""math-container"">$$G_{t:t+3} = R_{t+1} + R_{t+2} + R_{t+3}$$</span></p>

<p>There are a few different ways you could do this, but the simplest and closest to DQN would IMO be to:</p>

<ul>
<li>Store trajectories in order in the experience replay table</li>
<li>For each item in the training mini-batch:

<ul>
<li>Pick a start state/action pair to assess <span class=""math-container"">$s_t, a_t$</span> randomly from replay table</li>
<li>Check that <span class=""math-container"">$a_{t+1}$</span> and <span class=""math-container"">$a_{t+2}$</span> are maximising actions in your current target policy for <span class=""math-container"">$s_{t+1}$</span> and <span class=""math-container"">$s_{t+2}$</span>, reject the sample if not (note you don't need to check or reject <span class=""math-container"">$a_t$</span>, and that is how your code learns about exploratory actions)</li>
<li>Calculate the TD target, <span class=""math-container"">$g_t = r_{t+1} + r_{t+2} + r_{t+3}$</span></li>
<li>Your training data for that example is <span class=""math-container"">$s_t, a_t, g_t$</span></li>
</ul></li>
</ul>

<p>The checking for maximising action parts could be quite slow, so you might prefer to simplify the approach and not use off-policy. Alternatively, if <span class=""math-container"">$epsilon$</span> is low enough you could just store the three step returns directly in the experience replay table (wait until you have the data from <span class=""math-container"">$t+3$</span> before storing data for <span class=""math-container"">$t$</span>) and ignore the fact that some returns are from exploratory actions, thus noisy/biased . . . this approach is used in n-step returns in DQN ""Rainbow"" version and works well enough in practice on the Atari problems despite being on shaky theoretical ground.</p>

<hr>

<p>Note I am using the convention <span class=""math-container"">$s_t, a_t, r_{t+1}, s_{t+1}$</span> to represent a step in the trajectory, whilst in the question you appear to be using <span class=""math-container"">$s_t, a_t, r_t, s_{t+1}$</span> with a different reward index. You will need to convert back if you want to stick with your convention.</p>
","1","2","836","25573"
"65576","<p>I would not concern yourself too much about any structuring of knowledge that declares that one subject is categorised as one thing or another. These structures are often wrong and knowledge in general is more fluid and difficult to define than can be viewed as some kind of Venn diagram.</p>

<p>In addition, both Data Science and AI are poorly defined, and have more in common with marketing terms used to recruit staff and sell products than stricter academic subject areas such as Maths or Modern Languages. You will find Data Scientists working on ""AI"" projects and ""AI Engineers"" using basic statistics to analyse data.</p>

<p>Having said that, I would prefer to say that Data Science and AI share a common set of tools, especially when it comes to machine learning. Neither contains the other in my opinion.</p>
","3","2","836","25573"
"65779","<blockquote>
  <p>But since we already compute the Q-value of action a in state old_state in order to choose a as the best action, why don't we simply store directly the Q-value ?</p>
</blockquote>

<p>That is because calculating a relevant TD Target e.g. <span class=""math-container"">$R + \gamma \text{max}_{a'}Q(S', a')$</span> requires the <em>current</em> target policy estimates for action value <span class=""math-container"">$Q$</span>. The action value at the time of making the step can be out of date for two reasons:</p>

<ol>
<li><p>The estimates have changed due to other updates, since the experience was stored</p></li>
<li><p>The target policy has changed due to other updates, since the experience was stored</p></li>
</ol>

<p>Storing the <span class=""math-container"">$Q$</span> value at the time the experience was made should still work to some degree, provided you don't keep the experience for so long that the values are radically different. However, it will usually be a lot less efficient, as updates will be biased towards older less accurate values.</p>

<p>There is a similar, but less damaging, effect from the experience replay table even with <span class=""math-container"">$Q$</span> recalculations. That is because the <em>distribution</em> of experience may not match what the current policy generates - something that most function approximators (e.g. neural networks used in DQN) are sensitive to. However, there are other factors in play here too, and it can be beneficial to train on a deliberately different distribution of experience - for instance prioritising experiences with larger update steps can speed learning and keeping older experiences available can reduce instances of catastrophic forgetting.</p>

<p>Note that if you were using an off-policy Monte Carlo method, you <em>could</em> store the Monte Carlo return in the experience replay table, since it does not bootstrap by using current value estimates. However, the early parts of older less relevant trajectories would stop contributing to updates in that case once the target policy had changed significantly during learning.</p>
","2","2","836","25573"
"72182","<p>The sources are both correct, they implement bias in different ways, and are counting slightly different things:</p>

<ul>
<li><p>Your first source implements a separate bias vector in each <em>output</em> layer in addition to the weights, and is referring to the dimension of the bias vector when it is counting biases.</p></li>
<li><p>Your second source implements bias as both a separate fixed value <span class=""math-container"">$1.0$</span> in each <em>input</em> layer, and a larger weights matrix with an extra column containing the actual learned bias value. It is referring to the extra added value in each layer when counting the ""biases"" - more accurately it is counting the added bias ""signals"" and not the learned bias values, because the learned bias values are implemented inside the weights matrix when using this approach.</p></li>
</ul>

<p>In both cases, the number of learned values added due to bias are the same, and are the same as this:</p>

<blockquote>
  <p>the answer of the first should be 5 and for the second 4 + 3. Each neuron except for in the input-layer has a bias</p>
</blockquote>

<p>In the second case the equivalent values appear in weight matrices as extra columns. It's really just an implementation difference. By adding a fixed bias signal to inputs, it simplifies the learning update for each layer to occur to only one matrix, as opposed to a matrix plus a separate bias vector (requiring different update rules for each). This is at the expense of needing to manipulate the inputs each time, and there is no great difference between the approaches in terms of efficiency. Some libraries take one approach, some take the other.</p>
","1","2","836","25573"
"73146","<blockquote>
  <p>here precision at threshold 0.85 > precision at threshold 0.90. shouldnt it be the other way round? increasing threshold will reduce False positive and precision will be greater than before?</p>
</blockquote>

<p>Precision is <span class=""math-container"">$\frac{\text{TP}}{\text{TP}+\text{FP}}$</span></p>

<p><em>Both</em> <span class=""math-container"">$\text{TP}$</span> and <span class=""math-container"">$\text{FP}$</span> are reduced when you increase the threshold. If both decrease in proportion to the current precision (i.e. they are spread evenly at each confidence value), then precision will remain the same. Most models on most datasets <em>will</em> tend to increase precision as the threshold increases, at least initially (e.g. moving from 0.5 to 0.6) as false positives may commonly be found as uncertain edge cases with low confidence, i.e. false positives tend to occur more frequently at low confidence, so increasing threshold will exclude a higher ratio of false positives than true positives than the current precision. However, there is no guarantee of that.</p>

<p>The value of precision will vary in practice depending on what the model predicted for each example. If you have a cluster of highly confident false positives, they can cause precision to drop as threshold grows, until they get excluded. The most extreme example would be where the most confident classification is incorrect, in which case the highest possible threshold will score zero precision.</p>
","6","2","836","25573"
"74336","<p>It is relatively common when learning basics of RL (as opposed to Deep RL with neural networks etc), to consider a single discrete state variable. For instance many grid worlds, maze solvers etc just enumerate the positions. For practical learners, the variable is effectively one-hot encoded, but it is still a single variable.</p>

<p>The number of state features is not relevant to whether RL is applicable or not.</p>

<p>Assuming you have correctly formulated your problem as a Markov Decision Process (MDP), then the important things to consider are:</p>

<ul>
<li><p>Do the state variables capture enough data about the current state so that the problem has the <a href=""https://en.wikipedia.org/wiki/Markov_property"" rel=""noreferrer"">Markov property</a> (that predictions of future rewards and state can be made using only the current state and action choices)?</p></li>
<li><p>Do the state variables need to be pre-processed before being used as features for your learning algorithm?</p></li>
<li><p>Is your state space, once turned into features, small and suitable for learning with a simple tabular method? </p></li>
<li><p>If the state space is not small, does a simple transformation (such as using a discrete tiling) make the problem simple, possibly linear, or is it complex? Simple problems can be adjusted to work with tabular approaches or linear approximators, complex ones will need approaches from Deep RL, such as DQN or A3C.</p></li>
</ul>
","5","2","836","25573"
"74678","<p>The DQN agent does not need to care what the actions represent, in your case it only needs to make a discrete choice, and it is simple to enumerate the action space. Ignoring the meaning of them for a moment, you have 16 discrete actions. The simplest way to model that is to have a single index discrete action space of 16 actions for the agent which you then map to the selections that you need in order to assess the results. As long as you do this consistently (e.g. take the binary representation of the action index number), this is fine. </p>

<p>It is also possible that using a more direct representation would help, depending on the true nature of the value function. In which case you could use it, provided you model the neural network for <span class=""math-container"">$\hat{q}(s,a,\theta)$</span> with action vector concatenated to state vector in the input, and a single output of the estimated action value for that specific combination. To assess which action to take, you would create a minibatch of 16 inputs, all of which have the same state component, and cover the 16 possible input variations. Then you would pick the combination with the highest estimate and look at the action part of the input vector to discover which action was estimated to be best.</p>

<p>If you are not sure which approach would suit the problem best, you could try both.</p>
","0","2","836","25573"
"74854","<blockquote>
  <p>Now, if I deploy this model (i.e., use this model to make predictions), does it keep learning (i.e., updating the Q values)?</p>
</blockquote>

<p>If you want it to (and understand how to code it) then yes a reinforcement agent - including a DQN-based one - can do this. This is <a href=""https://en.wikipedia.org/wiki/Online_machine_learning"" rel=""nofollow noreferrer"">online learning</a>, and is possible also with many supervised learning techniques.</p>

<p>Because there is risk of the agent learning incorrectly, you may choose to limit online learning, or disable it in production.  It could also happen by mistake if you are not sure how to stop it, and simply deploy your existing agent code from the training scripts into production. So make sure that you, or whoever you get to implement production version of the agent, understands how to control whether learning is still occurring.</p>

<p>You may choose to enable online learning if your initial training was in simulation, and you would like the agent to learn more from real-world interactions. Or you might choose to do so if the problem was non-stationary because the environment description changes over time. Many problems involving interacting with a population of people change over time with demographic changes.</p>

<p>If the environment is stochastic, then in theory you could also switch off exploration - typically by setting <span class=""math-container"">$\epsilon = 0$</span> in an <span class=""math-container"">$\epsilon$</span>-greedy behaviour policy. That would allow the agent a (limited) ability to refine its Q value estimates with a relatively low risk. The agent would continue to attempt to act optimally, but may learn enough to decide that different actions were optimal. Note this is still not without risk, because the learning process could fail in some way, leading the agent to predict a wildly incorrect optimal action.</p>

<p>Allowing Q learning to explore non-optimal actions - typically by setting <span class=""math-container"">$\epsilon &gt; 0$</span> in an <span class=""math-container"">$\epsilon$</span>-greedy behaviour policy - is more risky in production, because the agent will occasionally pick a non-optimal action in order to refine its estimates of that action. It may result in improved learning in the longer term though, so you might decide to do that if the consequences of non-optimal actions were mild.</p>
","1","2","836","25573"
"77789","<blockquote>
<p>Can someone explain what exactly breaks down for non-episodic tasks for Monte Carlo methods in Reinforcement Learning?</p>
</blockquote>
<p>It is this first part of the sentence that you quote:</p>
<blockquote>
<p>&quot;To ensure that well-defined returns are available...&quot;</p>
</blockquote>
<p>In more detail, the return distribution in an episodic MDP can be defined as</p>
<p><span class=""math-container"">$$G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}$$</span></p>
<p>where <span class=""math-container"">$R_x$</span> is reward distribution at time step <span class=""math-container"">$x$</span>, <span class=""math-container"">$\gamma$</span> is discount factor, and <span class=""math-container"">$T$</span> is the last time step of the episode.</p>
<p>From this you can see that you will need to collect all of <span class=""math-container"">$r_{t+1}$</span> to <span class=""math-container"">$r_T$</span> for an episode in order to calculate a single concrete return value <span class=""math-container"">$g_t$</span>. You can only collect <span class=""math-container"">$r_T$</span> at the end of an episode.</p>
<p>In non-episodic tasks, there is no terminal time step <span class=""math-container"">$T$</span>, and the return definition becomes:</p>
<p><span class=""math-container"">$$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$</span></p>
<p>This cannot be resolved directly by collecting data for infinite time steps. Or in other words you can never sample an actual return value as it is defined. To get around this you could do one or more of the following:</p>
<ul>
<li><p>You could use an artifical time horizon <span class=""math-container"">$h$</span>, and define <span class=""math-container"">$T = t + h$</span>. This will result in a biased sample. However, you can make the bias arbitrarily small relative to total return by making <span class=""math-container"">$h$</span> large. If you are using a discount factor, that may allow for smaller value of <span class=""math-container"">$h$</span>.</p>
</li>
<li><p>You could use temporal difference (TD) learning to bootstrap return estimates without needing a full sample of return. This will result in a biased sample, but the bias due to bootstrapping should reduce as more samples are collected.</p>
</li>
<li><p>You could use TD with eligibilty traces and a high trace decay parameter <span class=""math-container"">$\lambda = 1$</span>, which is a way of getting very similar learning behaviour to Monte Carlo, but still allowing learning to occur on each step in a continuing problem.</p>
</li>
</ul>
<p>These options technically stop the approach being true Monte Carlo. The last two are described in the book under TD methods.</p>
","1","2","836","25573"
"80260","<p>It follows very simply from the definitions of V and Q.</p>
<p>These are the relevant definitions:</p>
<p><span class=""math-container"">$$G_t \space \dot{=} \space \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$</span></p>
<p><span class=""math-container"">$$V^{\pi}(s) \space \dot{=} \space \mathbb{E}_{A \sim \pi}[G_t|S_t=s]$$</span></p>
<p><span class=""math-container"">$$Q^{\pi}(s, a) \space \dot{=} \space \mathbb{E}_{A \sim \pi}[G_t|S_t=s, A_t=a]$$</span></p>
<p>You can ignore the definiton of return, <span class=""math-container"">$G_t$</span>, I included it for completeness, and to simplify the other definitions.</p>
<p>The <em>only</em> difference between the definitions of V and Q is adding the selection of <span class=""math-container"">$a$</span> in the conditions. If you remove the action selection in the definition of Q, and condition it instead on following the policy, you are back to the definition of V.</p>
<p>You can also expand the expectation in the definition of V using the policy function to show the same thing:</p>
<p><span class=""math-container"">$$V^{\pi}(s) = \mathbb{E}_{A \sim \pi}[G_t|S_t=s]$$</span></p>
<p><span class=""math-container"">$$V^{\pi}(s) = \sum_{a}\pi(a|s) \mathbb{E}_{A \sim \pi}[G_t|S_t=s, A_t=a]$$</span></p>
<p><span class=""math-container"">$$V^{\pi}(s) = \sum_{a}\pi(a|s) Q^{\pi}(s,a)$$</span></p>
<p>Convert this back to an expectation over the policy:</p>
<p><span class=""math-container"">$$V^{\pi}(s) = \mathbb{E}_{A \sim \pi}[Q^{\pi}(S_t,A_t)|S_t=s]$$</span></p>
","1","2","836","25573"
"80540","<p>I think your main problem is use of relative distance as the core feature. It has two major weaknesses:</p>
<ul>
<li><p>The distance to an object does not give the direction to the object. The best action choices are all critically dependent on direction. For example an enemy laser bolt 0.1 units directly above the player is an immediate danger requiring evasive action, whilst one 0.1 units to the left or right is not a danger and about to leave the game window. Your feature of relative distance does not distinguish between those scenarios, but it is a critical difference.</p>
</li>
<li><p>Slightly less important, but the raw distance does not capture any sense of movement. If enemies move consistently turn by turn, but not always in the exact same direction or same speed, then their velocities should also be part of the state.</p>
</li>
</ul>
<p>One way you could improve the features is to add a velocity component for each item, showing how quickly it is <em>approaching</em> or <em>receding from</em> the player. This might help a little, but my feeling is that you need more data than distance and speed.</p>
<p>I think you should use normalised <span class=""math-container"">$x, y$</span> position as features for each item being tracked, <em>plus</em> normalised velocity <span class=""math-container"">$dx, dy$</span> for any object type that can change direction (if enemy lasers are always falling straight down you might not need anything for those).</p>
<p>In addition:</p>
<ul>
<li><p>If the window edges are important, you should include at least the relative <span class=""math-container"">$x$</span> of one of them, so the agent knows its absolute position on screen and how much space it has to maneuver. This is true whether the player is blocked from moving further left or right, or whether the player &quot;wraps around&quot; to the other side of the screen. Both types of effect will significantly affect how the game plays near the screen edge.</p>
</li>
<li><p>In order to track predicted value, you need to track location of player missiles. It is not enough to just let the agent predict when it is best to fire - in order to accurately track a value function it needs to &quot;see&quot; whether the missile it fired some time steps ago is likely to hit or miss a target.</p>
</li>
<li><p>For both enemy lasers and player missiles, it is OK to filter and sort the data by some criteria (such as distance to player). As long as this is consistent it may even help a lot to have such pre-processing.</p>
</li>
</ul>
","0","2","836","25573"
"81200","<p><strong>TL;DR</strong> Not usually, because Deep Dream ignores the distribution of your input population.</p>
<p>You cannot extract additional <em>informative</em> data using generators, only remix and recombine what you already have. There may be a few situations where that is useful because you can use a generator trained on <em>other</em> data to bring in extra relevant information that was not in your main dataset.</p>
<hr />
<p>Deep Dream logically alters an input structure to maximise some measure within a neural network based on neuron activations. This might be the activation of a single neuron, or of a layer of neurons.</p>
<p>I have worked with Deep Dream scripts based on Google's Inception v5 network which was trained on ImageNet, just for fun.</p>
<p>Here's a relatively extreme example of output, with a few specific neuron activatioons maximised and a long zoom so that the data is essentially only a super-stimulous for those artifical neurons:</p>
<p><a href=""https://i.stack.imgur.com/20mXN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/20mXN.png"" alt=""Deep dream image by Neil Slater"" /></a></p>
<p>One thing you can say with reasonable certainty is that this image does not look like anything you might draw at random from ImageNet, from any category. It would be a poor choice as augmentation for ImageNet classification tasks, because it is not from the target population.</p>
<p>The same is true in general for all obvious Deep Dream modifications. The modifications do not look natural and produce inputs that would not reasonably be in the input distribution. Once you are clearly outside the input distribution for your use case, then learning how to process those inputs may not give you any benefit related to the goals of your machine learning task.</p>
<p>More subtle changes might be useful, if on inspection it is clear that they should not change the categorisation. This might be similar augmentation to adding noise, and if the alterations were chosen carefully, might help defend against adversarial inputs. In fact generating adversarial inputs is an almost identical process to Deep Dream, usually focusing on the output categories and updates that look like high frequency noise so that humans tend not to see the difference.</p>
<p>There are better data generating models that attempt to create data that could have come from the input population. Two major ones are <a href=""https://jaan.io/what-is-variational-autoencoder-vae-tutorial/"" rel=""nofollow noreferrer"">variational autoencoders</a> (VAE) and <a href=""https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/"" rel=""nofollow noreferrer"">generative adversarial networks</a> (GAN). These might be useful for data generation.</p>
<p>However, if you are considering somehow &quot;bootstrapping&quot; a small dataset by training one of these generators on it then using its output to augment your data, it is unlikely that this will work. The quality of the output will usually depend on its training data such that generated data will not truly add information that you did not already have in the original training set. In most cases you will better off looking at simpler logical transformations of data that you are confident will not change the target label (or that change it in predictable ways) - for image-based CNNs that might include changing rotation, crop, brightness/contrast and other basic image manipulations.</p>
<p>There might be a few cases in which working with a generator helps. For instance if you can train it on far more data because you have lots of unlabelled data, or dat afrom a different but similar task. Then the quality of generated inputs could be high enough that you effectively get to make use of unsupervised learning from the same dataset. You might still find it easier to do transfer learning, or train an autoencoder and re-use its first few layers though.</p>
","1","2","836","25573"
"81659","<blockquote>
<p>This sounds like the t means time and the eight features are the latest 8 board states.</p>
</blockquote>
<p>That is correct. The description in the article is clear.</p>
<blockquote>
<p>I have the feeling the eight features should be the combination of rotated (4) and reflected (2) board states of the current board state. Please correct me if I am wrong.</p>
</blockquote>
<p>That is wrong.</p>
<p>This data augmentation can be done by modifying state data stored from each episode, and applied during weight update step (either randomly or exhaustively), because any rotation/reflection of a board should have the same expected value. It is not necessary to provide that augmentation at the feature level.</p>
<blockquote>
<p>Board repetition can be easily prevented by keeping tracking of all previous board states and when a new move is attempted, just compare against all previous board states.</p>
</blockquote>
<p>A game engine can do that, as could bespoke code added to the agent that was specific to Go. However, a neural network cannot do that unless those previous states are part of the input.</p>
<blockquote>
<p>The same is for komi.</p>
</blockquote>
<p>If you want the board to correctly predict the value of a state, the nerual network needs to have the current player identity encoded in some way. Having it tracked outside of the neural network's inputs (i.e. outside of state representation) does not help with this.</p>
<blockquote>
<p>What if the repetition happened to the last 9th board state?</p>
</blockquote>
<p>Most typically in Go, the repetition rule has effects over a relatively short duration, because moves add stones in new positions. The relevant rules are called <a href=""https://en.wikipedia.org/wiki/Rules_of_Go#Ko_and_Superko"" rel=""nofollow noreferrer"">ko and superko</a>, with superko being optional. When applying the standard ko rule, the repetition that it blocks is only likely to be achievable through play on short timescales (2 steps) that are covered by the AlphaGo Zero representation.</p>
<p>In the unlikely event that a play is blocked due to superko rule and repetition from earlier than 8 steps ago, the game engine would prevent a play that the agent may have assigned a high value to, and the agent could get the value wrong. I suspect that this has little or no impact on performance, as loops in Go board state of this duration are rare and do not occur much in practice.</p>
","2","2","836","25573"
"81855","<p>In Sutton &amp; Barto, vectors are considered column vectors by default. So if you have this kind of product:</p>
<p><span class=""math-container"">$$\mathbf{a}\mathbf{b}^T$$</span></p>
<p>where <span class=""math-container"">$\mathbf{a}$</span> and <span class=""math-container"">$\mathbf{b}$</span> are <span class=""math-container"">$d$</span> dimensional vectors, it does <em>not</em> calculate the scalar product. Instead it treats both vectors as matrices and calculates a matrix product, which will be a <span class=""math-container"">$d \times d$</span> matrix because you are multiplying a <span class=""math-container"">$d \times 1$</span> matrix by a <span class=""math-container"">$1 \times d$</span> matrix.</p>
<p>Worthing noting that the scalar product can also be calculated as a <span class=""math-container"">$1 \times 1$</span> matrix if follow the same matrix multiplication rules but with the first vector transposed instead:</p>
<p><span class=""math-container"">$$\mathbf{a}^T\mathbf{b}$$</span></p>
<p>which leads to multiplying a <span class=""math-container"">$1 \times d$</span> matrix by a <span class=""math-container"">$d \times 1$</span> matrix. This is why the value function approximation can be written as <span class=""math-container"">$\mathbf{w}^T\mathbf{x}_t$</span> (there is a small liberty taken of assuming a <span class=""math-container"">$1 \times 1$</span> matrix is the same as a scalar value in terms of notation).</p>
","2","2","836","25573"
"82896","<p>Linear regression will not work for this problem because the relationship between the board features and target variable that you are using is not linear.</p>
<blockquote>
<p>Is this how data scientists would go about creating the training set for tic tac toe?</p>
</blockquote>
<p>It is not 100% clear what your goal is. For simplicity I will select that your goal as &quot;Predict the probability of X winning eventually given the current board state and completely random play in future by both sides.&quot; That appears to be what you are doing.</p>
<p>As an aside, this is not a direct path to training a neural network to predict the best moves to make in a game. For this simple game, it might work acceptably if that is your eventual goal, but if you want machine learning for game playing you should probably look into reinforcement learning, and specifically self-play with reinforcement learning, as a framework to manage the training data.</p>
<p>Back to your question, what you are doing is acceptable for creating a data set, although I would want to check:</p>
<blockquote>
<p>For every tie game(full board) I do not count it as anything</p>
</blockquote>
<p>If that means you are still storing the states that lead to a tie, but with a different label, then that is ok. If you are discarding data about ties, then that will skew the dataset and might impact your predictions - unless you are also discarding ties when testing.</p>
<p>This is also slightly unusual:</p>
<blockquote>
<p>At the end I get my training data set which includes the board state (the feature set) and the outcome which is the percentage (a floating pint value. eg .8 is 80%) of games won for that state.</p>
</blockquote>
<p>This is unusual in that you have pre-processed the data into a summary row when features are identical. This skews the dataset when used with an approximation function (linear regression - like most ML statistical learners - is an approximation function), because you lose the number of times those features occurred. Any balancing the prediction function does to make itself more accurate for common states is lost when you do this. It is more normal to keep all records separate and have the ML method resolve the best way to take averages. If you measure the accuracy of your completed model by taking random samples of new played games, it could have lower accuracy than possible due to this.</p>
<p>For data collection of records, it is more usual to keep all observations separate and not to summarise them before training a classifier. The classifier can then fit the data allowing for the frequency of each observation.</p>
<p>Other than the caveats about ties (which you may well have right), and premature taking of averages, plus the limitation that your dataset will only help predict outcomes in fully random games, then the dataset collection looks ok to me. Neither of the above problems are major enough to cause the problem that you noticed. The reason your predictions are not working with linear regression is mainly due to needing non-linearity in the prediction function.</p>
<p>A simple fix for this would be to use a non-linear predictor such as a neural network or maybe a decision-tree algorithm like xgboost.</p>
<p>If you use a neural network, the following may help:</p>
<ul>
<li><p>Use sigmoid activation in the output layer and binary cross-entropy loss. This should help when your output is a probability.</p>
</li>
<li><p>Use the value <span class=""math-container"">$-1$</span> instead of <span class=""math-container"">$2$</span> for marking positions in the board played by <strong>O</strong>. This is not strictly required, but neural networks tend to learn faster and more accurately when the input data in centered around zero with close to 1 standard deviation.</p>
</li>
</ul>
<p>It is worth noting that your averaged win rate table is already quite a reasonable predictive model for game playing. For TicTacToe it should work quite well because there are a limited number of states. After 20k games with random play, you will have a record of nearly every possible state, and some will have reasonably accurate average values (for instance each initial play by <strong>X</strong> will have ~2000 sampled continuations which should give you the win rate within a few percent). The weakness of this approach is that it cannot generalise to new unseen states, but actually that is quite hard to do in board games where fine detail matters.</p>
","1","2","836","25573"
"84540","<blockquote>
<p>I wanted to know what is the answer to the question 'how is your policy expressed'?</p>
</blockquote>
<p>Typically when this is asked, what the asker wants to know is something like:</p>
<p><em>How do you represent the action choice in the output?</em></p>
<p>For example, if there are N discrete action choices, and your policy is stochastic, you could say something like: &quot;The policy is expressed as a vector size N, with each value being the probability of choosing a matching action.&quot; You may also want to enumerate the actions - to say which index of the vector corresponds to which specific action. That depends on what need the person communicating with you has for understanding your policy.</p>
<p>Plus maybe:</p>
<p><em>How do you represent the state in the input?</em></p>
<p>The same thing here, give the state representation required to make the policy function work according to your implementation. Again, whether you should supply this depends on why the person asking you wants to know.</p>
<blockquote>
<p>Am I supposed to give an analytical function?</p>
</blockquote>
<p>With the right level of detail, that should be fine. The input and output domains plus maybe the index map (of which input and output features are in which positions). Note that because you are asked for how the policy is expressed, you would usually stop at the point that you have defined the probability distribution over the action space.</p>
","1","2","836","25573"
"85689","<p>In general, there is more than one kind of optimiser. The gradient-based optimisers such as gradient descent, stochastic gradient descent, Adam, Adagrad, RMSProp, form one broad category of optimiser that can find minima or maxima of scalar functions provided it is possible to calculate a gradient.</p>
<p>In your question however, you are effectively focusing just on gradient-based optimisers - why can they be used to alter input images or latent spaces, as well as neural network weights?</p>
<p>The answer is that these optimisers are not specific to neural network weights, or even to neural networks. They are appropriate whenever:</p>
<ul>
<li><p>You can measure success at a task with a scalar function - typically this is a loss or cost function to minimise, but it can also be a numerical score that you wish to maximise.</p>
</li>
<li><p>The task involves parameters that you can control. This is a set of variables that can be changed in order to solve a problem. The weights of a neural network are one example, but so are the pixels in an image if your goal is to find an image that matches some criteria (such as looking like a photo of a face).</p>
</li>
<li><p>You have a way to estimate gradients of that function with respect to the parameters that you can control. Usually some form of back propagation is used to get from the loss function to the parameters you care about. That is the case in style transfer or stylegan, but it is not a requirement.</p>
</li>
</ul>
<p>The basic requirement is being able to estimate the gradient of your scoring function with respect to parameters that can be changed. Then you can minimise or maximise that function - or at least find local minima and maxima. Neural network weights are just one common example of the approach.</p>
<p>Likewise, using minibatches of input/output pair examples is one common way of estimating gradients. It is commonly used when learning how to generalise from a data set, but is not the only possible way to get gradients - which approaches are available and make sense depends on the optimisation problem you need to solve.</p>
","1","2","836","25573"
"86186","<p>MADDPG can be used to model agents that have limited observation and communication capabilities after training, which is an interesting and useful real world scenario.</p>
<blockquote>
<p>why not simply treat all cooperating agents as a single meta-agent with a concatenated observation space and a concatenated action space?</p>
</blockquote>
<p>Any real world implementation will then require resources to provide and manage that overview. This may not be practical or desirable in all cases.</p>
<p>There is no single fix for this, it is an open area of research. Whether to invest in better communication and central processing, or better autonomy for multiple agents is likely to have different answers depending on the problem and current technology limits for either approach.</p>
<p>MADDPG reduces the role of central processing to assessment of global reward signals during <em>training</em>. That means:</p>
<ul>
<li><p>Each agent works with local signals only, and simpler observation and action spaces as a result. Only the reward signal processing is handled externally.</p>
</li>
<li><p>Trained agents can theoretically be used in environments where a central processor is not available.</p>
</li>
</ul>
<p>So, for example, agents can be trained in simulation with all the oversight that allows, or with a carefully instrumented environment including high bandwidth connections between agents and central processing. They can then be deployed into matching environments where the central oversight is not available, or too costly.</p>
","0","2","836","25573"
"90063","<p>The way you have set your DQN up, it is designed to solve just one maze at a time. It has not (and cannot) learn to solve mazes in general, because it has no access to data about the layout of the maze, and a basic DQN agent has no capability to memorise layout seen so far.</p>
<p>You <em>could</em> view the training process as general algorithm for &quot;solving the maze&quot;. That is, given a new maze, you can take your written agent, run it for a while in training mode and it will produce a policy that solves that maze. You may be able to tune or adjust the algorithm so that it does it the most efficiently in terms of time or number of steps. It will not be able to do so as well as a hard-coded maze-solving algorithm, because DQN and Q-learning is a very generic learning algorithm that takes no advantage of consisent logic seen in mazes.</p>
<p>If you want to train a more generic maze-solving agent, this becomes a harder problem to solve using reinforcement learning, but it is achievable. There are two key - and linked - assumptions of the theory behind Q learning that you would need to address before you could make a general maze solver that coped with variations such s moving the walls, but without needing to be re-trained:</p>
<ul>
<li><p>The <a href=""https://en.wikipedia.org/wiki/Markov_property"" rel=""nofollow noreferrer"">Markov property</a> is assumed for the state variables, which means that the state description contains all the relevant information about future state transitions and rewards. Without some way to know where the walls are, or at least note the position of walls it has seen before, the agent does not have access to a state representation with the Markov property.</p>
</li>
<li><p>There is at least one deterministic optimal policy. If you are intending to withold information from the agent (and not give it capabilities such as memory in order to construct missing information), then there may not be a deterministic optimal policy.</p>
</li>
</ul>
<p>So it is possible to use reinforcement learning to teach an agent about solving mazes in general, and gain the ability to attempt to navigate out of a new unseen before maze. If that is of interest to you, you need to first decide what capabilities and knowledge about the environment the agent is allowed to observe. E.g. what specifically interests you about writing a maze solver?</p>
<p>Which approaches you could use to make a more general maze solver depends critically on how you want to frame the problem. For instance it is entirely valid to give the solver an image of the maze as input if you want the agent to learn to solve mazes in a similar manner to a human solving a printed puzzle.</p>
<p>There is one common factor that you will need for training a more generic maze solver - you will want to train on many example mazes taken from the population of all possible mazes. Training on just one maze as an example will typically be about as successful as training a supervised classifier or regression task on one example. Adding many more mazes, or better a maze generator, and training on many of them is likely to make training time much longer than before.</p>
","4","2","836","25573"
"92306","<p>The full method is explained in the paper <a href=""https://arxiv.org/abs/1707.02286"" rel=""nofollow noreferrer"">Emergence of Locomotion Behaviours in Rich Environments</a> by the DeepMind team.</p>
<p>Quoting from that paper:</p>
<blockquote>
<p>Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required
by the environment . . .</p>
</blockquote>
<p>So to answer your question, the researchers used a policy gradient method.</p>
","1","2","836","25573"
"94059","<p>Any exploration function that ensures the behaviour policy <em>covers</em> all possible actions will work in theory with Q learning. By <em>covers</em> I mean that there is a non-zero probability of selecting each action in each state. This is required so that all estimates will converge on true action values given enough time.</p>
<p>As a result, there are many ways to construct behaviour policies. It is possible in Q learning to use equiprobable random action choice - ignoring current Q values - as a behaviour policy, or even with some caveats learn from observations of an agent that uses an unknown policy.</p>
<p>There are practical concerns:</p>
<ul>
<li><p>If the behaviour policy is radically different from optimal policy, then learning may be slow as most information collected is not relevant or very high variance once adjusted to learning the value of the target policy.</p>
</li>
<li><p>When using function approximation - e.g. in DQN with a neural network - the distribution of samples of state, action pairs seen has an impact on the approximation. It is desirable to have similar population of input data to that which the target policy would generate*.</p>
</li>
<li><p>In some situations, a consistent policy over multiple time steps gives better exploration characteristics. Examples of this occur when controlling agents navigating physical spaces that may have to deviate quite far from current best guess at optimal in order to discover new high value rewards.</p>
</li>
</ul>
<p>These concerns drive designs of different exploration techniques.</p>
<p>The epsilon-greedy approach is very popular. It is simple, has a single parameter which can be tuned for better learning characteristics for any environment, and in practice often does well.</p>
<p>The exploration function you give attempts to address the last bullet point. It adds complexity, but may be useful in a non-stationary environment since it encourages occasional exploration paths far away from the current optimal one.</p>
<blockquote>
<p>Now my question is, are these 2 above strategies just 2 different ways of Q learning?</p>
</blockquote>
<p>Yes.</p>
<blockquote>
<p>Or can the exploration function be used along with the epsilon greedy Q learning algorithm as a form of some optimization?</p>
</blockquote>
<p>Yes it should be possible to combine the two approaches. It would add complexity, but it might offer some benefit in terms of learning speed, stability or ability to cope with non-stationary environments.</p>
<p>Provided the behaviour policy covers all possible actions over the long term, then your choice of how exploration for off-policy reinforcement learning works is one of the hyperparameters for a learning agent. Which means that if you have an idea for a different approach (such as combining two that you have read about), then you need to try it to see if it helps for your specific problem.</p>
<hr />
<p>* But not necessarily identical, because the approximate Q function would then lose learning inputs that differentiate between optimal and non-optimal behaviour. This can be a tricky balance to get right, and is an issue with <em>catastrophic forgetting</em> in reinforcement learning agents.</p>
","3","2","836","25573"
"94777","<p>The &quot;hidden&quot; layers and state in a neural network are those that do not have direct input or output with respect to the function being learned.</p>
<p>I don't think you need to read anything deep into the analogy. The term &quot;hidden&quot; was, according to Geoffrey Hinton, coined by him partly because he liked the mysterious sound of it (from hidden markov models). He freely admits that it is partly an awkward name since in the case of deep neural networks, the data in the hidden layers is not inaccessible or unknown.</p>
<p>I heard him state this in one of the lectures in a now closed Coursera course.</p>
<p>Sadly I am not able to find a current reference to this in his own words. I have found a reference to the same story in the book <a href=""https://books.google.co.uk/books?id=B71nu3LDpREC&amp;pg=PA24&amp;lpg=PA24&amp;dq=geoffrey%20hinton%20talking%20about%20hidden%20markov%20models%20and%20hidden%20layers&amp;source=bl&amp;ots=KluCSxz9UU&amp;sig=ACfU3U3n0nW0hfbLN37zPaMAhmvKonj6UQ&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwj7uOrAi97wAhW1ShUIHb_nCjQQ6AEwCXoECAgQAw#v=onepage&amp;q=geoffrey%20hinton%20talking%20about%20hidden%20markov%20models%20and%20hidden%20layers&amp;f=false"" rel=""nofollow noreferrer"">Backpropagation: Theory, Architectures, and Applications</a>.</p>
","2","2","836","25573"
"96576","<blockquote>
<p>I think I have understood that it consists in choosing the bandit that maximizes the future reward.</p>
</blockquote>
<p>Yes, in expectation.</p>
<p>The target of finding the best action is often easy to get eventually correct - you could for instance try each action 1000 times in turn and calculate the average reward from it. So, there are usually two other important goals that bandit algorithms try to solve:</p>
<ul>
<li><p>Finding the best action with the least number of trials.</p>
</li>
<li><p>Scoring the highest reward <em>during learning</em>. This may also be expressed as &quot;minimising regret&quot;, the idea of looking back once you know what the best action is and figuring out how far away from optimal all your previous choices were.</p>
</li>
</ul>
<blockquote>
<p>Can I set the rewards of a multi armed bandit problem with deterministic values?</p>
</blockquote>
<p>You can. A deterministic value is a special case of a probability distribution - it has <span class=""math-container"">$p(X = r) = 1$</span> and <span class=""math-container"">$p(X \ne r) = 0$</span> - so you can even use exactly the same description of the problem.</p>
<p>However, deterministic mulit-armed bandits are not very challenging. If you know in advance that the results are deterministic then you could try each action once and then pick the maximising result from then on. Although technically this <em>is</em> still a learning algorithm, it is not a very interesting one.</p>
<p>Deterministic <em>contextual</em> bandits, where the agent is given an observation that is known to influence the rewards obtained from different actions, do still have some challenge when solving for deterministic rewards. The agent then has to learn the mapping from input features to rewards whilst exploring different actions.</p>
","1","2","836","25573"
"21899","<p>When you do <code>model.predict(X)</code> the first axis in <code>X</code> is always an index in a batch. So if you want to predict on one sample, do something like <code>X = np.expand_dims(X, axis=0)</code></p>

<p>In your case, this should work:</p>

<pre><code>old_qval = model.predict( np.expand_dims(old_state_m, axis=0) )
</code></pre>
","2","2","836","25573"
"22971","<p>This is a Community Wiki answer, everyone is welcome to add references.</p>

<ul>
<li>Kaggle ran a competition to predict whether a question would be closed on Stack Overflow: <a href=""https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow"" rel=""nofollow noreferrer"">https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow</a></li>
</ul>
","1","2","836","25573"
"24321","<p>There are no <em>unlimited</em> free services*, but some have starting credit or free offers on initial signup. Here are some suggested to date:</p>

<ul>
<li><p>AWS: If specifically deep learning on a large data set, then probably AWS is out - their free offer does not cover machines with enough processing power to tackle deep learning projects. </p></li>
<li><p>Google Cloud might do, the starting credit offer is good enough to do a little deep learning (for maybe a couple of weeks), although they have signup and tax restrictions. </p></li>
<li><p>Azure have a free tier with limited processing and storage options.</p></li>
</ul>

<p>Most free offerings appear to follow the ""Freemium"" model - give you limited service that you can learn to use and maybe like. However not enough to use heavily (for e.g. training an image recogniser or NLP model from scratch) unless you are willing to pay.</p>

<p>This best advice is to shop around for a best starting offer and best price. A review of services is not suitable here, as it will get out of date quickly and not a good use of Stack Exchange. But you can find <a href=""https://www.quora.com/What-is-best-cloud-solution-for-deep-learning"" rel=""noreferrer"">similar questions on Quora</a> and other sites - your best bet is to do a web search for ""cloud compute services for deep learning"" or similar and expect to spend some time comparing notes. A few specialist deep learning services have popped up recently such as <a href=""https://www.nimbix.net/"" rel=""noreferrer"">Nimbix</a> or <a href=""https://www.floydhub.com/"" rel=""noreferrer"">FloydHub</a>, and there are also the big players such as Azure, AWS, Google Cloud.</p>

<p>You won't find anything completely free and unencumbered, and if you want to do this routinely and have time to build and maintain hardware then it is cheaper to buy your own equipment in the long run - at least at a personal level.</p>

<p>To decide whether to pay for cloud or build your own, then consider a typical price for a cloud machine suitable for performing deep learning at around \$1 per hour (prices do vary a lot though, and it is worth shopping around, if only to find a spec that matches your problem). There may be additional fees for storage and data transfer. Compare that to pre-built deep learning machines costing from \$2000, or <a href=""https://medium.com/towards-data-science/build-a-deep-learning-pc-for-1-000-cad-f3c5f26ba134"" rel=""noreferrer"">building your own for \$1000</a> - such machines might not be 100% comparable, but if you are working by yourself then the payback point is going to be after only a few months use. Although don't forget the electricity costs - a powerful machine can draw 0.5kW whilst being heavily used, so this adds up to more than you might expect.</p>

<p>The advantages of cloud computing are that someone else does the maintenance work and takes on the risk of hardware failure. These are valuable services, and priced accordingly. </p>

<hr>

<p>* But see <a href=""https://datascience.stackexchange.com/a/27085"">Jay Speidall's answer</a> about Google's colab service, which appears to be free to use, but may have some T&amp;C limitations which may affect you (for instance I doubt they will be happy for you to run content production of Deep Dream or Style Transfer on it)</p>
","22","2","836","25573"
"6107","<p>I recently read <a href=""http://arxiv.org/abs/1411.4038"">Fully Convolutional Networks for Semantic Segmentation</a> by Jonathan Long, Evan Shelhamer, Trevor Darrell. I don't understand what ""deconvolutional layers"" do / how they work.</p>

<p>The relevant part is</p>

<blockquote>
  <p>3.3. Upsampling is backwards strided convolution</p>
  
  <p>Another way to connect coarse outputs to dense pixels
  is interpolation. For instance, simple bilinear interpolation
  computes each output $y_{ij}$ from the nearest four inputs by a
  linear map that depends only on the relative positions of the
  input and output cells.<br/>
  In a sense, upsampling with factor $f$ is convolution with
  a fractional input stride of 1/f. So long as $f$ is integral, a
  natural way to upsample is therefore backwards convolution
  (sometimes called deconvolution) with an output stride of
  $f$. Such an operation is trivial to implement, since it simply
  reverses the forward and backward passes of convolution.<br/>
  Thus upsampling is performed in-network for end-to-end
  learning by backpropagation from the pixelwise loss.<br/>
  Note that the deconvolution filter in such a layer need not
  be fixed (e.g., to bilinear upsampling), but can be learned.
  A stack of deconvolution layers and activation functions can
  even learn a nonlinear upsampling.<br/>
  In our experiments, we find that in-network upsampling
  is fast and effective for learning dense prediction. Our best
  segmentation architecture uses these layers to learn to upsample
  for refined prediction in Section 4.2.</p>
</blockquote>

<p>I don't think I really understood how convolutional layers are trained. </p>

<p>What I think I've understood is that convolutional layers with a kernel size $k$ learn filters of size $k \times k$. The output of a convolutional layer with kernel size $k$, stride $s \in \mathbb{N}$ and $n$ filters is of dimension $\frac{\text{Input dim}}{s^2} \cdot n$. However, I don't know how the learning of convolutional layers works. (I understand how simple MLPs learn with gradient descent, if that helps).</p>

<p>So if my understanding of convolutional layers is correct, I have no clue how this can be reversed.</p>

<p>Could anybody please help me to understand deconvolutional layers?</p>
","259","1","8820","16250"
"8642","<p>I've recently read about concept learning in a machine learning class. They defined a concept as (translated from German):</p>

<blockquote>
  <p>Concept:</p>
  
  <ul>
  <li>Describes a subset of objects or events defined on a bigger set.</li>
  <li>Boolean function defined on bigger set</li>
  </ul>
  
  <p>Given: Examples which are labeled as members or not-members</p>
  
  <p>Searched: Automatically infer the definition of the underlying concept</p>
  
  <p>Definition of concept learning: Infering a boolean-valued function from training data of its input and its output.</p>
</blockquote>

<p>I would call this a 2-class classification problem. Is there a difference?</p>
","5","1","8820","16250"
"8855","<p>As far as I understood it, the pooling layer doesn't learn anything. It has several parameters, most important its <code>pool_size</code> and <code>stride</code> (see <a href=""http://lasagne.readthedocs.org/en/latest/modules/layers/pool.html#lasagne.layers.MaxPool2DLayer"" rel=""nofollow"">Lasagne documentation</a> for more), but none of those is learned.</p>

<p>Is it possible to learn these two (or one of those) parameters? Are there papers about it?</p>

<p>(I would guess that it is not possible to add this to the objective function in a meaningful way... but I'd like to see if people thought about it.)</p>
","3","1","8820","16250"
"8986","A Markov process is a stochastic process for which the Markov property holds: If you know the current state, then the next state is independent of all past states.","0","4","8820","16250"
"9073","<p>If I understand it correctly, then the labels of <a href=""http://image-net.org/"" rel=""nofollow noreferrer"">ImageNet</a> are based on WordNet:</p>

<blockquote>
  <p>ImageNet is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a ""synonym set"" or ""synset"". There are more than 100,000 synsets in WordNet, majority of them are nouns (80,000+). In ImageNet, we aim to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated. In its completion, we hope ImageNet will offer tens of millions of cleanly sorted images for most of the concepts in the WordNet hierarchy.</p>
</blockquote>

<p>Source: <a href=""http://image-net.org/about-overview"" rel=""nofollow noreferrer"">http://image-net.org/about-overview</a></p>

<p>The text above states already that WordNet defines a hierarchy. You can see that by using the Python <a href=""http://www.nltk.org/"" rel=""nofollow noreferrer""><code>nltk</code> package</a>:</p>

<pre><code>from nltk.corpus import wordnet
nltk.download()
def get_hyponyms(synset):
    hyponyms = set()
    for hyponym in synset.hyponyms():
        hyponyms |= set(get_hyponyms(hyponym))
    return hyponyms | set(synset.hyponyms())
dog = wordnet.synset('dog.n.01')
print(get_hyponyms(dog))
harrier = wordnet.synset('harrier.n.02')
print(get_hyponyms(harrier))
</code></pre>

<p>(thanks to <a href=""https://stackoverflow.com/a/33662890/562769"">Stephan Ds answer</a>)</p>

<p>gives</p>

<blockquote>
  <p>set([Synset('harrier.n.02'), Synset('water_spaniel.n.01'), Synset('standard_poodle.n.01'), Synset('dandie_dinmont.n.01'), Synset('courser.n.03'), Synset('wirehair.n.01'), Synset('toy_manchester.n.01'), Synset('puppy.n.01'), Synset('briard.n.01'), Synset('beagle.n.01'), Synset('siberian_husky.n.01'), Synset('manchester_terrier.n.01'), Synset('bloodhound.n.01'), Synset('gordon_setter.n.01'), Synset('leonberg.n.01'), Synset('king_charles_spaniel.n.01'), Synset('yorkshire_terrier.n.01'), Synset('sealyham_terrier.n.01'), Synset('american_water_spaniel.n.01'), Synset('skye_terrier.n.01'), Synset('clumber.n.01'), Synset('pembroke.n.01'), Synset('wire-haired_fox_terrier.n.01'), Synset('shih-tzu.n.01'), Synset('newfoundland.n.01'), Synset('retriever.n.01'), Synset('cocker_spaniel.n.01'), Synset('springer_spaniel.n.01'), Synset('american_foxhound.n.01'), Synset('large_poodle.n.01'), Synset('lapdog.n.01'), Synset('bull_mastiff.n.01'), Synset('affenpinscher.n.01'), Synset('irish_water_spaniel.n.01'), Synset('terrier.n.01'), Synset('keeshond.n.01'), Synset('vizsla.n.01'), Synset('dalmatian.n.02'), Synset('bird_dog.n.01'), Synset('irish_terrier.n.01'), Synset('miniature_poodle.n.01'), Synset('dachshund.n.01'), Synset('australian_terrier.n.01'), Synset('blenheim_spaniel.n.01'), Synset('weimaraner.n.01'), Synset('soft-coated_wheaten_terrier.n.01'), Synset('doberman.n.01'), Synset('kelpie.n.02'), Synset('water_dog.n.02'), Synset('feist.n.01'), Synset('attack_dog.n.01'), Synset('french_bulldog.n.01'), Synset('papillon.n.01'), Synset('bedlington_terrier.n.01'), Synset('foxhound.n.01'), Synset('labrador_retriever.n.01'), Synset('great_dane.n.01'), Synset('kerry_blue_terrier.n.01'), Synset('miniature_schnauzer.n.01'), Synset('pariah_dog.n.01'), Synset('border_terrier.n.01'), Synset('staghound.n.01'), Synset('norwegian_elkhound.n.01'), Synset('redbone.n.01'), Synset('pooch.n.01'), Synset('old_english_sheepdog.n.01'), Synset('police_dog.n.01'), Synset('welsh_terrier.n.01'), Synset('spitz.n.01'), Synset('boxer.n.04'), Synset('tibetan_terrier.n.01'), Synset('shetland_sheepdog.n.01'), Synset('boarhound.n.01'), Synset('border_collie.n.01'), Synset('wolfhound.n.01'), Synset('lhasa.n.02'), Synset('scotch_terrier.n.01'), Synset('coondog.n.01'), Synset('giant_schnauzer.n.01'), Synset('japanese_spaniel.n.01'), Synset('german_short-haired_pointer.n.01'), Synset('entlebucher.n.01'), Synset('griffon.n.03'), Synset('griffon.n.02'), Synset('welsh_springer_spaniel.n.01'), Synset('clydesdale_terrier.n.01'), Synset('hound.n.01'), Synset('brittany_spaniel.n.01'), Synset('corgi.n.01'), Synset('pekinese.n.01'), Synset('mastiff.n.01'), Synset('flat-coated_retriever.n.01'), Synset('sennenhunde.n.01'), Synset('schipperke.n.01'), Synset('english_toy_spaniel.n.01'), Synset('ibizan_hound.n.01'), Synset('airedale.n.01'), Synset('cardigan.n.02'), Synset('miniature_pinscher.n.01'), Synset('bluetick.n.01'), Synset('west_highland_white_terrier.n.01'), Synset('seizure-alert_dog.n.01'), Synset('pomeranian.n.01'), Synset('english_foxhound.n.01'), Synset('bernese_mountain_dog.n.01'), Synset('norfolk_terrier.n.01'), Synset('greater_swiss_mountain_dog.n.01'), Synset('collie.n.01'), Synset('chow.n.03'), Synset('pug.n.01'), Synset('scottish_deerhound.n.01'), Synset('groenendael.n.01'), Synset('golden_retriever.n.01'), Synset('schnauzer.n.01'), Synset('irish_setter.n.01'), Synset('german_shepherd.n.01'), Synset('walker_hound.n.01'), Synset('english_setter.n.01'), Synset('english_springer.n.01'), Synset('sporting_dog.n.01'), Synset('afghan_hound.n.01'), Synset('cairn.n.02'), Synset('rhodesian_ridgeback.n.01'), Synset('chesapeake_bay_retriever.n.01'), Synset('irish_wolfhound.n.01'), Synset('fox_terrier.n.01'), Synset('sled_dog.n.01'), Synset('toy_dog.n.01'), Synset('staffordshire_bullterrier.n.01'), Synset('bullterrier.n.01'), Synset('seeing_eye_dog.n.01'), Synset('samoyed.n.03'), Synset('bouvier_des_flandres.n.01'), Synset('otterhound.n.01'), Synset('kuvasz.n.01'), Synset('cur.n.01'), Synset('guide_dog.n.01'), Synset('malinois.n.01'), Synset('malamute.n.01'), Synset('poodle.n.01'), Synset('curly-coated_retriever.n.01'), Synset('toy_spaniel.n.01'), Synset('basset.n.01'), Synset('toy_terrier.n.01'), Synset('tibetan_mastiff.n.01'), Synset('basenji.n.01'), Synset('field_spaniel.n.01'), Synset('mexican_hairless.n.01'), Synset('setter.n.02'), Synset('great_pyrenees.n.01'), Synset('american_staffordshire_terrier.n.01'), Synset('rottweiler.n.01'), Synset('standard_schnauzer.n.01'), Synset('black-and-tan_coonhound.n.01'), Synset('borzoi.n.01'), Synset('bulldog.n.01'), Synset('sausage_dog.n.01'), Synset('sussex_spaniel.n.01'), Synset('spaniel.n.01'), Synset('working_dog.n.01'), Synset('belgian_sheepdog.n.01'), Synset('watchdog.n.02'), Synset('silky_terrier.n.01'), Synset('eskimo_dog.n.01'), Synset('brabancon_griffon.n.01'), Synset('whippet.n.01'), Synset('plott_hound.n.01'), Synset('liver-spotted_dalmatian.n.01'), Synset('coonhound.n.01'), Synset('saluki.n.01'), Synset('toy_poodle.n.01'), Synset('hearing_dog.n.01'), Synset('chihuahua.n.03'), Synset('lakeland_terrier.n.01'), Synset('shepherd_dog.n.01'), Synset('rat_terrier.n.01'), Synset('italian_greyhound.n.01'), Synset('komondor.n.01'), Synset('saint_bernard.n.01'), Synset('norwich_terrier.n.01'), Synset('pointer.n.04'), Synset('appenzeller.n.01'), Synset('maltese_dog.n.01'), Synset('hunting_dog.n.01'), Synset('smooth-haired_fox_terrier.n.01'), Synset('housedog.n.01'), Synset('boston_bull.n.01'), Synset('pinscher.n.01'), Synset('greyhound.n.01')])</p>
  
  <p>set([])</p>
</blockquote>

<p>Now my question:</p>

<p>Are all images of ImageNet in the leaves, e.g. <code>'harrier.n.02'</code> or are some images only labeled as <code>'dog.n.01'</code>?</p>

<p>(Sub-question: ImageNet was labeled by ordinary people (not specialists in the topic, e.g. for dogs not biologists with specialization in dogs or something similar) via Amazon Mechanical Turk, if I remember it correctly. How did the people there know all these different kinds of dogs? I wouldn't know what a borzoi is... so how did they check if the label was correct and specific enough?)</p>

<p>Are the 21841 synsets (<a href=""http://image-net.org/about-stats"" rel=""nofollow noreferrer"">source</a>) leaves or also inner nodes?</p>

<p>Are all images in ImageNet in the leaves? How many leaves are there?</p>
","4","1","8820","16250"
"9074","<p>Until recently, I thought that ""labeling"" and ""classification"" are synonyms. But when I started another question about <a href=""https://stackoverflow.com/q/33947823/562769"">terminology in computer vision</a> I thought about it: Is there a difference between ""labeling"" and ""classification""?</p>

<p>I thought that the ""class"" is the concept you want to detect and ""label"" is what you assign to data. So the ""class"" is a concept which leads to the data and ""label"" is only the name. Hence ""labeling"" would be the same as ""classification"" as both want to make a statement about the underlying class which lead to the data.</p>

<h2>Articles</h2>

<p>A quick search via Google Scholar revealed that some articles use both terms in the title:</p>

<ul>
<li>Markus Eich, Malgorzata Dabrowska, and Frank Kirchner: ""Semantic Labeling: Classification of 3D Entities Based on Spatial Feature Descriptors""</li>
<li>Chunlin Li, Dmitry B. Goldgof, and Lawrence 0. Hall: ""Knowledge-based classification and tissue labeling of MR images of human brain""</li>
<li>Ray Blanchard: ""The classification and labeling of nonhomosexual gender dysphorias"" - another research area but probably it is the same difference between the two words?</li>
</ul>

<p>So I guess there is a difference between ""labeling"" and ""classification"". What is the difference?</p>

<h2>Google N-Gram</h2>

<p><a href=""https://i.stack.imgur.com/zpn9y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zpn9y.png"" alt=""enter image description here""></a></p>

<p>classification seems to be a much boarder term.</p>
","6","1","8820","16250"
"9172","<p>Imagine you have $k$ classes. Every class $i$ has points which are follow a probability distribution, such that their distance to 0 is $i$ in mean, but this distance follows a normal distribution. The direction is uniformly distributed. So all classes are in shells around the origin 0.</p>

<p>Can $k$-means get these shells when you choose the ""right"" distance metric?
(Obviously it can't find it if you take the euclidean metric, but I wonder if there is any metric at all or if this problem is inherently unsolvable by $k$-means, even if you know the number of clusters $k$)</p>
","1","1","8820","16250"
"9175","<p>This question boils down to ""how do convolution layers <em>exactly</em> work.</p>

<p>Suppose I have an $n \times m$ greyscale image. So the image has one channel.
In the first layer, I apply a $3\times 3$ convolution with $k_1$ filters and padding. Then I have another convolution layer with $5 \times 5$ convolutions and $k_2$ filters. How many feature maps do I have?</p>

<h2>Type 1 convolution</h2>

<p>The first layer gets executed. After that, I have $k_1$ feature maps (one for each filter). Each of those has the size $n \times m$. Every single pixel was created by taking $3 \cdot 3 = 9$ pixels from the padded input image.</p>

<p>Then the second layer gets applied. Every single filter gets applied separately to <strong>each of the feature maps</strong>. This results in $k_2$ feature maps for every of the $k_1$ feature maps. So there are $k_1 \times k_2$ feature maps after the second layer. Every single pixel of each of the new feature maps got created by taking $5 \cdot 5 = 25$ ""pixels"" of the padded feature map from before.</p>

<p>The system has to learn $k_1 \cdot 3 \cdot 3 + k_2 \cdot 5 \cdot 5$ parameters.</p>

<h2>Type 2.1 convolution</h2>

<p>Like before: The first layer gets executed. After that, I have $k_1$ feature maps (one for each filter). Each of those has the size $n \times m$. Every single pixel was created by taking $3 \cdot 3 = 9$ pixels from the padded input image.</p>

<p>Unlike before: Then the second layer gets applied. Every single filter gets applied to the same region, but <strong>all feature maps</strong> from before. This results in $k_2$ feature maps in total after the second layer got executed. Every single pixel of each of the new feature maps got created by taking $k_2 \cdot 5 \cdot 5 = 25 \cdot k_2$ ""pixels"" of the padded feature maps from before.</p>

<p>The system has to learn $k_1 \cdot 3 \cdot 3 + k_2 \cdot 5 \cdot 5$ parameters.</p>

<h2>Type 2.2 convolution</h2>

<p>Like above, but instead of having $5 \cdot 5 = 25$ parameters per filter which have to be learned and get simply copied for the other input feature maps, you have $k_1 \cdot 3 \cdot 3 + k_2 \cdot k_1 \cdot 5 \cdot 5$ paramters which have to be learned.</p>

<h2>Question</h2>

<ol>
<li>Is type 1 or type 2 typically used?</li>
<li>Which type is used in <a href=""http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"" rel=""noreferrer"">Alexnet</a>?</li>
<li>Which type is used in <a href=""http://arxiv.org/abs/1409.4842"" rel=""noreferrer"">GoogLeNet</a>?

<ul>
<li>If you say type 1: Why do $1 \times 1$ convolutions make any sense? Don't they only multiply the data with a constant?</li>
<li>If you say type 2: Please explain the quadratic cost (""For example, in a deep vision network, if two convolutional layers are chained,
any uniform increase in the number of their filters results in a quadratic increase of computation"")</li>
</ul></li>
</ol>

<p>For all answers, please give some evidence (papers, textbooks, documentation of frameworks) that your answer is correct.</p>

<h2>Bonus question 1</h2>

<p>Is the pooling applied always only per feature map or is it also done over multiple feature maps?</p>

<h2>Bonus question 2</h2>

<p>I'm relatively sure that type 1 is correct and I got something wrong with the GoogLe paper. But there a 3D convolutions, too. Lets say you have 1337 feature maps of size $42 \times 314$ and you apply a $3 \times 4 \times 5$ filter. How do you slide the filter over the feature maps? (Left to right, top to bottom, first feature map to last feature map?) Does it matter as long as you do it consistantly?</p>

<h2>My research</h2>

<ul>
<li>I've read the two papers from above, but I'm still not sure what is used.</li>
<li>I've read the <a href=""http://lasagne.readthedocs.org/en/latest/modules/layers/conv.html"" rel=""noreferrer"">lasagne documentation</a></li>
<li>I've read the <a href=""http://deeplearning.net/software/theano/library/tensor/nnet/conv.html"" rel=""noreferrer"">theano documentation</a></li>
<li>I've read the answers on <a href=""https://stats.stackexchange.com/q/85767/25741"">Understanding convolutional neural networks</a> (without following all links)</li>
<li>I've read <a href=""http://deeplearning.net/tutorial/lenet.html"" rel=""noreferrer"">Convolutional Neural Networks (LeNet)</a>. Especially figure 1 makes me relatively sure that Type 2.1 is the right one. This would also fit to the ""quadratic cost"" comment in GoogLe Net and to some practical experience I had with Caffee.</li>
</ul>
","46","1","8820","16250"
"9177","<p>I am currently reading slides about the $k$-means algorithm. In the analysis, the professor writes</p>

<blockquote>
  <p>Minimize Schwarz Criterion: $W(C) + \lambda m k \log R$</p>
</blockquote>

<p>$W(C)$ is Within-class scatter. I guess $\lambda$ is a weighting factor which has to be chosen by the developer and $k$ is the number of clusters. But what is $m$ and what is $R$?</p>
","2","1","8820","16250"
"9195","<p>Are there any papers published which show differences of the regularization methods for neural networks, preferably on different domains (or at least different datasets)?</p>

<p>I am asking because I currently have the feeling that most people seem to use only dropout for regularization in computer vision. I would like to check if there would be a reason (not) to use different ways of regularization.</p>
","9","1","8820","16250"
"9205","Computer Vision is a subfield of computer science which deals with analyzing and understanding images. This includes detection of objects like faces in images or segmenting images.","0","4","8820","16250"
"9212","<p>I've recently read about maxout in slides of a lecture and in the paper.</p>

<p>Is maxout the same as max pooling?</p>
","4","1","8820","16250"
"9233","<p>I am currently preparing for an exam on neural networks. In several protocols from former exams I read that the activation functions of neurons (in multilayer perceptrons) have to be monotonic.</p>

<p>I understand that activation functions should be differentiable, have a derivative which is not 0 on most points, and be non-linear. I do not understand why being monotonic is important/helpful.</p>

<p>I know the following activation functions and that they are monotonic:</p>

<ul>
<li>ReLU</li>
<li>Sigmoid</li>
<li>Tanh</li>
<li>Softmax: I'm not sure if the definition of monotonicity is applicable for functions <span class=""math-container"">$f: \mathbb{R}^n \rightarrow \mathbb{R}^m$</span> with <span class=""math-container"">$n, m &gt; 1$</span></li>
<li>Softplus</li>
<li>(Identity)</li>
</ul>

<p>However, I still can't see any reason why for example <span class=""math-container"">$\varphi(x) = x^2$</span>.</p>

<p>Why do activation functions have to be monotonic?</p>

<p>(Related side question: is there any reason why the logarithm/exponential function is not used as an activation function?)</p>
","16","1","8820","16250"
"9234","<p>A good answer to this question has to rely on the specific dataset / domain.</p>

<p>The questions I would ask myself are (in this order):</p>

<ul>
<li>Can I solve my classification task without those features? → Just remove those features</li>
<li>Do I know of a relationship between features I know and the (partially) missing feature? → Find hard rules to fill those missing ones</li>
<li>Otherwise: Train a classifier / regression model to fill those missing features</li>
</ul>
","1","2","8820","16250"
"9235","<h2>SOMs</h2>

<p>Self-organizing maps (SOMs) are one specific type of neural networks. In contrast to multilayer perceptrons (MLPs; they are used much more often) the SOMs neurons have a position on a regular grid.</p>

<h2>Training of SOMs</h2>

<p>SOMs are usually trained in a stochastic way (source: [1]). This means one training example is used at a time. After every single training example the network learns something.</p>

<p>In contrast, batch learning means the network is presented the complete batch of all training-examples (or a big part of usually 128 examples; then you call it mini-batch). The parameters are only updated after all of the examples were presented to the network.</p>

<p>In contrast to MLPs, you don't apply gradient descent (as far as I know; I've never used them, but only read about SOMs). I wouldn't even know to which function you would apply the gradient descent algorithm, because there is no error function in SOMs. (At least no standard error function).</p>

<p>For MLPs, I know that the advantage of batch training compared to stochastic training is that the weights are not jumping as much. So you rather go in the right direction. However, the advantage of stochastic training is that you make updates much more often. You want to get the best compromise, so you make mini-batch training.</p>

<h2>Online Learning</h2>

<p>""Online"" has quite a lot of different meanings:</p>

<ol>
<li>""online"" as in ""in the internet"".</li>
<li>""online"" as in ""<a href=""https://en.wikipedia.org/wiki/Online_algorithm"" rel=""nofollow"">online algorithms</a>"" which get input piece-by-piece and have to make a decision before knowing all of the input (e.g. when you have severe time constraints or when you can't store all the data like in CERN where you have to throw most of the data away right after you get it)</li>
<li>""online"" in handwriting recognition in contrast to the offline OCR methods (see <a href=""http://arxiv.org/abs/1511.09030"" rel=""nofollow"">On-line Recognition of Handwritten Mathematical Symbols</a> if you're interested). Online methods use the way how symbols were written, offline methods only use the image.</li>
</ol>

<p>According to <a href=""https://en.wikipedia.org/wiki/Online_machine_learning"" rel=""nofollow"">Wikipedia</a>:</p>

<blockquote>
  <p>Online machine learning is used when data becomes available in a sequential order to determine a mapping from data set corresponding labels. The difference between online learning and batch learning (or ""offline"" learning) techniques, is that in online learning the mapping is updated after the arrival of every new data point in a scale fashion, whereas batch techniques are used when one has access to the entire training data set at once.</p>
</blockquote>

<h2>Sources</h2>

<p>[1] Andries P. Engelbrecht: Computational Intelligence: An Introduction. Page 62.</p>
","1","2","8820","16250"
"9302","<p>In the <a href=""https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html"" rel=""noreferrer"">MNIST For ML Beginners</a> they define cross-entropy as</p>

<p>$$H_{y'} (y) := - \sum_{i} y_{i}' \log (y_i)$$</p>

<p>$y_i$ is the predicted probability value for class $i$ and $y_i'$ is the true probability for that class.</p>

<h2>Question 1</h2>

<p>Isn't it a problem that $y_i$ (in $\log(y_i)$) could be 0? This would mean that we have a really bad classifier, of course. But think of an error in our dataset, e.g. an ""obvious"" <code>1</code> labeled as <code>3</code>. Would it simply crash? Does the model we chose (softmax activation at the end) basically never give the probability 0 for the correct class?</p>

<h2>Question 2</h2>

<p>I've learned that cross-entropy is defined as </p>

<p>$$H_{y'}(y) := - \sum_{i} ({y_i' \log(y_i) + (1-y_i') \log (1-y_i)})$$</p>

<p>What is correct? Do you have any textbook references for either version? How do those functions differ in their properties (as error functions for neural networks)?</p>
","141","1","8820","16250"
"9406","<p>Suppose I have a smooth function like $f(x, y) = x^2+y^2$. I have a training set $D \subsetneq \{((x, y), f(x,y)) | (x,y) \in \mathbb{R}^2\}$ and, of course, I don't know $f$ although I can evaluate $f$ wherever I want.</p>

<p>Are regression trees capable of finding a smooth model of the function (hence a tiny change in the input should only give a tiny change in the output)?</p>

<p>From what I've read in <a href=""http://www.stat.cmu.edu/~cshalizi/350-2006/lecture-10.pdf"">Lecture 10: Regression Trees</a> it seems to me that regression trees basically put the function values into bins:</p>

<blockquote>
  <p>For classic regression trees, the model in each cell is just a constant estimate of
  Y .</p>
</blockquote>

<p>As they write ""classic"" I guess there is a variant where the cells do something more interesting?</p>
","11","1","8820","16250"
"9495","<p>I've heard that a multilayer perceptron can approximate any function arbitrarily exact, given enough neurons. I wanted to try it, so I wrote the following code:</p>



<pre><code>#!/usr/bin/env python

""""""Example for learning a regression.""""""


import tensorflow as tf
import numpy


def plot(xs, ys_truth, ys_pred):
    """"""
    Plot the true values and the predicted values.

    Parameters
    ----------
    xs : list
        Numeric values
    ys_truth : list
        Numeric values, same length as `xs`
    ys_pred : list
        Numeric values, same length as `xs`
    """"""
    import matplotlib.pyplot as plt
    truth_plot, = plt.plot(xs, ys_truth, '-o', color='#00ff00')
    pred_plot, = plt.plot(xs, ys_pred, '-o', color='#ff0000')
    plt.legend([truth_plot, pred_plot],
               ['Truth', 'Prediction'],
               loc='upper center')
    plt.savefig('plot.png')


# Parameters
learning_rate = 0.1
momentum = 0.6
training_epochs = 1000
display_step = 100

# Generate training data
train_X = []
train_Y = []

# First simple test: a linear function
f = lambda x: x+4

# Second, more complicated test: x^2
# f = lambda x: x**2

for x in range(-20, 20):
    train_X.append(float(x))
    train_Y.append(f(x))
train_X = numpy.asarray(train_X)
train_Y = numpy.asarray(train_Y)
n_samples = train_X.shape[0]

# Graph input
X = tf.placeholder(tf.float32)
reshaped_X = tf.reshape(X, [-1, 1])
Y = tf.placeholder(""float"")

# Create Model
W1 = tf.Variable(tf.truncated_normal([1, 100], stddev=0.1), name=""weight"")
b1 = tf.Variable(tf.constant(0.1, shape=[1, 100]), name=""bias"")
mul = tf.matmul(reshaped_X, W1)
h1 = tf.nn.sigmoid(mul) + b1
W2 = tf.Variable(tf.truncated_normal([100, 100], stddev=0.1), name=""weight"")
b2 = tf.Variable(tf.constant(0.1, shape=[100]), name=""bias"")
h2 = tf.nn.sigmoid(tf.matmul(h1, W2)) + b2
W3 = tf.Variable(tf.truncated_normal([100, 1], stddev=0.1), name=""weight"")
b3 = tf.Variable(tf.constant(0.1, shape=[1]), name=""bias"")
# identity as activation to get arbitrary output
activation = tf.matmul(h2, W3) + b3

# Minimize the squared errors
l2_loss = tf.reduce_sum(tf.pow(activation-Y, 2))/(2*n_samples)
optimizer = tf.train.MomentumOptimizer(learning_rate, momentum).minimize(l2_loss)

# Initializing the variables
init = tf.initialize_all_variables()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)

    # Fit all training data
    for epoch in range(training_epochs):
        for (x, y) in zip(train_X, train_Y):
            sess.run(optimizer, feed_dict={X: x, Y: y})

        # Display logs per epoch step
        if epoch % display_step == 0:
            cost = sess.run(l2_loss, feed_dict={X: train_X, Y: train_Y})
            print(""cost=%s\nW1=%s"" % (cost, sess.run(W1)))

    print(""Optimization Finished!"")
    print(""cost=%s W1=%s"" %
          (sess.run(l2_loss, feed_dict={X: train_X, Y: train_Y}),
           sess.run(W1)))  # ""b2="", sess.run(b2)

    # Get output and plot it
    ys_pred = []
    ys_truth = []

    test_X = []
    for x in range(-40, 40):
        test_X.append(float(x))

    for x in test_X:
        ret = sess.run(activation, feed_dict={X: x})
        ys_pred.append(list(ret)[0][0])
        ys_truth.append(f(x))
    plot(train_X.tolist(), ys_truth, ys_pred)
</code></pre>

<p>This kind of works for linear functions (at least for the training data, not so much for the testing data outside of the range):</p>

<p><a href=""https://i.stack.imgur.com/cIRSR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cIRSR.png"" alt=""enter image description here""></a></p>

<p>However, it doesn't work at all for non-linear function $x^2$:</p>

<p><a href=""https://i.stack.imgur.com/q6JbH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q6JbH.png"" alt=""enter image description here""></a></p>

<p>Why does this neural network not work for such simple function approximation? What do I have to change to make the same network topology work for both functions?</p>
","1","1","8820","16250"
"9510","<p>Long short-term memory is a recurrent neural network architecture introduced in the paper <a href=""http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf"" rel=""noreferrer"">Long short-term memory</a>.</p>

<p>Can you please tell me where the name comes from?</p>

<p>(""Memory"", as the network can store information because of the recurrence - but where does the ""Long short-term"" come from?)</p>
","8","1","8820","16250"
"9512","<p>When reading about SVMs (e.g. on the German Wikipedia) there is a sentence like ""an svm is a large-margin classifier).</p>

<p>Are there other large margin classifiers than SVMs?</p>
","2","1","8820","16250"
"9528","<p>I've understood that SVMs are binary, linear classifiers (without the kernel trick). They have training data $(x_i, y_i)$ where $x_i$ is a vector and $y_i \in \{-1, 1\}$ is the class. As they are binary, linear classifiers the task is to find a hyperplane which separates the data points with the label $-1$ from the data points with the label $+1$.</p>

<p>Assume for now, that the data points are linearly separable and we don't need slack variables.</p>

<p>Now I've read that the training problem is now the following optimization problem:</p>

<ul>
<li>${\min_{w, b} \frac{1}{2} \|w\|^2}$</li>
<li>s.t. $y_i ( \langle w, x_i \rangle + b) \geq 1$</li>
</ul>

<p>I think I got that minizmizing $\|w\|^2$ means maximizing the margin (however, I don't understand why it is the square here. Would anything change if one would try to minimize $\|w\|$?).</p>

<p>I also understood that $y_i ( \langle w, x_i \rangle + b) \geq 0$ means that the model has to be correct on the training data. However, there is a $1$ and not a $0$. Why?</p>
","7","1","8820","16250"
"9543","<p>There are several ways to make this big number trainable:</p>

<ul>
<li>Use <a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network"">CNNs</a></li>
<li>Auto-Encoders (see <a href=""https://www.cs.toronto.edu/~hinton/science.pdf"">Reducing the Dimensionality of Data with Neural Networks</a>)</li>
</ul>

<h2>Dimensionality reduction of the input</h2>

<ul>
<li>Scale the image down</li>
<li><a href=""https://en.wikipedia.org/wiki/Principal_component_analysis"">PCA</a> / <a href=""https://en.wikipedia.org/wiki/Linear_discriminant_analysis"">LDA</a></li>
</ul>

<h2>Troll-Answer</h2>

<p>If you really meant ""only a few neurons"" then you might want to have a look at <a href=""https://en.wikipedia.org/wiki/Spiking_neural_network"">Spiking neural networks</a>. Those are incredibly computationally intensive, need a lot of hand-crafting and still get worse performance than normal neural networks for most tasks ... but you only need very little of them.</p>
","6","2","8820","16250"
"9566","<p>A neural network is in principle a good choice when you have A LOT of similar data and classification tasks. Predicting the next character (or word... which is just multiple characters) is such a szenario. I don't think it really matters which kind of language you have, as long as you have enough training data of the same kind.</p>

<p>See <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow"">The Unreasonable Effectiveness of Recurrent Neural Networks</a> for a nice article where a recurrent neural network (RNN) was used as a character predictor to write complete texts. They also have code on <a href=""https://github.com/karpathy/char-rnn"" rel=""nofollow"">github.com/karpathy/char-rnn</a> ready to train / go. You can feed it with a start string and ask for the next characters / words.</p>
","4","2","8820","16250"
"9606","<h1>Question 1: Category prediction</h1>

<p>To predict the category of a new blog post, you could do the following:</p>

<ul>
<li>Build a MLP (multilayer Perceptron, a very simple neural network). Each category gets an output node, each tag is a binary input node. However, this will only work if the number of tags is very little. As soon as you add new tags, you will have to retrain the network.</li>
<li>Build a MLP with ""important words"" as features.</li>
<li>If you have internal links, you might want to have a look at ""On Node Classification in Dynamic Content-based Networks"". In case you're German, you might also like <a href=""http://arxiv.org/abs/1512.04469"" rel=""nofollow"">Über die Klassifizierung von Knoten in dynamischen Netzwerken mit Inhalt</a></li>
<li>You could take all words you currently have, see those as a vector space. Fix that vocabulary (and probably remove some meaningless words like ""with"", ""a"", ""an"" - this is commonly called a ""stopword""). For each text, you count the different words you have in your vocabulary. A new blog post is point in this space. Use $k$ nearest neighbor for classification.</li>
<li>Use combinations of different predictiors by letting each predictor give a vote for a classification.</li>
</ul>

<h2>See also</h2>

<ul>
<li>Yiming Yang, Jan O. Pedersen: <a href=""http://www.surdeanu.info/mihai/teaching/ista555-spring15/readings/yang97comparative.pdf"" rel=""nofollow"">A Comparative Study on Feature Selection in Text Categorization</a>, 1997.</li>
<li>Scikit-learn: <a href=""http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"" rel=""nofollow"">Working With Text Data</a></li>
</ul>

<h1>Question 2: Tagging texts</h1>

<p>This can be treated the same way like question 1.</p>

<h1>Question 3: Finding locations</h1>

<p>Download a database of countries / cities (e.g. <a href=""https://www.maxmind.com/de/free-world-cities-database"" rel=""nofollow"">maxmind</a>) and just search for a match.</p>
","2","2","8820","16250"
"9617","<p>In <a href=""https://www.youtube.com/watch?v=ZYUnyyVgtyA"" rel=""nofollow"">Neural networks [3.8] : Conditional random fields - Markov network</a> by Hugo Larochelle it seems to me that a Markov Random Field is a special case of a CRF.</p>

<p>However, in the Wikipedia article <a href=""https://en.wikipedia.org/w/index.php?title=Markov_random_field&amp;oldid=695876526#Conditional_random_fields"" rel=""nofollow"">Markov random field</a> it says:</p>

<blockquote>
  <p>One notable variant of a Markov random field is a conditional random field, in which each random variable may also be conditioned upon a set of global observations o.</p>
</blockquote>

<p>This would mean that CRFs are a special case of MRFs.</p>

<h2>Definitions</h2>

<h3>Markov Random Field</h3>

<p>Again, according to Wikipedia</p>

<blockquote>
  <p>Given an undirected graph $G=(V,E)$, a set of random variables $X = (X_v)_{v\in V}$ indexed by $V$  form a Markov random field with respect to $G$  if they satisfy the local Markov properties:</p>
  
  <p><strong>Pairwise Markov property</strong>: Any two non-adjacent variables are conditionally independent given all other variables:
  $X_u \perp\!\!\!\perp X_v \mid X_{V \setminus \{u,v\}} \quad \text{if } \{u,v\} \notin E$</p>
  
  <p><strong>Local Markov property</strong>: A variable is conditionally independent of all other variables given its neighbors:
  $X_v \perp\!\!\!\perp X_{V\setminus \operatorname{cl}(v)} \mid X_{\operatorname{ne}(v)}$
  where ${\textstyle \operatorname{ne}(v)}$ is the set of neighbors of $v$, and $\operatorname{cl}(v) = v \cup \operatorname{ne}(v)$ is the closed neighbourhood of $v$.</p>
  
  <p><strong>Global Markov property</strong>: Any two subsets of variables are conditionally independent given a separating subset:
  $X_A \perp\!\!\!\perp X_B \mid X_S$
  where every path from a node in $A$ to a node in $B$ passes through $S$.</p>
</blockquote>

<p>Please note if you know a citable source which gives a good definition.</p>

<h3>Conditional Random Fields</h3>

<p>According to <a href=""https://en.wikipedia.org/w/index.php?title=Conditional_random_field&amp;oldid=695232246#Description"" rel=""nofollow"">Wikipedia</a>:</p>

<blockquote>
  <p>Lafferty, McCallum and Pereira[1] define a CRF on observations $\boldsymbol{X}$ and random variables $\boldsymbol{Y}$ as follows:</p>
  
  <blockquote>
    <p>Let $G = (V , E)$ be a graph such that</p>
    
    <p>$\boldsymbol{Y} = (\boldsymbol{Y}_v)_{v\in V}$, so that $\boldsymbol{Y}$ is indexed by the vertices of $G$. Then $(\boldsymbol{X}, \boldsymbol{Y})$ is a conditional random field when the random variables $\boldsymbol{Y}_v$, conditioned on $\boldsymbol{X}$, obey the Markov property with respect to the graph: $$p(\boldsymbol{Y}_v |\boldsymbol{X}, \boldsymbol{Y}_w, w \neq v) = p(\boldsymbol{Y}_v |\boldsymbol{X}, \boldsymbol{Y}_w, w \sim v)$$ where $\mathit{w} \sim v$ means that $w$ and $v$ are neighbors in G.</p>
  </blockquote>
  
  <p>What this means is that a CRF is an undirected graphical model whose nodes can be divided into exactly two disjoint sets $\boldsymbol{X}$ and $\boldsymbol{Y}$, the observed and output variables, respectively; the conditional distribution $p(\boldsymbol{Y}|\boldsymbol{X})$ is then modeled.</p>
</blockquote>

<h2>Question</h2>

<p>What is the relationship between Markov Random Fields and Conditional Random Fields?</p>
","4","1","8820","16250"
"9639","<p>Conditinal Random Fields (CRFs) are a special case of Markov Random Fields (MRFs).</p>

<blockquote>
  <p>1.5.4 Conditional Random Field</p>
  
  <p>A Conditional Random Field (CRF) is a form of MRF that defines a posterior for
  variables x given data z, as with the hidden MRF above. Unlike the hidden MRF,
  however, the factorization into the data distribution P (x|z) and the prior P
  (x) is not made explicit [288]. This allows complex dependencies of x on z to
  be written directly in the posterior distribution, without the factorization
  being made explicit. (Given P (x|z), such factorizations always exist,
  however—infinitely many of them, in fact—so there is no suggestion that the CRF
  is more general than the hidden MRF, only that it may be more convenient to
  deal with.)</p>
</blockquote>

<p>Source: Blake, Kohli and Rother: Markov random fields for vision and image processing. 2011.</p>

<blockquote>
  <p>A conditional random field or CRF (Lafferty et al. 2001), sometimes a
  discriminative random field (Kumar and Hebert 2003), is just a version of an
  MRF where all the clique potentials are conditioned on input features:
  [...]</p>
  
  <p>The advantage of a CRF over an MRF is analogous to the advantage of a discriminative
  classifier over a generative classifier (see Section 8.6), namely, we don’t need to “waste resources”
  modeling things that we always observe. [...]</p>
  
  <p>The disadvantage of CRFs over MRFs is that they require labeled training data, and they are slower to train[...]</p>
</blockquote>

<p>Source: Kevin P. Murphy: <a href=""https://www.cs.ubc.ca/~murphyk/MLbook/pml-print3-ch19.pdf"" rel=""nofollow"">Machine Learning: A Probabilistic Perspective</a></p>
","2","2","8820","16250"
"9672","<p>I've just read <a href=""http://papers.nips.cc/paper/207-the-cascade-correlation-learning-architecture.pdf"" rel=""nofollow"">The Cascade-Correlation Learning Architecture</a> by Scott E. Fahlman and Christian Lebiere.</p>

<p>I think I've got the overall concept (or at least the ""cascade"" part - a <a href=""https://www.youtube.com/watch?v=1E3XZr-bzZ4"" rel=""nofollow"">4min YouTube video</a> how I think it works):</p>

<ol>
<li>Start with a minimal network with input and output units only</li>
<li>Learn those weights with standard algorithms (e.g. gradient descent - they seem to use another training objective which I don't quite understand, so it is gradient ascent in the paper)</li>
<li>When the network doesn't improve, add a single new hidden unit. This unit gets input from all input nodes and all hidden nodes which were added before. Its output goes to all output nodes only.</li>
<li>Repeat step 3</li>
</ol>

<p>However, I don't understand the details of step 3: The input weights to hidden units are frozen (indicated by boxes in the paper). When exactly do they get frozen? Are they just initialized by random and never learned at all?</p>

<p>I also don't understand this paragraph:</p>

<blockquote>
  <p>To create a new hidden unit, we begin with a candidate unit that receives
  trainable input connections from all of the network's external inputs and from
  all pre-existing hidden units. The output of this candidate unit is not yet
  connected to the active network. We run a number of passes over the examples of
  the training set, adjusting the candidate unit's input weights after each pass.
  The goal of this adjustment is to maximize $S$, the sum over all output units $o$
  of the magnitude of the correlation (or, more precisely, the covariance)
  between $V$, the candidate unit's value, and Eo, the residual output error
  observed at unit o. We define S as</p>
  
  <p>$$S = \sum_{o} | \sum_p (V_p - \bar V) (E_{p,o} - \bar{E_o}) |$$</p>
  
  <p>where $o$ is the network output at which the error is measured and p is the
  training pattern. The quantities $\bar V$ and $\bar{E_o}$ are the values of
  $V$ and $E_o$ averaged over all patterns.</p>
</blockquote>

<p>What is an ""residual output error""? Is $V_p$ simply the activation of the unit given the pattern $p$? What does the term $S$ mean and why do we want to maximize it?</p>
","5","1","8820","16250"
"9678","<p>Machine Learning is a really big field. Depending on what exactly you want to do, there might be huge differences.</p>

<p>Having said that, the following skills are helpful:</p>

<ul>
<li>Programming, especially Python / C++</li>
<li>Frameworks like <a href=""http://tensorflow.org"" rel=""nofollow"">TensorFlow</a>, <a href=""http://scikit-learn.org/stable/"" rel=""nofollow"">sklearn</a>, <a href=""http://torch.ch/"" rel=""nofollow"">Torch</a>, ...</li>
<li>Algorithms like neural networks (especially gradient descent), SVM, decision trees, clustering algorithms, Q-Learning, ...</li>
<li>Mathematics (e.g. probability theory / statistics, linear algebra, analysis for calculating gradients</li>
</ul>

<p>Knowing how to program the GPU directly (CUDA) is probably not always necessary, but certainly a big bonus.</p>

<p>In case you're interested what you learn at KIT (Karlsruhe, Germany) in an introductory machine learning course, see <a href=""http://martin-thoma.com/machine-learning-1-course/"" rel=""nofollow"">my blog post for exam preparation</a> (I'm sorry, it is mostly in German).</p>

<h2>Online Courses</h2>

<p>There are lots of online material for machine learning. For example an <a href=""https://www.coursera.org/learn/machine-learning"" rel=""nofollow"">Coursera ML course</a> </p>
","4","2","8820","16250"
"9723","<p>There are three training modes for neural networks</p>

<ul>
<li><strong>stochastic gradient descent</strong>: Adjust the weights after every single training example</li>
<li><strong>batch training</strong>: Adjust the weights after going through all data (an epoch)</li>
<li><strong>mini-batch training</strong>: Adjust the weights after going through a <em>mini-batch</em>. This is usually 128 training examples.</li>
</ul>

<p>Most of the time, mini-batch training seems to be used.</p>

<p>So the answer is:</p>

<p>No, the neural network learning algorithm is not online algorithm.</p>
","4","2","8820","16250"
"9765","<p>While Kasra Manshaei gives a good general answer (+1), I would like to give an easy to understand example.</p>

<p>Think of a very simple problem: Fitting a function $f:[0, 1] \rightarrow \mathbb{R}$. To do so, you take a model out of the polynomial class. For the sake of argument, let's say you take a polynomial of degree 0. This models <strong>capacity</strong> is very limited as it can only fit constants. It will basically guess the mean value (depends on the error function, of course, but keep it simple). So relatively quick you will have a pretty good estimate of what the best parameters for this kind of model are. Your test- and training error will be almost identical, no matter how many examples you add. The problem is not that you don't have enough data, the problem is that your model is not powerful enough: You <strong>underfit</strong>.</p>

<p>So lets go the other way around: Say you have 1000 data points. Knowing a bit of math, you choose a polynomial of degree 999. Now you can fit the training data perfectly. However, your data might just fit the data too perfectly. For example, see (from <a href=""http://martin-thoma.com/spline-interpolation/"" rel=""nofollow noreferrer"">my blog</a>)</p>

<p><a href=""https://i.stack.imgur.com/fYSlo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fYSlo.png"" alt=""enter image description here""></a></p>

<p>In this case, you have other models which also fit the data perfectly. Obviously, the blue model seems kind of unnatural between the datapoints. The model itself might not be able to capture the kind of distribution well, so restricting the model to something simpler might actually help it. This can be an example of <strong>overfitting</strong>.</p>
","5","2","8820","16250"
"9818","<p>Neural networks get top results in Computer Vision tasks (see <a href=""http://yann.lecun.com/exdb/mnist/"">MNIST</a>, <a href=""http://www.image-net.org/challenges/LSVRC/"">ILSVRC</a>, <a href=""http://blog.kaggle.com/2014/04/18/winning-the-galaxy-challenge-with-convnets/"">Kaggle Galaxy Challenge</a>).  They seem to outperform every other approach in Computer Vision. But there are also other tasks:</p>

<ul>
<li><a href=""http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/"">Kaggle Molecular Activity Challenge</a></li>
<li>Regression: <a href=""http://blog.kaggle.com/2016/01/04/how-much-did-it-rain-ii-winners-interview-1st-place-pupa-aka-aaron-sim/"">Kaggle Rain prediction</a>, also the <a href=""http://blog.kaggle.com/2015/12/17/how-much-did-it-rain-ii-2nd-place-luis-andre-dutra-e-silva/"">2nd place</a></li>
<li><a href=""http://blog.kaggle.com/2015/09/29/grasp-and-lift-eeg-detection-winners-interview-2nd-place-daheimao/"">Grasp and Lift 2nd</a> also <a href=""http://blog.kaggle.com/2015/10/05/grasp-and-lift-eeg-detection-winners-interview-3rd-place-team-hedj/"">third place</a> - Identify hand motions from EEG recordings</li>
</ul>

<p>I'm not too sure about ASR (automatic speech recognition) and machine translation, but I think I've also heard that (recurrent) neural networks (start to) outperform other approaches.</p>

<p>I am currently learning about Bayesian Networks and I wonder in which cases those models are usually applied. So my question is:</p>

<p><strong>Is there any challenge / (Kaggle) competition, where the state of the art are Bayesian Networks or at least very similar models?</strong></p>

<p>(Side note: I've also seen <a href=""http://blog.kaggle.com/2015/12/21/rossmann-store-sales-winners-interview-1st-place-gert/"">decision trees</a>, <a href=""http://blog.kaggle.com/2015/11/09/profiling-top-kagglers-gilberto-titericz-new-1-in-the-world/"">2</a>, <a href=""http://blog.kaggle.com/2015/10/30/dato-winners-interview-2nd-place-mortehu/"">3</a>, <a href=""http://blog.kaggle.com/2015/10/21/recruit-coupon-purchase-winners-interview-2nd-place-halla-yang/"">4</a>, <a href=""http://blog.kaggle.com/2015/10/20/caterpillar-winners-interview-3rd-place-team-shift-workers/"">5</a>, <a href=""http://blog.kaggle.com/2015/09/28/liberty-mutual-property-inspection-winners-interview-qingchen-wang/"">6</a>, <a href=""http://blog.kaggle.com/2015/09/22/caterpillar-winners-interview-1st-place-gilberto-josef-leustagos-mario/"">7</a> win in several recent Kaggle challenges)</p>
","55","1","8820","16250"
"9829","<p>Examples for Probabilistic Graphical Models (PGMs) are:</p>

<ul>
<li>Markov Random Fields (MRFs, undirected)

<ul>
<li>Conditional Random Fields (CRFs)</li>
</ul></li>
<li>Bayesian networks (BNs, directed)</li>
</ul>
","0","5","8820","16250"
"9830","A probabilistic graphical model (PGM) is a probabilisic model for which conditional dependencies are expressed with a graph G = (X, E) where X are random variables.","0","4","8820","16250"
"9832","<p>It seems to me that the $V$ function can be easily expressed by the $Q$ function and thus the $V$ function seems to be superfluous to me. However, I'm new to reinforcement learning so I guess I got something wrong.</p>

<h2>Definitions</h2>

<p>Q- and V-learning are in the context of <a href=""https://en.wikipedia.org/wiki/Markov_decision_process#Definition"" rel=""noreferrer"">Markov Decision Processes</a>. A <strong>MDP</strong> is a 5-tuple $(S, A, P, R, \gamma)$ with</p>

<ul>
<li>$S$ is a set of states (typically finite)</li>
<li>$A$ is a set of actions (typically finite)</li>
<li>$P(s, s', a) = P(s_{t+1} = s' | s_t = s, a_t = a)$ is the probability to get from state $s$ to state $s'$ with action $a$.</li>
<li>$R(s, s', a) \in \mathbb{R}$ is the immediate reward after going from state $s$ to state $s'$ with action $a$. (It seems to me that usually only $s'$ matters).</li>
<li>$\gamma \in [0, 1]$ is called discount factor and determines if one focuses on immediate rewards ($\gamma = 0$), the total reward ($\gamma = 1$) or some trade-off.</li>
</ul>

<p>A <strong>policy $\pi$</strong>, according to <a href=""http://incompleteideas.net/book/the-book-2nd.html"" rel=""noreferrer"">Reinforcement Learning: An Introduction</a> by Sutton and Barto is a function $\pi: S \rightarrow A$ (this could be probabilistic).</p>

<p>According to <a href=""http://www.cs.upc.edu/~mmartin/Ag4-4x.pdf"" rel=""noreferrer"">Mario Martins slides</a>, the <strong>$V$ function</strong> is
$$V^\pi(s) = E_\pi \{R_t | s_t = s\} = E_\pi \{\sum_{k=0}^\infty \gamma^k r_{t+k+1} | s_t = s\}$$
and the <strong>Q function</strong> is
$$Q^\pi(s, a) = E_\pi \{R_t | s_t = s, a_t = a\} = E_\pi \{\sum_{k=0}^\infty \gamma^k r_{t+k+1} | s_t = s, a_t=a\}$$</p>

<h2>My thoughts</h2>

<p>The $V$ function states what the expected overall value (not reward!) of a state $s$ under the policy $\pi$ is.</p>

<p>The $Q$ function states what the value of a state $s$ and an action $a$  under the policy $\pi$ is.</p>

<p>This means,
$$Q^\pi(s, \pi(s)) = V^\pi(s)$$</p>

<p>Right? So why do we have the value function at all? (I guess I mixed up something)</p>
","48","1","8820","16250"
"9869","<p>I am currently reading <em>Boosting the Performance of RBF Networks with Dynamic Decay Adjustment</em> by Michael R. Berthold and Jay Diamond (<a href=""http://kops.uni-konstanz.de/handle/123456789/5427"" rel=""nofollow"">online</a>) to understand how Dynamic Decay Adjustment (DDA; a constructive trainining algorithm for RBF networks). Doing so, I stumbled over the word <strong>prototype</strong> a couple of times:</p>

<blockquote>
  <p>Unfortunately PRCE networks do not adjust the standard deviation of their prototypes individually, using only one global value for this parameter.</p>
  
  <p>[...]</p>
  
  <p>This paper introduces the Dynamic Decay Adjustment (DDA) algorithm which utilizes the constructive nature of the PRCE algorithm together with independent adaptation of each prototype's decay factor.</p>
  
  <p>[...]</p>
  
  <p>PNNs are not suitable for large databases because they commit one new prototype for each training pattern they encounter, eeffectively becoming a referential memory scheme.</p>
</blockquote>

<p>I've tried to find it in the only resource they referenced in this context (D.L. Reilly, L.N. Cooper, C. Elbaum: ""A Neural Model for Category Learning""), but sadly I don't have access to that one.</p>

<p>I found an explanation on <a href=""https://chrisjmccormick.wordpress.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/"" rel=""nofollow"">https://chrisjmccormick.wordpress.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/</a>:</p>

<blockquote>
  <p>An RBFN performs classification by measuring the input’s similarity to examples from the training set. Each RBFN neuron stores a “prototype”, which is just one of the examples from the training set. When we want to classify a new input, each neuron computes the Euclidean distance between the input and its prototype. Roughly speaking, if the input more closely resembles the class A prototypes than the class B prototypes, it is classified as class A.</p>
</blockquote>

<p>So a prototype is just the parameters (center and radius, assuming Gaussians are used) of an RBFs neuron?</p>

<p>Rephrasing the first quoted sentence, does it mean that the RBF networks usually only learn the center and the radius is fixed?</p>

<p>My question is if I understood it correct. Please add a reference (which is not a random blog article) which makes it more clear.</p>
","1","1","8820","16250"
"9930","<p>A recent paper by He et al. (<a href=""http://arxiv.org/pdf/1512.03385v1.pdf"" rel=""noreferrer"">Deep Residual Learning for Image Recognition</a>, Microsoft Research, 2015) claims that they use up to 4096 layers (not neurons!).</p>

<p>I am trying to understand the paper, but I stumble about the word ""residual"".</p>

<p>Could somebody please give me an explanation / definition what residual means in this case?</p>

<h2>Examples</h2>

<blockquote>
  <p>We explicitly reformulate the layers as learning
  residual functions with reference to the layer inputs, instead
  of learning unreferenced functions.</p>
  
  <p>[...]</p>
  
  <p>Instead
  of hoping each few stacked layers directly fit a
  desired underlying mapping, we explicitly let these layers
  fit a residual mapping. Formally, denoting the desired
  underlying mapping as $\mathcal{H}(x)$, we let the stacked nonlinear
  layers fit another mapping of $\mathcal{F}(x) := \mathcal{H}(x)−x$. The original
  mapping is recast into $\mathcal{F}(x)+x$. We hypothesize that it
  is easier to optimize the residual mapping than to optimize
  the original, unreferenced mapping</p>
</blockquote>
","7","1","8820","16250"
"9970","<p>When you want to use Auto-Encoders (AEs) for dimensionality reduction, you usally add a bottleneck layer. This means, for example, you have 1234-dimensional data. You feed this into your AE, and - as it is an AE - you have an output of dimension 1234. However, you might have many layers in that network and one of them has significantly less dimensions. Lets say you have the topology <code>1234:1024:784:1024:1234</code>. You train it like this, but you only use the weights from the <code>1234:1024:784</code> part.</p>

<p>When you get new input, you just feed it into this network. You can see it as a kind of preprocessing. For the later stages, this is a black box.</p>

<p>This is manly useful when you have a lot of unlabeled data. It is called <em>Semi Supervised Learning</em> (SSL).</p>
","2","2","8820","16250"
"9983","<p>Naive Bayes classifiers make use of Bayes theorem:</p>

<p>$$\overbrace{P(c | X)}^{\text{A posteriori}} = \frac{\overbrace{P(X | c)}^{\text{Likelihood}} \cdot \overbrace{P(c)}^{\text{A priori} } }{\underbrace{P(X)}_{\text{evidence}}}$$</p>

<h2>See also</h2>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Naive_Bayes_classifier"" rel=""nofollow"">Wikipedia</a></li>
</ul>
","0","5","8820","16250"
"9984","Naive Bayes classifiers makes the naive assumption that the features are independent. They make use of Bayes theorem.","0","4","8820","16250"
"9985","<p>The hinge loss function $\mathcal{l}$ is defined as</p>

<p>$$\mathcal{l}(y, t) = \max(0, 1 - t \cdot y)$$</p>

<p>where $t$ is the target value and $y$ is the prediction.</p>

<p>It  has values in $[0,  \infty)$</p>

<h2>See also</h2>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Hinge_loss"" rel=""nofollow"">Wikipedia</a></li>
</ul>
","0","5","8820","16250"
"9986","The hinge loss function is defined as $l(y, t) = max(0, 1 - t \cdot y)$","0","4","8820","16250"
"10000","<p>I have read that HMMs, Particle Filters and Kalman filters are special cases of dynamic Bayes networks. However, I only know HMMs and I don't see the difference to dynamic Bayes networks.</p>

<p>Could somebody please explain?</p>

<p>It would be nice if your answer could be similar to the following, but for bayes Networks:</p>

<h2>Hidden Markov Models</h2>

<p>A Hidden Markov Model (HMM) is a 5-tuple $\lambda = (S, O, A, B, \Pi)$:</p>

<ul>
<li>$S \neq \emptyset$: A set of states (e.g. ""beginning of phoneme"", ""middle of phoneme"", ""end of phoneme"")</li>
<li>$O \neq \emptyset$: A set of possible observations (audio signals)</li>
<li>$A \in \mathbb{R}^{|S| \times |S|}$: A stochastic matrix which gives probabilites $(a_{ij})$ to get from state $i$ to state $j$.</li>
<li>$B \in \mathbb{R}^{|S| \times |O|}$: A stochastic matrix which gives probabilites $(b_{kl})$ to get in state $k$ the observation $l$.</li>
<li>$\Pi \in \mathbb{R}^{|S|}$: Initial distribution to start in one of the states.</li>
</ul>

<p>It is usually displayed as a directed graph, where each node corresponds to one state $s \in S$ and the transition probabilities are denoted on the edges.</p>

<p>Hidden Markov Models are called ""hidden"", because the current state is hidden. The algorithms have to guess it from the observations and the model itself. They are called ""Markov"", because for the next state only the current state matters.</p>

<p>For HMMs, you give a fixed topology (number of states, possible edges). Then there are 3 possible tasks</p>

<ul>
<li><strong>Evaluation</strong>: given a HMM $\lambda$, how likely is it to get observations $o_1, \dots, o_t$ (Forward algorithm)</li>
<li><strong>Decoding</strong>: given a HMM $\lambda$ and a observations $o_1, \dots, o_t$, what is the most likely sequence of states $s_1, \dots, s_t$ (Viterbi algorithm)</li>
<li><strong>Learning</strong>: learn $A, B, \Pi$: <a href=""https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm"" rel=""noreferrer"">Baum-Welch algorithm</a>, which is a special case of Expectation maximization.</li>
</ul>

<h2>Bayes networks</h2>

<p>Bayes networks are directed acyclical graphs (DAGs) $G = (\mathcal{X}, \mathcal{E})$. The nodes represent random variables $X \in \mathcal{X}$. For every $X$, there is a probability distribution which is conditioned on the parents of $X$:</p>

<p>$$P(X|\text{parents}(X))$$</p>

<p>There seem to be (please clarify) two tasks:</p>

<ul>
<li><strong>Inference</strong>: Given some variables, get the most likely values of the others variables. Exact inference is NP-hard. Approximately, you can use MCMC.</li>
<li><p><strong>Learning</strong>: How you learn those distributions depends on the exact problem (<a href=""http://www.eng.tau.ac.il/~bengal/BN.pdf"" rel=""noreferrer"">source</a>):</p>

<ul>
<li>known structure, fully observable:  maximum likelihood estimation (MLE)</li>
<li>known structure, partially observable: Expectation Maximization (EM) or Markov Chain Monte Carlo (MCMC)</li>
<li>unknown structure, fully observable: search through model space</li>
<li>unknown structure, partially observable: EM + search through model space</li>
</ul></li>
</ul>

<h2>Dynamic Bayes networks</h2>

<p>I guess dynamic Bayes networks (DBNs) are also directed probabilistic graphical models. The variability seems to come from the network changing over time. However, it seems to me that this is equivalent to only copying the same network and connecting every node at time $t$ with every the corresponding node at time $t+1$. Is that the case?</p>
","15","1","8820","16250"
"10064","<p>Today, in a lecture it was claimed that the direction of edges in a Bayes network doesn't really matter. They don't have to represent causality.</p>

<p>It is obvious that you cannot switch any single edge in a Bayes network. For example, let $G = (V, E)$ with $V = \{v_1, v_2, v_3\}$ and $E=\{(v_1, v_2), (v_1, v_3), (v_2, v_3)\}$. If you would switch $(v_1, v_3)$ to $(v_3, v_1)$, then $G$ would no longer be acyclical and hence not a Bayes network. This seems to be mainly a practical problem how to estimate the probabilities then. This case seems to be much more difficult to answer, so I will skip it.</p>

<p>This made me ask the following questions for which I hope to get answers here:</p>

<ol>
<li>Is it possible for any directed acyclical graph (DAG) to reverse all edges and still have a DAG?</li>
<li>Assume a DAG $G$ and data is given. Now we construct the inverse DAG $G_\text{inv}$. For both DAGs, we fit the data to the corresponding Bayes networks. Now we have a set of data for which we want to use the Bayes network to predict the missing attributes. Could there be different results for both DAGs? (Bonus if you come up with an example)</li>
<li>Similar to 2, but simpler: Assume a DAG $G$ and data is given. You may create a new graph $G'$ by inverting any set of edges, as long as $G'$ remains acyclical. Are the Bayes networks equivalent when it comes to their predictions?</li>
<li>Do we get something if we have edges which do represent causality?</li>
</ol>
","10","1","8820","16250"
"10126","<p>You might be interested in reading the following paper by researchers of Microsoft Research:</p>

<blockquote>
  <p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun: <a href=""http://arxiv.org/abs/1512.03385"">Deep Residual Learning for Image Recognition</a> on arxiv, 2015.</p>
</blockquote>

<p>They had similar problems as you had:</p>

<blockquote>
  <p>When deeper networks are able to start converging, a
  degradation problem has been exposed: with the network
  depth increasing, accuracy gets saturated (which might be
  unsurprising) and then degrades rapidly. <strong>Unexpectedly,
  such degradation is not caused by overfitting, and adding
  more layers to a suitably deep model leads to higher training
  error</strong>, as reported in [11, 42] and thoroughly verified by
  our experiments.</p>
</blockquote>

<p>To solve the problem, they have made use of a skip architecture. With that, they trained very deep networks (1202 layers) and achieved the best result in the ILSVRC 2015 challenge.</p>
","8","2","8820","16250"
"10127","<p>Restricted Boltzmann machines are stochastic neural networks. The neurons form a complete bipartite graph of visible units and hidden units. The ""restricted"" is exactly the bipartite property: There may not be a connection between any two visible units and there may not be a connection between two hidden units.</p>

<p>Restricted Boltzmann machines are trained with Contrastive Divergence (CD-k, see <a href=""https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf"" rel=""noreferrer"">A Practical Guide to Training
Restricted Boltzmann Machines</a>).</p>

<p>Now I wonder: How are non-restricted Boltzmann Machines trained?</p>

<p>When I google for ""Boltzmann Machine"", I only finde RBMs.</p>
","7","1","8820","16250"
"10128","<p>A restricted Boltzmann maschine (RBM) is a stochastic neural network. RBMs can be stacked to deep networks.</p>

<h2>See also</h2>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine"" rel=""nofollow"">Wikipedia</a></li>
<li><a href=""http://www.scholarpedia.org/article/Boltzmann_machine"" rel=""nofollow"">Scholarpedia</a></li>
</ul>
","0","5","8820","16250"
"10129","A restricted Boltzmann machine (RBM) is a stochastic neural network.","0","4","8820","16250"
"10134","<p>This is a special tag that is designed to be used only by the system for questions that have had all of their other tags removed.</p>

<p>All questions in the system must have at least one tag. Normally, it is not possible to submit new or edited questions that do not have tags; the system will reject the submission until a valid tag is applied.</p>

<p>There are a few unusual situations that can cause this tag to be applied. When a tag has existed for six months and only been used on one question, and no tag wiki has been created for the tag, the system deletes the tag. When the community decides that a given tag can never be appropriate &#8212; for example, because it is misspelled or is off-topic &#8212; moderators may remove the tag from the site <em>en masse</em>.</p>

<p>When one of these deletions removes the last tag from a question, the system adds the <a href=""/questions/tagged/untagged"" class=""post-tag"" title=""show questions tagged &#39;untagged&#39;"" rel=""tag"">untagged</a> tag to that question so that the ""all questions must have at least one tag"" rule is not broken.</p>

<p>You can help clean up some of these questions! If they're good questions, edit them to remove the <a href=""/questions/tagged/untagged"" class=""post-tag"" title=""show questions tagged &#39;untagged&#39;"" rel=""tag"">untagged</a> tag and add more descriptive tags. If they're not so good, use the usual tools: downvoting, flagging and voting to close/delete.</p>
","0","5","8820","16250"
"10135","This is a special tag that is designed to be used only by the system for questions that have had all of their other tags removed.  Do not add this tag to existing questions, it is reserved for use by the system to identify posts with no valid tags.","0","4","8820","16250"
"10137","Interpolation is a set of methods to construct intermediate points between known points.","0","4","8820","16250"
"10147","<p>Multi-State Time Delay Neural Networks (MS-TDNNs) were introduced in</p>

<blockquote>
  <p>Haffner, Patrick and Waibel, Alex: <a href=""http://isl.anthropomatik.kit.edu/downloads/0135_Kopie_.pdf"" rel=""nofollow"">Multi-state time delay networks for continuous speech recognition</a>. In Advances in neural information processing systems, 1992.</p>
</blockquote>

<p>They are an extension of TDNNs. TDNNs are convolutional neural networks for automatic speech recognition (ASR), where the convolution happens over the time.</p>

<p>The aim of MS-TDNNs seems to be to get rid of the hybrid approach in ASR, where you need dynamic programming / HMMs to chunk the audio stream and then neural networks to recognize the phonemes. Somehow MS-TDNNs seem to do the segmentation as well.</p>

<p>I don't understand how. Could somebody please explain it to me?</p>

<p>(Related side questions: Are MS-TDNNs recurrent networks? Where exactly does the name ""multi-state"" come from?)</p>
","3","1","8820","16250"
"10149","<p>You can do <a href=""https://en.wikipedia.org/wiki/Topological_sorting"" rel=""nofollow"">topological sorting</a>. For example, here is some pythonic pseudocode</p>

<pre><code>def order(current_neuron, level):
    current_neuron.level = max(level, current_neuron.level)
    for child in sorted(current_neuron.children, key=children level, reverse=True):
        order(child, level + 1)

for input_neuron in input_neurons:
    order(input_neuron, level=0)
</code></pre>

<p>(Take this pseudo code with a grain of salt. This is just what came directly to my mind; it is not tested.)</p>

<p>Then you can calculate the activations in order of <code>node.level</code>.</p>
","1","2","8820","16250"
"10150","<p>Recurrent Neural Networks can deal with variable length data. You might want to have a look at:</p>

<ul>
<li>Andrej Karpathy: <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow"">The Unreasonable Effectiveness of Recurrent Neural Networks</a>.</li>
<li>Christopher Olah: <a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""nofollow"">Understanding LSTM Networks</a>.</li>
<li>Hochreiter, Schmidthuber: <a href=""http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=6795963"" rel=""nofollow"">Long short-term memory</a>.</li>
</ul>

<p>Another idea (which I have not tested so far and  just came to my mind) is using a histogram approach: You could probably make fixed-size windows, get the data from those windows and make it discrete (e.g. vector quantization, k-means). After that, you can make a histogram of how often those vectors appeared.</p>

<p>You could also use <a href=""https://en.wikipedia.org/wiki/Hidden_Markov_model"" rel=""nofollow"">HMMs</a> for recognition of variable length data.</p>

<p>Transformations (e.g. <a href=""https://en.wikipedia.org/wiki/Fourier_transform"" rel=""nofollow"">Fourier transform</a> from the time domain in the <a href=""https://en.wikipedia.org/wiki/Frequency_domain"" rel=""nofollow"">frequency domain</a>) might also come in handy.</p>
","4","2","8820","16250"
"10151","<p>I am currently reading:</p>

<blockquote>
  <p>Stephen Jose Hanson: <a href=""http://papers.nips.cc/paper/227-meiosis-networks.pdf"" rel=""nofollow"">Meiosis Networks</a>, 1990.</p>
</blockquote>

<p>and I stumbled about this:</p>

<blockquote>
  <p>It is possible to precisely characterize the search problem in terms of the resources or degress of freedom in the learning model. If the task the learning system is to perform is classification then the system can be analyzed in terms of its ability to dichotomize stimulus points in feature space.</p>
  
  <p><em>Dichotomization Capability: Network Capacity</em> Using a linear fan-in or hyperplane type neuron we can characterize the degrees of freedom inherent in a network of units with thresholded output. For example, with linear boundaries, consider 4 points, well distributed in a 2-dimensional feature space. There are exactly 14 linearly separable dichotomies that can be formed with the 4 target points. However, there are actually 16 ($2^4$) possible dichotomies of 4 points in 2 dimensions consequently, the number of possible dichotomies or arbitrary categories that are linearly implementable can be thought of as a capacity of the linear network in $k$ dimensions with $n$ examples. </p>
</blockquote>

<p>What is a ""dichonomy"" in this case?</p>

<p>(Side questions: what is a fan-in type neuron?)</p>
","5","1","8820","16250"
"10204","<p>When implementing mini-batch gradient descent for neural networks, is it important to take random elements in each mini-batch? Or is it enough to shuffle the elements at the beginning of the training once?</p>

<p>(I'm also interested in sources which definitely say what they do.)</p>
","9","1","8820","16250"
"10250","<p>The title says it all: I have seen three terms for functions so far, that seem to be the same / similar:</p>

<ul>
<li>error function</li>
<li>criterion function</li>
<li>cost function</li>
<li>objective function</li>
<li>loss function</li>
</ul>

<p>I was working on classification problems</p>

<p>$$E(W) = \frac{1}{2} \sum_{x \in E}(t_x-o(x))^2$$</p>

<p>where $W$ are the weights, $E$ is the evaluation set, $t_x$ is the desired output (the class) of $x$ and $o(x)$ is the given output. This function
seems to be commonly called ""error function"".</p>

<p>But while reading about this topic, I've also seen the terms ""criterion function"" and ""objective function"". Do they all mean  the same for neural nets?</p>

<ul>
<li><a href=""https://class.coursera.org/neuralnets-2012-001/lecture/47"" rel=""noreferrer"">Geoffrey Hinton</a> called cross-entropy for softmax-neurons and $E(W) = \frac{1}{2} \sum_{x \in E}(t_x-o(x))^2$ a <em>cost function</em>.</li>
</ul>
","8","1","8820","16250"
"10282","<p>Yes, that is correct.</p>

<p>For example, think of a polynomial $a_n x^n + a_{n-1} x^{n-1} + \dots + a_2 x^2 + a_1 x^1 + a_0 x^0$ which should fit 100 data points $(x_, y_i)$ where all $y_i$ were generated by one polynomial with some noise. Of course, you could always perfectly fit the model to the data making the MSE error</p>

<p>$$MSE = \sum_{(x_i, y_i)} (y_i - \text{predict}_{\text{Model}}(x_i))^2$$</p>

<p>be 0. But considering that there is noise, you might actually prefer a ""simpler"" model. One way to think about simplicity is having weights $a_i$ which are smaller. This is done by adding a regularization term to the error. One common regularization is $L_1$ regularization ($+ \sum_{1}^{100} | a_i |$), another one is $L_2$ regulariazion ($+ \sum_{1}^{100} a_i^2$).</p>
","4","2","8820","16250"
"10286","<p>I am thinking of the following image where we have two weights and an error (so we can make a 3D visualization). In this case a ""long valley"" looks like $x^2$ in the plane of the gradient, but in a perpendicular plane the function can still be minimized:</p>

<p><a href=""https://i.stack.imgur.com/5JyJB.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5JyJB.gif"" alt=""enter image description here""></a></p>

<p>According to my understanding, all optimization algorithms which are based on the gradient should be ""trapped"" at the saddle point like SGD is. Why does having a momentum term help in this case?</p>

<p>(I'm sorry, I don't know who made those images. <a href=""https://imgur.com/a/Hqolp"" rel=""nofollow noreferrer"">More of them</a> are available.)</p>
","4","1","8820","16250"
"10361","<p>Linear, binary classifiers can choose either class (but consistently) when the datapoint which is to classify is on the hyperplane. It just depends on how you programmed it.</p>

<p>Also, it doesn't really matter. This is very unlikely to happen. In fact, if we had arbitrary precision computing and normal distributed features, there would be a probability of 0 (exactly, not rounded) that this would happen. We have IEEE 754 floats, so the probability is not 0, but still so small that there are much more important factors to worry about.</p>
","6","2","8820","16250"
"10638","<blockquote>
  <p>I can’t figure about which variables the lecturer is talking about. Are they weights and biases or are they soft-max function and labels in the Big-loss function?</p>
</blockquote>

<p>In the case of neural networks, the loss function depends on the weights and the biases (which are often not mentioned extra; they are also weights).</p>

<p>The loss function itself is parametrized heavily. Its parameters is the data (input and labels). Its variables (by which the derivatives are calculated) are weights.</p>

<blockquote>
  <p>And how would zero mean and equal variance help in optimization?</p>
</blockquote>

<p>Have a look at the derivative of the sigmoid function. It is biggest at 0. This means the gradient there can be big. This helps by learning, because the basic learning rule is</p>

<p>$$w \gets w_i + \Delta w_i\;\;\; \text{ with } \Delta w_i = - \eta \frac{\partial E}{\partial w_i}$$</p>

<p>So you can get bigger adjustments if you normalize it to have mean of 0.</p>

<p>The part about the variance ... hm. That's harder to explain. I'm not entirely sure about it. One thought is that you want the data to be in a very restricted, similar domain (independent of the application) so that you can treat results independently from your application. Also, it might help with mini-batches not varying too much.</p>

<blockquote>
  <p>in the video the lecturer talks about weight initialization randomly using Gaussian distribution, I cannot understand how can we initialize weights using Gaussian distribution with zero mean and standard deviation sigma?</p>
</blockquote>

<p>I'm not sure what exactly the question is.</p>

<ul>
<li>Practically: Just use a function. For example, <a href=""http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.random.normal.html"" rel=""nofollow""><code>numpy.random.normal</code></a> if you're using Python.</li>
<li>Mathematically: This is called <em>sampling</em>. You have a process which has a random distribution and you take examples from that.</li>
<li>Technically: If you really want to implement this yourself, you will most likely end up using a random number generator which generates samples from a uniform distribution in [0, 1].</li>
</ul>

<p>Once you have those sample numbers, you simply assign those numbers to the weights.</p>

<blockquote>
  <p>And where optimizer will move the point (initialized weight), up or down to find the local minima?</p>
</blockquote>

<p>The optimizer calculates the gradient of the error function. I recommend having a look at the early chapters of the Udacity course. I'm pretty sure this was covered there, too. Another resource would be <a href=""http://neuralnetworksanddeeplearning.com/chap2.html"" rel=""nofollow"">neuralnetworksanddeeplearning.com</a></p>

<blockquote>
  <p>So can you recommend me a book that I should study before taking this course, so that problems like these don't occur?</p>
</blockquote>

<p>Tom Mitchells book ""Machine Learning"" covers similar topics as the course. </p>
","2","2","8820","16250"
"10707","<p>Without code you could have made to many mistakes to answer your question directly.</p>

<p>However, I suggest two things:</p>

<ol>
<li>Gradient checking (e.g. <a href=""http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization"" rel=""nofollow noreferrer"">this explanation</a>, but there are many more - pretty sure also in the course)</li>
<li>XOR problem with a 2:2:1 network with only sigmoid activations. Plot the error surface. Compare the decision surface with that of <a href=""https://phiresky.github.io/neural-network-demo/"" rel=""nofollow noreferrer"">this interactive demo</a>.</li>
</ol>

<p>It should look similar to this:
<a href=""https://i.stack.imgur.com/pqGoh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pqGoh.png"" alt=""enter image description here""></a></p>

<p>However, please keep in mind that the problem might not be your implementation, but rather the network architecture / hyperparameters such as the number of epochs you're training or the training data.</p>

<p><strong>Also, very important</strong>: I doubt that you will get good results for $y = x^2$, except if $x$ is restricted to $[-1,1]$ or something similar simple. Please keep the domain of your output layer in mind.</p>
","1","2","8820","16250"
"10854","<p>A graphical explanation how I understood bias and variance from <a href=""https://www.coursera.org/learn/machine-learning"" rel=""noreferrer"">Andrew Ngs course</a>:</p>

<p><a href=""https://i.stack.imgur.com/n6Tfr.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/n6Tfr.png"" alt=""enter image description here""></a></p>
","4","2","8820","16250"
"11148","<blockquote>
  <p>How are you solving this? How are you keeping track of the work done? What's your logbook tool?</p>
</blockquote>

<p>For <a href=""http://arxiv.org/abs/1511.09030"" rel=""nofollow"">my bachelors thesis</a> (<a href=""http://www.martin-thoma.de/write-math/classify/"" rel=""nofollow"">write-math.com</a>) I wrote my own little toolkit to go through different models / preprocessing steps very fast. Each experiment had one configuration file (see <a href=""https://github.com/MartinThoma/hwr-experiments"" rel=""nofollow"">hwr-experiments repository</a>). For example:</p>

<pre><code>data-source: feature-files/baseline-3-points
training: '{{nntoolkit}} train --epochs 1000 --learning-rate 0.1 --momentum 0.1 --print-errors --hook=''!detl
    test {{testing}},err=testresult_%e.txt'' {{training}} {{validation}}
    {{testing}} &lt; {{src_model}} &gt; {{target_model}} 2&gt;&gt; {{target_model}}.log'
model:
    type: mlp
    topology: 24:500:369
</code></pre>

<p>The trained model is stored; it is pretty fast to get the evaluation results (e.g. accuracy, confuscation matrix).</p>
","2","2","8820","16250"
"11298","<h2>Question 3</h2>

<p><a href=""http://synergy.st-andrews.ac.uk/vannesmithlab/research/bayesian-networks/"" rel=""nofollow"">synergy.st-andrews.ac.uk/vannesmithlab</a> claims that the graphs</p>

<pre><code>G1 = o-&gt;o-&gt;o and
G2 = o&lt;-o-&gt;o
</code></pre>

<p>are in one equivalence class. According to that source, the models  represents exactly the same joint probability distribution.</p>
","0","2","8820","16250"
"11379","<p>Suppose we use a decision tree to predict if a bank customer can pay back a credit. So it is a two class classification problem. Now we can make two mistakes:</p>

<ul>
<li>$\alpha$ error: The customer can back the credit, but we predict he can't.</li>
<li>$\beta$ error: The customer can't pay back the credit, but we predict he can.</li>
</ul>

<p>Now we know that $\beta$ errors are 123.4 times as expensive as $\alpha$ errors. But we only have a given set of data. In this set we have $n_1=10000$ customers who paid back the credit and $n_2 = 100$ customers who didn't.</p>

<p>How can the training of the decision tree be adjusted to account for the fact that $\beta$ errors are more expensive?</p>

<p>(Note: This is a theoretical question to learn about decision trees. I know about other classifiers like neural networks and I know of ensembles. However, I only want to know about decision trees here.)</p>
","2","1","8820","16250"
"11548","<p>Adding more data does not always help. However, you can get an estimate if more data will help you by the following procedure: Make a plot. On the $x$-axis is the amount of training examples, starting at one example per class going to wherever you are currently. The $y$-axis shows the error. Now you should add two curves: Training and test error. For low $x$, the training error should be very low (almost 0) and the test error very high. With enough data, they should be ""about the same"". By plotting those curves you can make an educated guess how much more data will give you how much improvement.</p>

<blockquote>
  <p>When doing this should I add data to both the training set and the test set?</p>
</blockquote>

<p>Depends on what you want to achieve. If only getting a better classifier, then you can only add it to the training set. However, if you're doing this in a scientific setting this might be more difficult. (I assume that your test set is of reasonable size).</p>

<p>You might want to have a look at <a href=""https://en.wikipedia.org/wiki/Cross-validation_(statistics)"" rel=""nofollow"">cross-validation</a>.</p>
","2","2","8820","16250"
"11628","<p>The <a href=""https://en.wikipedia.org/wiki/OPTICS_algorithm"" rel=""noreferrer"">OPTICS clustering algorithm</a> defines</p>

<p>$$\text{core-dist}_{\varepsilon,MinPts}(p)=\begin{cases}\text{UNDEFINED} &amp; \text{if } |N_\varepsilon(p)| &lt; MinPts\\ MinPts\text{-th smallest distance to } N_\varepsilon(p) &amp; \text{otherwise}\end{cases}$$</p>

<p>and</p>

<p>$$\text{reachability-dist}_{\varepsilon,MinPts}(o,p) = \begin{cases}\text{UNDEFINED} &amp; \text{if } |N_\varepsilon(p)| &lt; MinPts\\ \max(\text{core-dist}_{\varepsilon,MinPts}(p), \text{dist}(p,o)) &amp; \text{otherwise}\end{cases}$$</p>

<p>why isn't it simply</p>

<p>$$\text{reachability-dist}_{\varepsilon,MinPts}(o,p) = \begin{cases}\text{UNDEFINED} &amp; \text{if } |N_\varepsilon(p)| &lt; MinPts\\ \text{dist}(p,o) &amp; \text{otherwise}\end{cases}$$</p>
","6","1","8820","16250"
"11641","<p>I don't know what you mean by ""In multi label classification we have to share the subspace."", but the concept of subspaces is fairly easy to explain.</p>

<blockquote>
  <p>In mathematics, a space is a set (sometimes called a universe) with some added structure.</p>
</blockquote>

<p>Source: <a href=""https://en.wikipedia.org/wiki/Space_(mathematics)"" rel=""nofollow"">Wikipedia</a></p>

<p>The most common spaces are $\mathbb{R}$ (the number line), $\mathbb{R}^2$ (the plane) and $\mathbb{R}^3$ (the space we often think in, with $x,y,z$ axes). Lets assume our main space is $\mathbb{R}^3$. Our data are datapoints in this space. However, we have to write a report. This report has to be printed on paper. Hence we project the data points from a 3-dimensional space to a two-dimensional subspace. So a subspace is just a part of the original space.</p>
","1","2","8820","16250"
"11657","<blockquote>
  <p>Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). </p>
</blockquote>

<p>Source: <a href=""https://en.wikipedia.org/w/index.php?title=Cluster_analysis&amp;oldid=718035863"" rel=""nofollow"">Wikipedia</a></p>

<blockquote>
  <p>Association rule learning is a method for discovering interesting relations between variables in large databases.</p>
</blockquote>

<p>Source: <a href=""https://en.wikipedia.org/w/index.php?title=Association_rule_learning&amp;oldid=714074922"" rel=""nofollow"">Wikipedia</a></p>

<p>So both, clustering and association rule mining (ARM), are in the field of unsupervised machine learning. Clustering is about the data points, ARM is about finding relationships between the attributes of those datapoints.</p>

<p>However, I wonder if there are more relationships. For example, given a clustering, can this enhance / simplify ARM or vice versa?</p>
","0","1","8820","16250"
"11674","<p>When using OPTICS, one wants a reachability-plot as output from which one can read the number of clusters depending on $\varepsilon$. If $\text{dist}$ was used instead of $\text{reachability-dist}$, then some points would have a lower distance in the reachability plot. This could lead to the wrong conclusion that there would be more clusters for some small $\varepsilon$.</p>
","2","2","8820","16250"
"11716","<p>Suppose you have a classification task and accuracy is what you care about. Now an old system $s_1$ has an accuracy of $a(s_1) = 0.9$ and a new system $s_2$ has an accuracy of $a(s_2) = 0.99$. This is an absolute improvement of $a(s_2) - a(s_1) = 0.09$ percentage points and a relative improvement of $\frac{a(s_2)-a(s_1)}{a(s_1)} = \frac{0.09}{0.9} = 0.1$.</p>

<p>However, when you now try to get a system of $a(s_3) = 0.999$ this seems to be much more difficult, although it is only an absolute improvement of $0.009$ and a relative improvement of $0.00\overline{90}$. So neither the absolute nor the relative difference in accuracy seems to capture this well.</p>

<p>Is there a common other way to quantify how much better the system is?</p>
","2","1","8820","16250"
"11821","<p><a href=""http://www.dbs.informatik.uni-muenchen.de/Publikationen/Papers/OPTICS.pdf"" rel=""nofollow noreferrer"">OPTICS</a> gets rid of $\varepsilon$, you might want to have a look at it. Especially the <strong>reachability plot</strong> is a way to visualize what good choices of $\varepsilon$ in DBSCAN might be.</p>

<p>Wikipedia (<a href=""https://en.wikipedia.org/wiki/OPTICS_algorithm"" rel=""nofollow noreferrer"">article</a>) illustrates it pretty well. The image on the top left shows the data points, the image on the bottom left is the reachability plot:</p>

<p><a href=""https://i.stack.imgur.com/TamFM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TamFM.png"" alt=""enter image description here""></a></p>

<p>The $y$-axis are different values for $\varepsilon$, the valleys are the clusters. Each ""bar"" is for a single point, where the height of the bar is the minimal distance to the already printed points.</p>
","4","2","8820","16250"
"12278","<p>If you have a classification problem, you should you LDA instead of PCA. PCA ignores classes, whereas LDA is class-aware.</p>

<p>For example, if your data is 2D and you use PCA in the following example, you get:</p>

<p><a href=""https://i.stack.imgur.com/Ylm1X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ylm1X.png"" alt=""enter image description here""></a></p>

<p>So before PCA, the classes were perfectly linearly separable, but after PCA they are not separable at all. I'm not saying this happens in your case, but it could be.</p>
","2","2","8820","16250"
"12687","<p>The precision is defined as </p>

<p>$$\text{precision} = \frac{\text{true positive}}{\text{true positive} + \text{false positive}}$$</p>

<p>Is there any definition what this value should be if there is no positive classification (but of course positive elements)?</p>
","4","1","8820","16250"
"12691","<p>I just found that <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"" rel=""nofollow""><code>sklearn.metrics.precision_score</code></a> handles it like this:</p>

<pre><code>&gt;&gt;&gt; sklearn.metrics.precision_score(y_true, y_pred)
/home/moose/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074:
  UndefinedMetricWarning: Precision is ill-defined and being
    set to 0.0 due to no predicted samples.
    'precision', 'predicted', average, warn_for)
0.0
</code></pre>

<p>So they give a warning and set it to 0.</p>
","2","2","8820","16250"
"12830","<p>I've recently read <a href=""https://www.facebook.com/yann.lecun/posts/10152820758292143"" rel=""noreferrer"">Yan LeCuns comment on 1x1 convolutions</a>:</p>

<blockquote>
  <p><strong>In Convolutional Nets, there is no such thing as ""fully-connected layers"". There are only convolution layers with 1x1 convolution kernels and a full connection table.</strong></p>
  
  <p>It's a too-rarely-understood fact that ConvNets don't need to have a fixed-size input. You can train them on inputs that happen to produce a single output vector (with no spatial extent), and then apply them to larger images. Instead of a single output vector, you then get a spatial map of output vectors. Each vector sees input windows at different locations on the input.
  <strong>In that scenario, the ""fully connected layers"" really act as 1x1 convolutions.</strong></p>
</blockquote>

<p>I would like to see a simple example for this.</p>

<h2>Example</h2>

<p>Assume you have a fully connected network. It has only an input layer and an
output layer. The input layer has 3 nodes, the output layer has 2 nodes. This
network has <span class=""math-container"">$3 \cdot 2 = 6$</span> parameters. To make it even more concrete, lets say you have a ReLU activation function in the output layer and the weight matrix</p>

<p><span class=""math-container"">$$
\begin{align}
W &amp;= 
\begin{pmatrix}
 0 &amp; 1 &amp; 1\\
 2 &amp; 3 &amp; 5\\
\end{pmatrix} \in \mathbb{R}^{2 \times 3}\\
b &amp;= \begin{pmatrix}8\\ 13\end{pmatrix} \in \mathbb{R}^2
\end{align}
$$</span></p>

<p>So the network is <span class=""math-container"">$f(x) = ReLU(W \cdot x + b)$</span> with <span class=""math-container"">$x \in \mathbb{R}^3$</span>.</p>

<p><strong>How would the convolutional layer have to look like to be the same? What does LeCun mean with ""full connection table""?</strong></p>

<p>I guess to get an equivalent CNN it would have to have exactly the same number of parameters. The MLP from above has <span class=""math-container"">$2 \cdot 3 + 2 = 8$</span> parameters.</p>
","80","1","8820","16250"
"12851","<p>When writing a paper / making a presentation about a topic which is about neural networks, one usually visualizes the networks architecture.</p>

<p>What are good / simple ways to visualize common architectures automatically?</p>
","136","1","8820","16250"
"12856","<h2>Tensorflow, Keras, MXNet, PyTorch</h2>

<p>If the neural network is given as a Tensorflow graph, then you can <a href=""https://www.tensorflow.org/guide/graph_viz"" rel=""noreferrer"">visualize this graph with TensorBoard</a>.</p>

<p>Here is how the MNIST CNN looks like:</p>

<p><a href=""https://i.stack.imgur.com/zJHpV.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zJHpV.png"" alt=""enter image description here""></a></p>

<p>You can add names / scopes (like ""dropout"", ""softmax"", ""fc1"", ""conv1"", ""conv2"") yourself.</p>

<h3>Interpretation</h3>

<p>The following is only about the left graph. I ignore the 4 small graphs on the right half.</p>

<p>Each box is a layer with parameters that can be learned. For inference, information flows from bottom to the top. Ellipses are layers which do not contain learned parameters.</p>

<p>The color of the boxes does not have a meaning.</p>

<p>I'm not sure of the value of the dashed small boxes (""gradients"", ""Adam"", ""save"").</p>
","40","2","8820","16250"
"12996","<p>Probabilistic Graphical Models (PGMs) are:</p>
<ul>
<li>Connectionist: RBMs are PGMs and neural networks (<a href=""http://image.diku.dk/igel/paper/AItRBM-proof.pdf"" rel=""nofollow noreferrer"">source</a>)</li>
<li>Bayesian: Bayes Networks are bayesian (<a href=""https://en.wikipedia.org/wiki/Bayesian_network"" rel=""nofollow noreferrer"">Wikipedia article</a>)</li>
<li>Symbolist: Markov Logic Networks (<a href=""http://link.springer.com/article/10.1007/s10994-006-5833-1"" rel=""nofollow noreferrer"">source</a>)</li>
<li>Analogizers and Evolutionaries: <a href=""http://www.kdnuggets.com/2015/11/domingos-5-tribes-machine-learning-acm-webinar.html"" rel=""nofollow noreferrer"">According to Domingos</a>, they are also in Markov Logic Networks.</li>
</ul>
<p>So the answer is that you can't simply categorize such a general technique as probabilistic graphical models in a single one of those categories.</p>
<p>See also: <a href=""https://www.youtube.com/watch?v=E8rOVwKQ5-8"" rel=""nofollow noreferrer"">&quot;The Five Tribes of Machine Learning (And What You Can Learn from Each),&quot; Pedro Domingos</a></p>
","8","2","8820","16250"
"13029","<p>There are various algorithms for reinforcment learning (RL). One way to group them is by ""off-policy"" and ""on-policy"". I've heard that SARSA is on-policy, while Q-Learning is off-policy.</p>

<p>I think they work as follows:</p>

<p><a href=""https://i.stack.imgur.com/1CB4h.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1CB4h.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/vJKzA.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vJKzA.png"" alt=""enter image description here""></a></p>

<p>My questions are:</p>

<ul>
<li>How exactly is ""on-policy RL"" and ""off-policy RL"" defined?</li>
<li>What are the advantages / disadvantages of both?</li>
</ul>
","6","1","8820","16250"
"13231","<p><strong>Image classification</strong> is the task of assigning one of $n$ previously known labels to a given image. For example, you know that you will be given a couple of photos and each single image has exactly one of $\{cat, dog, car, stone\}$ in it. The algorithm should say what the photo shows.</p>

<p>The benchmark dataset for image classification is <strong>ImageNet</strong>; especiall thy <strong>large scale visual recognition challenge (LSVRC)</strong>. It has exactly 1000 classes and a huge amount of training data (I think there is a down-sampled version with about 250px x 250px images, but many images seem to be from Flicker).</p>

<p>This challenge is typically solved with CNNs (or other neural networks).</p>

<p><strong>Is there any paper which tries an approach which does not use neural networks in LSVRC?</strong></p>

<p>To clarify the question: Of course, there are other classification algorithms like $k$ nearest neighbors or SVMs. However, I doubt they work at all for that many classes / that much data. At least for $k$-NNs I'm sure that prediction would be extremely slow; for SVMs I guess both fitting and prediction would be much to slow (?).</p>
","8","1","8820","16250"
"13438","<p>I have recently downloaded the small image net dataset:</p>

<p><a href=""http://image-net.org/small/download.php"" rel=""noreferrer"">http://image-net.org/small/download.php</a></p>

<p>The archives contain a lot of images, but no other files. How do I know which label the images have?</p>
","7","1","8820","16250"
"13450","<p>After downloading the imagenet urls (<a href=""http://image-net.org/download-imageurls"" rel=""nofollow"">link</a>), I see that it is a single 1.1 GB text file which starts like this:</p>

<pre><code>n00004475_6590    http://farm4.static.flickr.com/3175/2737866473_7958dc8760.jpg
n00004475_15899    http://farm4.static.flickr.com/3276/2875184020_9944005d0d.jpg
n00004475_32312    http://farm3.static.flickr.com/2531/4094333885_e8462a8338.jpg
n00004475_35466    http://farm4.static.flickr.com/3289/2809605169_8efe2b8f27.jpg
n00004475_39382    http://2.bp.blogspot.com/_SrRTF97Kbfo/SUqT9y-qTVI/AAAAAAAABmg/saRXhruwS6M/s400/bARADEI.jpg
n00004475_41022    http://fortunaweb.com.ar/wp-content/uploads/2009/10/Caroline-Atkinson-FMI.jpg
n00004475_42770    http://farm4.static.flickr.com/3488/4051378654_238ca94313.jpg
n00004475_54295    http://farm4.static.flickr.com/3368/3198142470_6eb0be5f32.jpg
n00005787_13    http://www.powercai.net/Photo/UploadPhotos/200503/20050307172201492.jpg
n00005787_32    http://www.web07.cn/uploads/Photo/c101122/12Z3Y54RZ-22027.jpg
</code></pre>

<p>I'm pretty sure <code>n00004475_6590</code> is the class of the image. How can I look up what this means (in natural language)?</p>
","3","1","8820","16250"
"13451","<p>The first part before the underscore can be entered in this URL:</p>

<p><a href=""http://www.image-net.org/api/text/wordnet.synset.getwords?wnid=n00004475"" rel=""nofollow"">http://www.image-net.org/api/text/wordnet.synset.getwords?wnid=n00004475</a></p>
","1","2","8820","16250"
"13452","<p>I couldn't find an URL text file for the ILSVRC2012 training set, but for complete imagenet you can download the URLs only as a text file: <a href=""http://image-net.org/download"" rel=""noreferrer"">http://image-net.org/download</a></p>

<p>I wrote the following script to get a feeling for the data:</p>

<pre><code>#!/usr/bin/env python

""""""Analyze the distribution of classes in ImageNet.""""""

classes = {}
images = 0

with open(""fall11_urls.txt"") as f:
    for i, line in enumerate(f):
        label, _ = line.split(""\t"", 1)
        wnid, _ = label.split(""_"")
        if wnid in classes:
            classes[wnid] += 1
        else:
            classes[wnid] = 1
        images += 1

# Output
print(""Classes: %i"" % len(classes))
print(""Images: %i"" % images)

class_counts = [count for _, count in classes.items()]
import matplotlib.pyplot as plt
plt.hist(class_counts, bins=range(max(class_counts)))
plt.show()
</code></pre>

<p>which gave:</p>

<pre><code>Classes: 21841
Images: 14197122
</code></pre>

<p><a href=""https://i.stack.imgur.com/8iqYO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8iqYO.png"" alt=""enter image description here""></a></p>

<p>Classes which have less than 100 examples are pretty much useless. Lets remove them from the plot. Also increase the bin size to 25:</p>

<pre><code>#!/usr/bin/env python

""""""Analyze the distribution of classes in ImageNet.""""""

classes = {}
images = 0

with open(""fall11_urls.txt"") as f:
    for i, line in enumerate(f):
        label, _ = line.split(""\t"", 1)
        wnid, _ = label.split(""_"")
        if wnid in classes:
            classes[wnid] += 1
        else:
            classes[wnid] = 1
        images += 1

# Output
print(""Classes: %i"" % len(classes))
print(""Images: %i"" % images)

class_counts = [count for _, count in classes.items()]
import matplotlib.pyplot as plt
plt.title('ImageNet class distribution')
plt.xlabel('Amount of available images')
plt.ylabel('Number of classes')
min_examples = 100
bin_size = 25
plt.hist(class_counts, bins=range(min_examples, max(class_counts), bin_size))
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/Jm4ft.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Jm4ft.png"" alt=""enter image description here""></a></p>

<p>Or with seaborn:</p>

<pre><code>import seaborn as sns
sns.distplot(class_counts, kde=True, rug=False);
sns.plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/0lxqi.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0lxqi.png"" alt=""enter image description here""></a></p>

<h2>Top 10</h2>

<p>The top 10 classes with most data are:</p>

<pre><code>top10 = sorted(classes.items(), key=lambda n: n[1], reverse=True)[:10]
for class_label, count in top10:
    print(""%s:\t%i"" % (class_label, count))

n02094433:    3047 (Yorkshire terrier)
n02086240:    2563 (Shih-Tzu)
n01882714:    2469 (koala bear, kangaroo bear, native bear, )
n02087394:    2449 (Rhodesian ridgeback)
n02100735:    2426 (English setter)
n00483313:    2410 (singles)
n02279972:    2386 (monarch butterfly, Danaus plexippus)
n09428293:    2382 (seashore)
n02138441:    2341 (meerkat)
n02100583:    2334 (vizsla, Hungarian pointer)
</code></pre>

<p>Using <a href=""http://www.image-net.org/api/text/wordnet.synset.getwords?wnid=n02094433"" rel=""noreferrer"">http://www.image-net.org/api/text/wordnet.synset.getwords?wnid=n02094433</a> you can look the names up.</p>
","6","2","8820","16250"
"13864","<p>I would like to see if I can reproduce some of the image net results. However, I could not find the data (the list of URLs) used for training / testing in the ILSVRC 2012 (or later) classification challenges. I only found <a href=""http://www.image-net.org/download-imageurls"" rel=""noreferrer"">http://www.image-net.org/download-imageurls</a>.</p>

<p>Where is the data used for the ImageNet ILSVRC 2012 (or later) classification challenge?</p>
","9","1","8820","16250"
"14524","<p>In Hintons talk ""<a href=""https://web.archive.org/web/20150929075327/http://d1baxxa0joomi3.cloudfront.net/69bf620e48d60761118dc4328f5d6e9c/basic.mp4"" rel=""nofollow"">What's wrong about convolutional nets</a>"" (Late 2016 or early 2015, I guess) he talks about capsules to make a modular CNN.</p>

<p>Is there any publicly available implementation or papers about this idea?</p>
","2","1","8820","16250"
"14635","<p>I can think of three ways to deal with the problem:</p>

<ol>
<li><strong>Treat ""missing value"" as another feature</strong>: Imagine you have a feature like ""date of graduation"". One possible (likely?) reason why this value is missing might be that the person did not graduate. So you could build a model which as a binary feature ""graduation date is available"" and the actual graduation date as another feature.</li>
<li><strong>Predict the missing values</strong>: If data is missing because of your lack of knowledge of it (in contrast to the first point), then you might think about trying to predict the missing value. You could  also add a feature which encodes the certainty of the predicted value being correct.</li>
<li><strong>Skip the feature</strong>: If it is missing very often and if it doesn't add much value to your prediction, you might simply want to remove it.</li>
</ol>
","2","2","8820","16250"
"14644","<p>I would like to elaborate a bit on the fact that data in high dimensional spaces is sparser.</p>

<p>Usually, we think of euclidean spaces. This means if we have points $p_1, p_2 \in \mathbb{R}^n$ we say their distance is</p>

<p>$$d(p_1, p_2) = \sqrt{\sum_{i=1}^n {\left (p_1^{(i)} - p_2^{(i)} \right )}^2}$$</p>

<p>So we take the square root of the sum of the component-wise squared difference.</p>

<p>Now think of two points in a unit hypercube in $\mathbb{R}^n$ (hence $[0, 1]^n$). The maximum distance two points can have in this hypercube is when they are on the diagonal. Hence the distance is $\sqrt{n}$, meaning the higher the dimension is, the farer away can points be in the unit hypercube.</p>

<p>See also:</p>

<ul>
<li><a href=""https://math.stackexchange.com/q/1976250/6876"">What is the maximum distance of k points in an n-dimensional hypercube?</a></li>
<li>Martin Thoma: <a href=""https://martin-thoma.com/average-distance-of-points/"" rel=""nofollow noreferrer"">Curse of Dimensionality</a></li>
</ul>
","1","2","8820","16250"
"15081","<p>Convolutional Neural Networks (CNNs) use almost always the rectified linear activation function (ReLU):</p>

<p>$$f(x) = max(0, x)$$</p>

<p>However, the derivative of this function is</p>

<p>$$f'(x) = \begin{cases} 0 &amp;\text{if } x \leq 0\\ 1&amp;\text{otherwise}\end{cases}$$</p>

<p>(ignoring that is not differentiable at $0$, as I think it is done in practice). For inputs > 0 this is fine, but <strong>why doesn't it matter that the gradient is 0 at every point &lt; 0?</strong> Or does it matter? (Are there publications about this problem?)</p>

<p>If a neuron outputs 0 for every sample of the training data, it is basically lost, correct? Its weights will never be adjusted again?</p>
","4","1","8820","16250"
"15091","<p>I found a couple of sources which mention mean pooling for convolutional neural networks (CNNs) - including all lectures I had about CNNs so far - but I could not find any paper with at least 10 citations which uses mean pooling.</p>

<p>Do you know of a paper which uses mean pooling?</p>
","3","1","8820","16250"
"15093","<p>I just found an answer myself:</p>

<blockquote>
  <p>Boureau, Y-Lan, Jean Ponce, and Yann LeCun. ""<a href=""http://yann.lecun.com/exdb/publis/pdf/boureau-icml-10.pdf"" rel=""nofollow noreferrer"">A theoretical analysis of feature pooling in visual recognition</a>."" In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 111-118. 2010.</p>
</blockquote>

<p>See also: <a href=""http://www.shortscience.org/paper?bibtexKey=conf/icml/BoureauPL10#martinthoma"" rel=""nofollow noreferrer"">shortscience summary</a></p>
","3","2","8820","16250"
"15188","<p>I currently tried to figure out which paddings are directly supported by the frameworks:</p>

<p>Tensorflow (<a href=""https://www.tensorflow.org/api_docs/python/nn.html#conv2d"" rel=""nofollow noreferrer""><code>tf.nn.conv2d</code></a>):</p>

<ul>
<li><code>padding='VALID'</code>: No padding</li>
<li><code>padding='SAME'</code>: 0-padding such that the output has the same size as the input</li>
</ul>

<p>By applying <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/array_ops.html#pad"" rel=""nofollow noreferrer""><code>tf.pad</code></a> and then <code>padding='VALID'</code> one can get reflect and symmetric padding.</p>

<p>Lasagne (<a href=""http://lasagne.readthedocs.io/en/latest/modules/layers/conv.html#lasagne.layers.Conv2DLayer"" rel=""nofollow noreferrer""><code>lasagne.layers.Conv2DLayer</code></a>) (and probably also Theano):</p>

<ul>
<li><code>pad='full'</code>: A variant of 0-padding which results in a bigger image</li>
<li><code>pad='same'</code>: Like <code>SAME</code> for TF</li>
<li><code>pad='valid'</code>: Like <code>SAME</code> for TF</li>
</ul>

<p>Caffe (<a href=""http://caffe.berkeleyvision.org/tutorial/layers.html"" rel=""nofollow noreferrer"">docs</a>): I have no idea which padding they support. As <code>pad_h</code> and <code>pad_w</code> seem to be in <code>convolution_param</code>, I guess padding is supported. Probably (only?) zero-padding.</p>

<p>Hence my question:</p>

<p><strong>Are there publications where CNNs with padding, which was neither <code>VALID</code> nor <code>SAME</code>, or not 0-padding, was used?</strong></p>

<p>For example, I know of the following other options:</p>

<ul>
<li>reflect</li>
<li>nearest</li>
</ul>

<p>both of them might have the advantage, that the filter does not detect a border, where no border is.</p>
","3","1","8820","16250"
"15214","<p>In the <a href=""https://arxiv.org/pdf/1503.03832v3.pdf"" rel=""nofollow noreferrer"">FaceNet paper</a>, they describe the <a href=""https://arxiv.org/abs/1311.2901"" rel=""nofollow noreferrer"">Zeiler&amp;Fergus model</a> like this:</p>

<blockquote>
  <p>[...] the
  Zeiler&amp;Fergus model which consists of multiple interleaved
  layers of convolutions, [...]</p>
</blockquote>

<p>What do they mean by <strong>interleaved</strong>? How does that compare to the <a href=""https://arxiv.org/abs/1409.4842"" rel=""nofollow noreferrer"">inception layers</a>?</p>

<p>Especially, as the Zeiler&amp;Fergus paper states</p>

<blockquote>
  <p>We use standard fully supervised convnet models
  throughout the paper, as defined by (LeCun et al.,
  1989) and (Krizhevsky et al., 2012).</p>
  
  <p>[...]</p>
  
  <p>The top few layers of the network are conventional
  fully-connected networks and the final layer is
  a softmax classifier.</p>
</blockquote>
","1","1","8820","16250"
"15215","<p>The <a href=""http://www.cs.tau.ac.il/~wolf/ytfaces/"" rel=""nofollow noreferrer"">YouTube Faces database</a> (YTF) consists of 3,425 videos of 1,595 different people. Given two videos, the task for YTF is to decide if they contain the same person or not. Having $n$ comparisons, the classifier might get $c \leq n$  right. Then the accuracy would be $\frac{c}{n}$.</p>

<p><a href=""https://arxiv.org/pdf/1503.03832v3.pdf"" rel=""nofollow noreferrer"">FaceNet</a> is a CNN which maps an image of a face on a unit sphere of $\mathbb{R}^{128}$. It was evaluated on YTF. How did they decide which person is in the video?</p>

<p>(I can imagine several procedures how this could be done, but I couldn't find it in the paper. One example, how it could be done, is by evaluating all images $x_i^{(k)}$ with $i = 1, \dots, \text{length of video }k$ and averaging the results - but I would like to know what they did / how this is usually done.) </p>
","3","1","8820","16250"
"15234","<blockquote>
  <p>I want to come up with a script/tool(Python) that <strong>leverages machine learning</strong> and automatically analyze the results for me, instead of me looking at the results manually every day.</p>
</blockquote>

<p>This is the wrong approach. Choose your tools to solve tasks, not because they sound cool. Imagine a craftsman saying ""I want to use a hammer"". (However, machine learning might still suit to this task.)</p>

<blockquote>
  <p>From what I read online, I think I need to [...] train it for a few days.</p>
</blockquote>

<p>Depends on how much data, how much features, how much computing power and which algorithm you use. In many cases you can train a model within seconds on a normal computer.</p>

<h2>The real question</h2>

<p>The problem: You have some characteristics (features) like CPU usage, memory usage, ... for every automated test. For some instances (meaning a test and the associated features) you have the binary information ""normal"", ""abnormal"". Now, when you get the characteristics of a new test, you want to automatically decide if it was normal or abnormal.</p>

<p>This is a binary classification problem. If you have the label (""normal"", ""abnormal"") for all instances, you can apply supervised learning methods. If you have the label only for some instances, it is semi-supervised learning. If you don't have any labels it is unsupervised.</p>

<h2>The Answer</h2>

<p>I assume you are in a supervised learning case. Then, I would suggest to use a <a href=""https://en.wikipedia.org/wiki/Decision_tree"" rel=""nofollow noreferrer"">decision tree</a> (see <a href=""http://scikit-learn.org/stable/modules/tree.html"" rel=""nofollow noreferrer""><code>sklearn.tree.DecisionTreeClassifier</code></a> - sklearn is pretty easy to use and has many standard algorithms implemented) for this decision making. It basically automatically generates many nested if/else statements to get to the decision. Hence it is interpretable and you could manually adjust or improve it.</p>
","4","2","8820","16250"
"15328","<p>The paper <a href=""https://arxiv.org/pdf/1409.4842v1.pdf"" rel=""noreferrer"">Going deeper with convolutions</a> describes GoogleNet which contains the original inception modules:</p>

<p><a href=""https://i.stack.imgur.com/zTinD.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zTinD.png"" alt=""enter image description here""></a></p>

<p>The change to inception v2 was that they replaced the 5x5 convolutions by two successive 3x3 convolutions and applied pooling:</p>

<p><a href=""https://i.stack.imgur.com/eindA.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eindA.png"" alt=""enter image description here""></a></p>

<p>What is the difference between Inception v2 and Inception v3?</p>
","20","1","8820","16250"
"15470","<p>See <a href=""https://arxiv.org/pdf/1707.09725.pdf#page=21"" rel=""nofollow noreferrer"">Analysis and Optimization of Convolutional Neural Network Architectures</a> for the pooling types in general.</p>

<blockquote>
  <p>What is the difference between max pooling, average pooling and min pooling?</p>
</blockquote>

<p>A pooling function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is a function which is very symmetric:</p>

<p>$$f(x_1, x_2, \dots, x_n) = f(x_2, x_1, \dots, x_n) = f(x_n, x_1, \dots, x_2) = \dots$$</p>

<p>So the function does not care about the order of the arguments. This is the case for $\min$, $\max$ and average.</p>

<p>In the context of CNNs, you apply the pooling function to each feature map independantly. The output of a pooling layer has exactly as many feature maps as the input. For each feature map, you take a $p \times p$ section and apply the pooling function $f$ to it. Usually, you apply it with a stride $s &gt; 1$ so that the output is much smaller than the input (only $\frac{1}{s^2}$).</p>

<blockquote>
  <p>I have learnt that pooling is a sort of downsampling where we can preserve the locations of the original image that showed the strongest correlation to the specific features we searching for. </p>
</blockquote>

<p>This is only true if you talk about max-pooling with $s &gt; 1$. Which is usually the case. Although I would not say ""correlation"" but ""activation"".</p>

<blockquote>
  <p>But what is the point of find weakest correlations with min pooling, for example?</p>
</blockquote>

<p>I don't know. I have never seen min pooling even be mentioned. Where have you seen it?</p>
","4","2","8820","16250"
"15816","<p>Any train-test split which has more data in the training set will most likely give you better accuracy as calculated on that test set. So the direct answer to your question is 60:40. And 99:1 would even give better accuracy...</p>

<p>However, at the end of the day, you are not interested in the accuracy on your test set. You are interested in the ""real"" accuracy, which gets estimated by your test set. And you better want to make sure that the test set can predict that accuracy well.</p>

<p>So, which split should you pick?</p>

<ul>
<li>Make sure ""enough"" data is in the test set. What ""enough"" means depends on your dataset (the number of classes, number of features)</li>
<li>If you rather want a good estimate of your real error, make the test set bigger</li>
<li>If you doubt that you can get more training data and you think the more data in your training set will improve your model (in real, not on the test set) a lot, then ""sacrifice"" a bit for the training set.</li>
</ul>

<p>Whatever you do, make sure you define your training set before you start your experiments.</p>
","0","2","8820","16250"
"15856","<p>Are the target values ordered? Then it is likely to be regression.</p>

<p>Otherwise classification.</p>

<p>See <a href=""https://en.wikipedia.org/wiki/Level_of_measurement#Interval_scale"" rel=""nofollow noreferrer"">Level of measurement</a>.</p>

<h2>Examples</h2>

<p>Regression</p>

<ul>
<li>Price prediction</li>
<li>Rating prediction</li>
</ul>

<p>Classification</p>

<ul>
<li>Predict the gender of a user</li>
<li>Predict the class (dog, cat, house, airplane, ...) of an object in an image</li>
</ul>

<p>However, I'd say for two classes it doesn't really make a big difference. In the case of predicting the gender of a user, you could also predict the probability of the user being female. It's just a minor variation of the same problem (ignoring <a href=""https://medium.com/gender-2-0/falsehoods-programmers-believe-about-gender-f9a3512b4c9c#.77b5ni8g0"" rel=""nofollow noreferrer"">this</a>).</p>
","0","2","8820","16250"
"15934","<p>When you have sensors, the values you receive change even if the signal that was recorded didn't change. This is one example of noise.</p>
<p>When you have a model of the world, it abstracts from the real relationships by simplifying things which are not too important. To take into account for the simplification, you model the error as noise (e.g. in a <a href=""https://martin-thoma.com/kalman-filter/"" rel=""nofollow noreferrer"">Kalman filter</a>).</p>
<p>But noise sources can be anything. For example, in an image classification problem, data compression can distort an image. Images can have different resolution; low resolution signals are harder to classify than high resolution figures. Aliasing effects can also distort images.</p>
<blockquote>
<p>And what is additive noise?</p>
</blockquote>
<p>Suppose your system equation is</p>
<p><span class=""math-container"">$$z = H \cdot x$$</span></p>
<p>where <span class=""math-container"">$z \in \mathbb{R}^{n_m}$</span> is the observation, <span class=""math-container"">$x \in \mathbb{R}^{n_x}$</span> is the state you're interested in and <span class=""math-container"">$H \in \mathbb{R}^{n_m \cdot n_x}$</span> is a transformation matrix. Then the noise could interact with your system in any way. But most of the time it is logical and practical that the noise is additive, meaning your model is</p>
<p><span class=""math-container"">$$z = H \cdot x + r$$</span></p>
<p>where <span class=""math-container"">$r$</span> is sampled from a random variable of any distribution.</p>
<blockquote>
<p>What is (additive) Gaussian noise?</p>
</blockquote>
<p><span class=""math-container"">$$r \sim \mathcal{N}(\mu, \sigma^2)$$</span></p>
<p>See <a href=""https://en.wikipedia.org/wiki/Normal_distribution"" rel=""nofollow noreferrer"">normal distribution</a></p>
","6","2","8820","16250"
"16010","<p>It's a binary semi-supervised classification problem. First, establish a base-line for the supervised case. Then try if the unlabeled data helps</p>

<h2>Supervised</h2>

<ol>
<li>From your labeled data: create a training, validation and test set. Don't touch the test set until the very end.</li>
<li>Try something simple, e.g. a multilayer Perceptron (MLP) with 350 input nodes and 1 output node (giving the probability of ""true"").</li>
<li>Try more stuff (e.g. <a href=""https://github.com/MartinThoma/algorithms/blob/master/ML/mnist/many-classifiers/python.py#L92"" rel=""nofollow noreferrer"">https://github.com/MartinThoma/algorithms/blob/master/ML/mnist/many-classifiers/python.py#L92</a>).</li>
<li>Try to combine classifiers in ensembles (see <a href=""https://martin-thoma.com/images/2015/12/ml-ensemble-learning.png"" rel=""nofollow noreferrer"">examples</a>).</li>
<li>A (naive) bayes classifier might be worth being investigated.</li>
</ol>

<h2>Semi-supervised</h2>

<ul>
<li>You could train an auto-encoder with a bottleneck on the unlabeled data. Then remove everything after the bottleneck. Use this network as a preprocessing step. The idea is that this network finds a more meaningful abstraction of the relevant data. However, similar as PCA can have a very low projection error and still destroy the possibility to distinguish the classes.</li>
<li>SVMs can also be used in a semi-supervised setting.</li>
<li>You could try to find clusters in the unlabeled data + labeled data. Then you can get an a priori probability of ""true"" for each cluster (ignoring the unlabeled data). You could train very small models (overfitting!) on the data in the clusters.</li>
</ul>

<p>Usually, you can interpret features. This might help to develop new features.</p>
","3","2","8820","16250"
"16045","<p>Dropout (<a href=""https://arxiv.org/abs/1207.0580"" rel=""noreferrer"">paper</a>, <a href=""https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"" rel=""noreferrer"">explanation</a>) sets the output of some neurons to zero. So for a MLP, you could have the following architecture for the <a href=""https://en.wikipedia.org/wiki/Iris_flower_data_set"" rel=""noreferrer"">Iris flower dataset</a>:</p>

<pre><code>4 : 50 (tanh) : dropout (0.5) : 20 (tanh) : 3 (softmax)
</code></pre>

<p>It would work like this:</p>

<p>$$softmax(W_3 \cdot \tanh(W_2 \cdot \text{mask}(D, \tanh(W_1 \cdot input\_vector)))$$</p>

<p>with $input\_vector \in \mathbb{R}^{4 \times 1}$, $W_1 \in \mathbb{R}^{50 \times 4}$, $D \in \{0, 1\}^{50 \times 1}$, $W_2 \in \mathbb{R}^{20 \times 50}$, $W_3 \in \mathbb{R}^{20 \times 3}$ (ignoring biases for the sake of simplictiy).</p>

<p>With $D = (d)_{ij}$ and</p>

<p>$$d_{ij} \sim B(1, p=0.5)$$</p>

<p>where the $\text{mask}(D, M)$ operation multiplies $D$ point-wise with $M$ (see <a href=""https://en.wikipedia.org/wiki/Hadamard_product_(matrices)"" rel=""noreferrer"">Hadamard product</a>).</p>

<p>Hence we just sample the matrix $D$ each time and thus the dropout becomes a multiplication of a node with 0.</p>

<p>But for CNNs, it is not clear to me what exactly is dropped out. I can see three possibilities:</p>

<ol>
<li>Dropping complete feature maps (hence a kernel)</li>
<li>Dropping one element of a kernel (replacing an element of a kernel by 0)</li>
<li>Dropping one element of a feature map</li>
</ol>

<p>Please add a reference / quote to your answer.</p>

<h2>My thoughts</h2>

<p>I think Lasagne does (3) (see <a href=""https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/noise.py#L20-L106"" rel=""noreferrer"">code</a>). This might be the simplest to implement. However, closer to the original idea might be (1).</p>

<p>Seems to be similar for Caffe (see <a href=""https://github.com/BVLC/caffe/blob/master/src/caffe/layers/dropout_layer.cu"" rel=""noreferrer"">code</a>). For tensorflow, the user has to decide (<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/core.py#L215-L293"" rel=""noreferrer"">code</a> - I'm not sure what happens when <code>noise_shape=None</code> is passed).</p>

<h2>How it should be</h2>

<p>(2) and (3) don't make much sense as it would cause the network to add invariance to spacial positions, which is probably not desired. Hence (1) is the only variant which makes sense. But I'm not sure what happens if you use the default implementation.</p>
","11","1","8820","16250"
"16072","<p>What you are looking for is called <em>Reinforcement Learning</em>. At my university, there is a complete course ($15 \cdot 3h = 45h$) only to introduce students to this topic. Here are my (mostly german) <a href=""https://martin-thoma.com/probabilistische-planung/"" rel=""nofollow noreferrer"">lecture notes to probabilistic planning</a>. I would say this is definitively an advanced topic for machine learning.</p>

<h2>Topcis to learn about</h2>

<ul>
<li>Markov Decision Processes (MDPs)

<ul>
<li>Policy and Value iteration</li>
<li>Project: Rock-Paper-Scissors / <a href=""https://en.wikipedia.org/wiki/Tic-tac-toe"" rel=""nofollow noreferrer"">Tic-Tac-Toe</a></li>
</ul></li>
<li>Partially Obversable Markov Decision Processes

<ul>
<li>Project: Black Jack</li>
</ul></li>
<li>Reinforcment learning

<ul>
<li>Q-Learning</li>
<li>SARSA</li>
</ul></li>
</ul>

<h2>Other simple games</h2>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Pong"" rel=""nofollow noreferrer"">Pong</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Inverted_pendulum"" rel=""nofollow noreferrer"">Inverted Pendulum</a></li>
<li>Backgammon</li>
</ul>

<h2>Other resources</h2>

<ul>
<li><a href=""https://gym.openai.com/"" rel=""nofollow noreferrer"">OpenAI Gym</a></li>
<li><a href=""http://incompleteideas.net/book/the-book-2nd.html"" rel=""nofollow noreferrer"">Book by Sutton</a></li>
</ul>
","3","2","8820","16250"
"16088","<p>I have heard of genetic algorithms, but I have never seen practical examples and I've never got a systematic introduction to them.</p>

<p>I am now looking for a textbook which introduces genetic algorithms in detail and gives practical examples how they are used, what their strengths are compared to other solution methods and what their weaknesses are.</p>

<p>Is there any standard textbook for this?</p>
","6","1","8820","16250"
"16144","<p>The VC dimension of a classifier is determined the following way:</p>

<pre><code>VC = 1
found = False
while True:
    for point_distribution in all possible point distributions of VC+1 points:
        allcorrect = True
        for classdist in every way the classes could be assigned to the classes:
            adjust classifier
            if classifier can't classify everything correct:
                allcorrect = False
                break
        if allcorrect:
            VC += 1
            continue
    break
</code></pre>

<p>So there has only to be one way to place three points such that all possible class distributions among this point-placement can be classified the correct way.</p>

<p>If you don't place the three points on a line, the perception gets it right. But there is no way to get the perception classify all possible class distributions of 4 points, no matter how you place the points</p>

<h2>Your example</h2>

<p>Your features are in $\mathbb{R}$. Every classifier has at least dimension 1.</p>

<p>VC-Dimension 2: It can classify all four situations correctly.</p>

<ol>
<li>Points: 0 and 42</li>
<li>Distributions:

<ul>
<li>class(0) = False, class(42) = False => $a = 1337, b=3141$ classifies this correctly</li>
<li>class(0) = False, class(42) = True => $a = 40, b = 1337$ classifies this correctly</li>
<li>class(0) = True, class(42) = False => $a = -1, b = 1$ classifies this correctly</li>
<li>class(0) = True, class(42) = True => $a = -1, b = 1337$ classifies this correctly.</li>
</ul></li>
</ol>

<p>VC-Dimension 3: No, that doesn't work. Imagine the classes <code>true</code> and <code>false</code> being ordered like <code>True False True</code>. Your classifier can't deal with that. Hence it has a VC-Dimension of 2.</p>

<h3>Proof</h3>

<p>Obviously, the points $x_1, x_2, x_3 \in \mathbb{R}$ can only be distinguished if they have different values. Without loss of generality, we can assume that $x_1 &lt; x_2 &lt; x_3$. Hence the classifier has to be able to classify</p>

<blockquote>
  <p>class($x_1$) = True, class($x_2$) = False, class($x_3$) = True</p>
</blockquote>

<p>correctly to have VC dimension 3. For $x_1$ to be classified as True,
$$a \leq x_1 \leq b$$
is required. For $x_2$ to be False,
$$x_2 &lt; a \qquad\text{ or }\qquad b &lt; x_2$$
is required. As $a \leq x_1$ and $x_1 &lt; x_2$, it has to be $b &lt; x_2$. So the situation is currently:
$$a \leq x_1 \leq b &lt; x_2 &lt; x_3$$
For $x_3$ to be classified as True, 
$$a \leq x_3 \leq b$$
is required. But the other constraints already required $b &lt; x_3$. Hence it is not possible to classify all class distributions of any 3 points correctly with this classifier. Hence it does not have VC dimension 3.</p>
","3","2","8820","16250"
"16165","<p>I am currently wondering if the following graphic of <a href=""https://arxiv.org/abs/1512.03385"" rel=""nofollow noreferrer"">deep residual networks</a> is wrong:</p>

<p><a href=""https://i.stack.imgur.com/msvse.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/msvse.png"" alt=""enter image description here""></a></p>

<p>I would say the graphic describes</p>

<p>$$\varphi \left (W_2 \varphi(W_1 x) + x \right ) \qquad \text{ with } \varphi = ReLU$$</p>

<p>The $\mathcal{F}(x)$ does not make sense to me. Assuming both weight layers are simple MLPs without bias, where the first one has a weight matrix $W_1$ and the second one has a weight matrix $W_2$, what is $\mathcal{F}$?</p>

<p>In the text, they define</p>

<p>$$\mathcal{F}(x) := \mathcal{H}(x) - x$$</p>

<p>where $\mathcal{H}(x)$ is ""the desired underlying mapping"" (whatever that exactly means).</p>

<p>Also, equation (1) seems strange to me:
$$y = \mathcal{F}(x, \{W_i\}) + x$$
In figure 5 they have two weight layers and call this a building block. Why is there only one weight matrix in this equation?</p>

<h2>My thoughts</h2>

<p>I think the authors could mean
$$\mathcal{F}_i = \varphi(W_i x)$$
In that case, in the image where $\mathcal{F}(x)$ is it should be
$$\mathcal{F}_1(x) = \varphi(W_1 x)$$
and where $\mathcal{F}(x) + x$ is should be
$$\mathcal{F}_2(\mathcal{F}_1(x)) + x = \varphi \left (W_2 \varphi(W_1 x) + x \right )$$</p>
","1","1","8820","16250"
"16184","<p>Reading the <a href=""https://arxiv.org/pdf/1311.2901v3.pdf"" rel=""nofollow noreferrer"">Zeiler&amp;Fergus paper</a> (<a href=""http://www.shortscience.org/paper?bibtexKey=journals/corr/ZeilerF13#martinthoma"" rel=""nofollow noreferrer"">my summary</a>), I wonder how exactly they trained the deconv net. What was their data?</p>

<p>I think for one CNN which they want to analyze, they train exactly one deconv net (in contrast to training one deconv net per layer). The featuers (inputs) of the deconv net are the activations of the layer they want to analyze. The output they train them on are the activations that actually was the input of the layer they want to analyze. So although they have one deconv-net in total, they train it layer-wise. So for each training run, the weights of only one deconv layer are adjusted.</p>

<p>However, I wonder why the images look that unrealistic:</p>

<p><a href=""https://i.stack.imgur.com/hCqz2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hCqz2.png"" alt=""enter image description here""></a></p>

<p>Is it gray because MSE is the training objective? Why aren't the first layer filter outputs gray then, too?</p>
","3","1","8820","16250"
"16187","<p>I've just been reading</p>

<blockquote>
  <p>Zeiler, M.D. and Fergus, R., 2014, September. Visualizing and understanding convolutional networks. In European Conference on Computer Vision (pp. 818-833). Springer International Publishing. (<a href=""https://arxiv.org/abs/1311.2901"" rel=""nofollow noreferrer"">Link</a>, <a href=""http://www.shortscience.org/paper?bibtexKey=journals/corr/1512.02017#martinthoma"" rel=""nofollow noreferrer"">my summary</a>)</p>
</blockquote>

<p>and (partially)</p>

<blockquote>
  <p>Mahendran, A. and Vedaldi, A., 2016. Visualizing deep convolutional neural networks using natural pre-images. International Journal of Computer Vision, pp.1-23. (<a href=""https://arxiv.org/abs/1512.02017v3"" rel=""nofollow noreferrer"">link</a>, <a href=""http://www.shortscience.org/paper?bibtexKey=journals/corr/1512.02017#martinthoma"" rel=""nofollow noreferrer"">my summary</a>)</p>
</blockquote>

<p>They are both about visualizing features being learned in CNNs.</p>

<p>Although both papers have an introduction where you can read things like:</p>

<ul>
<li>our understanding of [CNN features] remains limited</li>
<li>While the performance of representations has been improving significantly in the past few years, their design remains eminently empirical</li>
<li>In this paper, with the aim of obtaining a better understanding of representations, we develop a family of methods to investigate CNNs and other image features by means of visualizations</li>
<li>there is no clear understanding of why they perform so well, or how they might be improved</li>
<li>there is still little insight into the internal operation and behavior of these complex models, or how they achieve such good performance</li>
</ul>

<p>Although I like the images, I don't see how these methods are better than simply pushing all images through the network and showing the top-$n$ images which activate the neuron of interest most. Was this evaluated? Did the authors have any insights into the features which other authors didn't have before / without those techniques?</p>

<p>(The Zeiler&amp;Fergus paper at least added the occlusion sensitivity analysis which does help. However, a big part of the paper is this filter visualization by deconv-nets. And I don't see how this helps to address any of the points mentioned above)</p>
","2","1","8820","16250"
"16292","<p>I was recently thinking about the memory cost of (a) training a CNN and (b) inference with a CNN. Please note, that I am not talking about the storage (which is simply the number of parameters).</p>

<p>How much memory does a given CNN (e.g. VGG-16 D) need for </p>

<ul>
<li>(a) Training (with ADAM)</li>
<li>(b) Inference on a single image?</li>
</ul>

<h2>My thoughts</h2>

<p>Basically, I want to make sure that I didn't forget anything with this question. If you have other sources which explain this kind of thought, please share them with me.</p>

<h3>(a) Training</h3>

<p>For training with ADAM, I will now assume that I have a Mini-batch size of <span class=""math-container"">$B \in \mathbb{N}$</span> and <span class=""math-container"">$w \in \mathbb{N}$</span> is the number of parameters of the CNN. Then the memory footprint (the maximum amount of memory I need at any point while training) for a single training pass is:</p>

<ul>
<li><span class=""math-container"">$2w$</span>: Keep the weights and the weight updates in memory</li>
<li><span class=""math-container"">$B \cdot $</span> Size of all generated feature maps (forward pass)</li>
<li><span class=""math-container"">$w$</span>: Gradients for each weight (backpropagation)</li>
<li><span class=""math-container"">$w$</span>: Learning rates for each weight (ADAM)</li>
</ul>

<h3>(b) Inference</h3>

<p>In inference, it is not necessary to store a feature map of layer <span class=""math-container"">$i-1$</span> if the feature maps of layer <span class=""math-container"">$i$</span> are already calculated. So the memory footprint while inference is:</p>

<ul>
<li><span class=""math-container"">$w$</span>: The model</li>
<li>The two most expensive successive layers (one which is already calculated, the net one which gets calculated)</li>
</ul>
","8","1","8820","16250"
"16311","<p>For my answers, I assume you are talking about batch (not mini-batch or stochastic) gradient descent.</p>

<ol>
<li>No. Assume you initialize all weights with the same value. Then all gradients (in the same layer) will be the same. Always. Hence the network effectively only learns one parameter per layer. It is possible (and likely) that this is neither a gloabl nor a local minimum of the network (which has more parameters).</li>
<li>Yes, as the learning rate is ""small enough for all practical purposes"". (No, if you use SGD or mini-batch gradient descent)</li>
<li>Usure. I think the correct answer is ""No, the network can make more mistakes in between with cross entropy."". It is certainly sure to improve CE loss while at the same time getting worse at accuracy (see proof below). However, I'm not sure if the gradient would ever lead to such a result.</li>
</ol>

<h2>Example for 3</h2>

<pre><code>#!/usr/bin/env python

from math import log

def ce(vec):
    """"""index 0 is the true class.""""""
    return -(log(vec[0]) + sum([log(1-el) for el in vec[1:]]))

a = [0.1001, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0999]
print(ce(a))
b = [0.49, 0.51, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
print(ce(b))
</code></pre>

<p>gives:</p>

<pre><code>ce(a) = 3.24971912864
ce(b) = 1.42669977575
</code></pre>

<p>Hence the cross entropy loss dropped (as expected), the probability for the correct class increased (as expecte) but it makes a mistake (if you simply take the argmax).</p>
","3","2","8820","16250"
"16411","<p>This question is only about the vocabulary.</p>

<p>Do / can you say</p>

<ol>
<li>data item</li>
<li>data sample</li>
<li>recording</li>
<li>sample</li>
<li>data point</li>
<li>something else</li>
</ol>

<p>when you talk about elements of the training / test set? For example:</p>

<blockquote>
  <p>The figure shows 100 data items of the training set.</p>
  
  <p>Database A contains the same data items as database B, but in another format.</p>
  
  <p>The remaining data items were removed from the dataset.</p>
  
  <p>Those 10 classes have 123456 data items.</p>
</blockquote>

<p>Please provide papers with examples.</p>

<p>According to <a href=""https://books.google.com/ngrams/graph?content=data+item%2Cdata+point%2Cdata+sample&amp;case_insensitive=on&amp;year_start=1920&amp;year_end=2020&amp;corpus=15&amp;smoothing=3&amp;share=&amp;direct_url=t4%3B%2Cdata%20item%3B%2Cc0%3B%2Cs0%3B%3Bdata%20item%3B%2Cc0%3B%3BData%20Item%3B%2Cc0%3B%3BData%20item%3B%2Cc0%3B%3BDATA%20ITEM%3B%2Cc0%3B.t4%3B%2Cdata%20point%3B%2Cc0%3B%2Cs0%3B%3Bdata%20point%3B%2Cc0%3B%3BData%20Point%3B%2Cc0%3B%3BData%20point%3B%2Cc0%3B%3BDATA%20POINT%3B%2Cc0%3B.t4%3B%2Cdata%20sample%3B%2Cc0%3B%2Cs0%3B%3Bdata%20sample%3B%2Cc0%3B%3BData%20Sample%3B%2Cc0%3B%3Bdata%20Sample%3B%2Cc0%3B%3BDATA%20SAMPLE%3B%2Cc0%3B%3BData%20sample%3B%2Cc0%3B%3BDATA%20Sample%3B%2Cc0"" rel=""nofollow noreferrer"">Google n-grams</a>:</p>

<p><a href=""https://i.stack.imgur.com/1tzNa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1tzNa.png"" alt=""enter image description here""></a></p>
","2","1","8820","16250"
"16797","<p>For detection, a common way to determine if one object proposal was right is <em>Intersection over Union</em> (IoU, IU). This takes the set $A$ of proposed object pixels and the set of true object pixels $B$ and calculates:</p>

<p>$$IoU(A, B) = \frac{A \cap B}{A \cup B}$$</p>

<p>Commonly, IoU > 0.5 means that it was a hit, otherwise it was a fail. For each class, one can calculate the</p>

<ul>
<li>True Positive ($TP(c)$): a proposal was made for class $c$ and there actually was an object of class $c$</li>
<li>False Positive ($FP(c)$): a proposal was made for class $c$, but there is no object of class $c$</li>
<li>Average Precision for class $c$: $\frac{\#TP(c)}{\#TP(c) + \#FP(c)}$</li>
</ul>

<p>The mAP (mean average precision) = $\frac{1}{|classes|}\sum_{c \in classes} \frac{\#TP(c)}{\#TP(c) + \#FP(c)}$</p>

<p>If one wants better proposals, one does increase the IoU from 0.5 to a higher value (up to 1.0 which would be perfect). One can denote this with mAP@p, where $p \in (0, 1)$ is the IoU.</p>

<p>But what does <code>mAP@[.5:.95]</code> (as found in <a href=""https://arxiv.org/abs/1512.04412"" rel=""noreferrer"">this paper</a>) mean?</p>
","22","1","8820","16250"
"16814","<blockquote>
  <p>Unless I'm mistaken, the batch size is the number of training instances let seen by the model during a training iteration</p>
</blockquote>

<p>Correct (although I would call it ""weight update step"")</p>

<blockquote>
  <p>and epoch is a full turn when each of the training instances have been seen by the model</p>
</blockquote>

<p>Correct</p>

<blockquote>
  <p>If so, I cannot see the advantage of iterate over an almost insignificant subset of the training instances several times in contrast with applying a ""max batch"" by expose all the available training instances in each turn to the model (assuming, of course, enough the memory). What is the advantage of this approach?</p>
</blockquote>

<p>Well, pretty much that. You usually don't have enough memory. Lets say we are talking about image classification. ImageNet is a wildly popular dataset. For quite a while, VGG-16D was one of the most popular mod.els. It needs calculcate 15 245 800 floats (in the feature maps) for one 224x224 image. This means about 61MB per image. This is just a rough lower bound on how much memory you need during training for each image.  ImageNet contains several thousand (I think about 1.2 million?) images. While you might have that much main memory, you certainly do not have that much GPU memory. I've seen GPU speeding up things to about 21x. So you definitely want to use the GPU.</p>

<p>Also: The time for one mini-batch is much lower. So the question is: Would you rather do n update steps with mini-batch per hour on a GPU or m update steps with batch without GPU, where n >> m.</p>
","3","2","8820","16250"
"16828","<p>If you want to use SVMs: <a href=""https://www.csie.ntu.edu.tw/~cjlin/libsvm/"" rel=""nofollow noreferrer""><code>libsvm</code></a> is written in C.</p>
","2","2","8820","16250"
"17079","<p>I have recently published a dataset (<a href=""https://arxiv.org/abs/1701.08380"" rel=""noreferrer"">link</a>) with 369 classes. I ran a couple of experiments on them to get a feeling for how difficult the classification task is. Usually, I like it if there are confusion matrices to see the type of error being made. However, a $369 \times 369$ matrix is not practical.</p>

<p>Is there a way to give the important information of big confusion matrices? For example, usually there are a lot of 0s which are not so interesting. Is it possible to sort the classes so that most non-zero entries are around the diagonal in order to allow showing multiple matrices which are part of the complete confusion matrix?</p>

<p>Here is <a href=""https://github.com/MartinThoma/algorithms/tree/master/ML/confusion-matrix"" rel=""noreferrer"">an example for a big confusion matrix</a>.</p>

<h2>Examples in the Wild</h2>

<p>Figure 6 of <a href=""https://arxiv.org/pdf/1702.05373.pdf"" rel=""noreferrer"">EMNIST</a> looks nice:</p>

<p><a href=""https://i.stack.imgur.com/NKEgU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NKEgU.png"" alt=""enter image description here""></a></p>

<p>It is easy to see where many cases are. However, those are only $26$ classes. If the whole page was used instead of only one column this could probably be 3x as many, but that would still only be $3 \cdot 26 = 78$ classes. Not even close to 369 classes of HASY or 1000 of ImageNet.</p>

<h2>See also</h2>

<p>My similar question on <a href=""https://cs.stackexchange.com/q/70627/2914"">CS.stackexchange</a></p>
","12","1","8820","16250"
"17421","<p>The classes of <a href=""http://ieeexplore.ieee.org/abstract/document/5206848/"" rel=""noreferrer"">ImagNet</a> have a hierarchy. Did anybody try to build a hierarchy of classifiers to use this fact?</p>

<p>Searching for ""multistage classification"" leads to different results.</p>
","4","1","8820","16250"
"17562","<ol>
<li>Decistion Tree: $\mathcal{O}(1)$ for constant depth (you might have bad accuracy, though)</li>
<li>SVM: $\mathcal{O}(K)$ if you use a linear kernel and one-vs-all</li>
<li>ANN: I don't know what ""2/3rd neurons of input data"" means</li>
<li>KNN: $\mathcal{O}(n)$ - you have to store all $n$ samples.</li>
</ol>
","2","2","8820","16250"
"17953","<p>It seems to me that t-SNE and other dimensionality reduction algorithms which reduce the dimensionality to two dimensions are mainly used to get an impression of the dataset. If done well, they look nice (e.g. <a href=""https://blog.sourced.tech/post/lapjv/"" rel=""nofollow noreferrer"">like this</a>), but I wonder if this is better than just showing random images / grouping them by class on a grid.</p>

<p>I would like to get an answer to the following aspects:</p>

<ol>
<li>How is t-SNE* better than just taking a random (probably stratified) sample of the data?</li>
<li>How is t-SNE* better than just fitting a neural network with a 2-neuron bottleneck to the data and then taking the (normalized) value of the 2 neurons for an embedding?</li>
<li>Does t-SNE* give any guarantees?</li>
<li>Is t-SNE* good for the construction of classifiers? I mean: If you already have a classifier which is much better than random / guessing the most frequent class, does t-SNE help you to make better classifiers? How?</li>
<li>There are <a href=""https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction"" rel=""nofollow noreferrer"">many dimensionality reduction algorithms</a>. How does one compare the non-linear ones? When is one algorithm better than others? Especially: Are they better than bottleneck features of neural networks?</li>
</ol>

<p>*: You could probably answer this for other dimensionality reduction algorithms as well, but t-SNE seems to be the most popular one.</p>

<p>Please note: I do see the advantage of a reduced dimensionality for compression / easier optimization / faster inference. However, the reduction to 2 dimensions seems to be only for visualization. Hence my question if one can see more in those embeddings than a visually pleasing image of the dataset.</p>
","2","1","8820","16250"
"17987","<p>I've read a couple of papers about kernel initialization and many papers mention that they use L2 regularization of the kernel (often with $\lambda = 0.0001$).</p>

<p>Does anybody do something different than initializing the bias with constant zero and not regularizing it?</p>

<h2>Kernel initialization papers</h2>

<ul>
<li>Mishkin and Matas: <a href=""https://arxiv.org/abs/1511.06422"" rel=""noreferrer"">All you need is a good init</a></li>
<li>Xavier Glorot and Yoshua Bengio: <a href=""http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf"" rel=""noreferrer"">Understanding the difficulty of training deep feedforward neural networks</a></li>
<li>He et al: <a href=""https://arxiv.org/abs/1502.01852"" rel=""noreferrer"">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
</ul>
","16","1","8820","16250"
"18159","<p>Take this with a grain of salt, but I think this is simply not true. You can evaluate it with the <a href=""https://github.com/MartinThoma/algorithms/tree/master/ML/filter-kernels"" rel=""nofollow noreferrer"">code I just wrote</a>:</p>

<p><a href=""https://i.stack.imgur.com/3OMdh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3OMdh.jpg"" alt=""enter image description here""></a></p>

<p>Only 2, probably 3 of the 25 look like edge filters to me. The result of an edge filter looks like this:</p>

<p><a href=""https://i.stack.imgur.com/bOciP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bOciP.png"" alt=""enter image description here""></a></p>
","3","2","8820","16250"
"18217","<p>I know that there are various pre-trained models available for ImageNet (e.g. <a href=""https://github.com/fchollet/keras/blob/master/keras/applications/vgg16.py"" rel=""nofollow noreferrer"">VGG 16</a>, <a href=""https://github.com/fchollet/keras/blob/master/keras/applications/inception_v3.py"" rel=""nofollow noreferrer"">Inception v3</a>, <a href=""https://github.com/fchollet/keras/blob/master/keras/applications/resnet50.py"" rel=""nofollow noreferrer"">Resnet 50</a>, <a href=""https://github.com/fchollet/keras/blob/master/keras/applications/xception.py"" rel=""nofollow noreferrer"">Xception</a>). Is there something similar for the tiny datasets (CIFAR-10, CIFAR-100, SVHN)?</p>
","1","1","8820","16250"
"18312","<p>What you are looking for is called <strong>Collaborative Filtering</strong> / <a href=""https://en.wikipedia.org/wiki/Matrix_completion"" rel=""nofollow noreferrer"">Matrix completion</a>. See <a href=""https://martin-thoma.com/collaborative-filtering/"" rel=""nofollow noreferrer"">my blog post</a> for a short introduction.</p>
","4","2","8820","16250"
"18315","<blockquote>
  <p>Why doesn't table-look up generalize?</p>
</blockquote>

<p>Generalization means you find rules which apply to unseen situations. For example, let's say I have a function $f: \mathbb{R} \rightarrow \mathbb{R}$ and I give you the (input, output) pairs (0, 1), (1, 2), (3, 4), (3.141, 4.141).</p>

<p>If you learn by table look-up, you know exactly those 4 tuples. But If I ask you what $f(5)$ is, you have a problem. Because you didn't find the general rule/pattern, but you simply memorized the data.</p>

<p>Another example: Imagine you have $n$ data points $(x, y)$ and you decide to fit a polynomial to it. As you know, you can fit any $n$ points (with the x's pairwise different) to a polynomial of degree $n-1$. But if you do that, even the slightest noise or a different unterlying model causes your predictions to be awefully wrong because your polynomial bounces like crazy.</p>

<blockquote>
  <p>What does he mean by hand generated features?</p>
</blockquote>

<p>If you have a vector of $n$ numbers $(x_1, \dots, x_n)$ as input, you might decided that the pair-wise multiplication $x_3 \cdot x_{42}$ helps the classification process. Hence you add $x_{n+1} = x_3 \cdot x_{42}$. This is a hand generated feature. In contrast, neural networks learn non-linear combinations of the input.</p>
","2","2","8820","16250"
"18406","<blockquote>
  <p>Do I have to take an ensemble learning that</p>
</blockquote>

<p>You don't have to do anything. As so often with machine learning questions, there is no obvious answer what will work better / best. Most of the time, you just have to try it.</p>

<p>For a general overview over ensembling techniques, see my blog post: <a href=""https://martin-thoma.com/ensembles/"" rel=""nofollow noreferrer"">https://martin-thoma.com/ensembles/</a></p>

<p>Keywords to look for are: Bagging, boosting, stacking</p>
","-1","2","8820","16250"
"18414","<p>When training neural networks, one hyperparameter is the size of a minibatch. Common choices are 32, 64, and 128 elements per mini batch.</p>
<p>Are there any rules/guidelines on how big a mini-batch should be? Or any publications which investigate the effect on the training?</p>
","30","1","8820","16250"
"18557","<p>It is commonly seen as something bad if the decision boundary of a neural network is too sharp, meaning if slight changes in the input change the class prediction completely.</p>

<p>Given a trained CNN, is it possible to measure / calculate the ""sharpness"" of its decision boundaries? Did somebody do that already?</p>
","3","1","8820","16250"
"18583","<p>I thought both, PReLU and Leaky ReLU are
<span class=""math-container"">$$f(x) = \max(x, \alpha x) \qquad \text{ with } \alpha \in (0, 1)$$</span></p>
<p>Keras, however, has both functions in <a href=""https://keras.io/layers/advanced_activations/"" rel=""noreferrer"">the docs</a>.</p>
<h2>Leaky ReLU</h2>
<p><a href=""https://github.com/fchollet/keras/blob/master/keras/layers/advanced_activations.py#L41"" rel=""noreferrer"">Source of LeakyReLU</a>:</p>
<pre><code>return K.relu(inputs, alpha=self.alpha)
</code></pre>
<p>Hence (see <a href=""https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L2529"" rel=""noreferrer"">relu code</a>)
<span class=""math-container"">$$f_1(x) = \max(0, x) - \alpha \max(0, -x)$$</span></p>
<h2>PReLU</h2>
<p><a href=""https://github.com/keras-team/keras/blob/50d83f7ab80c04283236a986a1c99550cecc5140/keras/layers/advanced_activations.py#L121"" rel=""noreferrer"">Source of PReLU</a>:</p>
<pre><code>def call(self, inputs, mask=None):
    pos = K.relu(inputs)
    if K.backend() == 'theano':
        neg = (K.pattern_broadcast(self.alpha, self.param_broadcast) *
               (inputs - K.abs(inputs)) * 0.5)
    else:
        neg = -self.alpha * K.relu(-inputs)
    return pos + neg
</code></pre>
<p>Hence
<span class=""math-container"">$$f_2(x) = \max(0, x) - \alpha \max(0, -x)$$</span></p>
<h2>Question</h2>
<p>Did I get something wrong? Aren't <span class=""math-container"">$f_1$</span> and <span class=""math-container"">$f_2$</span> equivalent to <span class=""math-container"">$f$</span> (assuming <span class=""math-container"">$\alpha \in (0, 1)$</span>?)</p>
","53","1","8820","16250"
"18677","<p>Look at the CUDA compute capability. They are a mixture of hardware and software features a GPU has (<a href=""http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities"" rel=""nofollow noreferrer"">see guide</a>).</p>

<p>I benchmarked the GTX 1070, Titan Black, GTX 970, GTX 980, GTX 980Ti. The numbers can be found in <a href=""https://arxiv.org/abs/1707.09725"" rel=""nofollow noreferrer"">my masters thesis</a> (Table 5.3 and Table 5.16), but the gist is:</p>

<ul>
<li>GTX 1070 is by far the fastest</li>
<li>GTX 980 and 980 Ti are pretty much the same, but only ~half as fast as the GTX 1070,</li>
<li>GTX 970 needs 1.5 the time of the 980</li>
</ul>

<p>You should look at how much memory you need. This is sometimes a limiting factor.</p>
","5","2","8820","16250"
"18840","<p>LDA (linear discriminant analysis), SVMs with a linear kernel, and perceptrons are linear classifiers. Is there any other relationship between them, e.g.:</p>

<ol>
<li>Every decision boundary that can be found by LDA can be found by linear SVM</li>
<li>Every decision boundary that can be found by linear SVM can be found by LDA.</li>
<li>Every decision boundary that can be found by LDA can be found by a perceptron</li>
<li>Every decision boundary that can be found by linear SVM can be found by a perceptron.</li>
<li>Every decision boundary that can be found by a perceptron can be found by LDA</li>
<li>Every decision boundary that can be found by a perceptron can be found by an SVM with a linear kernel</li>
</ol>

<p>Always on the same data, of course.</p>

<p>For example, I think the linear SVM can find more decision boundaries than a perceptron due to slack variables. While the perceptron finds just an arbitrary hyperplane which separates the data linearly (if such a hyperplane exists), the linear SVM will always find the same hyperplane due to the optimality criterium.</p>
","3","1","8820","16250"
"20271","<p>Are there publications which mention numerical problems in neural network optimization?</p>

<p>(Blog posts, articles, workshop notes, lecture notes, books - anything?)</p>

<h2>Background of the question</h2>

<p>I've recently had a strange phenomenon: When I trained a convolutional network on the GTSRB dataset with a given script on my machine, it got state of the art results (99.9% test accuracy). 10 times. No outlier. When I used the same scripts on another machine, I got much worse results (~ 80% test accuracy or so, 10 times, no outliers). I thought that I probably didn't use the same scripts and as it was not important for my publication I just removed all results of that dataset. I thought I probably made a mistake one one of the machines (e.g. using different pre-processed data) and I couldn't find out where the mistake happened.</p>

<p>Now a friend wrote me that he has a network, a training script and a dataset which converges on machine A but does not converge on machine B. Exactly the same setup (a fully connected network trained as an autoencoder).</p>

<p>I have only one guess what might happen: The machines have different hardware. It might be possible that Tensorflow uses different algorithms for matrix multiplication / gradient calculation. Their numeric properties might be different. Those differences might cause one machine to be able to optimize the network while the other can't.</p>

<p>Of course, this needs further investigation. But no matter what is happening in those two cases, I think this question is interesting. Intuitively, I would say that numeric issues should not be important as sharp minima are not desired anyway and differences in one multiplication are less important than the update of the next step.</p>
","4","1","8820","16250"
"21862","<p>Copy-pasted from <a href=""https://arxiv.org/pdf/1707.09725.pdf"" rel=""noreferrer"">my masters thesis</a>:</p>

<ul>
<li>If the loss does not decrease for several epochs, the learning rate might be too low.
The optimization process might also be stuck in a local minimum.</li>
<li>Loss being NAN might be due to too high learning rates. Another reason is division
by zero or taking the logarithm of zero.</li>
<li>Weight update tracking: Andrej Karpathy proposed in the 5th lecture of CS231n to track weight updates to check if
the learning rate is well-chosen. He suggests that the weight update should be in the order
of 10−3. If the weight update is too high, then the learning rate has to be decreased. If the
weight update is too low, then the learning rate has to be increased.</li>
<li>Typical learning rates are in [0.1, 0.00001]</li>
</ul>
","9","2","8820","16250"
"22132","<blockquote>
  <p>Why do we need for Shortcut Connections to build Residual Networks,</p>
</blockquote>

<p>Because otherwise the network would not be a residual network.</p>

<blockquote>
  <p>how [do residual connections] help to train neural networks for classification and detection?</p>
</blockquote>

<p>They add shortcuts for the gradient. This way the first few layers get updates very quick, the vanishing gradient problem is no longer a problem. So you can easily make networks with a 1000 layers. See <a href=""https://arxiv.org/abs/1512.03385"" rel=""nofollow noreferrer"">Residual Networks paper</a>.</p>

<p>There is nothing specific about classification / detection. Its about very deep networks.</p>
","4","2","8820","16250"
"22312","<p>I have three classifiers for language identification:</p>

<pre><code>A: en, de, ru, fr, ij, kl
B: en, de, ru, fr, xy
C: en, de, ru, fr, no, pq, rs
</code></pre>

<p>and I have a balanced dataset which matches the classes of A.</p>

<p>What is a fair way to compare those classifiers?</p>

<h2>My thoughts</h2>

<ul>
<li><strong>Accuracy</strong> on classes of A: Not fair, because B can't possible recognize ij, kl and will likely make mistakes due to the fact that it knows language xy. (Similar for C)</li>
<li><strong>Smallest common subset</strong>: Possible, but not so interesting as those are the ""easy"" classes.</li>
<li><strong>Precision</strong>: Given a language the classifier knows, how often does it actually recognize it?

<ul>
<li>Unfair for C, as it has more possibilities to make mistakes. Probably this could be somehow compensated? E.g. if it recognizes a class which is not in A, just the next best is taken (until the correct class is predicted or a wrong class from A is preditected)</li>
</ul></li>
</ul>

<p>There could also be a possibility to use the classifier A in combination with another classifier. The weight / importance (however that is measured) of the contribution of the other classifier is its score.</p>

<p>Are there any publications which do something similar?</p>
","1","1","8820","16250"
"22367","<p>While I agree on <a href=""https://datascience.stackexchange.com/a/22338/8820"">ncasas answer</a> in most points (+1), I beg to differ on some:</p>

<ul>
<li>Decision Trees can be used as black box models, too. In fact, I'd say in most cases they are used as black-box models. If you have 10,000 features and a tree of depth of 50 you cannot reasonably expect a human to understand it.</li>
<li>Neural Networks can be understood. There are many analyzation techniques (see <a href=""https://arxiv.org/abs/1707.09725"" rel=""noreferrer"">chapter 2.5 of my master thesis</a> for some which are aimed at improving the model). Especially occlusion analysis (Figure 2.10), Filter visualization (Figure 2.11). Also the <a href=""https://arxiv.org/abs/1602.04938v3"" rel=""noreferrer"">Why Should I Trust You?</a> paper (<a href=""http://www.shortscience.org/paper?bibtexKey=journals/corr/1602.04938"" rel=""noreferrer"">my notes</a>).</li>
</ul>

<p>Explaining the prediction of a black-box model by fancy occlusion analysis (from ""Why should I trust you?""):
<a href=""https://i.stack.imgur.com/uy6NK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/uy6NK.png"" alt=""enter image description here""></a></p>

<p>I would like to point out <a href=""https://arxiv.org/abs/1606.03490"" rel=""noreferrer"">The Mythos of Model Interpretability</a>. It formulates some ideas about interpretability in a concise way.</p>

<h2>Your question</h2>

<blockquote>
  <p>Why are Machine Learning models called black boxes?</p>
</blockquote>

<p><strong>How people use it</strong>: Because they do not model the problem in a way which allows humans to directly say what happens for any given input.</p>

<h2>Personal thoughts</h2>

<p>I don't think this notion of a ""black box model"" makes much sense. For example, think of weather forecasting. You cannot expect any human to say which weather will be predicted if he is only given the data. Yet most people would not say that physical weather models are black box models. So where is the difference? Is it only the fact that one model was generated using data and the other one was generated using insights into physics?</p>

<p>When people speak of black box models they usually say it as if it is a bad thing. But humans are black box models, too. The critical difference I see here is that the class of errors humans make is easier to predict for humans. Hence it is a training problem (adverserial examples on the NN side) and an education problem (teaching humans how NNs work).</p>

<p><strong>How the term 'black-box model' should be used</strong>: An approach which makes more sense to me is to call the problem a ""black box problem"", similar to what <a href=""https://datascience.stackexchange.com/a/22349/8820"">user144410</a> (+1) writes. Hence any model which only treats the problem as a black box - hence something you can put input in and get output out - is a black box model. Models which have insights (not only assume!) about the problem are not black-box models. The insight part is tricky. Every model makes restrictions on the possible function which it can model (yes, I know about the universal approximation problem. As long as you use a fixed-size NN it doesn't apply). I would say something is an insight into the problem if you know something about the relationship of input and output without poking the problem (without looking at data).</p>

<p>What follows from this:</p>

<ul>
<li>Neural Networks can be non-blackbox (whitebox?)</li>
<li>Logistic Regression can be a black-box model.</li>
<li>It's more about the problem and your insights about it, less about the model.</li>
</ul>
","19","2","8820","16250"
"22626","<p>See <a href=""https://arxiv.org/abs/1707.09725"" rel=""nofollow noreferrer"">my masters thesis</a>. Chapter 2.2 and 2.3 for a high-level overview over the building blocks. Appendix D describes a couple of really well known CNN architectures in detail:</p>

<ul>
<li>Lenet-5 (historic)</li>
<li>AlexNet (still in use, but not SotA)</li>
<li>VGG-16 D (Still in use, but not SotA)</li>
<li>Googlenet and the Inception-Nets (v2, v3, v4)</li>
</ul>

<p>Not described in detail, but still important</p>

<ul>
<li>Resnets</li>
<li>Densenets</li>
</ul>
","1","2","8820","16250"
"22930","<p>It is just an arbitrary choice. You have to choose one number and the order of magnitude matters, but not the exact value. Powers of two just feel natural.</p>

<p>If you don't think so: Evaluate it on a given architecture. Lower the number of neurons from a power of two to a smaller number. If the time increases, you've proven me wrong.</p>
","-1","2","8820","16250"
"23267","<h2>When should I use machine learning?</h2>

<p>Machine learning should only be used if there is no reasonable other way:</p>

<ul>
<li><strong>Automatic Speech Recognition (ASR)</strong>: We can't manually design how we recognize language by rules. It is too complex, often fuzzy and a lot depends on context. The same is true for various <strong>Computer Vision (CV)</strong> tasks.</li>
<li><strong>Natural Language Processing (NLP)</strong>: Let it be automatic summarization, translation or sentiment analysis. Additionally to the problems in ASR and CV it has a lot of special cases. While we might be able to recognize those if phrased correctly, it is in many cases absolutely hopeless to write them all down and keep them up to date.</li>
</ul>

<h2>What is the perceptron good for?</h2>

<p>It's good for learning how multilayer perceptrons (MLPs) work. I'm not sure if there is any real use case where you would want to apply a perceptron.</p>

<p>Decision stumps (if-statements) are extremely simple, just like perceptrons. If such a simple model is enough, you could probably also just set it by hand.</p>

<p>(Please let me know in the comments if you know an example where a single perceptron / decision stump with the associated learning method provides value compared to an ad-hoc decision method)</p>
","0","2","8820","16250"
"23676","<p>I would use the functional interface.</p>

<p>Something like this:</p>

<pre><code>from keras.layers import Activation, Input, Dense
from keras.models import Model
from keras.layers.merge import Concatenate

input_ = Input(shape=input_shape)

x = input_
x1 = Dense(4, x)
x2 = Dense(4, x)
x3 = Dense(4, x)
x1 = Activation('softmax')(x1)
x2 = Activation('softmax')(x2)
x3 = Activation('softmax')(x3)
x = Concatenate([x1, x2, x3])

model = Model(inputs=input_, outputs=x)
</code></pre>
","9","2","8820","16250"
"23933","<p>Assume I want to predict if I'm fit in the morning. One feature is the last time I was online. Now this feature is tricky: If I take the hour, then a classifier might have a difficult time with it because 23 is numerically closer to 20 than to 0, but actually the time 23 o'clock is closer to 0 o'clock.</p>

<p>Is there a transformation to make this more linear? Probably into multiple features? (Well, hopefully not 60 features if I do the same for minutes)</p>
","1","1","8820","16250"
"23954","<p>Sklearn has a <a href=""http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html"" rel=""noreferrer""><code>feature_importances_</code></a> attribute, but this is highly model-specific and I'm not sure how to interpret it as removing the most important feature does not necessarily decrease the models quality most.</p>

<p><strong>Is there a model-agnostic way to tell which features are important for a prediction problem?</strong></p>

<p>The only way I could see is the following:</p>

<ul>
<li>Use an ensemble of different models</li>
<li>Either start with a big set of features and remove one at a time. To find the features ""uplift"", compare the ensembles quality with the full feature set against the ensembles quality against the removed feature set.</li>
</ul>

<p>(What this can't do is to find connected features: Some features might be not exactly the same, but have a common underlying cause which is important for the prediction. Hence removing either of them doesn't change much, but removing both might change a lot. I ask <a href=""https://datascience.stackexchange.com/q/23955/8820"">another question</a> for that.)</p>
","6","1","8820","16250"
"24414","<p>I had the following idea of Reinforcement Learning:</p>

<ul>
<li>There is the RL agent and the environment. The agent can execute actions, sends them to the environment and receives an observation and a reward (a float).</li>
<li>Accounting problem: The reward might be delayed.</li>
<li>Learning algorithms: Q-learning is one example. It basically tries to learn the mapping between State x Action to reward and works <a href=""https://martin-thoma.com/probabilistische-planung/#q-learning"" rel=""nofollow noreferrer"">like this</a></li>
</ul>

<p><strong>Can Reinforcement Learning work for a setting similar to Dutch auctions?</strong></p>

<ul>
<li>The agent can make 3 actions: Increase, keep or lower the price</li>
<li>The agents goal is to sell the item for the highest price. So the agent is the auctioneer.</li>
<li>The auction item does not change. The bidders are anonymous and thus don't give a hint about their behavior.</li>
<li>This means the observations are the same, only the time to the end gets lower - independently of the action of the agent.</li>
</ul>
","3","1","8820","16250"
"24484","<p>Input: $x \in \mathbb{R}^+$</p>

<p>Expected output:</p>

<p>$f(x) = \begin{cases}x &amp;\text{for } x \leq 5\\10 -
 x &amp;\text{for } x &gt; 5\end{cases}$</p>

<p>So basically you have a neural network with one input neuron and one output neuron. Let's say the output has the identity as an activation.</p>

<p>What is the simplest network with ReLU activations that fits this function?</p>
","2","1","8820","16250"
"24513","<p>I solved the <a href=""https://gym.openai.com/envs/CartPole-v0/"" rel=""nofollow noreferrer"">CartPole-v0</a> with a CEM agent pretty easily (<a href=""https://martin-thoma.com/rl-agents/"" rel=""nofollow noreferrer"">experiments and code</a>), but I struggle to find a setup which works with DQN.</p>

<p>Do you know which parameters should be adjusted so that the mean reward is about 200 for this problem?</p>

<h2>What I tried</h2>

<ul>
<li>Adjustments in the model: Deeper / less deep, neurons per layer</li>
<li>Memory size (how many steps are stored for replay)</li>
</ul>

<h2>What I'm unsure about</h2>

<ul>
<li>How should I choose the memory? Is higher always better? - Some quick experiments indicate that there might be a sweet-spot - not too high, but also not too low. I have no idea how to figure out the region of that sweet spot.</li>
<li>Window size: Having a window size of 1 seems to work well in this case. Bigger window sizes seem to be worse. Is there any indicator when to increase the Window size?</li>
<li>How to deal with delayed rewards: Suppose the CartPole did not start upright, but down. Then it would only get rewards late. Would this be a case for increasing the window size?</li>
</ul>

<h2>My current code</h2>

<p>I use <a href=""https://github.com/matthiasplappert/keras-rl"" rel=""nofollow noreferrer"">Keras-RL</a> for the model and <a href=""https://github.com/openai/gym"" rel=""nofollow noreferrer"">OpenAI gym</a> for the environment.</p>

<p>Here is my code</p>

<pre><code>#!/usr/bin/env python

import numpy as np
import gym

from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from keras.optimizers import Adam

from rl.agents.dqn import DQNAgent
from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy
from rl.memory import EpisodeParameterMemory


def main(env_name, nb_steps):
    # Get the environment and extract the number of actions.
    env = gym.make(env_name)
    np.random.seed(123)
    env.seed(123)

    nb_actions = env.action_space.n
    input_shape = (1,) + env.observation_space.shape
    model = create_nn_model(input_shape, nb_actions)

    # Finally, we configure and compile our agent.
    memory = EpisodeParameterMemory(limit=2000, window_length=1)

    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1.,
                                  value_min=.1, value_test=.05,
                                  nb_steps=1000000)
    agent = DQNAgent(model=model, nb_actions=nb_actions, policy=policy,
                     memory=memory, nb_steps_warmup=50000,
                     gamma=.99, target_model_update=10000,
                     train_interval=4, delta_clip=1.)
    agent.compile(Adam(lr=.00025), metrics=['mae'])
    agent.fit(env, nb_steps=nb_steps, visualize=False, verbose=2)

    # After training is done, we save the best weights.
    agent.save_weights('dqn_{}_params.h5f'.format(env_name), overwrite=True)

    # Finally, evaluate the agent
    history = agent.test(env, nb_episodes=100, visualize=False)
    rewards = np.array(history.history['episode_reward'])
    print((""Test rewards (#episodes={}): mean={:&gt;5.2f}, std={:&gt;5.2f}, ""
           ""min={:&gt;5.2f}, max={:&gt;5.2f}"")
          .format(len(rewards),
                  rewards.mean(),
                  rewards.std(),
                  rewards.min(),
                  rewards.max()))


def create_nn_model(input_shape, nb_actions):
    """"""
    Create a neural network model which maps the input to actions.

    Parameters
    ----------
    input_shape : tuple of int
    nb_actoins : int

    Returns
    -------
    model : keras Model object
    """"""
    model = Sequential()
    model.add(Flatten(input_shape=input_shape))
    model.add(Dense(32))
    model.add(Activation('relu'))
    model.add(Dense(64))
    model.add(Activation('relu'))
    model.add(Dense(64))
    model.add(Activation('relu'))
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dense(nb_actions))
    model.add(Activation('linear'))
    print(model.summary())
    return model


def get_parser():
    """"""Get parser object.""""""
    from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
    parser = ArgumentParser(description=__doc__,
                            formatter_class=ArgumentDefaultsHelpFormatter)
    parser.add_argument(""--env"",
                        dest=""environment"",
                        help=""OpenAI Gym environment"",
                        metavar=""ENVIRONMENT"",
                        default=""CartPole-v0"")
    parser.add_argument(""--steps"",
                        dest=""steps"",
                        default=10000,
                        type=int,
                        help=""how many steps is the model trained?"")
    return parser


if __name__ == ""__main__"":
    args = get_parser().parse_args()
    main(args.environment, args.steps)
</code></pre>
","4","1","8820","16250"
"24514","<h2>Code</h2>

<ul>
<li><a href=""https://github.com/matthiasplappert/keras-rl"" rel=""nofollow noreferrer"">https://github.com/matthiasplappert/keras-rl</a></li>
</ul>
","0","5","8820","16250"
"24515","A framework for Reinforcement Learning with Keras","0","4","8820","16250"
"24517","<p>In order to find the network, I tried a couple of different small networks:</p>

<h3>One hidden layer with two neurons</h3>

<p>This would be the function</p>

<p>$$out = \max(a_1 x + b_1, 0) + \max(a_2x+ b_2, 0)$$</p>

<p>$$a_1 = 1, b_1 = 0, a_2=1, b_2 = -1$$</p>

<p><a href=""https://i.stack.imgur.com/ASZLU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ASZLU.png"" alt=""enter image description here""></a></p>

<p>$$a_1 = 1, b_1 = -5, a_2=-1, b_2 = 5$$</p>

<p><a href=""https://i.stack.imgur.com/GIHrT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GIHrT.png"" alt=""enter image description here""></a></p>

<h2>2 hidden layers, 3 neurons in total</h2>

<p>This is pretty close:</p>

<p><a href=""https://i.stack.imgur.com/KkLFb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KkLFb.png"" alt=""enter image description here""></a></p>

<p>I guess the pattern is right, but the values are just not quite right. Also, I'm not totally sure if only one hidden layer with 3 neurons (or more) might also work.</p>

<p>I always include biases.</p>

<h2>The solution</h2>

<pre><code>    / o (ReLU) \
IN o            o (linear)
    \ o (ReLU) /
</code></pre>

<p>It is basically the 1 hidden layer with 2 neurons, but also using the scaling (-1?) and the bias (+5?) of the output layer.</p>

<p>$$(-1) \cdot (\max(x - 5, 0) + \max(-x + 5, 0))+5$$</p>
","2","2","8820","16250"
"24728","<p>Let's say we have a neural network with one input neuron and one output neuron. The training data $(x, f(x))$ is generated by a process</p>

<p>$$f(x) = ax + \mathcal{N}(b, c)$$</p>

<p>with $a, b, c \in \mathbb{R}^+$, e.g. something like</p>

<pre><code>feature  | target
-----------------
0             0.0
0             1.0
0             1.5
0            -1.2
0            -0.9
...
</code></pre>

<p>I know that neural networks can deal pretty well with labeling errors in classification problems. Meaning if you have a large dataset and a couple of examples have the wrong label, they get basically ignored.</p>

<p>But for this kind of problem I'm not too sure. A first experiment indicates that they do smooth values.</p>

<p><strong>Are there choices in architecture / training which help the smoothing / averaging / removal of noise?</strong></p>

<h2>What I tried</h2>

<p>I created a network which can solve this kind of regression problem without noise. It gets a MSE of about <code>0.0005</code>. When I add a bit of noise to the training set only, I get an MSE of <code>0.001</code>:</p>

<pre><code>#!/usr/bin/env python

# core modules
import random

# 3rd party modules
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import train_test_split
import numpy as np


def main(add_noise=True):
    # Get data
    xs, ys = create_data_points(10000)
    x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.20)

    # Add noise to training data
    if add_noise:
        noise = np.random.normal(0, 0.1, len(x_train))
        x_train = x_train + noise

    # Create model
    model = create_model()
    model.compile(optimizer='rmsprop',
                  loss='mse',
                  metrics=['mse'])

    # Fit model to data.
    model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=1)

    # Evaluate
    y_pred = model.predict(x_test, batch_size=100).flatten()
    print(""MSE on test set:"")
    print(((y_pred - y_test)**2).sum() / len(y_test))


def create_data_points(nb_points):
    xs = []
    ys = []
    for i in range(nb_points):
        x = random.random()
        xs.append(x)
        ys.append(2 * x)
    return np.array(xs), np.array(ys)


def create_model(input_dim=1, output_dim=1):
    model = Sequential()
    model.add(Dense(200, input_dim=input_dim, activation='relu'))
    model.add(Dense(200, input_dim=input_dim, activation='relu'))
    model.add(Dense(output_dim, activation='linear'))
    return model


if __name__ == '__main__':
    main()
</code></pre>

<h2>Outliers</h2>

<p>In an earlier version of this question I wrote ""outlier"" when I meant ""label noise"". For outliers, there is:</p>

<ul>
<li><a href=""http://www.scialert.net/fulltext/?doi=jas.2005.1394.1398&amp;org=11"" rel=""noreferrer"">The Effects of Outliers Data on Neural Network Performance</a></li>
</ul>
","6","1","8820","16250"
"24760","<p>I would like to know the difference in terms of applications (e.g. which one is credit card fraud detection?) and in terms of used  techniques.</p>

<p>Example papers which define the task would be welcome.</p>
","10","1","8820","16250"
"25008","<p>You can apply a technique I described in <a href=""https://arxiv.org/pdf/1707.09725.pdf#page=62"" rel=""nofollow noreferrer"">my masters thesis (page 48ff)</a> and called Confusion Matrix Ordering (CMO):</p>
<ol>
<li>Order the columns/rows in such a way, that most errors are along the diagonal.</li>
<li>Split the confusion matrix into multiple blocks such that the single blocks can easily printed / viewed - and such that you can remove some of the blocks because there are to few data points.</li>
</ol>
<p>Nice side effect: This method also automatically clusters similar classes together. Figure 5.12 of my masters thesis shows that:</p>
<p><a href=""https://i.stack.imgur.com/dNw1T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dNw1T.png"" alt=""enter image description here"" /></a></p>
<p>You can apply confusion matrix ordering with <a href=""https://github.com/MartinThoma/clana/"" rel=""nofollow noreferrer""><code>clana</code></a></p>
","6","2","8820","16250"
"25439","<p>My question is about how to monitor RL agents in production. To make the question easier to discuss, here is a use case. Please don't focus on difficulties in implementing such an agent, but rather on how to monitor if it is still doing well:</p>

<blockquote>
  <p>Suppose Amazon would use Reinforcement Learning for optimizing the order of a search:</p>
  
  <ul>
  <li>An episode starts when a user starts a search</li>
  <li>An episode ends after a threshold or when the user buys things.</li>
  <li>The observation the agent gets is the search terms as well as 20 products for which he has to give an ordering.</li>
  <li>He gets a reward if one of those 20 things is bought.</li>
  </ul>
</blockquote>

<p>Of course, one can (should?) let the agent learn all the time as products change and probably search terms / language changes. But when do I know that the agent learned something weird / that I should stop it?</p>

<p>I can imagine the following:</p>

<ul>
<li>Case-based single examples</li>
<li>Having a ground-truth for some trivial searches + products and checking if the model does those right (in non-exploratory mode)</li>
<li>Letting the agent learn in batch-mode (e.g. update the model only once a week) and doing A/B tests for the current model / new model.</li>
<li>Measuring mean reward and setting a threshold. If the mean reward of the agent drops below the threshold, reset the agent to a past ""save"" state.</li>
</ul>

<p>Is there literature about it? Blog posts?</p>

<p>I know at least one example where RL went wrong / monitoring didn't quite work:</p>

<ul>
<li><a href=""https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist"" rel=""noreferrer"">Twitter taught Microsoft’s AI chatbot to be a racist asshole in less than a day</a> (<a href=""https://www.theguardian.com/technology/2016/mar/24/tay-microsofts-ai-chatbot-gets-a-crash-course-in-racism-from-twitter"" rel=""noreferrer"">The Guardian</a>)</li>
</ul>
","4","1","8820","16250"
"26468","<p>I would like to check JPG files if they were manipulated to change the content.</p>

<p>What I consider NOT photoshopped:</p>

<ul>
<li>Cropping</li>
<li>Rotating</li>
<li>(Scaling)</li>
<li>Image resolution</li>
<li>Automatic changes smartphones might make</li>
</ul>

<p>What I consider photoshopping:</p>

<ul>
<li>Adding a new image on top of parts of the old image</li>
<li>Changing text of a part of an image</li>
</ul>

<p><strong>How can this automatically be checked?</strong></p>

<p>(And: Are there ready-to-use libraries for it?)</p>
","10","1","8820","16250"
"26469","<p>Error Level Analysis as described Error Level Analysis, found at <a href=""https://github.com/afsalashyana/FakeImageDetection"" rel=""nofollow noreferrer"">https://github.com/afsalashyana/FakeImageDetection</a> seems to be one seems to be one way:</p>

<p>You exploit that local compression ratios might be different. And it seems to be possible to train neural networks on it.</p>

<p>I didn't find a paper which says how well this works so far</p>
","3","2","8820","16250"
"26495","<p>After having read a lot more papers and having talked to many people about machine learning topics, this is how I would define the words:</p>

<p>A <strong>class</strong> as an abstract concept which exists. Each class has properties and can have a lot of different labels. For example, the class <em>cat</em> has the properties ""feet"" (with the value 4), the property ""Genus"" with the value ""Felis"". There are many way members of the class can look like. Also many labels: cat, Katze, Felis silvestris, 🐱, 🐈.</p>

<p>A <strong>label</strong> is just a sticker you put on the concept. A name. We need a word to be able to talk about the concept.</p>

<p>I use <strong>labeling</strong> for the manual process of defining which parts of the dataset belong to which class. And I use <strong>classification</strong> for the process of the automatic classifier deciding which part of the data belongs to which class. So typically, labeling is done by a human and proceeds classification which is done by the machine.</p>
","1","2","8820","16250"
"26985","<blockquote>
  <p>You receive an email from a friend that says, ""let's have lunch next Tuesday"" and your email program detects it and asks if you want to save a new calendar entry for ""lunch on Tuesday"".</p>
</blockquote>

<p>What you describe is called <a href=""https://en.wikipedia.org/wiki/Information_extraction"" rel=""nofollow noreferrer"">Information extraction</a> and is a big field of NLP (Natural Language Processing). You are looking for <em>temporal expression identification</em>. You can have a look at the <a href=""http://nlp.stanford.edu:8080/sutime/process"" rel=""nofollow noreferrer"">Stanford Temporal Tagger: SUTime</a> to get a ""live"" demo. From what I see <a href=""https://nlp.stanford.edu/software/sutime.html"" rel=""nofollow noreferrer"">here</a> it is a regex-based rule system.</p>

<p>To give you an impression how powerful rule-based systems can be: </p>

<blockquote>
  <p>Weizenbaum’s own secretary reportedly asked Weizenbaum to leave the room so that she and ELIZA could have a real conversation. Weizenbaum was surprised by this, later writing, “I had not realized… that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.”</p>
</blockquote>

<p>Source: <a href=""https://en.wikipedia.org/wiki/ELIZA"" rel=""nofollow noreferrer"">Wikipedia: Eliza</a></p>

<h2>See also</h2>

<ul>
<li><a href=""http://www.miramontes.com/writing/add-cacm/"" rel=""nofollow noreferrer"">Apple Data Detectors</a></li>
<li><a href=""http://www.aaai.org/Papers/Symposia/Spring/2003/SS-03-07/SS03-07-005.pdf"" rel=""nofollow noreferrer"">TimeML: Robust specification of event and temporal expressions in text</a></li>
</ul>
","3","2","8820","16250"
"27495","<p>I recently wrote <a href=""https://stackoverflow.com/a/47781091/562769"">this</a> as a list of what I have seen / can think of as problem sources that make it hard to reproduce (replicate?) an experiment. I think I have seen most of them, except for hardware errors.</p>

<p>My thought was this: A typical computer vision experiment might run a training on a GPU for many hours / several days. Even if a bit-flip happens only once every $10^9$ FLOPs, that would be 11500 errors per second with an <a href=""http://www.gamestar.de/artikel/nvidia-geforce-gtx-1080-ti-schneller-als-eine-titan-x-fuer-699-us-dollar,3310460.html"" rel=""nofollow noreferrer"">Nvidia Titan GTX 1080 Ti</a>. I don't know how this error would affect later calculations (how the problem is numerically conditioned).</p>

<p>So: <strong>Are there any reports on Hardware errors affecting experiments?</strong></p>

<p>(Blog posts, journal articles, posters?)</p>
","2","1","8820","16250"
"27705","<p>Although I didn't implement it so far, I am pretty sure natural language text vs code snippets is easy:</p>

<p>For each block, you make compare the distribution of characters to ground-truth natural language text vs. code. See my paper <a href=""https://arxiv.org/pdf/1801.07779.pdf"" rel=""nofollow noreferrer"">The WiLI benchmark dataset for written language identification</a>, page 4 ""Single-Character Frequency Analysis"".</p>
","1","2","8820","16250"
"28328","<p><a href=""https://en.wikipedia.org/wiki/Multicollinearity"" rel=""noreferrer"">Multicollinearity</a> is a problem for linear regression because the results become unstable / depend too much on single elements (<a href=""http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/"" rel=""noreferrer"">source</a>).</p>

<p>(Also, the inverse of $X^TX$ doesn't exist so the standard OLS estimator does not exist ... I have no idea how, but <a href=""https://gist.github.com/MartinThoma/6e7714fd4774b5185ac7c3ace9a5b780"" rel=""noreferrer"">sklearn deals with it just fine</a>)</p>

<p>Is (perfect) multicollinearity also a problem for neural networks?</p>
","15","1","8820","16250"
"28631","<p>The curse of dimensionality means that your intuition fails at a certain number of features. See <a href=""https://martin-thoma.com/average-distance-of-points/"" rel=""nofollow noreferrer"">Average Distance of Random Points in a Unit Hypercube</a> for some examples. It does not matter how many points you have.</p>

<p>For example, at around 100 dimensions every two points randomly sampled points are really close.</p>
","1","2","8820","16250"
"28658","<p>(1) <strong>No</strong>. For example, if you have computer vision problems, then each pixel of the image is a feature. most neighboring pixels are highly correlated.</p>

<p>For example, see the following image from <a href=""https://arxiv.org/pdf/1701.08380.pdf"" rel=""nofollow noreferrer"">The HASYv2 dataset</a>:</p>

<p><a href=""https://i.stack.imgur.com/rdn19.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rdn19.jpg"" alt=""enter image description here""></a></p>
","1","2","8820","16250"
"29332","<p>Have a look at <a href=""https://arxiv.org/pdf/1701.08380.pdf"" rel=""nofollow noreferrer"">The HASYv2 dataset</a>. I tried to do as much of the exploratory work as possible to make sure that others can directly try more interesting thing with the dataset.</p>

<h2>Image format specific stuff</h2>

<ul>
<li>(Minimum / Median / Mean / Maximum) (width / height / area)</li>
<li>Image formats</li>
<li>Timestamps</li>
<li>Exiff meta data</li>
</ul>

<p>For this kind of stuff, you might want to have a look at <a href=""https://github.com/MartinThoma/edapy"" rel=""nofollow noreferrer""><code>edapy</code></a>.</p>

<h2>Image/ML specific stuff</h2>

<p>Things you can do with images:</p>

<ul>
<li>Compute the mean image

<ul>
<li>Mean image by class</li>
</ul></li>
<li><a href=""https://en.wikipedia.org/wiki/Eigenface"" rel=""nofollow noreferrer"">Eigenfaces</a> (or rather ""Eigenimages"")</li>
<li><a href=""https://en.wikipedia.org/wiki/Facial_recognition_system#Traditional"" rel=""nofollow noreferrer"">Fisher-Faces</a></li>
</ul>

<p>You can compute the correlation of pixels, e.g. <a href=""https://arxiv.org/pdf/1701.08380.pdf"" rel=""nofollow noreferrer"">Figure 3</a>:</p>

<p><a href=""https://i.stack.imgur.com/cOhqp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cOhqp.jpg"" alt=""enter image description here""></a></p>

<h2>Classification-specific stuff</h2>

<ul>
<li>Plot the distribution of classes.</li>
<li>Behaviour of standard classification algorithms (CNNs, VGG-16)</li>
<li><a href=""https://arxiv.org/pdf/1707.09725.pdf"" rel=""nofollow noreferrer"">Confusion Matrix Ordering</a> (page 48 - 52, especially Figure 5.12 and 5.13): Find similar classes</li>
</ul>

<p><a href=""https://i.stack.imgur.com/UPoVg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UPoVg.png"" alt=""enter image description here""></a></p>
","3","2","8820","16250"
"29803","<p>There are two things to consider:</p>

<ul>
<li>Sampling bias</li>
<li>Metric</li>
</ul>

<p>The sampling bias problem is that your test set is likely not the complete set of things you're interested in. So, no, you can't simply check MSE_1 &lt; MSE_2 and conclude it is always the case when it's ""just"" for your dataset the case. This is what significance tests are for. (Although this kind of reasoning is super common in machine learning and I did it myself already 🙈)</p>

<p>Then the question if the metric is the correct one for your application. Typical choices are: MSE, mean absolute error, custom cost functions</p>
","1","2","8820","16250"
"29804","<p>First, neural networks are good in dealing with""label noise"". I'm currently on mobile/vacation, so remind me to search the paper on Friday.</p>

<p>Second, the more important question is how to get a good ground truth. Without a good ground truth you can't evaluate your models, no matter how good they might be.</p>

<p>I see the ways: </p>

<p>(1) have multiple experts label the stuff. Then you can make the ground truth a probability, not a simple label. If 9 experts say it is cancer and 1 says it is not, you would label it with 90%</p>

<p>(2) wait. If you can access the patients data, it will likely be more obvious in a year (especially if it was not treated)</p>

<p>(3) other diagnostic methods: I'm not a medical doctor, but I'm pretty sure there are invasive methods to diagnose cancer which are reliable</p>
","2","2","8820","16250"
"29805","<h2>The simple solution</h2>

<p>Have one classifier which can classify all the combinations:</p>

<ul>
<li>Fresh banana</li>
<li>Rotten banana</li>
<li>Fresh mango</li>
<li>Rotten mango</li>
<li>...</li>
</ul>

<h2>The good solution</h2>

<p>Or have one network with multiple outputs. See the master Thesis of Marvin Teichmann for example. 
<a href=""https://arxiv.org/abs/1612.07695"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1612.07695</a></p>

<p>So one path of the network would be:</p>

<ul>
<li>Fresh vs Rotten</li>
</ul>

<p>The other path of the network would be </p>

<ul>
<li>Banana vs Mango vs ...</li>
</ul>

<p>The difference to the simple solution is that you can give a probability to the cases, e.g. for the ground truth ""fresh banana"" you might say 99% banana but only 42% fresh.</p>

<h2>The obvious solution</h2>

<p>This one should not be done: have multiple classifiers</p>
","2","2","8820","16250"
"29807","<blockquote>
  <p>My network does always predict the same class. What is the problem?</p>
</blockquote>

<p>I had this a couple of times. Although I'm currently too lazy to go through your code, I think I can give some general hints which might also help others who have the same symptom but probably different underlying problems.</p>

<h2>Debugging Neural Networks</h2>

<h3>Fitting one item datasets</h3>

<p>For every class i the network should be able to predict, try the following:</p>

<ol>
<li>Create a dataset of only one data point of class i.</li>
<li>Fit the network to this dataset.</li>
<li>Does the network learn to predict ""class i""?</li>
</ol>

<p>If this doesn't work, there are four possible error sources:</p>

<ol>
<li><strong>Buggy training algorithm</strong>: Try a smaller model, print a lot of values which are calculated in between and see if those match your expectation.

<ol>
<li>Dividing by 0: Add a small number to the denominator</li>
<li>Logarithm of 0 / negativ number: Like dividing by 0</li>
</ol></li>
<li><strong>Data</strong>: It is possible that your data has the wrong type. For example, it might be necessary that your data is of type <a href=""https://docs.scipy.org/doc/numpy/user/basics.types.html"" rel=""nofollow noreferrer""><code>float32</code></a> but actually is an integer.</li>
<li><strong>Model</strong>: It is also possible that you just created a model which cannot possibly predict what you want. This should be revealed when you try simpler models.</li>
<li><strong>Initialization / Optimization</strong>: Depending on the model, your initialization and your optimization algorithm might play a crucial role. For beginners who use standard stochastic gradient descent, I would say it is mainly important to initialize the weights randomly (each weight a different value). - see also: <a href=""https://stackoverflow.com/a/41565985/562769"">this question / answer</a></li>
</ol>

<h3>Learning Curve</h3>

<p>See <a href=""http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html"" rel=""nofollow noreferrer"">sklearn</a> for details.</p>

<p><a href=""https://i.stack.imgur.com/mhAUB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mhAUB.png"" alt=""enter image description here""></a></p>

<p>The idea is to start with a tiny training dataset (probably only one item). Then the model should be able to fit the data perfectly. If this works, you make a slightly larger dataset. Your training error should slightly go <strong>up</strong> at some point. This reveals your models capacity to model the data.</p>

<h3>Data analysis</h3>

<p>Check how often the other class(es) appear. If one class dominates the others (e.g. one class is 99.9% of the data), this is a problem. Look for ""outlier detection"" techniques.</p>

<h3>More</h3>

<ul>
<li><strong>Learning rate</strong>: If your network doesn't improve and get only slightly better than random chance, try reducing the learning rate. For computer vision, a learning rate of <code>0.001</code> is often used / working. This is also relevant if you use Adam as an optimizer.</li>
<li><strong>Preprocessing</strong>: Make sure you use the same preprocessing for training and testing. You might see differences in the confusion matrix (see <a href=""https://stackoverflow.com/q/42705400/562769"">this question</a>)</li>
</ul>

<h3>Common Mistakes</h3>

<p>This is inspired by <a href=""https://www.reddit.com/r/MachineLearning/comments/6xvnwo/d_my_neural_network_isnt_working_what_should_i_do/"" rel=""nofollow noreferrer"">reddit</a>:</p>

<ul>
<li>You forgot to apply preprocessing</li>
<li>Dying ReLU</li>
<li>To small / to big learning rate</li>
<li>Wrong activation function in final layer:

<ul>
<li>Your targets are not in sum one? -> Don't use softmax</li>
<li>Single elements of your targets are negative -> Don't use Softmax, ReLU, Sigmoid. tanh might be an option</li>
</ul></li>
<li>Too deep network: You fail to train. Try a simpler neural network first.</li>
</ul>
","2","2","8820","16250"
"30233","<p>For example, taking the image from <a href=""https://sebastianraschka.com/faq/docs/bagging-boosting-rf.html#boosting"" rel=""nofollow noreferrer"">sebastian raschkas post ""Machine Learning FAQ""</a>:</p>

<p><a href=""https://i.stack.imgur.com/YknB5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YknB5.png"" alt=""enter image description here""></a></p>

<p>I would expect a very similar (if not exactly the same) result for a decision tree: Given only two features, it finds the optimal feature (and value for that feature) to split the classes. Then, the decision tree does the same for each child considering only the data which arrives in the child. Of course, boosting considers all the data again, but at least in the given sample it leads to exactly the same decision boundary. <strong>Could you make an example where a decision tree would have a different decision boundary on the same training set than boosted decision stumps?</strong></p>

<p>I have the intuition that boosted decision stumps are less likely to overfit because the base classifier is so simple, but I couldn't exactly pin point why.</p>
","4","1","8820","16250"
"30377","<p>Let's say we want to predict the probability of rain. So just the binary case: rain or no rain.</p>

<p>In many cases it makes sense to have this in the [5%, 95%] interval. And for many applications this will be enough. And it is actually desired to make the classifier not too confident. Hence cross entropy (CE) is chosen:</p>

<p>$$H_{y'} (y) := - \sum_{i} y_{i}' \log (y_i)$$</p>

<p>But cross entropy practically makes it very hard for the classifier to learn to predict 0. Is there another objective function that does not behave that extreme around 0?</p>

<h2>Why it matters</h2>

<p>There might be cases where it is possible to give the prediction 0% (or at least something much closer to 0 like $10^{-6}$). Like in a desert. And there might be applications where one needs this (close to) zero predictions. For example, when you want to predict the probability that something happens at least once. If the classifier always predicts at least a 1% chance, then having rain at least once in 15 days is</p>

<p>$$1 - (1-0.05)^{15} \approx 54\%$$</p>

<p>but if the classifier can <em>practically</em> output 0.1% as well, then this is only</p>

<p>$$1 - (1-0.001)^{15} \approx 1.5\%$$</p>

<p>I could also imagine this to be important for medical tests or for videos.</p>
","4","1","8820","16250"
"30437","<p>I just had a similar issue with a dataset which contains only 115 elements and only one single feature (international airline data). The solution was to scale the data. What I missed in answers so far was the usage of a Pipeline:</p>

<pre><code>from sklearn.svm import SVR
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler

model = Pipeline([('scaler', StandardScaler()),
                  ('svr', SVR(kernel='linear'))])
</code></pre>

<p>You can train <code>model</code> like a usual classification / regression model and evaluate it the same way. Nothing changes, only the definition of the model.</p>
","1","2","8820","16250"
"30608","<ol>
<li>First try a simple model: The input layer and the output layers dimension are defined by your data / your problem definition. Then train a model without any hidden layer.</li>
<li>See how good it performs. Is it good enough? If yes, you're done. If no, continue</li>
<li>Add a hidden layer of reasonable size or adjust a hidden layers size. Go to step (2).</li>
</ol>

<p>The ""reasonable"" size part might be difficult. As a guidance: If you have only a single node, it is certainly too small for a 1000 class problem. It might be big enough for a 2 - 3 class problem. I would usually suggest to keep the size of the features per layer roughly constant or at most reduce it by 1/10 or triple it. But that is only gut feeling.</p>

<p>The reason for my preference for simple models is <a href=""https://en.wikipedia.org/wiki/Occam%27s_razor"" rel=""nofollow noreferrer"">Occam's razor</a>, the fact that they are often faster, easier to analyze and to manually improve.</p>

<p>For more information about topology learning and rules how to design neural networks, see:</p>

<blockquote>
  <p>Thoma, Martin. ""<a href=""https://arxiv.org/pdf/1707.09725.pdf"" rel=""nofollow noreferrer"">Analysis and Optimization of Convolutional Neural Network Architectures</a>."" arXiv preprint arXiv:1707.09725 (2017).</p>
</blockquote>

<p>Especially chapter 2.5 and chapter 3.</p>
","3","2","8820","16250"
"30859","<p>Suppose I know that I want to use a ResNet-101 architecture for my specific problem. There are ReseNet-101 models trained on ImageNet.</p>

<p>Is there any disadvantage of using those pre-trained models and just resetting the last (few) layers to match the specific number of classes or should that be the default option?</p>

<p>Please don't simply post your gut feeling... I have a gut feeling as well, but I want to know.</p>
","7","1","8820","16250"
"30975","<p>Networks like VGGNet have huge numbers of parameters (see <a href=""https://arxiv.org/pdf/1707.09725.pdf"" rel=""nofollow noreferrer"">Appendix D for details</a>, but it's something like 135 million). Training such a big network takes a lot of data and a lot of time. There is ImageNet, which has 1000 classes and many thousand images. For ImageNet, some people already trained VGGNet and provided the parameters.</p>

<p>One way of finetuning is to replace only the last layer (about 4 million parameters) and add a new layer which has the number of classes you need. Then you can freeze all other weights and train on your data. Hence you will have <strong>lower training time</strong> and you will likely get <strong>better results</strong> (at least on similar datasets ... for datasets which look very different, <a href=""https://datascience.stackexchange.com/q/30859/8820"">there seems not to be documented conclusive evidence</a>)</p>
","2","2","8820","16250"
"31212","<p>Let's make an example: I want to build a neural network which should predict if a person is obese. It gets the following features:</p>

<ul>
<li>Weight</li>
<li>Height</li>
<li>Number of hours the person makes sports</li>
<li>superspecial health index</li>
<li>foobar</li>
</ul>

<p>The output of the model is a probability that the given person is obese. The higher, the more likely.</p>

<p>Now I know, if everything else stays the same, that a higher weight should always be the same or a higher output probability. But I don't know the exact relationship, only that it changes monothonically.</p>

<p>How can I include this knowledge into the network and force it to show the desired behaviour?</p>
","3","1","8820","16250"
"31846","<p>It's not important to adjust to the opponent, as Go is only about winning or losing. There is no bigger reward the faster / the more obvious it wins.</p>

<p>Or to put it different: only the current board situation is important in a min-Max setting (although the value approximation of a state admittedly depends a bit on the opponent)</p>
","1","2","8820","16250"
"33654","<p>MinMax scaler is not the only way to scale. There is also the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"" rel=""nofollow noreferrer"">StandardScaler</a> which basically does:</p>

<p>$$
\begin{align}
x &amp;\sim \mathcal{N}(\mu, \sigma)\\
x' :&amp;= \frac{x-\mu}{\sigma}
\end{align}$$</p>

<p>This leads to $x' \sim \mathcal{N}(0, 1)$.</p>
","0","2","8820","16250"
"33747","<p>I use the two terms as follows:</p>

<p>A <strong>prediction model</strong> gets features (which can be a time series) as input and gives a fixed-length output (might be multiple values, but ""atomic"" in some sense)</p>

<p>Examples:</p>

<ul>
<li>Dog vs Cat</li>
<li>Spam vs No Spam</li>
<li>Weather for tomorrow</li>
<li>Sales of next month</li>
</ul>

<p>A <strong>forecasting model</strong> gets a time series as input and outputs a single time series.</p>

<p>Examples:</p>

<ul>
<li>Amount of airline passengers: You can predict the value for the next month, but a forecast can be over an arbitrary time frame (with deteriating quality, of course)</li>
<li>Weather</li>
<li>Sales</li>
</ul>

<p>I've never seen textbook definitions of those terms. Do you know any source that compares the two terms?</p>

<p>Bonus points if you give me a good German translation for both terms. By now, I call both ""Vorhersage"" - I have the impression there is no other German word for it.</p>
","0","1","8820","16250"
"35575","<p>I doubt this is possible in general without knowing anything.</p>

<p>You can, of course, find outliers in the data and thus try to infer where something got mixed up. But this will likely be a lot more work than just fixing the CSV export.</p>
","0","2","8820","16250"
"36568","<p>Tom M. Mitchell defines machine learning as </p>

<blockquote>
  <p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.</p>
</blockquote>

<p>For some algorithms that are usually counted towards the field of machine learning, I heavily doubt that they are learning algorithms according to the definition. Interestingly, all of them are part of Tom Mitchell's book ""Machine Learning"".</p>

<h2>Deduction</h2>

<p>The ""symbolist tribe"" uses inverse deduction according to Pedro Domingos (see <a href=""https://medium.com/42ai/the-5-tribes-of-the-ml-world-670ebce96b4c"" rel=""nofollow noreferrer"">The 5 tribes of ML</a>). While I'm not too sure what inverse deduction is, the idea of having a set of true statements and inference rules like <a href=""https://en.wikipedia.org/wiki/Modus_ponens"" rel=""nofollow noreferrer"">Modus Ponens</a> to create more/other true statements is clear.</p>

<p>Comming to the definition: What is experience here? How is performance measured? What does a typical task look like?</p>

<h2>Genetic programming</h2>

<p>From Wikipedia:</p>

<blockquote>
  <p>Genetic programming (GP) is a technique whereby computer programs are encoded as a set of genes that are then modified (evolved) using an evolutionary algorithm (often a genetic algorithm, ""GA"").</p>
</blockquote>

<p>What is experience in this case? Maybe ""iterations"" / ""epochs""? (I'm not sure about the terminology)</p>

<p>Tasks: <a href=""https://stackoverflow.com/q/355460/562769"">What are the typical use cases of Genetic Programming?</a></p>

<h2>PCA</h2>

<p>PCA and some other dimensionality reduction algorithms are in some way ""fixed"". The algorithm does not change with more data. It does not have parameters. Thinking about it, one might argue that it estimates the projection in a better way with more data.</p>

<p>But then: What is better? What is the performance measure P here?</p>

<h2>Clustering</h2>

<p>I don't see a clear performance measure for Clustering (e.g. <a href=""https://en.wikipedia.org/wiki/K-means_clustering"" rel=""nofollow noreferrer"">k-means</a>). Also, the only kind of experience I can see is the number of samples.</p>
","2","1","8820","16250"
"40966","<p>If you add a new feature to the perceptron, the perceptron actually gets one more parameter. So in some sense it is not the same model anymore. But lets ignore that.</p>

<p>Correlation is the key problem. Assume the new feature is a random number which happens to have positive correlation with the desired output for the training dataset, but negative correlation with the test dataset. Then you would expect this new feature to influence the test error negatively, right? At least I would.</p>

<p>But your question was about the <strong>training error</strong>. In that case I actually don't see a reason why it should be the case. I'd suggest to try it. </p>

<p>And anyways, ask the person/organisation, where you found this question. (And post the answer back)</p>
","1","2","8820","16250"
"41728","<p>First, you should get clear about your non-functional requirements (speed, memory consumption). Make sure you have a clear idea who is going to maintain the software and what the effort will be if you make some changes. Higher accuracy might not be worth it.</p>

<p>The simplest way to combine n models is averaging their predictions. This technique is often used on Kaggle. See chapter 2.6 of <a href=""https://arxiv.org/pdf/1707.09725"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1707.09725</a></p>

<p>Did you try a simple MLP with tf-idf features? How well does that perform? Usually it executes way faster than SVMs, if you have many classes.</p>
","0","2","8820","16250"
"41839","<p>Very likely, there is too much noise in the data. Meaning there are hidden pieces of information that are dominating the decision for acceptance or not, which you don't find in the data. For example, if the client finds the consultant sympathic. Or if the client had a good day when deciding about the consultant.</p>

<p>I doubt that the rule system is overly complicated, so my advise is to stay with it.</p>

<p>If you want to get started with machine learning, look for MNIST examples (a handwritten digits dataset) and I recommend to use Keras with Tensorflow as a Backend.</p>

<p>(by the way: I'm a machine learning ;-) )</p>
","0","2","8820","16250"
"41886","<blockquote>
  <p>my aim is to figure out the optimal multi class classification model which fits my dataset</p>
</blockquote>

<p>That is impossible for two reasons: (1) there are many reasonable ways to define ""optimal"", (2) there are infinitely many models and currently there is no analytical way to get a better one (not even speaking about the best one)</p>

<p>Have a look at <a href=""https://arxiv.org/pdf/1707.09725"" rel=""nofollow noreferrer"">my master thesis, chapter 2.5</a> for ideas how to continue.</p>
","0","2","8820","16250"
"42711","<blockquote>
  <p>Can date data be used as a feature?</p>
</blockquote>

<p>Yes.</p>

<blockquote>
  <p>If so how do I handle them ?</p>
</blockquote>

<p>Think about your problem. Why should the date be a reasonable indicator for the success of a startup? Answering this question tells you also in which way you need to transform it.</p>

<p>Most often, when I use some date information for models, I do the following:</p>

<ul>
<li>Day of the week: integer/one-hot encoding for Monday, Tuesday, ..., Sunday</li>
<li>Month: integer/one-hot encoding for January, February, ..., December</li>
<li>Hour of the day: integer/one-hot encoding for 0, ..., 23</li>
<li>Seconds/minutes/hours/days since XY: Usually normalized or at least scaled in some way</li>
</ul>
","1","2","8820","16250"
"42724","<p>Yes, accuracy measured on the training set over epochs is not monotonically decreasing.</p>

<p>Possible reasons:</p>

<ol>
<li>Software bug</li>
<li>Nan due to division by zero / log (0) / overflow</li>
<li>Too big learning rate</li>
</ol>
","1","2","8820","16250"
"42746","<p>Adding to J.C. answer, please note that you don't have to stick with one-hot encoding. For your hot-mild-cold-freezing a target could also be 0,0.3,0.5,0.2.</p>
","0","2","8820","16250"
"42760","<p>In regression problems, you can use various different metrics to check how well your model is doing:</p>

<ul>
<li>Mean Absolute Deviation (MAD): In <span class=""math-container"">$[0, \infty)$</span>, the smaller the better</li>
<li>Root Mean Squared Error (RMSE): In <span class=""math-container"">$[0, \infty)$</span>, the smaller the better</li>
<li>Median Absolute Error (MAE): In <span class=""math-container"">$[0, \infty)$</span>, the smaller the better</li>
<li>Mean Squared Log Error (MSLE): In <span class=""math-container"">$[0, \infty)$</span>, the smaller the better</li>
<li>R², coefficient of determination: In <span class=""math-container"">$(-\infty, 1]$</span> <a href=""http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit"" rel=""noreferrer"">not necessarily the bigger the better</a></li>
</ul>

<p>Are there any strong reasons not to use one or the other?</p>
","10","1","8820","16250"
"42761","<p>I have very rough ideas for some:</p>

<ul>
<li>MAD if a deviation of 2 is ""double as bad"" than having a deviation of 1.</li>
<li>RMSE if the value deteriorates more quickly - punishes outliers hard! (can be good or bad)</li>
<li>MAE if I'm not interested in complete outliers, but only in ""typical"" cases (as I usually fence the outputs to a reasonable range, this is almost the same as MAD)</li>
</ul>

<p>For MSLE and R², I have no idea when it is better suited than the others.</p>
","0","2","8820","16250"
"43121","<p>I'm currently analyzing a WhatsApp chat history. One thing I'm interested in is the time when the two people communicated. I thought this is a perfect use case for a radar chart (aka spider chart, star chart). So here is one example:</p>

<p><a href=""https://i.stack.imgur.com/ry1L1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ry1L1.png"" alt=""enter image description here""></a></p>

<p>I find this super hard to read. In contrast, have a look at the following bar chart with exactly the same information:</p>

<p><a href=""https://i.stack.imgur.com/u39c6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u39c6.png"" alt=""enter image description here""></a></p>

<p>Here it is way easier for me to see patterns:</p>

<ul>
<li>0 - 5: No activity (sleeping)</li>
<li>6 - 9: A morning peak (messages after waking up)</li>
<li>10 - 16: Little activity (work times)</li>
<li>17 - 19: Increased activity (work is done)</li>
<li>20 - 21: High activity</li>
<li>22 - 23: Sinking activity (going to bed)</li>
</ul>

<h2>Question</h2>

<p>What are typical use-cases for radar charts? Are there specific requirements on the data? When are they better than bar charts? How should I set the ticks for radar charts?</p>

<p>Before people starting to close-vote: I could imagine very well that there are similar studies like for <a href=""https://bids.github.io/colormap/"" rel=""nofollow noreferrer"">color maps</a> - how many errors are done in analysis? How fast can people pick up main insights?</p>

<h2>Example code</h2>

<p>In case you want to improve the existing visualizations:</p>

<pre><code># core modules
from math import pi

# 3rd party modules
import matplotlib.pyplot as plt
import pandas as pd


def main():
    df = pd.DataFrame({'date': [209, 13, 1, 2, 1, 25, 809, 3571, 1952, 1448, 942, 1007, 1531, 1132, 981, 864, 975, 2502, 2786, 2717, 3985, 4991, 2872, 761]},
                      index=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])
    create_bar_chart(df)
    create_radar_chart(df)


def create_bar_chart(df, output_path='bar_chart.png'):
    df.plot(kind='bar')
    plt.savefig(output_path)


def create_radar_chart(df, output_path='radar_chart.png'):
    """"""
    Create a radar chart.

    Parameters
    ----------
    df : pandas.DataFrame
        Has a column 'date'
    """"""
    values = df['date'].tolist()

    df = df.T.reset_index(drop=True)
    df.insert(0, 'group', 'A')

    # number of variable
    categories = list(df)[1:]
    N = len(categories)

    # What will be the angle of each axis in the plot?
    # (we divide the plot / number of variable)
    angles = [n / float(N) * 2 * pi for n in range(N)]
    angles += angles[:1]

    # Initialise the spider plot
    ax = plt.subplot(111, polar=True)

    # Draw one axe per variable + add labels labels yet
    plt.xticks(angles[:-1], categories, color='grey', size=8)

    # We are going to plot the first line of the data frame.
    # But we need to repeat the first value to close the circular graph:
    values = df.loc[0].drop('group').values.flatten().tolist()
    values += values[:1]
    values

    # Plot data
    ax.plot(angles, values, linewidth=1, linestyle='solid')

    # Fill area
    ax.fill(angles, values, 'b', alpha=0.1)
    plt.savefig(output_path)


if __name__ == '__main__':
    main()
</code></pre>
","0","1","8820","16250"
"43187","<p>Yes, the test data might contain something important that you didn't learn from the test set. No, you're not allowed to use the test set for learning.</p>

<p>The purpose of the test set is to understand how the system would behave on new data not seen during training. Fitting your scaler in the test set would defeat that purpose.</p>

<blockquote>
  <p>1) Generate the mean and standard deviation stats on the 80% of training data and then apply the same mean/standard deviation to standardize the validation data.</p>
</blockquote>

<p>That's what you should do.</p>

<blockquote>
  <p>Or 2) Standardize the training data, and then standardize the validation data, i.e mean and SD is derived from the 80% for the training data, and then mean/SD is derived separately on the 20% of validation data?</p>
</blockquote>

<p>This is valid, if your production system makes it like that. So if you get data in batches and normalize the batches it might be fine as well.</p>
","1","2","8820","16250"
"43233","<blockquote>
  <p>in a computer it's impossible to have values <span class=""math-container"">$\in \mathbb{R}$</span></p>
</blockquote>

<p>Although this is technically right, practically it is practically irrelevant.</p>

<p>First, there are three important facts:</p>

<ul>
<li>You can represent any number <span class=""math-container"">$q \in \mathbb{Q}$</span> as a fraction <span class=""math-container"">$q = \frac{n}{d}$</span> where <span class=""math-container"">$n$</span> and <span class=""math-container"">$d$</span> are integers</li>
<li>There are arbitrary big integers (<a href=""https://rushter.com/blog/python-integer-implementation/"" rel=""nofollow noreferrer"">Explanation</a>, <a href=""https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic"" rel=""nofollow noreferrer"">Wikipedia</a>). They are only bound by your memory. Python uses them, for example.</li>
<li><span class=""math-container"">$\mathbb{Q}$</span> is dense in <span class=""math-container"">$\mathbb{R}$</span>: This means that for any number <span class=""math-container"">$r \in \mathbb{R}$</span> and any distance <span class=""math-container"">$\varepsilon &gt; 0$</span> there is a number <span class=""math-container"">$q \in \mathbb{Q}$</span> such that <span class=""math-container"">$|r-q| &lt; \varepsilon$</span>. So for the irrational numbers there is a rational one arbitary close by.</li>
</ul>
","0","2","8820","16250"
"43332","<p>I vaguely remember that there was a study / blog post which made a strong point against 3D bar charts. Do you have a source at hand which compares the two - 2D bar charts and 3D bar charts?</p>
","2","1","8820","16250"
"43334","<p>A relevant source seems to be</p>

<blockquote>
  <p>A Psychophysical analysis of chart readability, 2001</p>
</blockquote>

<p>Sadly, I don't have access to it.</p>

<h2>Anecdotes</h2>

<p>If you have an area chart, giving it a 3D-effect and rotating slightly can help readability. But I don't remember a context where you could not have used a line chart in the first place.</p>

<h2>Bad sources</h2>

<ul>
<li>PRO: Looks good (<a href=""https://www.targetdashboard.com/blog/75/3D-or-not-3D-when-to-add-a-3rd-dimension-to-your-KPI-charts.aspx"" rel=""nofollow noreferrer"">targetdashboard.com</a>)</li>
<li>CON: Values are harder to read from the axes only - you need to add values to
the bars themselves (targetdashboard.com)</li>
</ul>
","1","2","8820","16250"
"43369","<p>You might want to have a look at the hierarchical classification idea I described in <a href=""https://arxiv.org/pdf/1707.09725.pdf"" rel=""nofollow noreferrer"">my masters thesis</a>.</p>

<p>In short:</p>

<ul>
<li>If I had a problem where I needed to distinguish 42 breeds of dogs and 30 breeds of cats, I would very likely just create one classifier for the <span class=""math-container"">$42+30=72$</span> classes. Making a two-step approach (first cat vs dog, then cat-breed / dog-breed) seems not to give better results and for sure is more complicated</li>
<li>Analysis of errors becomes simpler the less classes you have</li>
<li>A single accuracy score for more than a handfull of classes / skewed data is not meaningful. For example, if you have two models with both 90% accuracy, one might be way better than the other one.</li>
</ul>
","1","2","8820","16250"
"43686","<p>From <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn-tree-decisiontreeclassifier"" rel=""nofollow noreferrer""><code>sklearn.tree.DecisionTreeClassifier</code> help</a>:</p>

<blockquote>
  <p>The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed.</p>
</blockquote>

<p>Also, you might want to have a look at my <a href=""https://martin-thoma.com/feature-importance/"" rel=""nofollow noreferrer"">critique on feature importance</a>.</p>
","4","2","8820","16250"
"43705","<p>I agree with Skiddles, but would like to add an extreme example: let the training examples be ordered by class. Hence you only have training samples of one class in the data.</p>

<p>You could make a two class example. Assuming one hot encoding of the targets, gradient descent would push weights to make one output 1 and all others 0. There is no balancing between classes for that single step.</p>

<p>To convince yourself that sorting is a bad idea, just train MNIST on a sorted dataset and an unsorted one (I'll post training curves in roughly 15 hours, after work)</p>
","0","2","8820","16250"
"44588","<p>From <a href=""https://en.wikipedia.org/wiki/Self-organizing_map"" rel=""nofollow noreferrer"">Wikipedia</a>:</p>

<blockquote>
  <p>A self-organizing map (SOM) or self-organizing feature map (SOFM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction.</p>
</blockquote>

<p>So you only have the original data itself; no additional data (like labels in a supervised setting). If you are also say the result has to have two dimensions, you basically look at functions</p>

<p><span class=""math-container"">$$f: X \rightarrow \mathbb{R}^2$$</span></p>

<p>where <span class=""math-container"">$X \subsetneq \mathbb{R}^n$</span> in most cases. You already mentioned quantization error.</p>

<p>Up to my knowledge <strong>there is nothing better measure</strong> which does not include getting more knowledge about the data itself by human inspection / using other datasets.</p>

<p>With human inspection you can, of course, tell for a given dataset and a given human if one mapping seems to make more sense.</p>

<p>You might also consider other dimensionality reduction techniques:</p>

<ul>
<li><a href=""http://www.cs.toronto.edu/~hinton/absps/tsne.pdf"" rel=""nofollow noreferrer"">t-SNE</a></li>
<li>Auto-Encoders</li>
</ul>
","0","2","8820","16250"
"44892","<p>The question of the measured test error of a classification model is reliable, hence if the test error on unknown set <span class=""math-container"">$T_1$</span> is the same as on <span class=""math-container"">$T_2$</span> is hard two answer. It depends on the following factors:</p>

<ul>
<li>How many digits of the error are reported?</li>
<li>How many samples have <span class=""math-container"">$T_1$</span> and <span class=""math-container"">$T_2$</span>? The more digits are reported, the more samples you need. As a rule of thumb, make sure that any change in the reported error means at least 3 samples have changed. So if you use accuracy and report two decimal places (e.g. 12.34%), then 0.01% must be bigger than 3 => <span class=""math-container"">$3 &lt; \frac{0.01}{100} \cdot |T_1| \Leftrightarrow 30000 &lt; |T_1|$</span></li>
<li>The distribution must be similar. The simplest part is the distribution of classes. The more difficult part is how the features look like.</li>
</ul>

<p>For other forms of error analysis, you might want to look into my Master's thesis <a href=""https://arxiv.org/pdf/1707.09725"" rel=""nofollow noreferrer"">Analysis and Optimization of Convolutional Neural Network Architectures</a></p>
","1","2","8820","16250"
"45253","<blockquote>
  <p>When to use cosine similarity over Euclidean similarity</p>
</blockquote>

<p>Cosine similarity looks at the angle between two vectors, euclidian similarity at the distance between two points.</p>

<p>Let's say you are in an e-commerce setting and you want to compare users for product recommendations:</p>

<ul>
<li>User 1 bought 1x eggs, 1x flour and 1x sugar.</li>
<li>User 2 bought 100x eggs, 100x flour and 100x sugar</li>
<li>User 3 bought 1x eggs, 1x Vodka and 1x Red Bull</li>
</ul>

<p>By cosine similarity, user 1 and user 2 are more similar. By euclidean similarity, user 3 is more similar to user 1.</p>

<h2>Questions in the text</h2>

<p>I don't understand the first part.</p>

<blockquote>
  <p>Cosine similarity is specialized in handling scale/length effects. For case 1, context length is fixed -- 4 words, there's no scale effects. In terms of case 2, the term frequency matters, a word appears once is different from a word appears twice, we cannot apply cosine.</p>
</blockquote>

<p>This goes in the right direction, but is not completely true. For example:</p>

<p><span class=""math-container"">$$
\cos \left (\begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}2\\1\end{pmatrix} \right) = \cos \left (\begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}4\\2\end{pmatrix} \right) \neq \cos \left (\begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}5\\2\end{pmatrix} \right)
$$</span></p>

<p>With cosine similarity, the following is true:</p>

<p><span class=""math-container"">$$
\cos \left (\begin{pmatrix}a\\b\end{pmatrix}, \cdot \begin{pmatrix}c\\d\end{pmatrix} \right) = \cos \left (\begin{pmatrix}a\\b\end{pmatrix}, n \cdot \begin{pmatrix}c\\d\end{pmatrix} \right) \text{ with } n \in \mathbb{N}
$$</span></p>

<p>So frequencies are only ignored, if all features are multiplied with the same constant.</p>

<h2>Curse of Dimensionality</h2>

<p>When you look at the table of <a href=""https://martin-thoma.com/curse-of-dimensionality/"" rel=""noreferrer"">my blog post</a>, you can see:</p>

<ul>
<li>The more dimensions I have, the closer the average distance and the maximum distance between randomly placed points become.</li>
<li>Similarly, the average angle between uniformly randomly placed points becomes 90°.</li>
</ul>

<p>So both measures suffer from high dimensionality. More about this: <a href=""https://stats.stackexchange.com/q/341535/25741"">Curse of dimensionality - does cosine similarity work better and if so, why?</a>. A key point:</p>

<ul>
<li>Cosine is essentially the same as Euclidean on normalized data.</li>
</ul>

<h2>Alternatives</h2>

<p>You might be interested in metric learning. The principle is described/used in <em>FaceNet: A Unified Embedding for Face Recognition and Clustering</em> (<a href=""https://www.shortscience.org/paper?bibtexKey=journals/corr/1503.03832#martinthoma"" rel=""noreferrer"">my summary</a>). Instead of taking one of the well-defined and simple metrics. You can learn a metric for the problem domain.</p>
","26","2","8820","16250"
"45399","<p>Have a look at where the reshaping happens. Just before that, you can insert a global average pooling layer in. This way you can handle any size.</p>

<p>However, I recommend cropping and scamming. Create multiple crops if necessary and average the results. That is likely still faster than using a bigger image.</p>

<h2>How to use Xception</h2>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-
""""""See https://martin-thoma.com/image-classification/ for details.""""""
from __future__ import print_function

import numpy as np
import json
import os
import time

from keras import backend as K
from keras.preprocessing import image
from keras.applications.xception import Xception
from keras.utils.data_utils import get_file

CLASS_INDEX = None
CLASS_INDEX_PATH = ('https://s3.amazonaws.com/deep-learning-models/'
                    'image-models/imagenet_class_index.json')


def preprocess_input(x, dim_ordering='default'):
    """"""
    Standard preprocessing of image data.

    1. Make sure the order of the channels is correct (RGB, BGR, depending on
       the backend)
    2. Mean subtraction by channel.

    Parameters
    ----------
    x : numpy array
        The image
    dim_ordering : string, optional (default: 'default')
        Either 'th' for Theano or 'tf' for Tensorflow

    Returns
    -------
    numpy array
        The preprocessed image
    """"""
    if dim_ordering == 'default':
        dim_ordering = K.image_dim_ordering()
    assert dim_ordering in {'tf', 'th'}

    if dim_ordering == 'th':
        x[:, 0, :, :] -= 103.939
        x[:, 1, :, :] -= 116.779
        x[:, 2, :, :] -= 123.68
        # 'RGB'-&gt;'BGR'
        x = x[:, ::-1, :, :]
    else:
        x[:, :, :, 0] -= 103.939
        x[:, :, :, 1] -= 116.779
        x[:, :, :, 2] -= 123.68
        # 'RGB'-&gt;'BGR'
        x = x[:, :, :, ::-1]
    return x


def decode_predictions(preds, top=5):
    """"""
    Decode the predictionso of the ImageNet trained network.

    Parameters
    ----------
    preds : numpy array
    top : int
        How many predictions to return

    Returns
    -------
    list of tuples
        e.g. (u'n02206856', u'bee', 0.71072823) for the WordNet identifier,
        the class name and the probability.
    """"""
    global CLASS_INDEX
    if len(preds.shape) != 2 or preds.shape[1] != 1000:
        raise ValueError('`decode_predictions` expects '
                         'a batch of predictions '
                         '(i.e. a 2D array of shape (samples, 1000)). '
                         'Found array with shape: ' + str(preds.shape))
    if CLASS_INDEX is None:
        fpath = get_file('imagenet_class_index.json',
                         CLASS_INDEX_PATH,
                         cache_subdir='models')
        CLASS_INDEX = json.load(open(fpath))
    results = []
    for pred in preds:
        top_indices = pred.argsort()[-top:][::-1]
        result = [tuple(CLASS_INDEX[str(i)]) + (pred[i],) for i in top_indices]
        results.append(result)
    return results


def is_valid_file(parser, arg):
    """"""
    Check if arg is a valid file that already exists on the file system.

    Parameters
    ----------
    parser : argparse object
    arg : str

    Returns
    -------
    arg
    """"""
    arg = os.path.abspath(arg)
    if not os.path.exists(arg):
        parser.error(""The file %s does not exist!"" % arg)
    else:
        return arg


def get_parser():
    """"""Get parser object.""""""
    from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
    parser = ArgumentParser(description=__doc__,
                            formatter_class=ArgumentDefaultsHelpFormatter)
    parser.add_argument(""-f"", ""--file"",
                        dest=""filename"",
                        type=lambda x: is_valid_file(parser, x),
                        help=""Classify image"",
                        metavar=""IMAGE"",
                        required=True)
    return parser


if __name__ == ""__main__"":
    args = get_parser().parse_args()

    # Load model
    model = Xception(include_top=True, weights='imagenet')

    img_path = args.filename
    img = image.load_img(img_path, target_size=(224, 224))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    print('Input image shape:', x.shape)
    t0 = time.time()
    preds = model.predict(x)
    t1 = time.time()
    print(""Prediction time: {:0.3f}s"".format(t1 - t0))
    for wordnet_id, class_name, prob in decode_predictions(preds)[0]:
        print(""{wid}\t{prob:&gt;6}%\t{name}"".format(wid=wordnet_id,
                                                 name=class_name,
                                                 prob=""%0.2f"" % (prob * 100)))
</code></pre>

<h2>Why it works with any size</h2>

<p>Look at the <a href=""https://gist.github.com/MartinThoma/6217d16264f3f9d627bfe96d591653e3"" rel=""nofollow noreferrer""><code>model.summary()</code> of Xception</a>, especially the output shape. Notice the global average pooling layer? Before that, the shape is determined by the input. Meaning until that point, it can be anything.</p>

<blockquote>
  <p>Global pooling is another type of transition layer. It applies pooling over the complete feature map size to shrink the input to a constant 1 × 1 feature map and hence allows one network to have different input sizes.</p>
</blockquote>

<p>Source: <a href=""https://arxiv.org/pdf/1707.09725.pdf"" rel=""nofollow noreferrer"">Me: Analysis and Optimization of Convolutional Neural Network Architectures</a></p>
","3","2","8820","16250"
"45479","<p>One single simple example: <a href=""https://en.wikipedia.org/wiki/Vanishing_gradient_problem"" rel=""nofollow noreferrer"">Vanishing Gradient problem</a> in Deep Learning. It's not mainly a finite precision problem, but that is also part of the problem.</p>
","0","2","8820","16250"
"45482","<blockquote>
  <p>Would my code be faster if I rewrite it with matrices?</p>
</blockquote>

<p>Without seeing the code it's impossible to know, but very likely. Also, I would never model single neurons. Too much overhead without any use. Model layers instead.</p>

<blockquote>
  <p>how can I get my performance to be comaparable to that of sklearn</p>
</blockquote>

<p>Sklearn is open source. Read the code: <a href=""https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/neural_network/multilayer_perceptron.py#L682"" rel=""nofollow noreferrer"">https://github.com/scikit-learn/scikit-learn/blob/7389dba/sklearn/neural_network/multilayer_perceptron.py#L682</a></p>
","2","2","8820","16250"
"45854","<p>I would like to extract all date information from a given document. Essentially, I guess this can be done with a lot of regexes:</p>

<ul>
<li>2019-02-20</li>
<li>20.02.2019 (""German format"")</li>
<li>02/2019 (""February 2019"")</li>
<li>""tomorrow"" (datetime.timedelta(days=1))</li>
<li>""yesterday"" (datetime.timedelta(days=-1))</li>
</ul>

<p>Is there a Python package / library which offers this already or do i have to write all of those regexes/logic myself?</p>

<p>I'm interested in Information Extraction from German and English texts. Mainly German, though.</p>

<h2>Constraints</h2>

<p>I don't have the complete dataset by now, but I have some idea about it:</p>

<ul>
<li>10 years of interesting dates which could be in the dataset</li>
<li>I guess the interesting date types are: (1) 28.02.2019, (2) relative ones like ""3 days ago"" (3) 28/02/2019, (4) 02/28/2019 (5) 2019-02-28 (6) 2019/02/28 (7) 2019/28/02 (8) 28.2.2019 (9) 28.2 (10) ... -- all of which could have spaces in various places</li>
<li>I have millions of documents. Every document has around 20 sentences, I guess.</li>
<li>Most of the data is in German</li>
</ul>
","7","1","8820","16250"
"48411","<p>I'm currently reading</p>

<blockquote>
  <p>Hu, Koren, Volinsky: <a href=""http://yifanhu.net/PUB/cf.pdf"" rel=""nofollow noreferrer"">Collaborative Filtering for Implicit Feedback Datasets</a></p>
</blockquote>

<p>One thing that confuses me is the ""expected percentile ranking"", an function the authors define to evaluate the goodness of their recommendations. They define it in the Evaluation methodology on page 6 as:</p>

<p><span class=""math-container"">$$\overline{\text{rank}} = \frac{\sum_{u,i} r^t_{ui} \text{rank}_{ui}}{\sum_{u,i} r^t_{ui}}$$</span></p>

<p>where <span class=""math-container"">$u$</span> is a user, <span class=""math-container"">$i$</span> is an item (e.g. a TV show), <span class=""math-container"">$r_{ui} \in [0, \infty)$</span> is the amount how much user <span class=""math-container"">$u$</span> did watch show <span class=""math-container"">$i$</span>. <span class=""math-container"">$\text{rank}_{ui} \in [0, 1]$</span> is the percentage rank of item <span class=""math-container"">$i$</span> for user <span class=""math-container"">$u$</span>. For example, it is 0 if for user <span class=""math-container"">$u$</span> the item <span class=""math-container"">$i$</span> has the highest <span class=""math-container"">$r$</span> value and 1 if the item <span class=""math-container"">$i$</span> for user <span class=""math-container"">$u$</span> has the lowest <span class=""math-container"">$r$</span> value.</p>

<p>I'm not super sure if I understood it correctly.</p>

<p>The authors write that lower values of <span class=""math-container"">$\overline{\text{rank}}$</span> are more desirable and for random predictions would lead to an expected value of <span class=""math-container"">$\overline{\text{rank}}$</span> of 0.5.</p>

<h2>Examples</h2>

<ul>
<li>Assume there is only one item. In this case <span class=""math-container"">$\text{rank} = 0$</span>. Makes sense, as there cannot be any predictions.</li>
<li>Assume there is only one user and two items with <span class=""math-container"">$r_{1,1} = 1$</span> and <span class=""math-container"">$r_{1,2} = 2$</span>. Then:</li>
</ul>

<p><span class=""math-container"">$$\overline{\text{rank}} = \frac{1 \cdot \text{rank}_{1, 1} + 2 \cdot \text{rank}_{1, 2}}{1+2}$$</span></p>

<p>This means <span class=""math-container"">$\overline{\text{rank}} \in \{2/3, 1/3\}$</span>.</p>

<ul>
<li>If there is only a single user and all <span class=""math-container"">$|I|$</span> values of <span class=""math-container"">$r_{ui}$</span> are the same, then <span class=""math-container"">$\overline{\text{rank}} = \sum_{ui} \text{rank}_{ui} = \frac{|I|}{2}$</span></li>
</ul>

<h2>Questions</h2>

<ol>
<li>Is my understanding of the metric correct? Especially my last example and the statement by the authors that <span class=""math-container"">$\overline{\text{rank}} \geq 50\%$</span> indicated an algorithm is no better than random seem off.</li>
<li>What is <span class=""math-container"">$t$</span>?</li>
</ol>
","3","1","8820","16250"
"48568","<p>I think there is a bug. Or too little data. The training accuracy is - in the best case - close to the validation accuracy. That the validation accuracy gets lower than the training accuracy is a strong indicator for a bug. Or too little data, which means the difference of 2% might not be relevant.</p>
","0","2","8820","16250"
"49581","<h2>Vocabulary</h2>

<ul>
<li>Face detection: Finding all faces in an image.</li>
<li>Face representation: The simplest way to represent a face is as an image (pixels / color values). This is not very space efficient and likely makes follow-up tasks hard. <a href=""https://www.shortscience.org/paper?bibtexKey=journals/corr/1503.03832"" rel=""nofollow noreferrer"">Face embeddings</a> are one other representation. In this case a face is a point on the unit-sphere in <span class=""math-container"">$\mathbb{R}^{128}$</span>, IIRC.</li>
<li>Face verification: Given two face representations, deciding if they are the same</li>
</ul>

<h2>Question</h2>

<p>I was just wondering how identifying a person with many potential people can work. So finding a face in an image works quite well and fast enough for most applications. Face verification as well. But I'm not sure how to scale this if you don't compare 1 face against 1 other face, but 1 against millions / billions of faces.</p>

<p>Suppose you have a lot of examples of faces with the identity of the person. Think of Facebook, where many people tagged friends. Or of countries with biometric passports.</p>

<p>In the real applications, the face verification task is easy because you can just brute-force compare against all candidates:</p>

<ul>
<li>Facebook: Only candidates are your friends, so ~200 candidates. </li>
<li>Airport EU fast entry: Your face is compared against the passport. So only one candidate.</li>
</ul>

<p>But then think about some dystopian books / movies, where cameras identify anybody. While tracking helps to reduce that problem, finding a match from millions / billions of examples is computationally super heavy. Assuming a single face verification takes ~200ms, for a million candidates it would already take 60h. For a billion users it would already be 6 years. For all people on earth who currently live it is 48 years.</p>

<p>So with that many candidates, you don't want to compare against all candidates.</p>

<p>When you use the face-embedding, it becomes a nearest neighbor search in <span class=""math-container"">$\mathbb{R}^{128}$</span>.</p>

<p>Calculating the euclidean distance of two vectors in <span class=""math-container"">$\mathbb{R}^{128}$</span> takes roughly 15μs (see ""timing"" below). This means a single check over <span class=""math-container"">$7.5 \cdot 10^9$</span> people would take 31h. Way better, but still pretty long.</p>

<p>While the face-embedding approach pre-computes a good face representation, going over all examples is still a pretty dumb approach. If it was only <span class=""math-container"">$\mathbb{R}^1$</span>, one could make a simple binary tree. For few dimensions, I think something like a k-d-tree might work. But what about 128 dimensions?</p>

<p>Is there another approach to get the person quicker?</p>

<h2>Timing</h2>

<pre><code>import numpy as np
durations = timeit.repeat('np.linalg.norm(a-b)',
                          setup='import numpy as np;a=np.random.random(128);b=np.random.random(128)',
                          repeat=1000,
                          number=3)
print('min: {min:5.1f}μs, mean: {mean:5.1f}μs, max: {max:6.1f}μs'
      .format(min=min(durations) * 10**6,
              mean=np.mean(durations) * 10**6,
              max=max(durations) * 10**6,
              ))
</code></pre>
","4","1","8820","16250"
"52334","<blockquote>
  <p>Are there any standard procedure for designing a CNN?</p>
</blockquote>

<p>Not really.</p>

<p>If you want a more detailed answer, have a look at my masters thesis:</p>

<blockquote>
  <p>Thoma, Martin. ""<a href=""https://arxiv.org/pdf/1707.09725.pdf"" rel=""nofollow noreferrer"">Analysis and optimization of convolutional neural network architectures</a>."" arXiv preprint arXiv:1707.09725 (2017).</p>
</blockquote>

<p>Especially chapter 3 (Topology Learning) and 2.5 (Analysis Techniques) might be of interest to you.</p>
","3","2","8820","16250"
"54576","<p>Assume you have the ratings of <span class=""math-container"">$n$</span> users for <span class=""math-container"">$m$</span> movies in a matrix <span class=""math-container"">$R \in \mathbb{R}^{n \times m}$</span>. You compute a representation</p>

<p><span class=""math-container"">$$R = U \times \Sigma \times V$$</span></p>

<p>by initializing <span class=""math-container"">$u_i, v_j \forall i \in 1, \dots, n \forall j \in 1, \dots, m$</span> randomly and optimizing the following expression through gradient descent: </p>

<p><span class=""math-container"">$$\min_{u_i, v_i} \sum_{p_{ij}} \left ( p_{ij} - u_i \cdot v_j \right )^2 \text{ with } u_i \in \mathbb{R}^{1 \times r}, v_j \in \mathbb{R}^{r \times 1}$$</span></p>

<p>This is how I understand how Simon Funk did it.</p>

<p>But how would you deal with a new user? How would you tell what that user likes?</p>

<p>(Or similarly, with a new movie?)</p>
","2","1","8820","16250"
"54811","<p>I found <a href=""https://github.com/CVxTz/Recommender_keras"" rel=""nofollow noreferrer"">Basic recommendation system for Movilens dataset using Keras</a> which has a solution which works ok (MAE 0.84).</p>

<p>What is the current state of the art for this dataset?</p>
","1","1","8820","16250"
"55181","<p>If you look at publications, you can have a dataset</p>

<ul>
<li>title of publication</li>
<li>list of authors</li>
<li>number of pages</li>
<li>year of publication</li>
</ul>

<p>The <a href=""https://en.wikipedia.org/wiki/Level_of_measurement"" rel=""nofollow noreferrer"">Level of measurement</a> of ""number of pages"" is  interval scale, the year of publication is interval scale as well, the title is nominal. But what is the list of authors? Simply saying that it is nominal seems not to capture a major part of this attribute.</p>
","1","1","8820","16250"
"56167","<p>I just found <a href=""https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture25-mf.pdf"" rel=""nofollow noreferrer"">slides from Matt Gormley (CMU)</a> about recommendation systems. Under the heading ""Unconstrained Matrix Factorization"" he mentions:</p>

<ul>
<li>Optimization problem</li>
<li>SGD</li>
<li>SGD with Regularization</li>
<li>Alternating Least Squares</li>
<li>User/item bias terms (matrix trick)</li>
</ul>

<p>What does ""User/item bias terms (matrix trick)"" mean?</p>
","3","1","8820","16250"
"57191","<p><strong>What I tried:</strong></p>

<pre><code># -*- coding: utf-8 -*-

from nltk.stem.snowball import GermanStemmer
st = GermanStemmer()

token_groups = [([""experte"", ""Experte"", ""Experten"", ""Expertin"", ""Expertinnen""], []),
                ([""geh"", ""gehe"", ""gehst"", ""geht"", ""gehen"", ""gehend""], []),
                ([""gebäude"", ""Gebäude"", ""Gebäudes""], []),
                ([""schön"", ""schöner"", ""schönsten""], [""schon""])]
header = ""{:&lt;15} [best expected: n/n| best variants: 1/n | overlap: m]: ..."".format(""name"")
print(header)
print('-' * len(header))
for token_group, different_tokens in token_groups:
    stemmed_tokens = [st.stem(token) for token in token_group]
    different_tokens = [st.stem(token) for token in different_tokens]
    nb_expected = sum(1 for token in stemmed_tokens if token == token_group[0])
    nb_variants = len(set(stemmed_tokens))
    overlap = set(stemmed_tokens).intersection(set(different_tokens))
    print(""{:&lt;15} [as expected: {}/{}| variants: {}/{} | overlap: {}]: {}"".format(token_group[0], nb_expected, len(token_group), nb_variants, len(token_group), len(overlap), stemmed_tokens))
</code></pre>

<p><strong>what I get:</strong></p>

<pre><code>experte  [as expected: 0/5| variants: 3/5 | overlap: 0]: ['expert', 'expert', 'expert', 'expertin', 'expertinn']
geh      [as expected: 3/6| variants: 4/6 | overlap: 0]: ['geh', 'geh', 'gehst', 'geht', 'geh', 'gehend']
gebäude  [as expected: 0/3| variants: 1/3 | overlap: 0]: ['gebaud', 'gebaud', 'gebaud']
schön    [as expected: 0/3| variants: 1/3 | overlap: 1]: ['schon', 'schon', 'schon']
</code></pre>

<p>The two main problems are:</p>

<ul>
<li>Overlaps: schön != schon</li>
<li>Non-working stemming, e.g. [experte, expertin, expertinnen], [ich gehe, du gehst, er geht]</li>
</ul>

<p>A not so serious side problem is matching my expectations. So if the stemmer could actually bring the word in a basic form (not simply the stem), then it would be easier to analyze.</p>

<h2>More Examples</h2>

<h3>Clashes</h3>

<ul>
<li><strong>Input -> Output != clash</strong></li>
<li>mittels -> mittel != ""Das Mittel""</li>
</ul>

<h3>Unmatched expectations</h3>

<ul>
<li><p><strong>Input -> Output / expected</strong></p></li>
<li><p>Mädchen -> madch / Mädchen</p></li>
<li>Behaarung -> behaar / Behaarung</li>
</ul>
","4","1","8820","16250"
"61398","<p>I have documents of pure natural language text. Those documents are rather short; e.g. 20 - 200 words. I want to classify them.</p>

<p>A typical representation is a bag of words (BoW). The drawback of BoW features is that some features might always be present / have a high value, simply because they are an important part of the language. Stopwords like the following are examples: is, are, with, the, a, an, ...</p>

<p>One way to deal with that is to simply define this list and remove them, e.g. by looking at the most common words and just deciding which of them don't carry meaning for the given task. Basically by gut feeling.</p>

<p>Another way is TF-IDF features. They weight the words by how often they occur in the training set overall vs. how often they occur in the specific document. This way, even words which might not directly carry meaningful information might be valuable.</p>

<p>The last part is my question: Should I remove stopwords when I use TF-IDF features? Are there any publications on this topic? (I'm pretty sure I'm not the first one to wonder about this question)</p>
","3","1","8820","16250"
"78177","<p>I've recently found the term &quot;Zonal OCR&quot;: (<a href=""https://www.chronoscan.org/features_zonal_ocr.asp"" rel=""nofollow noreferrer"">source 1</a>, <a href=""https://www.efilecabinet.com/zonal-ocr/"" rel=""nofollow noreferrer"">2</a>, <a href=""https://www.openkm.com/old/en/openkm-zone-ocr.html"" rel=""nofollow noreferrer"">3</a>, <a href=""https://www.logicaldoc.com/software-features/zonal-ocr"" rel=""nofollow noreferrer"">4</a>). It seems to be essentially OCR, but restricted to relevant parts of the document.</p>
<p>The interesting task about which I want to learn more is how to find those zones automatically. How is this called? Is this a field of research? Could you point me to a publication so that I can start learning about this topic?</p>
<p>(&quot;Field Level recognition&quot; and &quot;Template OCR&quot; seems to be a synonym for zonal OCR)</p>
<h2>What I found</h2>
<ul>
<li><a href=""https://arxiv.org/pdf/1812.07933.pdf"" rel=""nofollow noreferrer"">Dynamic Programming Approach to Template-based OCR</a>, but they don't reference anything OCR related</li>
</ul>
","1","1","8820","16250"
"78179","<p>I've just found the term &quot;Intelligent Character Recognition&quot; (ICR) on <a href=""https://en.wikipedia.org/wiki/Intelligent_character_recognition"" rel=""nofollow noreferrer"">Wikipedia</a> and <a href=""https://abbyy.technology/en:features:scenario:zonal_field-level-ocr"" rel=""nofollow noreferrer"">other</a> <a href=""https://www.chronoscan.org/doc/ocr_icr___optical_intelligent_character_recognition.htm"" rel=""nofollow noreferrer"">pages</a>. According to Wikipedia:</p>
<blockquote>
<p>In computer science, intelligent character recognition (ICR) is an advanced optical character recognition (OCR) or — rather more specific — handwriting recognition system that allows fonts and different styles of handwriting to be learned by a computer during processing to improve accuracy and recognition levels.</p>
</blockquote>
<p>Is this just a marketing stunt or are there actually techniques which are specified as OCR and other techniques which are ICR? Are there Conferences / Journals about OCR which distinguish between OCR and ICR? (If yes, please add a reference)</p>
","2","1","8820","16250"
"80883","<p>All three terms sound super similar:</p>
<blockquote>
<p>[...] <strong>document layout analysis</strong> is the process of identifying and categorizing the regions of interest in the scanned image of a text document. A reading system requires the segmentation of text zones from non-textual ones and the arrangement in their correct reading order. Detection and labeling of the different zones (or blocks) as text body, illustrations, math symbols, and tables embedded in a document is called geometric layout analysis.[2] But text zones play different logical roles inside the document (titles, captions, footnotes, etc.) and this kind of semantic labeling is the scope of the logical layout analysis.</p>
<p><strong>Document layout analysis is the union of geometric and logical labeling.</strong></p>
</blockquote>
<p>Source: <a href=""https://en.wikipedia.org/wiki/Document_layout_analysis"" rel=""nofollow noreferrer"">Wikipedia</a></p>
<blockquote>
<p>The process of <strong>document structure and layout analysis</strong> tries to decompose
a given document image into its component regions and understand their functional
roles and relationships. The processing is carried out in multiple steps, such as preprocessing, page decomposition, structure understanding, etc</p>
</blockquote>
<p>Source: <a href=""https://www.researchgate.net/publication/226300537_Document_Structure_and_Layout_Analysis"" rel=""nofollow noreferrer"">Document Structure and Layout Analysis</a> by Anoop M. Namboodiri</p>
<p>From this I would say that wikipedia calls it &quot;geometric layout analysis&quot; what Namboodiri calls &quot;layout analysis&quot;. Wikipedia calls &quot;logical layout analysis&quot; what Namboodiri calls &quot;document structure analysis&quot;.
Namboodiri uses &quot;document understanding&quot; as a broader term.</p>
<p>Do I understand this right?</p>
<p>(Is Document Understanding a sub-field of Information Extraction?)</p>
","0","1","8820","16250"
"6503","<p>I haven't tried R (well, a bit, but not enough to make a good comparison). However, here are some of Pythons strengths:</p>

<ul>
<li><strong>Very intuitive syntax</strong>: tuple unpacking, <code>element in a_list</code>, <code>for element in sequence</code>, <code>matrix_a * matrix_b</code> (for matrix multiplication), ...</li>
<li><strong>Many libraries</strong>:

<ul>
<li><a href=""http://www.scipy.org/"" rel=""nofollow noreferrer"">scipy</a>: Scientific computations; many parts of it are only wrappers for pretty fast Fortran code</li>
<li><a href=""http://deeplearning.net/software/theano/"" rel=""nofollow noreferrer"">theano</a> > <a href=""http://lasagne.readthedocs.org/en/latest/"" rel=""nofollow noreferrer"">Lasagne</a> > <a href=""https://github.com/dnouri/nolearn"" rel=""nofollow noreferrer"">nolearn</a>: Libraries for neural networks - they can be trained on GPU (nvidia, CUDA is required) without any adjustment</li>
<li><a href=""http://scikit-learn.org/"" rel=""nofollow noreferrer"">sklearn</a>: General learning algorithms</li>
</ul></li>
<li><strong>Good community</strong>:

<ul>
<li>Python has <a href=""https://stackoverflow.com/questions/tagged/python"">448,000+ questions on SO</a> (R has 99,000+)</li>
<li>Python has <a href=""https://datascience.stackexchange.com/questions/tagged/python"">103 questions on datascience.SE</a> (R has 140)</li>
<li><a href=""https://www.python.org/dev/peps/"" rel=""nofollow noreferrer"">PEPs</a>, 63,000+ packages on <a href=""https://pypi.python.org/pypi"" rel=""nofollow noreferrer"">PyPI</a></li>
</ul></li>
<li><a href=""http://ipython.org/"" rel=""nofollow noreferrer"">IPython notebooks</a></li>
<li><strong>Misc</strong>:

<ul>
<li>0-indexed arrays ... I made that error all the time with R.</li>
<li>Established package structures</li>
<li>Good support for testing your code</li>
</ul></li>
</ul>
","4","2","8820","16250"
"9213","<p>They are almost identical:</p>

<blockquote>
  <p>The second key reason that maxout performs well is
  that it improves the bagging style training phase of
  dropout. Note that the arguments in section 7 motivating
  the use of maxout also apply equally to rectified
  linear units (Salinas &amp; Abbott, 1996; Hahnloser, 1998;
  Glorot et al., 2011). <strong>The only difference between maxout
  and max pooling over a set of rectified linear units
  is that maxout does not include a 0 in the max</strong>.</p>
</blockquote>

<p>Source: <a href=""http://arxiv.org/pdf/1302.4389v4.pdf"" rel=""noreferrer"">Maxout Networks</a>.</p>
","5","2","8820","16250"
"9845","<p>I've posted <a href=""https://www.reddit.com/r/MachineLearning/comments/41dij6/is_there_any_domain_where_bayesian_networks/"" rel=""noreferrer"">this link on Reddit</a> and got a lot of feedback. Some have posted their answers here, others didn't. This answer should sum the reddit post up. (I made it community wiki, so that I don't get points for it)</p>

<ul>
<li><a href=""http://arxiv.org/abs/1312.6114v10"" rel=""noreferrer"">Auto-Encoding Variational Bayes</a> is a combination of a Bayes Network and a neural network. The paper <a href=""http://arxiv.org/abs/1506.02158v5"" rel=""noreferrer"">Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference</a> seems to go in the same direction.</li>
<li><a href=""https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"" rel=""noreferrer"">Dropout: A Simple Way to Prevent Neural Networks from
Overfitting</a> is an example where Bayesian neural networks outperform their dropout approach (see section 6.4 ""Comparison with Bayesian Neural Networks"")</li>
<li><a href=""http://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf"" rel=""noreferrer"">Human-level concept learning through probabilistic program induction</a> is a paper ""on a Bayesian net network that does one-shot classification that way outperformed neural networks"" (according to trashacount12345 - I didn't check that by now).</li>
<li>Yann LeCun wrote a <a href=""https://plus.google.com/+YannLeCunPhD/posts/gWE7Jca3Zoq"" rel=""noreferrer"">Google+ post</a> in which he argues that neural networks and probabilisitc graphical models are not orthogonal concepts.</li>
</ul>
","6","2","8820","16250"
"10152","<p>It seems to be about classification with 2 classes. To understand where the 14 comes from, just try all cases of 4 points being in one of two classes:</p>

<pre><code>#!/usr/bin/env python

""""""Print all cases of 4 points being in one of two classes in 2D.""""""


def print_pattern(pattern):
    """"""Print four points in 2D.""""""
    print(""%s\t%s"" % (pattern[0], pattern[1]))
    print(""%s\t%s"" % (pattern[2], pattern[3]))


get_bin = lambda x, n: format(x, 'b').zfill(n)

# 16 possible cases how 4 points can belong to 2 classes
for i in range(16):
    bin_ = get_bin(i, 4)
    print_pattern(bin_)
    print(""-""*60)
</code></pre>

<p>which gives:</p>

<pre><code>0    0
0    0
------------------------------------------------------------
0    0
0    1
------------------------------------------------------------
0    0
1    0
------------------------------------------------------------
0    0
1    1
------------------------------------------------------------
0    1
0    0
------------------------------------------------------------
0    1
0    1
------------------------------------------------------------
0    1        --- Not linearly seperable
1    0
------------------------------------------------------------
0    1
1    1
------------------------------------------------------------
1    0
0    0
------------------------------------------------------------
1    0        --- Not linearly seperable
0    1
------------------------------------------------------------
1    0
1    0
------------------------------------------------------------
1    0
1    1
------------------------------------------------------------
1    1
0    0
------------------------------------------------------------
1    1
0    1
------------------------------------------------------------
1    1
1    0
------------------------------------------------------------
1    1
1    1
------------------------------------------------------------
</code></pre>

<p>I'm still not quite sure what a dichtonomy is, but this seems to be part of the answer.</p>
","3","2","8820","16250"
"11660","<p>The paper</p>

<blockquote>
  <p>Marie Plasse et al: Combined use of association rules mining and clustering methods to find relevant links between binary rare attributes in a large data set. <a href=""https://cedric.cnam.fr/fichiers/RC1172.pdf"" rel=""nofollow"">Link</a></p>
</blockquote>

<p>combines both, clustering and association rule mining. They could improve ARM by association rule mining. From the abstract:</p>

<blockquote>
  <p>A method to analyse links between binary attributes in a large sparse data set is proposed. Initially the variables are clustered to
  obtain homogeneous clusters of attributes. Association rules are then mined in each cluster. A graphical comparison of some rule
  relevancy indexes is presented. It is used to extract best rules depending on the application concerned. The proposed methodology
  is illustrated by an industrial application from the automotive industry with more than 80 000 vehicles each described by more than
  3000 rare attributes.</p>
</blockquote>
","-1","2","8820","16250"
"5389","<p><a href=""http://tensorflow.org/"">Tensor Flow</a> (<a href=""http://www.tensorflow.org/api_docs/python/nn.html#neural-network"">docs</a>) by Google is another nice framework which has automatic differentiation. I've written down some <a href=""http://martin-thoma.com/tensor-flow-quick/"">quick thoughts about Google Tensor Flow</a> on my blog, together with the MNIST example which they have in their tutorial.</p>

<p>See also: My <a href=""https://martin-thoma.com/tf-xor-tutorial/"">Tensorflow XOR tutorial</a></p>

<p><a href=""https://github.com/benanne/Lasagne"">Lasagne</a> (<a href=""http://lasagne.readthedocs.org/en/latest/"">docs</a>) is very nice, as it uses theano (→ you can use the GPU) and makes it simpler to use. The author of lasagne won the Kaggle Galaxy challenge, as far as I know. It is nice with <a href=""http://pythonhosted.org/nolearn/"">nolearn</a>. Here is an MNIST example network:</p>

<pre><code>#!/usr/bin/env python

import lasagne
from lasagne import layers
from lasagne.updates import nesterov_momentum
from nolearn.lasagne import NeuralNet

import sys
import os
import gzip
import pickle
import numpy


PY2 = sys.version_info[0] == 2

if PY2:
    from urllib import urlretrieve

    def pickle_load(f, encoding):
        return pickle.load(f)
else:
    from urllib.request import urlretrieve

    def pickle_load(f, encoding):
        return pickle.load(f, encoding=encoding)

DATA_URL = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'
DATA_FILENAME = 'mnist.pkl.gz'


def _load_data(url=DATA_URL, filename=DATA_FILENAME):
    """"""Load data from `url` and store the result in `filename`.""""""
    if not os.path.exists(filename):
        print(""Downloading MNIST dataset"")
        urlretrieve(url, filename)

    with gzip.open(filename, 'rb') as f:
        return pickle_load(f, encoding='latin-1')


def load_data():
    """"""Get data with labels, split into training, validation and test set.""""""
    data = _load_data()
    X_train, y_train = data[0]
    X_valid, y_valid = data[1]
    X_test, y_test = data[2]
    y_train = numpy.asarray(y_train, dtype=numpy.int32)
    y_valid = numpy.asarray(y_valid, dtype=numpy.int32)
    y_test = numpy.asarray(y_test, dtype=numpy.int32)

    return dict(
        X_train=X_train,
        y_train=y_train,
        X_valid=X_valid,
        y_valid=y_valid,
        X_test=X_test,
        y_test=y_test,
        num_examples_train=X_train.shape[0],
        num_examples_valid=X_valid.shape[0],
        num_examples_test=X_test.shape[0],
        input_dim=X_train.shape[1],
        output_dim=10,
    )


def nn_example(data):
    net1 = NeuralNet(
        layers=[('input', layers.InputLayer),
                ('hidden', layers.DenseLayer),
                ('output', layers.DenseLayer),
                ],
        # layer parameters:
        input_shape=(None, 28*28),
        hidden_num_units=100,  # number of units in 'hidden' layer
        output_nonlinearity=lasagne.nonlinearities.softmax,
        output_num_units=10,  # 10 target values for the digits 0, 1, 2, ..., 9

        # optimization method:
        update=nesterov_momentum,
        update_learning_rate=0.01,
        update_momentum=0.9,

        max_epochs=10,
        verbose=1,
        )

    # Train the network
    net1.fit(data['X_train'], data['y_train'])

    # Try the network on new data
    print(""Feature vector (100-110): %s"" % data['X_test'][0][100:110])
    print(""Label: %s"" % str(data['y_test'][0]))
    print(""Predicted: %s"" % str(net1.predict([data['X_test'][0]])))


def main():
    data = load_data()
    print(""Got %i testing datasets."" % len(data['X_train']))
    nn_example(data)

if __name__ == '__main__':
    main()
</code></pre>

<p><a href=""http://caffe.berkeleyvision.org/"">Caffe</a> is a C++ library, but has Python bindings. You can do most stuff by configuration files (prototxt). It has a lot of options and can also make use of the GPU.</p>
","40","2","8820","16250"
"16168","<p>(edited) answer of <a href=""https://www.reddit.com/user/mostly_reasonable"" rel=""nofollow noreferrer"">/u/mostly_reasonable</a> on <a href=""https://www.reddit.com/r/MLQuestions/comments/5ml72j/is_the_graphic_of_deep_residual_networks_wrong/dc4frg3/"" rel=""nofollow noreferrer"">reddit</a></p>

<p>The thing to note here is that $F(x)$ can refer to the functioning of more than one layer.
The paper's authors use '$H(x)$' to mean something like 'the function we want to learn in some (possibly more than one) consecutive layers of a neural network', see their statement</p>

<blockquote>
  <p>[...] hoping each few stacked layers directly [...]</p>
</blockquote>

<p>Then '$F(x)$' is then that same possibly multi layer function, minus the residuals. The author of course hypothesizes that $F(x)$ is easier to learn than $H(x)$.
So I think that in the figure the $F(x)$ is supposed to refer to everything in the Figure besides the ('$+ x$') part. Note how the F(x) symbol is centered with respect the the network, not attached to either layer. Then $F(x) + x$ references the entire $F(x)$ two layer network above combined with the skip connections.</p>
","0","2","8820","16250"
"18415","<p>In <a href=""https://arxiv.org/abs/1609.04836"" rel=""noreferrer"">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a> there are a couple of intersting statements:</p>

<blockquote>
  <p>It has been observed in practice that
  when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize [...]</p>
  
  <p>large-batch methods tend to converge to sharp minimizers of the
  training and testing functions—and as is well known, sharp minima lead to poorer
  generalization. n. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation.</p>
</blockquote>

<p>From <a href=""https://arxiv.org/pdf/1707.09725.pdf#page=73"" rel=""noreferrer"">my masters thesis</a>: Hence the choice of the mini-batch size influences:</p>

<ul>
<li><strong>Training time until convergence</strong>: There seems to be a sweet spot. If the batch size is very small (e.g. 8), this time goes up. If the batch size is huge, it is also higher than the minimum.</li>
<li><strong>Training time per epoch</strong>: Bigger computes faster (is efficient)</li>
<li><strong>Resulting model quality</strong>: The lower the better due to better generalization (?)</li>
</ul>

<p>It is important to note <strong>hyper-parameter interactions</strong>: Batch size may interact with other hyper-parameters, most notably learning rate. In some experiments this interaction may make it hard to isolate the effect of batch size alone on model quality. Another strong interaction is with early stopping for regularisation.</p>

<h2>See also</h2>

<ul>
<li><a href=""https://stats.stackexchange.com/a/236393/25741"">this nice answer / related question</a></li>
<li><a href=""https://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf"" rel=""noreferrer"">Efficient Mini-batch Training for Stochastic Optimization</a></li>
<li><a href=""https://svail.github.io/rnn_perf/"" rel=""noreferrer"">this RNN study</a></li>
</ul>
","31","2","8820","16250"
"36341","<p>ML, by Tom M. Mitchell:</p>

<blockquote>
  <p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.</p>
</blockquote>

<p>AI, but not ML:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping"" rel=""nofollow noreferrer"">SLAM</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Pathfinding"" rel=""nofollow noreferrer"">Path finding</a>: <a href=""https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm"" rel=""nofollow noreferrer"">Bellman–Ford algorithm</a>, <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""nofollow noreferrer"">A* search algorithm</a>, <a href=""https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm"" rel=""nofollow noreferrer"">Dijkstra's algorithm</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Markov_chain"" rel=""nofollow noreferrer"">Markov chains</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Cellular_automaton"" rel=""nofollow noreferrer"">cellular automata</a></li>
</ul>

<p>Thank you, <a href=""https://twitter.com/SGruninger/status/1024536373531951111"" rel=""nofollow noreferrer"">Servan Grüninger</a>, for your help.</p>

<p>See also: <a href=""https://cs.stackexchange.com/a/86864/2914"">How does machine learning relate to artificial intelligence?</a></p>
","3","2","8820","16250"
"27686","<p>I think they share a lot (e.g. machine learning is a subset of both, right?), but maybe both have elements the other doesn't have?</p>

<p>Could you name some in that case? Or is one a subset of the other?</p>

<p><strong>What is the relationship between AI and data science?</strong></p>

<p>For example, when it comes to the relationship of AI and ML, I always say AI is a superset of ML. And the distinguishing set is search algorithms, which I would include in AI but not in ML. Would search algorithms be included in data science?</p>
","1","1","8820","16250"
"35621","<h2>Things Pandas can do, that SQL can't do</h2>

<ol>
<li><a href=""https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.describe.html"" rel=""nofollow noreferrer""><code>df.describe()</code></a></li>
<li>Plotting, e.g. <code>df['population'].plot(kind='hist')</code></li>
<li>Use a dataframe directly for training machine learning algorithms</li>
</ol>

<h2>Things Pandas can do, I wasn't aware that SQL can do as well</h2>

<ol>
<li>Export to csv: <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html"" rel=""nofollow noreferrer""><code>df.to_csv('foobar.csv')</code></a>. This is important when you want to show something to a business owner who wants to work with Excel. And there is <a href=""https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.to_excel.html"" rel=""nofollow noreferrer""><code>df.to_excel</code></a> as well. But in SQL, you can do <code>SELECT a,b,a+b INTO OUTFILE '/tmp/result.txt' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\n' FROM test_table;</code> (thank you, vy32!)</li>
</ol>
","7","2","8820","16250"
"52068","<p>Given the specific context of detecting abnormal changes in the amount of free space, I'd suggest that you use the <strong>variation over time</strong> instead of the raw amount. For instance: </p>

<pre><code>date                   free_space  variation
2019-05-15 09:00:00    102.65      NA
2019-05-15 09:05:00    102.69      0.04
2019-05-15 09:10:00    103.11      0.42
2019-05-15 09:15:00    102.58      -0.53
2019-05-15 09:20:00    102.55      -0.03
</code></pre>

<p>Whatever method you use, the variation is a much more relevant information to detect an unusual change than the raw size. You could also use a time window, e.g. calculate the variation over the last 30 minutes.</p>

<p>Personally I would simply use a heuristic for something like this: if the absolute value of the variation is higher than a threshold then label as outlier. The threshold could be a percentage of the size of the disk, e.g. 5%.</p>
","1","2","64377","14816"
"52069","<p>Yes it is. </p>

<p>For an evaluation measure independent from the threshold, look at the <a href=""https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve"" rel=""nofollow noreferrer"">Area Under the Curve (AUC)</a>.</p>
","3","2","64377","14816"
"52263","<p>Your description could corresponds to the NLP task of <strong>summarization</strong>. This is an active field of research: <a href=""https://scholar.google.com/scholar?q=text+summarization"" rel=""nofollow noreferrer"">https://scholar.google.com/scholar?q=text+summarization</a></p>

<p>A much simpler option is to only extract sentences from both: in this case the goal is not to produce a text which reads like a story, just an enumeration of sentences.</p>

<p>Even in this case you will have to define how to measure ""information value"", this is not easy afaik.</p>
","2","2","64377","14816"
"52264","<p>The traditional approach would be Conditional Random Fields (CRFs). CRFs models can be designed with a fixed-size ""memory"", i.e. taking into account the N previous states (where N is a constant).</p>

<p>There are some tools available like <a href=""https://taku910.github.io/crfpp/"" rel=""nofollow noreferrer"">CRF++</a> or <a href=""https://wapiti.limsi.fr/"" rel=""nofollow noreferrer"">Wapiti</a>, but I don't know about python or R libraries.</p>
","0","2","64377","14816"
"52313","<blockquote>
  <p>I would like to study the impact of Col_A (a continual feature) on the output result. </p>
</blockquote>

<p>If you want to <strong>study</strong> the impact of a feature, in the sense to obtain an interpretable observation of the relationship between the variables, neural networks are probably not a good idea.</p>

<p>I would go with (supervised) regression trees, as the model they produce is self-explanatory (and can be used for prediction too).</p>
","0","2","64377","14816"
"52355","<p>Without knowing the specifics I can only suggest general ideas:</p>

<ul>
<li>the most natural representation is a one dimension line, with each entry simply represented for instance as a colored or bold segment corresponding to its start and end time. Assuming that the full time span is not too long, this is ok since entries don't overlap. Short entries will appear only as a dot.</li>
<li>if appropriate, an alternative would be to represent the duration on the Y axis and use boxplots to represent a set of entries by unit of time on the X axis, for instance week or month. The disadvantage is that this wouldn't represent for instance the time of the day or day of the week at which entries occur.</li>
</ul>
","0","2","64377","14816"
"52499","<p>I don't know many examples but I'm aware of at least one such tool, specialized for the medical domain: <a href=""https://semrep.nlm.nih.gov/"" rel=""nofollow noreferrer"">SemRep</a></p>

<blockquote>
  <p>SemRep is a UMLS-based program that extracts three-part propositions,
  called semantic predications, from sentences in biomedical text.
  Predications consist of a subject argument, an object argument, and
  the relation that binds them. For example, from the sentence in (1),
  SemRep extracts the predications in (2).</p>
  
  <p>1.We used hemofiltration to treat a patient with digoxin overdose that was complicated by refractory hyperkalemia.</p>
  
  <ol start=""2"">
  <li>Hemofiltration-TREATS-Patients,
  Digoxin overdose-PROCESS_OF-Patients,
  hyperkalemia-COMPLICATES-Digoxin overdose,
  Hemofiltration-TREATS(INFER)-Digoxin overdose</li>
  </ol>
</blockquote>

<p>In general, this is closely related to the problem of <a href=""https://en.wikipedia.org/wiki/Semantic_role_labeling"" rel=""nofollow noreferrer"">semantic role labeling</a>:</p>

<blockquote>
  <p>semantic role labeling (also called shallow semantic parsing) is the process that assigns labels to words or phrases in a sentence that indicate their semantic role in the sentence, such as that of an agent, goal, or result.</p>
</blockquote>

<p>Apparently there are some implementations available: <a href=""https://framenet.icsi.berkeley.edu/fndrupal/ASRL"" rel=""nofollow noreferrer"">https://framenet.icsi.berkeley.edu/fndrupal/ASRL</a> </p>
","1","2","64377","14816"
"52602","<p>I second Anony-Mousse's answer. There's a fair amount of literature on record linkage, it's worth spending some time exploring it to obtain decent results.</p>

<p>I assume that you use cosine on tokens? This is not sufficient, for persons names and addresses one needs to use measures based on characters (e.g. Levenshtein edit distance, Jaro and variants) or characters n-grams (e.g. cosine, Jaccard, etc.). You can search for string similarity metrics. For names you should preferarbly use a combination of both characters and token levels.</p>

<p>Now since you are trying unsupervised methods I assume that you don't have labeled data. In this case, a standard workaround is to use some form of bootstrapping:</p>

<ol>
<li>using an simple unsupervised heuristic which scores pairs of records by their similarity, extract the top N pairs. This subset is likely to contain a large number of positives, whereas in the full data positives make up a very small proportion.</li>
<li>manually annotate the top N pairs: this is feasible as opposed to annotating the whole data, and that gives you a training set but not a representative one.</li>
<li>use a semi-supervised method (possibly active learning) to predict labels on the rest of the data, keeping in mind that the original training set is skewed towards positive. Typically it makes sense to assume a high probability of negative for the rest of the data.  </li>
</ol>
","1","2","64377","14816"
"52794","<blockquote>
  <p>Do I need to split data to TrainSet and TestSet?</p>
</blockquote>

<p>It depends:</p>

<ul>
<li>It is acceptable to cross-validate on the whole dataset to obtain the performance of a particular model, since in this case the model is always tested on unseen data.</li>
<li>In your case, the selection of the best generalized model/hyperparameters is part of the training stage, so yes it makes sense to cross-validate only on your training set and then measure the performance of your final model on some fresh data. This is because the selection of the best hyperparameters (especially from a large set of possibilities) can still be partly due to chance despite CV (especially if the dataset is small), so testing on unseen data avoids the risk of overestimating the performance.</li>
</ul>

<blockquote>
  <p>Then randomly select hyperparameters for RandomForest and a number of folds for CrossValidation between (2-15)</p>
</blockquote>

<p>Usually the number of folds k for k-fold CV is not really part of the hyper-parameters, so it's a quite unusual to do this (as far as I know). The risk is that your final model might be selected partly because a particular k happens to produce a higher performance on the training set. To see this intuitively: a larger training set is more likely to produce a better model, and a higher k makes CV use a larger training set for every fold. So a higher k is likely to artificially improve the performance, just because the training set is larger. This is why it's safer to use the same k for comparing different hyper-parameters, possibly repeating the whole process for different values of k.  Also for a particular value of k you can randomly reshuffle the partitions to minimize the effect of chance in the CV partitioning.</p>
","1","2","64377","14816"
"52921","<p>How many useful Excel files are we talking about? More importantly, how many potential variants for the columns names? </p>

<p>Because if it's less than say a thousand, you're probably better off manually curating the columns names: it's going to take less time and provide you with more accurate data than implementing and testing a sophisticated string matching system. You can't rely on the automatic matching, so you would need to evaluate how correct the result is. Using any predefined string similarity method will leave you with many mistakes.</p>

<p>I would proceed in the following way:</p>

<ol>
<li>Automatically extract all the column names from all the files</li>
<li>Manually group the ones which represent the same information</li>
<li>Automatically replace the variants with a standardized version of the name</li>
</ol>
","0","2","64377","14816"
"52928","<blockquote>
  <p>There 800 separate documents of any of the 3 labels or 3 big documents of each of the labels is the best way to go and why?</p>
</blockquote>

<p>The first thing you need to think about in any ML problem is: what is an instance for the problem? In other words, what is going to be the input for which you want a prediction at the end of the process? </p>

<p>Imagine you train your model with 3 big documents, one of each label. Then the input for such a model is a big set of documents with the same label. So it can only predict a label for a set of documents sharing the same label. This means that somehow you need to have the labels before applying your model... difficult isn't it? :)</p>

<p>This is why in this case an instance must be a single document. It's the job of the learning algorithm to learn to discover the label based on the instances, and for that it needs many instances of each possible label.</p>

<blockquote>
  <p>On the other hand, if I do (as we usually do actually) like in the former case then the TF-IDF will be categories/labels-agnostic and I do not know this helps things.</p>
</blockquote>

<p>This is where there is a confusion: the TF-IDF weights are not supposed to encode the label in any way, they represent the importance of a particular word in a document. The learning algorithm will use this information <strong>for all the words</strong>, that is it's going to learn the difference between when the word <em>delicious</em> has a high TF-IDF and when the word <em>disgusting</em> has a high TF-IDF (for instance).</p>

<blockquote>
  <p>Is the answer simply that this an interesting but pretty bad idea because in this way you simply massively decrease the number of the observations with which the model/algorithm is trained and so you make much harder for him to figure out how to successfully classify things?</p>
</blockquote>

<p>That would be true as well, but the main issue is the one I mentioned above: you won't be able to provide the same kind of input when you apply your model on your unlabeled data.</p>
","1","2","64377","14816"
"52930","<blockquote>
  <p>So in the example above if many of the positive reviews have the word 'positive' in them then this word will automatically have modified (and in general lower) TF-IDF scores simply because the majority of the documents in the dataset are positive (600 documents).</p>
</blockquote>

<p>You're thinking of the word <em>positive</em> as example because you are a human and you know what to select. The words which have the highest frequency in English are stop words: the, a, is... These are useless and would add a lot of noise in your model. You can remove them using a predefined list of stop words, but what about words which are not stop words but are frequent enough, for instance <em>food</em>, <em>place</em>, <em>chef</em>... This is where IDF helps.</p>
","0","2","64377","14816"
"53419","<p>I would start with a nice and simple decision tree regression to predict the number of trucks working based on date, time and trucks out of service (features). Visualizing this tree could give some decent insight on the big patterns at play, and applying it answers questions such as ""Can I create a probability model out of this data such as: at 08:00 during the month of July there is x% probability of N fire trucks working ?""</p>

<p>It might make sense to look at time-based methods, that is taking into account the sequential nature of the data. In the most simple form you can just add a feature ""how many trucks were available in average in the past N hours?"", and that will certainly already increase the predictive power of the model. </p>
","1","2","64377","14816"
"53420","<p>I don't perfectly understand the supervised part of the question but it might be useful to note that:</p>

<ul>
<li>the unsupervised classification of documents seems to correspond to the NLP task of <a href=""https://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow noreferrer"">topic modeling</a>, a task for which there are good methods and tools available.</li>
<li>in the proposed example about football, the polysemy of the target word calls for <a href=""https://en.wikipedia.org/wiki/Word-sense_induction"" rel=""nofollow noreferrer"">word sense discrimination</a>.</li>
</ul>

<p>The two tasks and the methods to solve them are quite similar.</p>
","0","2","64377","14816"
"53469","<p>There can be two distinct reasons to use instances annotated with the gold-standard class, i.e. the true answer for the target application:</p>

<ul>
<li>In order to perform proper <strong>evaluation</strong> your test set must contain the gold-standard labels. The principle of evaluation is to measure by how much the predictions deviate from the truth, but without the truth the performance that you obtain on the test set is meaningless for the task that you are doing.</li>
<li>In order to train a <strong>supervised</strong> or semi-supervised model, the training set must contain the gold-standard labels.  Semi-supervised methods offer some options to adapt a training set to a different task.</li>
</ul>

<p>You can't rely on a model if you can't evaluate it at least on a small sample, so yes you probably need to manually annotate a subset of the data. It's only after that you can start thinking about how to improve performance.</p>
","0","2","64377","14816"
"53501","<blockquote>
  <p>Now that I can predict P1 based on A1 and T1, how do I use this knowledge to actually assign ALL the tasks - T1...Tn amongst all the agents A1...Am as to maximise the performance: sum(P1, P2, ..., Pi)?</p>
</blockquote>

<p>In general <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow noreferrer"">Genetic algorithms</a> can deal with this kind of optimization problems. The tricky part is to define the population with this particular assignment problem. Assuming a given set of tasks T1...Tn to achieve, an ""individual"" in the population would probably be an assignment A1...An, that is a possible assignment of the tasks to a (subset of) the agents. Thus the tasks play the role of the ""genes"":</p>

<ul>
<li>a mutation consists in replacing an agent with another for a particular task/gene</li>
<li>a crossover consists in randomly combining the pairs (task,agent) from two assignments/individuals</li>
</ul>

<p>The rest is straightforward genetic learning:</p>

<ul>
<li>the fitness function of an assignment/individual is the sum of the performances for this assignment</li>
<li>at each generation the best assignments/individuals are selected to generate the next generation by cross-over</li>
</ul>

<p>After a few generations the population will converge to a set of optimal assignments. Mind that it might end up in a local optimum, you can tune the hyper-parameters to minimize this risk.</p>
","0","2","64377","14816"
"53526","<ul>
<li>column A seems to be a unique index independent from the features in column B, I don't understand why you want to predict it? If it's really unique it's impossible to predict for any new instance.</li>
</ul>

<p>The standard way is to split column B into as many columns as there are possible keywords, then the value for each instance is 1 if it has this keyword, 0 otherwise. From that you can train any supervised model, for instance decision trees or SVM, in order to predict column X.</p>

<p>If you really want to predict column A for some reason, you can do the same process with an independent model or try to learn it jointly with column X.</p>
","0","2","64377","14816"
"53648","<p>It depends on the answer to these questions:</p>

<ul>
<li>How is the real-time data provided?</li>
<li>How often and in which format are the predictions required?</li>
</ul>

<p>It's common to use a database in such a setting. So the ML prediction process would be called regularly, query this database to get the latest instances and generate predictions in the required format (possibly storing the predictions in the database as well).</p>
","1","2","64377","14816"
"53654","<p>You have a mistake somewhere in either the data you provide as training data or the model you use for predicting.</p>

<p>If it was really the same model trained on the full data in all the cases, then any given instance in set2 would always be predicted with the same class (right or wrong), independently from the other instances in the test set. This implies that it's not the same model being applied when you predict individual sets or both merged.</p>

<p>As a test, save your model somewhere first then apply it to your different cases (the model that you saved, don't train it again).</p>

<hr>

<p>Ok so you are trying to do 3 experiments:</p>

<ul>
<li>Training on full training set produces modelA, then apply modelA on set1</li>
<li>Training on full training set produces modelB, then apply modelB on set2</li>
<li>Training on full training set produces modelC, then apply modelC on both set1 and set2</li>
</ul>

<p>But if this was really what you are doing the three models would be the same: modelA = modelB = modelC. Basically you could do this instead:</p>

<ol>
<li>Training on full training set produces modelD</li>
<li>apply modelD on any set</li>
</ol>

<p>Based on your description what probably happens is something like this:</p>

<ul>
<li>Training on instances of class A (error) produces modelA, then apply modelA on set1 -> perfect results</li>
<li>Training on instances of class B (error) produces modelB, then apply modelB on set2 -> perfect results</li>
<li>Training on instances of class A (error) produces modelC = modelA, then apply modelC on both set1 and set2: perfect for class A, completely wrong for class B</li>
</ul>

<p>You need to check which instances you give as training set in each case.</p>
","0","2","64377","14816"
"53727","<p>The references are in the link that you give: <a href=""http://community.nzdl.org/kea/index_old.html"" rel=""nofollow noreferrer"">here</a> you can find references to two papers which describe the method:</p>
<ul>
<li><a href=""http://community.nzdl.org/kea/Frank-et-al-1999-IJCAI.pdf"" rel=""nofollow noreferrer"">Domain-Specic Keyphrase Extraction</a></li>
<li><a href=""http://community.nzdl.org/kea/Nevill-et-al-1999-DL99-poster.pdf"" rel=""nofollow noreferrer"">KEA: Practical Automatic Keyphrase Extraction</a></li>
</ul>
<p>Fyi it seems there's no neural nets involved (the question is tagged with neural-network)</p>
","1","2","64377","14816"
"53734","<p>In this case the easiest way is to use a simple single-label classifier with three labels:</p>

<ul>
<li>A0B1</li>
<li>A1B1</li>
<li>A0B0</li>
</ul>

<p>As long as the number of combinations is reasonable, that's what will give the best results while satisfying the constraints.</p>

<p>By the way ""If label B is false, then label A is false"" is logically implied by ""if label A is true, then label B is also true"". (contrapositive).</p>
","1","2","64377","14816"
"53735","<p><a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">Sequence labeling</a> seems like a good fit for your problem. Your features could be the frequency of each kind of basic-msg in the past 4 runs (but maybe it would work better normalized by unit of time).</p>

<p>There are a good few libraries for <a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">Conditional Random Fields</a> for example.</p>
","0","2","64377","14816"
"53788","<p>It's ok to compute the global performance on the concatenation of the predictions for all the K folds after running the cross-validation process, it depends on the goal and on the metric (for instance the mean accuracy over the folds gives the same result as the global accuracy, but that's not true for every evaluation measure).</p>

<p>But very often the goal involves not only measuring performance accurately but also measuring the variance of the performance across the folds, in order to detect instability. This can't be done from the concatenation of the predictions, so it's often more convenient to keep the folds results separated.</p>

<p>(this is my interpretation, there might be other reasons)</p>
","4","2","64377","14816"
"53797","<blockquote>
  <p>Does anyone have a better suggestion or are there just complete algorithms for this already available?</p>
</blockquote>

<p>Apparently you want to do information retrieval (IR) but without the information part: normally an important part of the IR process is the set of documents that the user is searching for (for example text, images video). This is important because knowing the documents which correspond to a query gives semantic information about the keywords.</p>

<p>Now, if you only have the keywords available then there's no semantic to help with the clustering. So you are right that the only thing you can use is the spelling. i assume that by "" simple fuzzywuzzy"" you mean ""fuzzy matching"", i.e. using string similarity measures. I can think of two options:</p>

<ul>
<li>You can compare each keyword with each other with some string similarity measure such as Levenshtein edit distance, Jaro, or characters n-gram based Jaccard, cosine etc.</li>
<li>You can represent each keyword as a vector based on the char n-grams in the word, then you can apply clustering with k-means for example.</li>
</ul>

<p>Topic modeling techniques wouldn't work because you don't have large text documents.</p>
","0","2","64377","14816"
"53826","<p>This is a reasonably standard problem for supervised ML:</p>

<ul>
<li>The class is the variable ""dropped_out""</li>
<li>Given the goal to predict a variable which is specific to a particular student, an instance must represent a student, not an exam.</li>
</ul>

<p>This definition of what an instance should consist of seems to be the part that you didn't reach yet: you correctly saw that you need to join the two datasets but in your example you join them <strong>by exam id</strong>. As a result you obtain ""instances"" which each represent a particular exam by a particular student, and of course the same student might appear several times in the data. The solution is to join your datasets <strong>by student id</strong> in order to make a single instance contain all the information for one student, i.e. something like this:</p>

<pre><code>AGE, RESULT_TEST1, RESULT_TEST2, SCORE_EXAM1, SCORE_EXAM2, SCORE_EXAM3,...., DROPPED_OUT
</code></pre>

<p>However it seems that the exams are not normalized, so I see two options:</p>

<ul>
<li>Simplification: for each student, give only some summary statistics about their performance at exams, for example min, max, avg, std dev for both the score and the complexity. This gives a fixed number of features (8 in my example), each with a specific role so that the ML method can ""make sense"" of it.</li>
<li>Refactor the data: if possible, rearrange the exam data so that a column corresponds to the same exam for different students. This would mean that the exam complexity is not needed anymore, because the distribution of the grades is the only thing which matters. It's ok to have some missing/undefined values for the students who didn't take a particular exam, most ML methods can deal with that. </li>
</ul>

<p>The second option is very likely to give better results than the first, but it might be impractical to transform the data this way.</p>
","1","2","64377","14816"
"53828","<p>In general a false positive <em>rate</em> is abbreviated FPR, so it's likely that ""FP: 128.52 [/case]."" is not the rate but simply the number of false positive instances. In this case they should also mention the total number of instances by case, and you can easily obtain the rate: FPR = FP/total</p>
","0","2","64377","14816"
"53829","<p>I'm an old man who likes simple things :D </p>

<p>So I would try a few more basic options for the level 2 model:</p>

<ul>
<li>majority voting (it hardly gets simpler than that!) </li>
<li>linear regression</li>
<li>single decision tree </li>
<li>SVM</li>
</ul>

<p>Apart from the fact that I'm old, there are two reasons why these could be useful:</p>

<ul>
<li>smaller risk of overfitting: If the level 2 model is too complex it tends to overfit, and in my experience there's a high price to pay for that in terms of performance at level 2 when stacking learners.</li>
<li>scrutiny: one can easily investigate what happens in the combination of predictions.  </li>
</ul>
","1","2","64377","14816"
"53851","<p>I don't know the framework but it seems that's what happens:</p>

<p>Since this is multi-label classification, it's possible that an instance is assigned no label at all by the model. Apparently at least one of the samples contains only instances which are predicted with no label. This means that there are no instances predicted positive for any label in this sample. This causes an NaN value for the precision of this particular sample, which in turn makes the average precision over all samples undefined (hence the error).</p>

<p>I assume that this is not related to the fact that all the classes have a positive precision and recall, since these values would be calculated over the whole dataset, not sample by sample.</p>
","1","2","64377","14816"
"53861","<p>Based on the comments, the problem is due to wrong file permissions for certain files. Assigning read permissions to the full directory should solve it:</p>

<pre><code>chmod -R u+r &lt;caffe directory&gt;
</code></pre>
","1","2","64377","14816"
"53872","<p>In general identifying similarities is done with clustering, but in this case what you're looking for is a potential pattern in the data leading to a missing value in a specific column, right? So I would try to train a decision tree using all the columns except the target one as features, and using a binary class indicating whether the target column has a missing value or not. Visualizing the decision tree obtained after training  should show the pattern if there is one.</p>
","0","2","64377","14816"
"53899","<p>Yes, Cosine TF-IDF is quite transparent so it's usually reasonably easy to visualize the words which contribute the most to a score. Cosine is defined as the dot product divided by the product of the norms, so you can isolate the terms:</p>

<pre><code>dotproduct(d_1,d_2) = tfidf(w1,d1) * tfidf(w1,d2) + tfidf(w2,d1) * tfidf(w2,d2) + ... + tfidf(wN,dN)
</code></pre>

<p>Ranking the words w_i by descending order of the term tfidf(w_i,d1) * tfidf(w_i,d2) gives the top most contributing words for the similarity score.</p>

<p>Mind that if the documents have big differences in size, this will have an effect since cosine is normalized with their norms. </p>
","1","2","64377","14816"
"53943","<p>There is no strict definition about when to call a dataset ""imbalanced"", but generally speaking it's when the imbalance is likely to cause a problem with the model. Typically the problem is that the model will use the majority class as a default, because assigning the majority class is far less likely to be an error: for example if the data consists of 99% class A and 1% class B, a model which always predicts A will achieve 99% accuracy.</p>

<p>Your case would not be called imbalanced: the minority class makes up a large proportion of the data so it's very unlikely that the model would ignore it.</p>
","1","2","64377","14816"
"54030","<p>This is normal: unless your training data covers the population very well, the test set is bound to contain instances which slightly deviate from the cases seen in the training data. With any regression method, this might cause predicted values to go slightly out of range. If the application requires normalized values, these deviations should be programmatically corrected post-process (i.e. anything negative changed to 0 and anything higher than 1 changed to 1).</p>
","0","2","64377","14816"
"54051","<blockquote>
  <p>Does anyone know where to source such a dataset or something similar for social-network analysis?</p>
</blockquote>

<p>Yes, directly from Twitter using their API: <a href=""https://developer.twitter.com/"" rel=""nofollow noreferrer"">https://developer.twitter.com/</a></p>
","1","2","64377","14816"
"54113","<p>In general the lower the number of classes the easier it is for a classifier to assign the right category. However this completely depends on the characteristics of the data, in particular how well the features match the classes.</p>

<p>For example if one tries to somehow classify pictures of dogs, cats and rabbits into two classes, it's possible that the performance will be poorer than into 3 classes. This is an obvious example but sometimes the data might contain patterns which fit more easily into 3 groups than 2.</p>
","0","2","64377","14816"
"54151","<p>The easiest way is to generate your data artificially. For example generate points from two circles with different radius and same center, that gives two groups of points which are not linearly separable.</p>
","0","2","64377","14816"
"54227","<p>What you describe is a supervised problem, an unsupervised system cannot guess which parts of the documents are relevant for your purpose. In this option you need to annotate a sample of documents with a binary class, then train a system using features based on the context (for example titles of the parts).</p>

<p>But imho this depends how many documents you have: if it's less than a few hundreds, semi-manual annotation is going to be faster and give better results.</p>
","0","2","64377","14816"
"54357","<p>Looks to me like a sequence labeling problem, where the class is binary indicating whether the component is still working or failed. In this option you should build a training dataset which each cycle which looks like this:</p>

<pre><code>No_of_runs    Para1    Para2    Para3    Para4    Para5    status
1              100      32        45      230       86       ok
2              101      34        65      234       90       ok
3              120      24        32      242       80       ok
4              105      45        40      213       75       ok
5              90       42        54      200       77       ok
...            ...      ...       ...     ...       ...
1234           ..       ..        ..      ..        ..       fail
</code></pre>

<p>The order of the instances matters. After training, the model can tell you the probability of failing for an instance given its <strong>sequence</strong> of runs.</p>

<p>Conditional Random Fields are a standard option for such problems.</p>
","0","2","64377","14816"
"54358","<p>I don't think you need any ML, in the best case it's going to be very slow compared to direct processing and it's very likely to cause errors. </p>

<p>For each address you can parse the url string and extract the domain substring (in your example they are all prefixes, if it's always the case it's very easy). Then you just group them by domain in a map... done.</p>
","1","2","64377","14816"
"54579","<p>The approach you're describing might be good but the main question is how the automatic labeling works. You say you can ""create a function to label the data based on behavior"": what is the behavior? Do you manually decide the label or is entirely automatic? If it's completely automatic and general enough so that it can work for any instance, then you don't need any ML since you can directly apply your function. On the other hand if it's specific to a subset of data (or requires some amount of manual decision) then it makes sense, and the challenge will be whether the features are informative enough to predict the label for fresh instances from a different subset. ML assumes that your test instances come from the same distribution as the training set.</p>

<blockquote>
  <p>Create a new feature column called ""label"" that classifies accounts based on a user defined function that IDs spikes in volume. Is this feature engineering ? </p>
</blockquote>

<p>It's not feature engineering if it's the class that you will try to predict later with new instances. Feature engineering would be selecting particular features, for example instead of volume amount you could have minimum, maximum and mean volume, or discretizing the time, etc.</p>
","1","2","64377","14816"
"54582","<p>This problem is called <a href=""https://en.wikipedia.org/wiki/Record_linkage"" rel=""nofollow noreferrer"">record linkage</a> and there are methods to avoid iterating the whole cartesian product. The main method I know was called ""blocking"" and consists in doing a first ""rough"" pass to create groups of matching candidates (the ""blocks""). For example you can create groups which contain at least X n-grams in common. This can be done through one linear iteration through all the entities, storing them in every applicable bin based on their n-grams (an entity can be stored in several bins). I assume that some kind of clustering could also be used to generate the groups of similar entities. Then you end up with multiple groups of smaller size, and you run the cartesian product comparison on every group individually. This can greatly reduce the complexity.</p>

<p>note: I was working on this maybe 10 years ago so there might be more recent approaches.</p>
","2","2","64377","14816"
"54583","<p>By definition an out of vocabulary word (OOV) is a word which haven't been seen in the training data, so it's virtually impossible to know which other word it is similar to since this would have to be determined with some training data. In the example that you mention (synonym), what you could use is a resource such as WordNet which tells you which word it's similar to, then you can use the other word embedding.</p>

<p>Cases of misspelled words is a different story: you could pass your OOV words through string matching techniques, or maybe use a character-based NN which would recognize the word despite the misspelling.</p>

<p>Both cases could be part of a pre-processing stage.</p>
","1","2","64377","14816"
"54703","<blockquote>
  <p>mean, reviewing a pull-request is not that big of a deal compared to writing commits and also this normally has higher priority than writing code, so why I see new commits in the project, but PRs are still not reviewed?</p>
</blockquote>

<p>I assume that it's not that simple if they want to maintain the stability and the homogeneity of their code base. If they were just quickly accepting PRs the whole software would become an unmanageable mess in no time, causing problems for a large community of users. It's almost unavoidable for any significant software project to become harder and slower to maintain and develop when it reaches a certain size, especially if many users depend on it.</p>

<p>Also it's maintained by a community of volunteers so there might not be that many contributors who are both knowledgeable enough about the code and have enough free time to study all the PRs coming regularly.</p>
","0","2","64377","14816"
"54706","<p>You'll find great datasets for this task called author verification from the PAN workshop series. Afaik the last one specifically on this task was in 2015: <a href=""https://pan.webis.de/clef15/pan15-web/author-identification.html"" rel=""nofollow noreferrer"">https://pan.webis.de/clef15/pan15-web/author-identification.html</a>. I recommend exploring the website, there are many other datasets for related tasks.</p>
","0","2","64377","14816"
"54728","<p>There have been many ways to measure text complexity proposed in the literature, I don't have any particular survey to recommend but google is your friend.</p>

<p>Many of these measures are heuristics, i.e. they work in an unsupervised way. I don't remember the details but I've seen some works using a combination of several of these measures to obtain more accurate results. </p>

<p>A basic way would be to be build a language model on the complex text, measure the complexity against this model for any new text and assume that if it's not similar then it's not complex, but as you rightly noticed it's not a very safe assumption.</p>

<p>At the most basic level, you can use the type token ratio (TTR): divide the number of types (unique tokens) by the total number of tokens. The TTR is a quite good indicator of lexical diversity, so complex text is likely to give a high value. It's a very crude measure but it's useful as a baseline: whatever system you try, if it doesn't give better results than a threshold on the TTR then it's not a good system :)</p>
","0","2","64377","14816"
"54746","<p><a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow noreferrer"">Genetic algorithms</a> are often a good option for optimization problems, assuming it's not a requirement for the solution to be the best one but only to be ""close enough"".</p>
","0","2","64377","14816"
"54760","<p>This is only a general answer but in case it helps:</p>

<ul>
<li><p>In general decision trees tend to be ""robust"" in the sense that they can work with practically any kind of data, in particular in cases where other methods such as linear or logistic regression might struggle. For example they have no problem in the case of heterogeneous features, e.g. when mixing categorical and numerical features, or mixing completely different ranges of values, etc. </p></li>
<li><p>Random forests add ensemble learning to the mix, making decision trees even more robust and especially well equipped to deal with noisy data, whereas standard regression methods can get easily confused by noise.</p></li>
</ul>

<p>Intuitively I see decision trees (random forests included) as the ""swiss army knife"" of supervised learning: efficient, versatile, easy to use.</p>
","1","2","64377","14816"
"54800","<p>Given your data sample, unless you have a more subtle way to measure similarity between different values for a given genre (e.g. some other resource indicating that football is closer to basketball than to tennis for instance), it seems that the only similarity measure that you can use is to count how many tastes two users have in common. </p>

<p>The similarity score can be only 0, 1, 2 or 3, so I don't think you really need clustering. You can simply build a map where the key is the concatenation of the 3 ""genre"" columns, and the value is the set of users which have these tastes. Some users might not have an exact match (3 identical genres), so you do the same process but for only 2 genres in common, and then for only one.</p>
","1","2","64377","14816"
"54837","<blockquote>
  <p>Will it be P(5+ and reawaken), or will it be something else?</p>
</blockquote>

<p>The events are not independent, so one cannot assume that p(A and B) = p(A) * p(B). </p>

<p>Let's denote the events as follows:</p>

<ul>
<li>A = reawakens (not A = remains dormant)</li>
<li>B = purchases at least 5 times</li>
</ul>

<p>The event ""reawakens and purchases more than 5 times"" is ""A and B"". In general we have:</p>

<pre><code>p(A and B) = p(A/B) p(B)
p(A and B) = p(A/B) (p(A and B) + p(not A and B))
</code></pre>

<p>But it's impossible for a customer to stay dormant and buy anything, so:</p>

<pre><code>p(not A and B) = 0 
</code></pre>

<p>Which gives us:</p>

<pre><code>    p(A and B) = p(A/B) p(A and B)
    p(A/B) = 1
</code></pre>

<p>Then we obtain:</p>

<pre><code>p(A and B) = p(B)
</code></pre>

<p>Note that this makes sense intuitively: the probability of buying at least 5 times is the same as awakening and buying at least 5 times.</p>

<p>Currently the labels of the two models overlap so I don't think you can infer much from combining their outputs. A way to make it usable would be to make the first model consider only awakening customers in order to avoid the overlap. But it might be more useful to train a single joint model to get a clear picture of your data. In general such a model would classify between 4 categories:</p>

<ul>
<li>not A and not B</li>
<li>not A and B</li>
<li>A and not B</li>
<li>A and B</li>
</ul>

<p>But since p(not A and B) = 0 there are actually only 3 labels corresponding to:</p>

<ul>
<li>stays dormant</li>
<li>awakens and buys less than 5 times</li>
<li>awakens and buys at least 5 times</li>
</ul>
","1","2","64377","14816"
"54844","<blockquote>
  <p>I want to have a local database of corpus of the whole internet </p>
</blockquote>

<p>Are you Google? If not storage might be an issue ;)</p>

<p>The PAN series have run various tasks related to plagiarism detection in the past: <a href=""https://pan.webis.de/tasks.html#task-originality"" rel=""nofollow noreferrer"">https://pan.webis.de/tasks.html#task-originality</a>. I think they provide annotated datasets and they used to provide a live search engine.</p>
","1","2","64377","14816"
"54909","<p>Normalization across instances should be done after splitting the data between training and test set, using only the data from the training set. </p>

<p>This is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance.</p>
","31","2","64377","14816"
"55057","<p>[This answer is based on my limited knowledge, please don't hesitate to edit or propose improvements in the comments]</p>

<p>Actually I think it's a bit misleading to say that <em>algorithms</em> can be affected by class imbalance, because it's not exactly the algorithm which is affected it's <em>the evaluation method</em> (I mean ""evaluation"" in a broad sense including the loss function used by the algorithm during training). Some algorithms may be closely related to a particular loss function or internal optimization strategy, so by association such algorithms have the same weaknesses. </p>

<p>A simple way to see that class imbalance issues completely depend on the evaluation method is to compare micro-average performance with macro-average over the classes in a case where 99% of the instances belong to the same class:</p>

<ul>
<li>micro-average gives the same weight to every instance, so a model which assigns the majority class will look as if it performs very well.</li>
<li>macro-average gives the same weight to every class, so assigning the majority class  won't work better than random. </li>
</ul>

<p>So technically the problem of class imbalance could (should?) be seen as a design choice between maximizing the number of instances correctly classified (default evaluation) and any other alternative, for example giving equal weight to every class.  But of course it's not practical nor common to design a specific evaluation measure or loss function specific to every problem.</p>
","1","2","64377","14816"
"55102","<p>If I understand correctly, you currently try to predict the next code among 5500 possibilities. I think that's too many options to choose from, you can't expect a good performance with that.</p>

<p>Since the task is about predicting the 15 error codes I would represent the problem as a <a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">sequence labeling problem</a>: the label at each step is either ""nothing"" (often represented as ""<code>_</code>"" in sequence labeling tasks) or one of the 15 error codes. The label would represent the most likely outcome <em>for the next step</em>, not for the current one. This way the system can use the history of the codes (regular features) without having to predict all of them, only the critical ones. Note that if several error codes can happen at the same time, you might have to select a single one as label (maybe they can be ranked by priority?).</p>

<p>As far as I know, the standard method for sequence labeling is <a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">Conditional Random Fields</a> (CRF). In the traditional approach you would have to do some feature engineering: for example try to not give only the codes as features but also features such as how many times a code happened in the last N steps. I think the modern approach consists in training a RNN and using the vector representation as features for the CRF, but I might not be up to date with this.</p>
","0","2","64377","14816"
"55150","<p>Normally the evaluation measure doesn't depend on the method used, it depends primarily on the <strong>task</strong> being carried out. </p>

<p>Of course there are standard evaluation measures associated with broad types of tasks, such as classification or regression. There are technical constraints to take into account, for example whether the output is categorical or numerical. It's a common mistake to overlook this question and/or treat it as purely technical, but the choice of an appropriate evaluation setup should be made based on how well it represents the quality of the output of the ML process. </p>

<p>To answer your question: if the task is the same, the evaluation measure should be the same whether the method is linear regression or non-linear regression. Otherwise one would be measuring only some technical aspect specific to the method, not estimating the quality of the output in a comparable way.</p>
","1","2","64377","14816"
"55160","<blockquote>
  <p>I have a few questions on which I can't find the answers elsewhere</p>
</blockquote>

<p>It's probably because there is no simple answer to these three questions :)</p>

<p>I doubt there's any state of the art approach, in such cases I simply try to determine the answer to these questions empirically. Basically I create a list of hyper-parameters including the type of algorithm, the algorithm-specific hyper-parameters and any other potentially relevant option. The goal is to determine the optimal combination of values for the set of parameters. If practical I run all the combinations and select the best one. If not practical, I use a simple genetic algorithm to find an optimal combination. Of course it's suitable only if you have a dataset large enough and if the training/testing process is not too computer-intensive. You also need to be very careful about overfitting by using cross-validation and re-sampling.</p>
","0","2","64377","14816"
"55178","<p>To me this problem looks similar to <a href=""https://en.wikipedia.org/wiki/Language_model"" rel=""nofollow noreferrer"">language modeling</a>: a model is trained on a large amount of sequences, and then it can predict the probability of any input sequence. In your case a low probability would indicate an abnormal sequence.</p>

<p>My background is in NLP that's why I think language modeling, but I guess the same techniques are used for other problems as well. The fact that you have transitions and states suggests Markov Models, for which there are known methods for inference and estimation. So maybe you could design a more specific kind of model for your case and use something like the <a href=""https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm"" rel=""nofollow noreferrer"">Baum–Welch algorithm</a>.</p>
","0","2","64377","14816"
"55192","<p>There are several options to keep your session running on the host machine even if it's disconnected, see for instance this: <a href=""https://www.tecmint.com/keep-remote-ssh-sessions-running-after-disconnection/"" rel=""nofollow noreferrer"">https://www.tecmint.com/keep-remote-ssh-sessions-running-after-disconnection/</a></p>

<p>Personally I use the <code>screen</code> command because I'm familiar with it but I think <code>tmux</code> is more recent. </p>
","0","2","64377","14816"
"55193","<p>With a list such as this list of authors, the only representation which truly capture the semantics is a (long) list of boolean features, one for each possible author in the set. Ideally one would also pre-process the whole set of authors in order to match the different ways a name can be written, e.g. ""J. Doe"" = ""John Doe"" = ""Doe, John"".</p>
","-1","2","64377","14816"
"55196","<p>I don't really have experience with such a massive dataset, but my first thought would be to explore the instances in order to see if so many are needed. I would start with an ablation experiment, trying various sizes of training data with a simple method (random forest seems a good idea) in order to observe the evolution of the performance w.r.t size of the training data. It's likely that the performance reaches a plateau at some point, and it would be useful to know this point.</p>

<p>It might also make sense to study if the data contains duplicates or near duplicates. You can't remove duplicates directly because the distribution matters, but it might be possible to replace them by assigning weights to instances. I'm not expert in this but there are methods for <a href=""https://en.wikipedia.org/wiki/Instance_selection"" rel=""nofollow noreferrer"">instance selection</a>.</p>
","0","2","64377","14816"
"55238","<p>Intuitively I think that the model would need some additional features based on the economic context in order to be more accurate, and that would also be a part where the evolution across time really matters.</p>

<blockquote>
  <ul>
  <li>Would it now be a good idea to create train/test folds based on the<code>client_nr</code>? Or should I make splits by month?</li>
  </ul>
</blockquote>

<p>If possible you need to have a full time series for a client as an instance, so I'd say splitting on the client number is much better. But select the client numbers randomly from the full set of client ids, because it's possible that the clients number are assigned following a particular order in time.</p>

<blockquote>
  <ul>
  <li>Should I then select the first <span class=""math-container"">$n$</span> months as features, and create one label from <span class=""math-container"">$k$</span> months after <span class=""math-container"">$n$</span> that indicates whether the client has applied for a credit in those <span class=""math-container"">$k$</span> months? Or is there a better way?</li>
  </ul>
</blockquote>

<p>That depends on the exact goal and on the kind of algorithm being used, but afaik usually with a time series the label is predicted at any given time given the past and current features and the past labels (or past predicted labels).</p>

<blockquote>
  <ul>
  <li>Would it be better to use regression using <code>nr_credit_applications</code>, or classification on <code>credit_application</code>?</li>
  </ul>
</blockquote>

<p>This is more a matter of convenience for your application or for the algorithm being used, as the accuracy should be very similar (assuming you use the same kind of method of course). </p>
","-1","2","64377","14816"
"55584","<blockquote>
  <p>In other words, do we need these features related to other tokens?</p>
</blockquote>

<p>No, these features are not needed. But they are often useful: CRFs handle sequential dependencies between the labels, however it's up to you to provide the relevant features, in particular some to represent the dependencies between (certain) features if needed. </p>

<p>With text, this relation between consecutive tokens is very often a relevant indicator. I'd suggest that you try both version, without and with this feature, and you're likely to observe a higher performance in the latter case. In my experience it's often worth trying different combinations, including trying features which go two or three steps back.</p>
","1","2","64377","14816"
"55596","<p>I haven't used SMOTE in Weka so I don't know about your specific question, but in general Weka allows you to apply some preprocessing and generate an .arff file as output (for example when doing feature selection). It's probably also possible to chain the different stages in a single command, but that would probably involve a complex list of arguments.</p>

<p>Hopefully reading some tutorials about preprocessing like the following will help you and you can adapt it to your case:
<a href=""http://facweb.cs.depaul.edu/mobasher/classes/ect584/WEKA/preprocess.html"" rel=""nofollow noreferrer"">http://facweb.cs.depaul.edu/mobasher/classes/ect584/WEKA/preprocess.html</a></p>
","0","2","64377","14816"
"55783","<p>The original BLEU scores 25.9 and 25.7 are very close, there might not even be any significant difference. It's totally possible that model B performs better than model A on the filtered data only by chance. It's also possible that model B actually performs better than model A on shorter sentences. And finally it's worth noting that BLEU score is based on the number of n-grams in common, so it's likely to be affected by the length of sentences independently from the model being tested.</p>

<p>Conclusion: based on the information provided, this difference seems perfectly reasonable.</p>
","1","2","64377","14816"
"55824","<p>It would be very convenient but I'm not aware of any such site. </p>

<p>Besides, it would be quite difficult to agree on what is the current best performing model in general, as this depends on the dataset, how it's been annotated and the evaluation method. Not to mention the multiplicity of languages, since a particular model is usually language specific. And of course it would be difficult to keep up with new methods and datasets being published constantly.</p>
","0","2","64377","14816"
"55915","<p>Rare words are not a problem only for NMT, they are a problem for MT in general. The reason is simple: in order to accurately translate a word in any particular context, the model needs to see as many examples as possible during the training stage. By definition the training data contains very few occurrences of rare words (especially <a href=""https://en.wikipedia.org/wiki/Hapax_legomenon"" rel=""nofollow noreferrer"">hapax</a> words which occur only once), so the model doesn't have enough information to learn their translation properly.</p>
","2","2","64377","14816"
"55947","<p>In general the advantage of repeated training/testing is to measure to what extent the performance is due to chance. The most common source of chance comes from which instances are selected as training/testing data. One can use k-fold cross-validation in order to mitigate the effect of chance in this case. Weka performs 10-fold CV by default, as far as I remember, but this is not compatible with providing a specific training/test set.</p>

<hr>

<p>[edit based on OP's comments]</p>

<p>In the <a href=""https://www.youtube.com/watch?v=7lFie7V__Gs"" rel=""nofollow noreferrer"">video</a> mentioned by OP, the author loads a dataset and sets the ""percentage split"" at 90%. This means that the full dataset will be split between training and test set <strong>by Weka itself</strong>. Weka randomly selects which instances are used for training, this is why chance is involved in the process and this is why the author proceeds to repeat the experiment with different values for the random seed: every time Weka will selects a different subset of instances as training set, resulting in a different accuracy. In other words, the purpose of repeating the experiment is to change how the dataset is split between training and test set. In this case (J48 with default options) <strong>there would be no point repeating the experiment with a fixed training set</strong>, because there's no chance involved in the process so there's no variation in the result.</p>

<p>It's worth noticing that this lesson by the author of the video seems to be used as an introduction to the more general concept of k-fold cross-validation, presented a couple of lessons later in the course.</p>
","1","2","64377","14816"
"55957","<p>I'm not familiar with NLTK but the tagset must come from the annotated corpus which was used to train the tagger. According to <a href=""https://www.nltk.org/book/ch05.html"" rel=""nofollow noreferrer"">https://www.nltk.org/book/ch05.html</a> (section 2.2), if you can find the name of the Hindi corpus you should be able to access the words with their tags with:</p>

<pre><code>nltk.corpus.&lt;corpus&gt;.tagged_words()
</code></pre>

<p>Apparently this is the list of corpora available in NLTK: <a href=""http://www.nltk.org/nltk_data/"" rel=""nofollow noreferrer"">http://www.nltk.org/nltk_data/</a>, it might help finding the one used to train the Hindi POS tagger.</p>

<p>Once you identify the corpus, it's likely that you can find explanations about the tagset and annotation process by searching for papers about its creation by the original authors.</p>
","1","2","64377","14816"
"56020","<p>I would start with a simple decision tree regression with the occupancy rate as target value. By visualizing the tree after training you can see which features impact the occupancy rate. You can do this in Weka with M5P regression: <a href=""http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/M5P.html"" rel=""nofollow noreferrer"">http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/M5P.html</a></p>
","0","2","64377","14816"
"56043","<p>I think a better idea would be to use <a href=""https://en.wikipedia.org/wiki/Approximate_string_matching"" rel=""nofollow noreferrer"">approximate string matching</a> techniques: the general idea would be to compute a similarity score for each candidate term, then consider it a match if the score is higher than some threshold. This approach can be refined by  computing a set of similarity scores according to different similarity measures (see e.g. <a href=""https://en.wikipedia.org/wiki/Approximate_string_matching#See_also"" rel=""nofollow noreferrer"">this list</a>) and train a classifier using these scores as features. It's even possible to use more sophisticated approaches involving training a model taking into account the probability of specific edits, but it shouldn't be necessary in a simple case like this.</p>
","1","2","64377","14816"
"56057","<p>I will assume that by ""random"" you mean that the numbers don't follow any particular mathematical function. If they were really random then there wouldn't be any pattern to discover so there would be no point trying to predict anything.</p>

<p>From your description I understand the following:</p>

<ul>
<li>The value of the digit doesn't have any numerical property. In particular the natural order doesn't play any role. This suggests that the digits can be considered as categorical variables.</li>
<li>The data is sequential (the order in the sequence of digits matters) but there is no notion of time involved.</li>
<li>Apparently the two devices produce two independent sequences. You might want to check but if this is the case this calls for two distinct models, one for each device (otherwise you should use a single joint model).</li>
</ul>

<p>Based on these observations I would use a simple sequential model such as Hidden Markov Model or Conditional Random Fields.</p>
","0","2","64377","14816"
"56078","<ul>
<li>Let <span class=""math-container"">$X$</span> represent the event ""a user clicks an ad"" with advertiser X,</li>
<li>Let event <span class=""math-container"">$Y$</span> represent a new user (<span class=""math-container"">$\neg Y$</span> a returning user),</li>
</ul>

<p>for any advertiser <span class=""math-container"">$X$</span> the probability of a user clicking an ad is:</p>

<p><span class=""math-container"">$p(X) = p(X\wedge Y) + p(X\wedge \neg Y)$</span></p>

<p><span class=""math-container"">$p(X) = p(X|Y)p(Y) + p(X| \neg Y)p(\neg Y)$</span></p>

<p><span class=""math-container"">$p(X) = p(X|Y)p(Y) + p(X| \neg Y)(1 - p(Y))$</span></p>

<p>If it's possible to obtain the information ""new user"" (event <span class=""math-container"">$Y$</span>) in the system, then obviously there should be two distinct rankings, one for <span class=""math-container"">$Y$</span> and one for <span class=""math-container"">$\neg Y$</span>. Phrased in a ML context: <span class=""math-container"">$Y$</span> is a binary feature and the predicted best advertiser depends on it. In this scenario a different advertiser would be selected depending on whether the user is new or not, here A for returning users and B for new users.</p>

<p>If not possible, then the advertisers should be ranked according to their overall probability <span class=""math-container"">$p(X)$</span>, which depends on the probability of a new user <span class=""math-container"">$Y$</span>. Based only on your initial values 10% vs. 9%, this means selecting advertiser <span class=""math-container"">$A$</span> is optimal overall. </p>

<p>To illustrate how this works we can calculate the detail based on the values provided. First we calculate <span class=""math-container"">$p(Y)$</span>:</p>

<p><span class=""math-container"">$p(X) = p(X|Y)p(Y) + p(X| \neg Y) - p(X| \neg Y)p(Y)$</span></p>

<p><span class=""math-container"">$p(X)-p(X| \neg Y) = (p(X|Y)- p(X| \neg Y))p(Y) $</span></p>

<p><span class=""math-container"">$(p(X)-p(X| \neg Y)) / (p(X|Y)- p(X| \neg Y)) = p(Y) $</span></p>

<p>We have <span class=""math-container"">$p(A)=0.1$</span>, <span class=""math-container"">$p(A|Y) = 0.08$</span> and <span class=""math-container"">$p(A| \neg Y) = 0.3$</span>, so we can derive <span class=""math-container"">$p(Y)=0.91$</span>.</p>

<p>What happens in this example is that the high value <span class=""math-container"">$p(A| \neg Y) = 0.3$</span> makes up for 
the low <span class=""math-container"">$p(A|Y) = 0.08$</span> despite <span class=""math-container"">$p(Y)=0.91$</span>: </p>

<ul>
<li>A: <span class=""math-container"">$0.91*0.08+0.09*0.3 = 0.1$</span> </li>
<li>B: <span class=""math-container"">$0.91*0.09+0.09*0.09 = 0.09$</span></li>
</ul>

<p>Of course this would be different if <span class=""math-container"">$p(A| \neg Y)$</span> was 0.15 instead of 0.3 or if <span class=""math-container"">$p(A|  Y)$</span> was 0.11 instead of 0.09, etc.</p>

<hr>

<p>[edit based on comments by OP]</p>

<p>Indeed it seems that a more accurate representation of the problem would take into account the probability that a user clicks <em>again</em> (and again, and again...) on an ad. </p>

<p>This reminds me of classical probability exercises such as the <a href=""https://math.stackexchange.com/questions/2681718/mean-value-in-a-russian-roulette-game"">Russian roulette game</a>, the tree of possible outcomes is somewhat similar:</p>

<ul>
<li>user doesn't click on the ad at all: <span class=""math-container"">$n=0$</span></li>
<li>user clicks on the ad at least once

<ul>
<li>user never clicks again: <span class=""math-container"">$n=1$</span></li>
<li>user clicks on the ad at least twice 

<ul>
<li>user never clicks again: <span class=""math-container"">$n=2$</span></li>
<li>user clicks on the ad at least 3 times

<ul>
<li>user never clicks again: <span class=""math-container"">$n=3$</span></li>
<li>.......</li>
</ul></li>
</ul></li>
</ul></li>
</ul>

<p>So the goal would be to represent as accurately as possible the random variable <span class=""math-container"">$n$</span> which represents how many times a user clicks on an ad, and to use the expected value of <span class=""math-container"">$n$</span> (that is, how many times a user clicks on average) to rank the advertisers.</p>

<p>I'm guessing that this can get pretty sophisticated depending on how we choose to represent the problem. I'm probably not able to do anything sophisticated so I'll stick to a simple representation where I assume that there are two probabilities involved:</p>

<ul>
<li>initial probability <span class=""math-container"">$F=p(n\geq 1)$</span> for a new user to click</li>
<li>probability for a user to click again after having clicked <span class=""math-container"">$m$</span> times: <span class=""math-container"">$G=p(n\geq m+1|n\geq m)$</span> for <span class=""math-container"">$m&gt;0$</span>.</li>
</ul>

<p>This representation has the advantage to be very close to the original probabilities in the question: new user clicks and returning user clicks.
Notice that the model assumes that the probability to click again is a constant, which is unlikely in reality. Also the model doesn't take into account any other features, such as for how long a user hasn't clicked. I'll  just assume that one estimate these probabilities from a training set for the sake of simplicity.</p>

<p>Now let's calculate the expected value [fair warning, I might get it wrong!]</p>

<ul>
<li><span class=""math-container"">$p(n=0) = 1 - F$</span></li>
<li><span class=""math-container"">$p(n=1) = F \times (1-G)$</span></li>
<li><span class=""math-container"">$p(n=2) = F \times G \times (1-G)$</span></li>
<li><span class=""math-container"">$p(n=3) = F \times G  \times G \times (1-G)$</span></li>
<li>...</li>
<li><span class=""math-container"">$p(n=i) = F \times G^{i-1} \times (1-G)$</span></li>
</ul>

<p>Therefore the expected value is:</p>

<p><span class=""math-container"">$\sum_{i\geq 0} (p(n=i) \times i) = \sum_{i\geq 1} (F \times G^{i-1} \times (1-G) \times i)$</span></p>

<p>[Ok so normally now we can plug in the values for advertisers A and B. The problem is that I just calculated the result for the first few values and it doesn't really work... I probably made a mistake somewhere, or maybe the values don't represent the same thing... Anyway I hope this gives you the idea, maybe I'll give it another try later]</p>
","0","2","64377","14816"
"56128","<p>I agree with the idea of using a similarity or distance measure (approximate string matching). I would try a bunch of them and test them on a sample: Levenshtein, Jaro, overlap coefficient or cosine (optionally with TF-IDF) over bi/tri-grams of characters. </p>

<p>I would also try to capture the most common abbreviations and have a lookup table for these common cases because:</p>

<ol>
<li>Computing similarity/distance measures takes time, so it's inefficient to compute the same result many times for the same string (and it's likely that some of these abbreviations are used many times).</li>
<li>That gives you an opportunity to check that the mapping is correct (or to fix if it's not) at least for the most common cases, thus minimizing the overall amount of noise in the data.</li>
</ol>
","2","2","64377","14816"
"56173","<p>It's not clear to me what is your data and what you are trying to do with it, but from what I gather you are trying to calculate cosine similarity for each pair in a cartesian product, right?</p>

<p>If yes then you might want to use ""blocking"" to reduce the number of comparisons, see <a href=""https://datascience.stackexchange.com/a/54582/64377"">https://datascience.stackexchange.com/a/54582/64377</a>.</p>
","0","2","64377","14816"
"56184","<p>A multi-label problem is when an instance can have several labels, for instance a system which classifies news articles by topic could do this:</p>

<ul>
<li>instance 1: politics, society</li>
<li>instance 2: sports</li>
<li>instance 3: culture, sports</li>
<li>instance 4: society</li>
<li>...</li>
</ul>

<p>To turn this into a multiclass problem and still do the exact same task, one needs to create one class for each possible subset existing in the data, for instance:</p>

<pre><code>{ politics-society, sports, culture-sports, society, ...}
</code></pre>

<p>If the original multi-labels problem contains <span class=""math-container"">$N$</span> labels, the number of classes in the multiclass problem is <span class=""math-container"">$2^N$</span> in the worst case (number of partitions of the set). </p>

<p>The main problem is that the classifier needs a representative sample of every class in order to perform well. The classes <code>sports</code> and <code>culture-sports</code> (for example) are now independent from the perspective of the classifier, so the class <code>sport</code> cannot benefit anymore from the instances belonging to <code>culture-sports</code>, as it would be the case in the multi-label problem. </p>

<p>So in general one would need many more instances to train a multiclass classifier than a multi-label classifier for doing the same thing.</p>
","3","2","64377","14816"
"56232","<p>This task would be very close to <a href=""https://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow noreferrer"">topic modeling</a>, which is usually addressed as a multi-label classification problem.</p>
","1","2","64377","14816"
"56326","<p>The general idea is to standardize the format of the data so that it can be used consistently across a wide range of methods for analysis and prediction. </p>

<p>The negative impact is the same for all 5 types of messy data: one would spend much more time implementing functions which do nothing more than converting from one very specific format to another. Additionally through these cumbersome conversions one is more likely to introduce errors or omissions in the data.</p>

<p>Basically following the ""tidy data philosophy"" can save a lot of time and makes your data readily usable with a wider range of methods.</p>
","1","2","64377","14816"
"56343","<p>It's just a simple idea but you could calculate the conditional probabilities of an event given another:</p>

<pre><code>library(plyr)

condProbEvents &lt;- function(d) {
  ldply(colnames(d), function(col1) {
    t(ldply(colnames(d), function(col2) {
      nrow(d[d[,col1] &amp; d[,col2],]) / nrow(d[d[,col2],]) # p(col1|col2)
    }))
  })
}
</code></pre>

<p>This gives you values which are easy to interpret, e.g. for <code>p(divorce|marriage)</code>: ""X% of the people who had a marriage also had a divorce""</p>
","1","2","64377","14816"
"56344","<blockquote>
  <p>This is counter-intuitive, because one would expect [0,2,0,0] to be more similar to [0,1,0,0] than [0,1,1,0]. </p>
</blockquote>

<p>No this is expected, since the two points are exactly at the same distance in the Euclidean space. To see it take a simplified 2D version of your points:</p>

<ul>
<li>A (1,0)</li>
<li>B (2,0)</li>
<li>C (1,1)</li>
</ul>

<p>Both B and C are exactly at distance 1 from A. </p>

<blockquote>
  <p>But for my dataset Jaccard metric makes the kNN queries taking very long time, perhaps it is more suited for binary features.</p>
</blockquote>

<p>Jaccard actually assumes binary features, it will consider all non-zero values the same way. Its result is based on how many non-zero dimensions the two points have in common. I'm assuming that the implementation follows the original definition, there might be variants. Normally it's a very simple measure which doesn't require any heavy computation so it's surprising that it takes a long time.</p>

<blockquote>
  <p>When I checked that other one I could not see ANY similarities, in fact most of the features where very dissimilar by value and/or by sign. A change of sign has no particular importance in the euclidean space, it's the distance which matters.</p>
</blockquote>

<p>Well the only way to really check would be to calculate the euclidean distance, with 52 dimensions just looking at the values is not going to give a good indication.</p>

<blockquote>
  <p>I could get the same distance with a made-up sample of the first row from the training set where any one feature was increased by 0.02.</p>
</blockquote>

<p>I'm not sure I understand this part but again this sounds perfectly correct: changing any feature by 0.02 will move the data point in the space by a distance of 0.02 so... it's going to be at distance of 0.02 from the original point.</p>

<blockquote>
  <p>I am wondering now how to overcome this problem and if there is an easy way (ie. by tweaking parameters of NearestNeighbors) or a hacky way (ie. custom metrics, feature weights etc.)</p>
</blockquote>

<p>If there is any data-specific way to measure the distance between two points, you should definitely define a custom metric. It's not a hack, as you can see NN relies entirely on the metric. The default euclidean distance is a generic measure, it might not be fit for your purpose.</p>
","3","2","64377","14816"
"56391","<p>There is a crucial assumption made by any supervised ML approach: both the training set and the set are samples drawn from the same population. This means that the model expects to see the same distribution of features in the test set as in the training set.</p>

<p>In order to make sure that this assumption is satisfied, it is important to shuffle the instances before splitting between training set and test set. This will avoid any visible or invisible bias due to the order in which the data was collected or assembled. That's the effect that you can observe in your first experiment: the main issue is not so much about overfitting due to the model relying too much on some parameters, it's that neither the training or test set are a random subsets of the population. It might make complete sense for the model to rely on these 3 parameters based on the training set, the problem is that they don't behave the same way in the test set. ""behaving in the same way"" includes their relation to other features, so the difference might not be visible by looking at these features independently. This bias (which probably affects more than the 3 features) causes a huge discrepancy when evaluating the model on the test set.</p>
","1","2","64377","14816"
"56411","<p>Based on the example I assume that the target is persons names. </p>

<p>Let's be clear, there's no such thing as an exhaustive dataset containing all possible persons names in the world. Also a crucial part of the question is: in What kind of names? In which language? Persons names in English are pretty different from names in Chinese for instance. And there is also the difficult question of <a href=""https://en.wikipedia.org/wiki/Transliteration"" rel=""nofollow noreferrer"">transliteration</a> of proper names.</p>

<p>That being said, a few resources exist. They would usually be found by searching resources for ""personal names"", ""record linkage"", ""named entities matching/coresolution"". The following ones probably don't cover all the requirements but it's a start:</p>

<ul>
<li><a href=""http://users.cecs.anu.edu.au/~Peter.Christen/Febrl/febrl-0.3/febrldoc-0.3/manual.html"" rel=""nofollow noreferrer"">Febrl</a> (see also <a href=""https://recordlinkage.readthedocs.io/en/latest/ref-datasets.html"" rel=""nofollow noreferrer"">here</a>) </li>
<li>Found <a href=""https://pdfs.semanticscholar.org/697c/57d9a05160cda2f674164aec0abc3ab9e8d0.pdf"" rel=""nofollow noreferrer"">This paper</a> which presents a large resource and <a href=""http://linghub.org/lremap/5285aafc6916c9e171fe52fb24d77b9f44e78196"" rel=""nofollow noreferrer"">this corresponding resource description</a> but couldn't find the data.</li>
<li>The <a href=""https://emm.newsexplorer.eu/NewsExplorer/home/en/latest.html"" rel=""nofollow noreferrer"">EMM news explorer</a> has an interesting <a href=""https://emm.newsexplorer.eu/NewsExplorer/entities/en/1350410.html"" rel=""nofollow noreferrer"">database of named entities</a> including persons names with all the spelling variants/transliterations.</li>
</ul>
","0","2","64377","14816"
"56468","<blockquote>
  <p>My intuition is if I arranged the data as individual interactions, with output of final score, that’ll give me what I want. Is that true?</p>
</blockquote>

<p>If I understand correctly, your goal is to predict the final score that a customer gives at the end of their dinner based on all the interactions they had during the dinner, right? If yes then I don't think you can keep individual interactions as instances, because then (1) the model would only be able to use one interaction to predict the score and (2) the model would predict a different score for each interaction.</p>

<p>So each instance should represent the full meal and somehow contain as features all the possible information extracted from the interactions. For example an instance could contain the number of interactions, total length, number of refills, etc.</p>

<p>(side note: I wouldn't like eating in a restaurant where customers and staff are monitored so closely, but maybe that's just me)</p>
","0","2","64377","14816"
"56502","<p>The comparison between named entity and string is irrelevant, because the concept of named entity doesn't belong to data types such as string or numeric values. Of course a named entity is usually represented as a string but so is a text document, a cryptographic key or a translation of a text to Vietnamese: what matters here is the semantics, not the technical representation. </p>

<p>The reason why <a href=""https://en.wikipedia.org/wiki/Named_entity"" rel=""nofollow noreferrer"">named entities (NEs)</a> are often offered special treatment is because they are particularly relevant for certain tasks and are notoriously difficult to detect and analyze:</p>

<ul>
<li>The task of <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition (NER)</a> consists in detecting named entities in a raw text document. It's not a trivial task, the problem is still an active area of research.</li>
<li>Even after having been identified, named entities are often difficult to <a href=""https://en.wikipedia.org/wiki/Entity_linking"" rel=""nofollow noreferrer"">disambiguate</a>. The same entity may appear under many different variants for various reasons:

<ul>
<li>""John Smith"", ""J. Smith"" and ""John A. Smith"" may or may not be the same person.</li>
<li>""The president"" and ""Donald Trump"" may or may not be the same entity.</li>
<li>""Big Apple"" and ""New York"" represent the same entity, but not ""big apple"" and ""New York"".</li>
<li>...</li>
</ul></li>
</ul>

<p>Of course when one is provided with the result of the NE recognition/disambiguation process, they can process NEs like any other piece of information... but that's because all the hard work about NEs has been done before.</p>
","0","2","64377","14816"
"56505","<p>You will need your data to look something like this:</p>

<pre><code>Blue Red Green Label
20   12  13    _
18   11  13    _
18   12  13    _
19   13  14    _
24  12   13    _
28  14   19    B
30  19   21    B
29  18   20    B
25  14   16    B
21  12   13    _
19  11   12    _
18  11   12    _
</code></pre>

<p>That is, every point in the data will need to be annotated with a label: <code>_</code> for nothing, <code>B</code> for blue, <code>Y</code> for yellow. </p>

<p>A <a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">sequence labeling</a> model can be trained to predict the label for each point in the input sequence. Note that it's common to use a ""Begin Inside Outside"" (BIO) scheme to help the model understand when an ""event"" starts and ends:</p>

<pre><code>Blue Red Green Label
20   12  13    Outside
18   11  13    Outside
18   12  13    Outside
19   13  14    Outside
24  12   13    Outside
28  14   19    B_Begin
30  19   21    B_Inside
29  18   20    B_Inside
25  14   16    B_Inside
21  12   13    Outside
19  11   12    Outside
18  11   12    Outside
</code></pre>

<p>(there are also variants of this scheme)</p>

<p>As far as I know <a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">Conditional Random Fields</a> would be the traditional approach for this kind of task, but there might more recent NN approaches that I'm not aware of.</p>
","1","2","64377","14816"
"56529","<blockquote>
  <p>Simple definitional question: In the context of machine learning, is the error of a model always the difference of predictions 𝑓(𝑥)=𝑦̂  and targets 𝑦? Or are there also other definitions of error?</p>
</blockquote>

<p>No, I don't think the word ""error"" can be considered as a technical term which always follows this specific definition. </p>

<p>The word ""error"" is frequently used in the context of evaluation: in a broad sense, an error happens every time a ML system predicts something different than the true value. It's obviously the same concept as in the definition, but it can be applied to non numerical values for which the mathematical difference doesn't make sense.</p>

<p>For example in the context of statistical testing and in classification it's common to talk about <a href=""https://en.wikipedia.org/wiki/Type_I_and_type_II_errors"" rel=""nofollow noreferrer"">""type I errors"" and ""type II errors""</a> for instances misclassified as false positive and false positive respectively.</p>
","0","2","64377","14816"
"56645","<p>I would try a different approach than clustering.</p>

<blockquote>
  <p>For now, I tried L1 distance, cosine similarity, Euclidean distance, Mahalanobis Distance</p>
</blockquote>

<p>First, you could have a look at <a href=""https://en.wikipedia.org/wiki/Approximate_string_matching"" rel=""nofollow noreferrer"">approximate string matching</a> measures. These are likely to give you much better similarity results on a pair of movie titles. It's usually a good idea to use not only word-based measures but also character-based or char n-grams based measures.</p>

<blockquote>
  <p>how can I compare them to see which method perform best?</p>
</blockquote>

<p>A proper evaluation framework would require annotating manually a large amount of pairs of titles as similar/not similar (or even a degree of similarity). Unless you have a lot of time, this is completely impractical because there is certainly a massive imbalance between positive and negative pairs. So instead you could use bootstrapping, which means running a few similarity measures on your data, extract the top N pairs for each measure, then manually annotate only these. It's likely that this would give you a high amount of (rare) positive cases, and you can build a labeled dataset by assuming that other instances are negative. It's obviously a simplification, otherwise you can take the time to annotate a lot of negative cases as well (it's still much faster than without bootstrapping, since you already have your positive cases).</p>

<blockquote>
  <p>my purpose is to find the most similar movie titles, I want to use different distance/similarity measurements and compare them, what is the best method to use?</p>
</blockquote>

<p>Based on the dataset you have built, you can now train a supervised model, with a pair of titles as instance. You can use various similarity measures as features, and you should vary the type of similarity (char-based, ngram-based, word-based) across these features in order to provide the model with a diversity of characteristics.</p>

<p>Then you can predict the similarity between any two pairs. This gives you a graph of similarity relations between all the movies, from which you can extract groups which are similar together.</p>

<p>Note that this is just a general strategy, many parts of it can be refined/adapted to your data and of course it depends how much time you want to spend on this problem.</p>
","2","2","64377","14816"
"56727","<p>You'd need to apply a tagger, either a generic NE tagger or a custom-trained one. The tagger works with each token as an instance, so that you can extract a particular sequence of tokens, e.g.:</p>

<pre><code>$15.00     Begin_Billing_rate
hour       Billing_rate
customer   _
service    _
,          _
open       _
to         _
industries _
</code></pre>

<p>Of course in order to train a custom tagger you will also have to annotate your data token by token. </p>
","2","2","64377","14816"
"56754","<p>[edited]</p>

<p>The <a href=""https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve"" rel=""nofollow noreferrer"">Area Under the Curve (AUC)</a> is a good fit for this kind of problem, but it's not exactly a standard case. The ROC curve is built by ranking your instances by their predicted probability of fault, for example:</p>

<pre><code>p(faulty)   gold
0.03        0
0.14        1
0.19        0
0.23        0
0.31        1
0.32        0
0.65        1
0.78        1
0.83        0
0.90        1
</code></pre>

<p>Each level of probability corresponds to a possible threshold for the binary classification problem. The number of True/False Positive/Negative can be calculated for every possible level of threshold:</p>

<pre><code>p(faulty)   gold TP TN FP FN
                 5  0  5  0
0.03        0    5  1  4  0
0.14        1    4  1  4  1   
0.19        0    4  2  3  1 
0.23        0    4  3  2  1
0.31        1    3  3  2  2
0.32        0    3  4  1  2
0.65        1    2  4  1  3
0.78        1    1  4  1  4
0.83        0    1  5  0  4
0.90        1    0  5  0  5
</code></pre>

<p>At this stage one obtains exactly the data required for a standard ROC curve: from these values precision and recall scores can be computed for every level and plotted on the ROC curve. The AUC is used in the usual way. </p>

<p>This method takes into account the continuity of the probabilities while matching predictions against binary classes. An example of problem where this evaluation method is used: <a href=""https://pan.webis.de/clef15/pan15-web/author-identification.html"" rel=""nofollow noreferrer"">https://pan.webis.de/clef15/pan15-web/author-identification.html</a></p>
","1","2","64377","14816"
"56839","<p>In general I don't see anything wrong with overlapping windows, it might make perfect sense depending on your task. In fact some learning models (e.g. for sequence labeling) do use features based on past data points, which is conceptually similar to having overlaps between them.</p>

<p>However you need to be careful about the fact that this makes data points depend on each other, so of course the preprocessing must be done separately for the training and test set.</p>
","2","2","64377","14816"
"56846","<p>You should definitely use a sliding window.</p>

<p>An n-gram language model represents the probabilities for <em>all</em> the n-grams. If it doesn't see a particular n-gram in the training data, for example ""sliding cat"", it will assume that this n-gram has probability zero (actually zero probabilities are usually replaced with very low probability by smoothing, in order to account for out-of-vocabulary n-grams). This would result in a zero probability for a sentence which was actually in the training case (or a very low probability with smoothing).</p>

<p>Also it's common to use ""padding"" at the beginning and end of every sentence, like this:</p>

<pre><code>#SENT_START# The
The sliding
sliding cat
cat is
is not
...
to dance
dance #SENT_END#
</code></pre>

<p>This gives the model indications about the words more likely to be at the beginning or end (it also balances the number of n-grams by word in a sentence: exactly <span class=""math-container"">$n$</span> even for the first/last word).</p>
","0","2","64377","14816"
"56870","<p>Late answer for an interesting question:</p>

<blockquote>
  <p>How can I calculate a similarity (coefficient) where the order of the items matters</p>
</blockquote>

<p>This is exactly what character-based <a href=""https://en.wikipedia.org/wiki/Approximate_string_matching"" rel=""nofollow noreferrer"">approximate string matching</a> measures do, since a string is an ordered list of characters. So the idea is to consider every element in the list as a character in a string and apply the algorithm. The main character-based measures are:</p>

<ul>
<li>The <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein edit distance</a>, for which there are many available variations</li>
<li><a href=""https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance"" rel=""nofollow noreferrer"">Jaro-Winckler</a></li>
</ul>

<p>I would recommend the former since it has a clearer interpretation and is probably more generally used.</p>
","0","2","64377","14816"
"56882","<p>A major difference is the job market: you'll find a lot of job ads for data analysts/scientists, very few for theoretical statisticians. In most sectors (there are some exceptions, in banking for example), companies are interested in applying existing models to their data because this is what can increase their profits. </p>

<p>Devising new theoretical models is more on the research side of innovation. Most ""pure research"" job opportunities are in academia, although some big companies have research departments as well.</p>
","1","2","64377","14816"
"56899","<p>Since your goal is to predict the price, I think it would be more useful to include features such as:</p>

<ul>
<li>distance between origin and destination</li>
<li>whether origin and destination are in the same state/country/continent/area...</li>
</ul>

<p>However the actual origin and destination might still be useful, at least the frequent ones, so it's worth experimenting. Those which appear only once or twice in your training data are unlikely to help the model, on the contrary they might cause overfitting. So you could filter out the origin/destination cities which appear less then <span class=""math-container"">$N$</span> times and keep the other ones. It's very likely that this will reduce the number of possible values a lot.</p>
","1","2","64377","14816"
"56969","<p>Your intuitions are good: in general you need your data to contain all the possible indications (features) which might help find the answer. </p>

<p>And your thinking is also correct about creating this ""other table"": the reason why you need it is because for this problem you need each instance to correspond to a customer. Your original data is not organized by customer, it's organized by meeting. So it makes sense to organize your data with features by customer such as these:</p>

<pre><code>days_since_last_visit   number_meeting_scheduled   number_meetings_attended   ...
15                      4                          2
189                     3                          1
24                      2                          2
...
</code></pre>

<p>In general it's not recommended to assign scores yourself, because you probably don't know what is the optimal value. For example is a customer better when they schedule 10 meetings and attend 3 or schedule 4 and attend 2? It's usually better to give all the raw values you have to the ML algorithm and let it calculate the best way to use it.</p>

<p>An important point to define is: what is a good customer exactly? If you think your scoring is really accurate to define a good customer, you can calculate the score and rank the customers, then you're done: this is a <strong>heuristic</strong>, because you calculate the answer directly based on your knowledge of the problem.</p>

<p>Now assuming you are not so sure and want to use ML:</p>

<ul>
<li>If you can have a sample with labels which say whether a customer is ""good"" or not, then you have a supervised <strong>classification</strong> problem: the goal will be to train a model able to predict the class (category) for any customer based on the features. </li>
<li>If you have a sample with numeric values indicating ""how good"" a customer is, then you have a supervised <strong>regression</strong> problem. Again the goal is to train a model which predicts the value for any customer based on the features.</li>
</ul>

<p>For both cases above I'd suggest you start with simple methods such as decision trees or SVM (both can do classification or regression). The former has the neat advantage that you can manually observe the tree and understand how the classifier works.</p>

<ul>
<li>If you don't have any labeled data, then your only option is <strong>unsupervised learning</strong>, which usually means some form of clustering: in this case there's no training stage, the algorithm is provided only with the features and it tries to group together the instances which are close to each other. A standard approach for that would be K-means, for example. Note that you're not sure to obtain the groups that you expect in this case.</li>
</ul>
","2","2","64377","14816"
"56999","<blockquote>
  <p>I would like to attempt to identify frequently used words or groups of words</p>
</blockquote>

<p>The difficulty here would be to capture multiword terms, as opposed to single words. This implies using n-grams for various values of <span class=""math-container"">$n$</span>, and that can cause a bias when comparing the frequency of two terms of different length (number of words).</p>

<blockquote>
  <p>I would also need some way of filtering out invalid words like ('I', 'and' etc)</p>
</blockquote>

<p>These are called <a href=""https://en.wikipedia.org/wiki/Stop_words"" rel=""nofollow noreferrer"">stop words</a> (sometimes function words or grammatical words). They are characterized by the fact that they appear very frequently even though they consist in a quite small subset of the vocabulary (fyi this is related to <a href=""https://en.wikipedia.org/wiki/Zipf&#39;s_law#Statistical_explanation"" rel=""nofollow noreferrer"">Zipf's law</a> for natural language). These two properties make them easy enough to list in a predefined list so that they can be excluded, there are many lists available (e.g. <a href=""https://gist.github.com/sebleier/554280"" rel=""nofollow noreferrer"">here</a> or <a href=""https://www.ranks.nl/stopwords"" rel=""nofollow noreferrer"">there</a>).</p>

<p>Since you don't have any predefined list of terms, a baseline approach could go along these lines:</p>

<ol>
<li>For every value of <span class=""math-container"">$n$</span> to consider, collect all the <span class=""math-container"">$n$</span>-grams</li>
<li>Remove any <span class=""math-container"">$n$</span>-gram which contains only stop words (or which contains mostly stop-words) (note: it might be better to do this step first, but only if it's safe to assume that multiword terms don't contain stop words)</li>
<li>Calculate the document frequency for every candidate term (the same DF as in <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">TF-IDF weights</a>)</li>
<li>Filter out the terms which have a very low document frequency (experiment with different values for the threshold). This step should eliminate a lot of noise, but probably not all of it.</li>
<li>You will probably still need a bit of manual filtering here if your goal is to obtain a clean list of actual terms. Normally there should be few long n-grams left and the ones left should be good for the most part, however there might still be a lot of false positive unigrams and bigrams.</li>
</ol>

<p>This approach is very basic but it's easily adjustable, you can adapt it to your data, possibly add steps etc. Otherwise there are probably specialized tools for <a href=""https://en.wikipedia.org/wiki/Terminology_extraction"" rel=""nofollow noreferrer"">terminology extraction</a>, but I'm not familiar with any.</p>
","0","2","64377","14816"
"57016","<blockquote>
  <p>What is the appropriate method to find n-grams/sub-phrases/parts-of-sequences that are referring to a specific topic or belong to a certain category?</p>
</blockquote>

<ul>
<li>An important question to solve this problem would be: what is the range of input topics? Is the topic selected among a predefined closed list? Can it be any search query?</li>
<li>A similar question might be asked about the target documents and/or terms: can they be processed so that any candidate term is extracted in advance, and the task only consists in identifying the right terms for the particular topic?</li>
</ul>

<p>Assuming the most open variant of the question (i.e. nothing is available beforehand), I think that one would need:</p>

<ul>
<li>a terminology extraction system which extracts any candidate term from the text (preferably specific to the data to be processed).</li>
<li>a third-party resource in order to calculate a semantic representation (typically a vector) of any possible term including the input topic query, so that the topic can be matched/compared against any term.</li>
</ul>
","1","2","64377","14816"
"57055","<p>It's very unlikely that anybody here has the expertise needed to assess the validity of a PhD topic, especially about a highly specialized (and unspecified) experimental system. You should discuss this with your potential academic supervisor, and I'd suggest you try to find a co-supervisor or collaborator expert in DL as well.</p>

<p>In general for a decent PhD topic you would need to make sure that it hasn't been done before and that there are reasonable grounds to think it's feasible. Study the state of the art and find out if anything similar has been published:</p>

<ol>
<li>If something very close already exists, it might be too trivial</li>
<li>If there's nothing even remotely close to it, it's probably not feasible or very hard.</li>
</ol>

<p>Good luck!</p>
","0","2","64377","14816"
"57060","<p>Well, any application which requires some level of semantic understanding could potentially benefit from this, obviously. This includes most major applications such as machine translation and question answering. Traditional approaches can give decent results in terms of grammatical structure or topic similarity, but they are prone to serious counter-sense errors because they lack any form of semantic logic.</p>
","0","2","64377","14816"
"57086","<p>As far as I know <a href=""https://en.wikipedia.org/wiki/Text_segmentation#Topic_segmentation"" rel=""nofollow noreferrer"">topic segmentation</a> is not a particularly easy task with clean data, so it's likely to be challenging with noisy old French. </p>

<p>It's not exactly the same problem so I'm not sure if this is useful but you might want to look into using stylistic features in order to help the model detect the changes between articles. There has been a fair amount of work on the task of <em>style change detection</em> as part of the <a href=""https://pan.webis.de/clef19/pan19-web/style-change-detection.html"" rel=""nofollow noreferrer"">PAN series</a> (the task has run for 3 years, results and papers are available from the previous years).</p>

<p>Hope this helps.</p>
","0","2","64377","14816"
"57274","<p>To be fair the concept of the graph is a bit special, I also struggled to get it.</p>

<p>As one can read in the source, the author takes number of ""likes"" on Tinder as a proxy for attractiveness. This way they can rank men and women separately by how attractive they are: that's what the two axes represent. </p>

<p>If one accepts a few questionable assumptions such as:</p>

<ul>
<li>equating ""relationship wealth"" with ""proportion of people of the opposite gender attracted""</li>
<li>the more a person is attractive the more they go for the top attractive people of the opposite gender,</li>
</ul>

<p>Then for each gender the average number of likes received <em>by level of attractiveness</em> is calculated, for example:</p>

<p>In average a woman who has say 50% of attractiveness ""likes"" only 10% of the men, and we assume that she's going to like <em>only the top 10%</em> attractive men. Since this implies that the top 50% women are interested only in the top 10% men, it can be deduced by contrapositive that only the other 50% least attractive women can be interested by the 90% remaining (least attractive) men.</p>

<p>Another example from the other side: a man who has a level of 50% attractiveness likes all the women with more than 5% attractiveness in average. Again it is assumed that a woman who can attract a man in the top 50% will not be interested in the top bottom 50%, therefore the bottom 50% men can only ""access"" the bottom 5% of the women.</p>

<p>The whole graph is based on this assumption: if somebody at a particular level of attractiveness can attract the N% most attractive of the opposite gender, then anybody below this level of attractiveness is stuck with the remaining proportion. In other words it's not about how many people of the opposite gender one likes, it's about which level of attractiveness one can ""afford"" given their own level of attractiveness. That's what the graph shows: for example the 80% least attractive men can only afford the 22% least attractive women. By contrast the 78% most attractive women can afford to take their pick among the 20% top attractive men.</p>

<p>Beyond the caveats, my personal conclusion is: all you need is love... but can you afford it?</p>
","1","2","64377","14816"
"57280","<p>You are right on all counts:</p>

<blockquote>
  <ol>
  <li>If DT splits a node with the above algorithm and treat those 10 values are true numeric values, will it not lead to wrong/misinterpreted splits? </li>
  </ol>
</blockquote>

<p>Yes absolutely, for exactly the reason you mention below:</p>

<blockquote>
  <ol start=""2"">
  <li>Should it rather perform the split based on <code>==</code> and <code>!=</code> for this variable? But then, how will the algorithm know that it is a categorical feature?</li>
  </ol>
</blockquote>

<p>Yes, as you correctly assume a (true) categorical variable should be compared only for equality, not order.</p>

<p>In general the algorithm cannot guess the nature of the feature, there has to be some parameters in the implementation which provide it with this information. Some implementations allow this, for example with <a href=""https://www.cs.waikato.ac.nz/ml/weka/arff.html"" rel=""noreferrer"">Weka the features are typed with either a ""numeric"" or ""nominal"" (categorical) type</a>. </p>

<blockquote>
  <ol start=""3"">
  <li>Also, will one-hot encoded values make more sense in this case?</li>
  </ol>
</blockquote>

<p>Correct again, that's what should be done for a categorical feature in case the implementation treats all the features as numeric values.</p>
","9","2","64377","14816"
"57311","<p>Evaluation is based on the task, not the type of model used for it. In the tutorial that you link the task would be simple document similarity. Afaik a more common variant is the <a href=""https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)"" rel=""nofollow noreferrer"">information retrieval setting</a>, where the goal is to rank documents according to their similarity against a query document.</p>

<p>The tutorial mentions that they use a test set for which a link to a paper is provided. <a href=""https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf"" rel=""nofollow noreferrer"">This paper</a> explains that human annotators rated the similarity level between pairs of documents. That would be the standard way to obtain annotated data, but of course it takes a lot of manpower. It doesn't seem that the ground truth information is provided with the test set used in the tutorial though, I assume that this is why the author only proposes a manual inspection of the results.</p>

<p>In general there are probably many benchmark datasets (with ground truth annotations) publicly available for similar tasks, but they are not always easy to find and I'm not very knowledgeable about these. I think a good place to start would be the <a href=""https://en.wikipedia.org/wiki/SemEval"" rel=""nofollow noreferrer"">SemEval series of competitions</a>, every year they release various datasets related to this kind of task.  </p>
","2","2","64377","14816"
"57334","<p>In general in probabilities the symbol <span class=""math-container"">$\sim$</span> means ""follows the distribution .."", for example:</p>

<p><span class=""math-container"">$$X \sim N(3,7)$$</span></p>

<p>would mean that the random variable <span class=""math-container"">$X$</span> follows a normal distribution with mean 3 and std. dev. 7. In your example the symbol is used to define the notations apparently: the training data follows the distribution of the real data denoted  <span class=""math-container"">$p_{data}$</span> and the generated samples (whatever these are) follow the distribution calculated by the model denoted <span class=""math-container"">$p_{model}$</span>. </p>

<p>The point seems to be about the distinction between the real distribution of the data and the distribution obtained by the model: naturally one wants the latter to represent the former as accurately as possible, that's the job of the estimation algorithm.</p>

<p>An alternative very common notation would be <span class=""math-container"">$p(x)$</span> for the true (i.e. data) distribution and <span class=""math-container"">$\hat{p}(x)$</span> for the estimated (i.e. model) distribution.</p>
","1","2","64377","14816"
"57359","<p>Since you don't seem to have any annotated data, the best you can do is probably this:</p>

<ol>
<li>Optional first step: remove <a href=""https://en.wikipedia.org/wiki/Stop_words"" rel=""nofollow noreferrer"">stop words</a> (there are many such lists available, for example <a href=""https://pythonspot.com/nltk-stop-words/"" rel=""nofollow noreferrer"">https://pythonspot.com/nltk-stop-words/</a>)</li>
<li>Calculate the <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">Inverse Document Frequency</a> for every word in the vocabulary. This is intended to measure the importance of a word based on how often it's used, in the sense that a word used in fewer sentences makes it more important for these sentences.</li>
<li>Calculate TF-IDF for every word in a sentence, then rank the words by their TF-IDF weights. In general the top ones are supposed to be the most relevant. Optionally you can decide to select the top N words as keywords.</li>
</ol>
","1","2","64377","14816"
"57365","<p>Apparently the program expects a JSON file (probably) like this:</p>

<pre><code>[
    {
    ""id"": ""......."",
    ""mailing_list_url"": ""......"",
    ""type_of_recipient"": ""......"",
    ""email_address"": ""......."",
    ""message_body"": ""......"",
    ""is_response_of"": ""......""
    },
    {
    ""id"": ""......."",
    ""mailing_list_url"": ""......"",
    ""type_of_recipient"": ""......"",
    ""email_address"": ""......."",
    ""message_body"": ""......"",
    ""is_response_of"": ""......""
    },

    ...

    {
    ""id"": ""......."",
    ""mailing_list_url"": ""......"",
    ""type_of_recipient"": ""......"",
    ""email_address"": ""......."",
    ""message_body"": ""......"",
    ""is_response_of"": ""......""
    }
]
</code></pre>

<p>So if your data can be somehow converted to this that would be ideal, I guess. Otherwise you might have to play with the code.</p>

<p>Alternatively you could contact the authors by opening an issue.</p>
","1","2","64377","14816"
"57369","<blockquote>
  <p>I see that this threshold always matches the percentage of observations equal to 1 in my original data. Is there any conceptual explanation for this?</p>
</blockquote>

<p>Yes, although the fact it always matches <em>exactly</em> is probably a coincidence or maybe due to a small sample. </p>

<p>The training data contains a proportion <span class=""math-container"">$p$</span> of instances labelled 1. From the ROC plot you can see all the possible values for setting the threshold at a certain level and the resulting performance; for every possible level you can calculate the corresponding proportion <span class=""math-container"">$q$</span> of instances predicted as 1:</p>

<ul>
<li>if <span class=""math-container"">$q$</span> is much lower than <span class=""math-container"">$p$</span>, then the system predicts many 0s, so there are many false negative errors and that makes the recall lower. Precision is high in this case.</li>
<li>if <span class=""math-container"">$q$</span> is much higher than <span class=""math-container"">$p$</span>, then the system predicts many 1s, so there are many false positive errors and that makes the precision lower. Recall is high in this case.</li>
</ul>

<p>I assume that you optimize on the F1-score right? The fact that the F1-score is based on the product of the precision and recall means that <strong>both</strong> values need to be reasonably high, otherwise the F1-score drops. As seen above, having very different values for <span class=""math-container"">$p$</span> and <span class=""math-container"">$q$</span> will cause either the precision or recall to be low. Therefore the optimal F1-score is achieved when <span class=""math-container"">$q$</span> is close to <span class=""math-container"">$p$</span>.</p>
","1","2","64377","14816"
"57392","<blockquote>
  <p>In other words, is hyperparameter tuning more affected by the task (which is constant) or by the input data?</p>
</blockquote>

<p>It's correct that the task is constant, but hyper-parameters are usually considered specific to a particular learning algorithm, or to a method in general. In a broad sense the method may include what type of algorithm, its hyper-parameters, which features are used (in your case which embeddings), etc. </p>

<p>The performance depends on both the data and the method (in a broad sense), and since hyper-parameters are parts of the method, there's no guarantee that the optimal parameters stay the same when any part of the method is changed <em>even if the data doesn't change</em>.</p>

<p>So for optimal results it's better to tune the hyper-parameters for every possible pair of ML model and words embeddings. You can confirm this experimentally: it's very likely that the selected hyper-parameters will be different when you change any part of the method.</p>
","0","2","64377","14816"
"57728","<p>Start by plotting some of your data in various ways in order to get more familiar with it and understand the big picture. For example plot the total amount of sales for every store, for every product, look at the different by store for the same product, difference promotion/not promotion for the same product, etc.</p>
","1","2","64377","14816"
"57729","<p>As for many questions, the answer is ""it depends"":</p>

<ul>
<li>features which have a low individual importance may still add predictive power to your model, because the model benefits from combining their information together with information of other features.</li>
<li>However they may introduce noise in the model and cause overfitting, thus decreasing the performance of the model.</li>
</ul>

<p>The best way to answer your question is to experiment: </p>

<ol>
<li>order the features by decreasing importance</li>
<li>loop from say 5 features to 30, each time selecting the top N features by importance, and training/testing a model based on this subset of features.</li>
<li>plot the performance</li>
</ol>

<p>You're likely to observe that the performance increases quite a lot at the start for each ""important"" feature added, then slows down as feature importance decreases and probably doesn't increase at all at some point, possibly even decreasing a bit.</p>
","3","2","64377","14816"
"57730","<p>Your question is actually the whole point of active learning. You probably need to read about existing approaches in active learning in order to find the one which suits your needs.</p>

<p>I'm not up to date at all on the topic but a traditional approach was to train several models on available data, make them predict on all the unannotated instances then use majority voting: instances for which the models tend to agree are ""easy"" to predict, whereas those for which models make different predictions are ""hard"" so potentially more valuable to improve performance.</p>
","1","2","64377","14816"
"58073","<p><a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition (NER)</a> is about identifying the <strong>position</strong> of the NEs in a text. This means that each instance must represent a particular position in a text, and the NER will predict whether this position corresponds to a NE or not. Currently your data is not formatted in this way so it's not surprising that it doesn't work: in a vector representation of the whole text the model cannot find the kind of indication it needs to identify NEs.</p>
","0","2","64377","14816"
"58395","<p>In order to maximize accuracy you would need to use not only a POS tagger but also a syntactic parser. Nevertheless for this task POS tags can probably give you reasonable results indeed, here is a general method:</p>

<ol>
<li>Segment the data into sentences and tokens</li>
<li>Apply the POS tagger (it predicts a POS tag for every token)</li>
<li>A <a href=""https://examples.yourdictionary.com/imperative-sentence-examples.html"" rel=""nofollow noreferrer"">sentence is (likely) imperative</a> if the following conditions are satisfied:

<ul>
<li>the sentence ends with a full stop or exclamation mark</li>
<li>the POS for the first token corresponds to a verb</li>
</ul></li>
</ol>

<p>This heuristic is probably all you need, but if you want to go further you could generate instances containing these features (and possibly add a few others)  for every sentence, annotate a training set and train a supervised model.</p>
","0","2","64377","14816"
"58460","<p>I'd suggest the following: </p>

<ul>
<li>3 features, one for each level <code>main_column</code>, <code>sub_column1</code>, <code>sub_column2</code></li>
<li>2 additional features representing the hierarchical relation:

<ul>
<li><code>main_column/sub_column1</code></li>
<li><code>main_column/sub_column1/sub_column2</code></li>
</ul></li>
</ul>

<p>This way the training can select the most informative level of information between <code>main_column</code>, <code>main_column/sub_column1</code>, <code>main_column/sub_column1/sub_column2</code>. Depending on the data and algorithm used it might also make sense to discard rare cases for a subcategory and use a kind of ""misc"" category instead.</p>
","1","2","64377","14816"
"58462","<p>The reason why one uses <strong>any</strong> particular evaluation measure <strong>should be</strong> based on the semantics of the task. Corollary: there's no unique evaluation measure which is perfect for every task.
Obviously there are also technical constraints to take into account, but imho considering only the technical aspect of performance optimization is the most common mistake made in ML applications.</p>

<p>For instance OP mentions non-symetry of the F1-score: in some tasks it makes complete sense to use a non-symmetric measure, for example any task where the goal is to extract specific elements, e.g. named entity recognition. Moreover the question of symmetry makes sense only for binary classification, and many tasks involve more than 2 possible classes. In such cases it's common to use micro or macro F-score, but again it depends what one wants to measure. It's also important to keep in mind that <strong>a single metric is always a simplification of the performance</strong>, often there are many other relevant aspects. For instance, mistakes for a particular class might be more costly than for another, like in the case of medical prediction systems where a false negative means that a patient doesn't get a potentially life-saving treatment (note that generic F-score can be weighted to account for that).</p>

<p>The <strong>interpretability</strong> of the performance measure is also very important. In many applications one needs to estimate some kind of average error rate that a non-expert can understand, otherwise the ML system is just a magic black box to its users and this can cause serious ethical and practical issues.</p>
","3","2","64377","14816"
"58499","<p>I'm just guessing but I can think of two options: </p>

<ol>
<li>Train the model from the original data, including features which represent the current trend so that the model can integrate it. It makes the job of the model more complex so it might not work very well.</li>
<li>Since you can untrend the data I assume that you can calculate the trend function. So you could train an independent model for this trend function and apply it on the predictions obtained with the untrended model. In this case performance depend how accurately the trend is predicted.</li>
</ol>
","0","2","64377","14816"
"58549","<p>This is a complex problem related to <a href=""https://en.wikipedia.org/wiki/Natural-language_understanding"" rel=""nofollow noreferrer"">Natural Language Understanding (NLU)</a>. The key part in such a system is certainly <a href=""https://en.wikipedia.org/wiki/Textual_entailment"" rel=""nofollow noreferrer"">textual entailment</a>, but it could also use techniques such as <a href=""https://en.wikipedia.org/wiki/Question_answering"" rel=""nofollow noreferrer"">Question Answering</a> and <a href=""https://en.wikipedia.org/wiki/Automatic_summarization"" rel=""nofollow noreferrer"">summarization</a>.</p>

<p>I'm not aware of any direct model or tool to carry out this task exactly, so I think you will have to study the literature in these fields.</p>
","1","2","64377","14816"
"58608","<blockquote>
  <p>If apply normalization on training and testing in a separate way, I get really good results 85% (and sometimes more) and the further steps I try to do next work better as well. </p>
</blockquote>

<p>The problem with applying normalization across instances on the test set separately is that the test set represents any new data. So in principle the model should be able to give a prediction for a single instance independently from any other instances, in which case there is no set of instances to obtain the mean/std dev from. More importantly, the prediction of the model for a given instance should always be the same. Normalizing on the test set breaches this principle, because it makes the prediction for a particular instance depends on the other instances in the test set.</p>

<p>I don't think ""separate normalization"" is unethical strictly speaking, because it doesn't imply using any of the test data at training stage (whereas normalizing before splitting the train/test sets would). However it's theoretically incorrect for the reasons I mentioned above. </p>

<p>The fact that you obtain such a big difference in performance by normalizing ""separately"" points to a very different distribution of the data between training and test set (or a bug somewhere along the process). I'd suggest investigating that, maybe there's some error in the data?</p>
","5","2","64377","14816"
"58638","<p>The n-gram model is often built after segmenting into words <em>and sentences</em>. If the data is segmented by sentence it's easy to avoid any overlap between sentences: one can simply extract n-grams sentence by sentence independently. In case it's more convenient to extract all the n-grams at once, padding can be used to mark the beginning/end of sentences like this: </p>

<blockquote>
  <p>The sign was red #SENT# Balls are the toys of children #SENT#</p>
</blockquote>

<p>Dealing with other punctuation signs which don't mark the end of a sentence can be a bit more tricky, especially if you want to keep the possibility of a keyword which spans over some punctuation signs (for instance in ""red-handed"" or ""tl;dr""). </p>
","1","2","64377","14816"
"58808","<blockquote>
  <p>Is there any simpler approach?</p>
</blockquote>

<p>Very unlikely, semantic similarity is a very complex problem related to <a href=""https://en.wikipedia.org/wiki/Natural-language_understanding"" rel=""nofollow noreferrer"">Natural Language Understanding (NLU)</a>. You could look at the techniques used for <a href=""https://en.wikipedia.org/wiki/Textual_entailment"" rel=""nofollow noreferrer"">textual entailment</a>, <a href=""https://en.wikipedia.org/wiki/Question_answering"" rel=""nofollow noreferrer"">Question Answering</a> and <a href=""https://en.wikipedia.org/wiki/Automatic_summarization"" rel=""nofollow noreferrer"">summarization</a>.</p>

<p>Simple methods like the baseline system proposed in the github link, but they don't really try to analyze the semantics.</p>
","0","2","64377","14816"
"58809","<p>It depends what it's going to be used for, but in general it can make sense to use TF-IDF with short sentence. The main difference with the more standard case of long sentences is that TF (Term Frequency) won't play any role since the frequency will almost always be 1. IDF can still be useful though, assuming it's relevant to assign more weight to rare words than to frequent words. </p>

<p>However the problem is that comparisons such as cosine are often going to be zero, since there will be little chance of words in common between two sentences.</p>
","1","2","64377","14816"
"58820","<p>Cross-validation is used to estimate the performance of a certain type of model on a specific dataset.</p>

<blockquote>
  <p>one model has to be chosen as the model which is actually used for prediction on real-world samples (e.g. in a product).</p>
</blockquote>

<p>Selecting one of the models obtained during cross-validation is not appropriate, and proceeding in this way would indeed cause the problem that you mention. The correct methodology is to train the final model on the full training data after cross-validation (i.e. independently from the models trained during CV). This way the performance obtained through CV is representative of the expected performance of the final model.</p>
","2","2","64377","14816"
"58840","<p>Yes, the length is very likely to influence any kind of similarity score. In general:</p>

<ul>
<li>String containing very few words will have their highest similarity scores with other short strings with which they share one or two words in common. However if the words they contain are not common at all, they will have a lot of zero similarity scores (possibly only zeros).</li>
<li>Long strings (i.e. with many words) usually have low similarity scores, simply because they are unlikely to have a high proportion of their words in common. However compared to short strings they rarely have no word in common so they are much less likely to have lots of zero similarity scores.</li>
</ul>

<p>As a result strings of similar length may tend to cluster together, depending on the data.</p>
","1","2","64377","14816"
"60206","<p>The <code>stem</code> function doesn't allow several digits as leaves, as far as I can tell. So you're left with implementing it from scratch. It's not too difficult but OMG it took me such a long time to get it right... I always get confused with R weird types conversions. Anyway, here it is:</p>

<pre><code>d&lt;-strtoi(unlist(strsplit('1717 1719 1645 3739 3024 3664 3830 2991 2430 2730 3469 5086 2119 3021 3292 2844 3426 2067 3215 2767 3124 2573 2840 2449 2584 1505 1390 1645 2497 3466 3228 3192',split=' ')))
dd&lt;-ldply(d,function(x) {data.frame(stem=x %/% 1000,leaf=sprintf(""%03d"",x %% 1000),stringsAsFactors=FALSE)})
for (i in 0:9) { cat(paste(i,""|"","""")); l&lt;-dd[dd<span class=""math-container"">$stem==i,]$</span>leaf; if (length(l)&gt;0) {cat(sort(l))}; cat(""\n"") }
</code></pre>

<p>Result:</p>

<pre><code>0 | 
1 | 390 505 645 645 717 719
2 | 067 119 430 449 497 573 584 730 767 840 844 991
3 | 021 024 124 192 215 228 292 426 466 469 664 739 830
4 | 
5 | 086
6 | 
7 | 
8 | 
9 | 
</code></pre>

<hr>

<p>Remark: there's no way to do this with the <code>stem</code> function simply because the purpose of a stem and leaves plot is not to display every exact value: the plot is only intended to give a ""big picture"" view of the distribution of the values. What matters for that is how many leaves there are for every stem, and it's enough to give a single digit since precision doesn't matter. From this point of view the <code>stem</code> function does the job:</p>

<pre><code>stem(d)
</code></pre>

<p>Or variant:</p>

<pre><code>stem(d,scale=0.5)
</code></pre>
","0","2","64377","14816"
"60249","<p>I can think of two options:</p>

<ul>
<li>Train a custom supervised tagger for your data, typically with a sequence labeling method such as <a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">CRF</a>. This will require a quite large amount of annotated data (and a specific formatting), but if done well it should give you quite accurate results.</li>
<li>Use manually defined patterns based on keywords (such as ""My * and I"", ""family"", ""friends"") directly associated with a predefined category, and match these patterns in the data. You can do several iterations of defining new patterns for cases that are not matched, thus refining progressively. Depending on your data and how far you go with this you should be able to correctly match most cases, possibly reaching as good accuracy as a tagger for much less work.</li>
</ul>

<p>Btw be careful how you represent your data: the same user id might not always give you the same group of people since they can go to a restaurant one day with their family, the next day with their wife, the day after with their mistress, etc. Also a ""family of 2, expecting"" usually becomes ""a family of 3"" after 9 months ;)</p>
","0","2","64377","14816"
"60270","<p>You could try to apply a French POS tagger, e.g. as suggested here: <a href=""https://stackoverflow.com/questions/44468300/how-to-pos-tag-a-french-sentence"">https://stackoverflow.com/questions/44468300/how-to-pos-tag-a-french-sentence</a></p>

<p>Note that a POS tagger usually works better from full sentences than isolated words.</p>
","1","2","64377","14816"
"60271","<p>Your reasoning is correct: for most tasks related to information retrieval and/or document classification based on the semantics of the documents, it's recommended to take into account the importance of the terms (both inside the document and across all documents, hence TF and IDF).</p>

<p>However TF-IDF is not necessarily <em>always</em> the best choice:</p>

<ul>
<li>There are some classification tasks which are not based on the semantics of the document. For example if the goal is to classify documents by writing style (e.g. find documents by the same author) then the topic doesn't matter and therefore IDF is not relevant. </li>
<li>In the case of a very small dataset and/or very short documents, using TF-IDF scores can lead to overfitting. In such cases using boolean values might perform better because it makes the job of the model easier.</li>
</ul>
","0","2","64377","14816"
"60357","<p>In a multiclass problem there is one score for each class, counting any other class as a negative. </p>

<p>For example for class 1:</p>

<ul>
<li>TP instances are gold standard class 1 predicted as class 1</li>
<li>FN instances are gold standard class 1 predicted as class 2,3 or 4</li>
<li>FP instances are gold standard class 2,3 or 4 predicted as class 1</li>
<li>TN instances are gold standard class 2,3 or 4 predicted as class 2,3 or 4 (here errors don't matter as long as class 1 is not involved)</li>
</ul>

<p>In other words, the problem is evaluated as if it was a binary classification problem for every class individually. Doing the same process for every class independently (since the status of an instance depends on the target class), one obtains a different F1-score for each class.</p>

<p>After that, one generally calculates either the <a href=""https://datascience.stackexchange.com/q/15989/64377"">macro F1-score or the micro F1-score</a> (or both) in order to obtain an overall performance statistic.</p>
","3","2","64377","14816"
"60397","<blockquote>
  <p>could we invent an algorithm like BM25 to compute the relevance score of question-answer pair? </p>
</blockquote>

<p>It depends:</p>

<ul>
<li>BM25 (actually cosine with BM25 weighted vectors) is a simple similarity measure, ultimately based on counting words in common. Proposing a different similarity measure is easy, for instance there are various measures used for MT evaluation (including some quite sophisticated ones) which could be used as well. Of course, these measures don't actually measure the relevance, they just offer a crude approximation.</li>
<li>However if there was such a rule-based algorithm which would be able to <em>actually measure the relevance of an answer in any context</em>, then for all means and purposes we would have solved AI: judging the <em>semantic</em> relevance is much more subtle than counting words in common. In particular if there is such an algorithm, then the problem of question answering is solved: you can just generate all the possible answers and loop until one is found relevant to the question.</li>
</ul>

<p>People have tried to do ""intelligent"" rule-based algorithms in NLP for decades, before realizing that ML is more efficient and performs much better in most tasks. So it's extremely unlikely that a rule-based algorithm would suddenly outperform ML on a non-trivial task like this.</p>
","0","2","64377","14816"
"60428","<p>A few comments on the top of my head:</p>

<ul>
<li>both parameters <code>min_samples_leaf</code> and <code>max_depth</code> are not very important for decision trees, so it's not surprising not to see much variation (or not all) across different values: 

<ul>
<li>the fact that <code>min_sample_leaf</code> doesn't influence the performance simply means that the algorithm finds enough good predictors in the features to create leaves with a high number of instances, apparently always more than 30.</li>
<li>I'm less sure about <code>max_depth</code>, but I assume that it's a parameter which is used to prune the tree in order to avoid overfitting. This is exactly what happens here as soon as it's increased above 5: the algorithm creates a deeper (more complex) tree by using very specific conditions on the features which turn out to be specific to the training set, hence the divergence between training score and CV score (that's a clear sign of overfitting).</li>
</ul></li>
<li>Value 5 for <code>max_depth</code> is indeed optimal, as seen on the second graph (due to overfitting with higher values). The value 19 returned by grid search for <code>min_samples_leaf</code> is as optimal as any other value between 1 and 30, as seen on the first graph: grid search just happened to pick this one but it doesn't have any impact anyway.</li>
<li>The 82% accuracy is completely normal: that's the Y axis on both graphs ;) It indeed looks like a decent performance, but we can't say for sure since there's no comparison: maybe the dataset is super easy and 82% is just the majority class, or maybe it's super hard and 82% is a great achievement.</li>
</ul>

<p>Actually these graphs don't show much about the learning, they just show that the parameters studied are not really relevant. To observe something more interesting try:</p>

<ul>
<li>Ablation study: pick a random subset of the training set instances and train only with this subset. Do this for say 10%, 20%, ..., 100% of the data, then plot the performance as a function of the size of the training set. This will show how many instances are needed for the model to reach its max performance (educated guess: not that many).</li>
<li>Feature selection: use only the N most informative features (as measured by information gain for instance) and plot the performance for different values of N. If you're lucky you might see an increase until a particular optimal value of N followed by a small decrease when uninformative features are added. If it happens, that last part would be due to overfitting.</li>
</ul>
","2","2","64377","14816"
"60455","<p>There are probably many options, I'd look at <a href=""https://en.wikipedia.org/wiki/Inter-rater_reliability"" rel=""nofollow noreferrer"">inter-annotator agreement</a>.</p>

<p>An alternative option would be to to evaluate one (e.g. by accuracy or f1-score) using the other as the gold standard.</p>
","1","2","64377","14816"
"60492","<blockquote>
  <p>But my concern is that my shops might be similar, it doesn't mean that they would spend the same amount of time to repair a bike. My second thought was to treat each shop independently, so basically one model per shop.</p>
</blockquote>

<p>You need to decide which problem you're trying to solve, e.g.:</p>

<ul>
<li>average duration of repair for a particular type of bike and repair id</li>
<li>average duration of repair for a particular shop (can be used to rank shops by their performance)</li>
<li>average duration of repair for a particular type of bike in a particular shop (can be used to predict the best shop for a particular bike)</li>
<li>...</li>
</ul>

<p>After having clearly defined the problem, you need to format the data so that every instance corresponds to the item for which you want to predict something. For instance if the goal is to predict duration by type of bike, then an instance represents a type of bike so the feature should contain information about the bike and the target variable could be the average duration for this bike. In this case you shouldn't have several instances for the same type of bike.</p>
","1","2","64377","14816"
"60509","<p>In my opinion there are two ways:</p>

<ul>
<li>Ask a few experts to assess the quality of the clusters based on a sample (<em>after</em> the clustering has been done, much easier than pre-annotating the whole data especially in the case of clustering)</li>
<li>If the clustering is done in the perspective of using the result in another task, the performance of this other task will reflect the quality of the clustering.</li>
</ul>

<p>Imho any measure based on the distance between clusters or other technical measure would be a flawed evaluation, because it would depend on the quality of the representation. Such measures might provide some useful indications though, just not a proper evaluation for the task.</p>
","4","2","64377","14816"
"60511","<p>It looks like you should redefine your task as a regression problem instead of a classification problem, because your target variable is numerical. </p>

<p>The performance will be much better and it will avoid the need for a questionable pseudo-regression-like measure. <a href=""https://en.wikipedia.org/wiki/Mean_absolute_error"" rel=""nofollow noreferrer"">Mean absolute error</a> and <a href=""https://en.wikipedia.org/wiki/Mean_squared_error"" rel=""nofollow noreferrer"">mean squared error</a> are standard regression evaluation measures.</p>
","1","2","64377","14816"
"60534","<p>""meaningful"" is a vague word anyway, but yes you got the idea: in the context of a particular task meaningful data is the information which contributes to solving the task. Non-meaningful is the opposite, so information which doesn't help for the task. Sometimes non-meaningful data makes it harder to the ML algorithm to capture the relevant information, since it has to correctly identify what is meaningful and what is not (as opposed to the case where it's provided only with relevant information).</p>

<p>Example: if you want to predict how fast a car can go, engine type and manufacturer are meaningful data. Colour is not.</p>
","0","2","64377","14816"
"60561","<p>You are perfectly right to pay attention to the std dev across CV folds, especially with a small dataset. As you observed, different models show different values for the performance but also for the std dev, so you have to arbitrate a tradeoff between performance and stability:</p>

<ul>
<li>The safe option is to choose the model with lower accuracy and low variance. It might not always perform optimally but at least it won't perform too bad.</li>
<li>The risky option is the high accuracy, high variance model: in average it will perform best, but you have a higher risk that it actually performs poorly.</li>
</ul>

<p>This choice depends on the context, i.e. what the model is intended for.</p>
","2","2","64377","14816"
"60562","<p>I assume that you are referring to the fact that training a decision tree (e.g. with the <a href=""https://en.wikipedia.org/wiki/C4.5_algorithm"" rel=""nofollow noreferrer"">C4.5 algorithm</a> or a variant) involves the selection of the features used as conditions in the nodes. Typically at each iteration the algorithm selects the most discriminative feature by ranking all the features using for instance their <a href=""https://en.wikipedia.org/wiki/Information_gain_in_decision_trees"" rel=""nofollow noreferrer"">information gain</a>. </p>

<p>This is indeed very close to a feature selection process:</p>

<ul>
<li>ranking features by their informative power w.r.t the label is a very common way to do feature selection</li>
<li>the decision tree can end up selecting only a subset of the available features, exactly as if feature selection had been applied.</li>
</ul>

<p>SVM works very differently and (in general) doesn't have a similar process of selecting particular features. But it's true that in a loose sense any classifier does some kind of feature selection: it measures how relevant each feature is in order to predict the correct class.</p>

<p>However it's important to understand that a real feature selection process consists in removing features <em>before</em> training, i.e. the learning algorithm doesn't get the option to use the features which have been removed. This differs a lot from the above cases because:</p>

<ul>
<li>if the feature selection removes too many features, the model might not be able to perform as well as with all the original features</li>
<li>if the feature selection correctly removes redundant and/or useless information, the model is likely to perform better than if all all the features were provided. This is because the algorithm can get confused by the abundance of information, either because it's harder to find what is relevant among many options or because it doesn't have enough instances to properly assess the relevance of all the features.</li>
</ul>
","0","2","64377","14816"
"60585","<p>The idea of splitting 80%-20% is to use as much data as possible for training while keeping enough (labeled) data in the test set in order to reliably evaluate the performance. It's fine to use a smaller test set if it's big enough, but if it's too small the evaluation might not reflect the real performance of the model. </p>

<p>Your idea of using future test data is fine, it simply means that the reliable evaluation is delayed until you obtain enough test data. In the meantime you cannot be sure that your model performs well, so I would avoid using it for real trading on the stock exchange ;)</p>
","4","2","64377","14816"
"60586","<p>As far as I know there's no specific task for this, it's general text classification.  It's also related to <a href=""https://en.wikipedia.org/wiki/Text_segmentation"" rel=""nofollow noreferrer"">text segmentation</a>. There are certainly existing systems for similar tasks, but probably specific to a certain type of data.</p>

<p>In general this would be a supervised process, you would need a sample containing many documents in which the parts are pre-annotated. I think s<a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">equence labeling</a> algorithms would be the standard approach.</p>
","1","2","64377","14816"
"60609","<p>Assuming the dimensionality is reasonable, I would not use K-means or any generic algorithm, instead I would write a code which directly gives me the exact result by building a map of the groups:</p>

<pre><code>// Assuming data is an array of size N containing all the arrays
// clusters is a map associating each group with a set of arrays
for i=0 to N-1
  for j=i+1 to N-1
    group = overlap(data[i], data[j])
    add data[i] to the set clusters[group]
    add data[j] to the set clusters[group]
</code></pre>

<p>An alternative version if the number of different values and size of the sets allow it and/or if it's possible to precompute the groups of interest:</p>

<pre><code>for i=0 to N-1
  for every subset S of data[i]
    add data[i] to the set clusters[S] 
</code></pre>
","0","2","64377","14816"
"60615","<p>Whenever possible, you should use the average performance when comparing different methods, and preferably even mention the standard deviation across different runs (see <a href=""https://datascience.stackexchange.com/q/60549/64377"">this question</a> for an example why it's important sometimes). It's perfectly fine to <em>also</em> provide the best performance, ideally you can even present a boxplot comparison of the different methods.</p>

<p>What is really unacceptable is to compare a best performance for one method against a mean performance for the other method (it should go without saying but I remember a paper where the authors were happily doing just that in order to make their method look better).</p>
","2","2","64377","14816"
"60707","<blockquote>
  <p>So, one document is a matrix. If I want to use some traditional method like random forest to classify documents, how to use such data?</p>
</blockquote>

<p>You can't, at least not directly because traditional methods require a fixed number of features for every instance. In the case of document classification the instance must represent the document, so unless all the documents have exactly the same length (unrealistic) it's impossible to use a set of vectors as features. </p>

<p>The traditional approach would consist in representing a document with a vector where each cell represents a word in the vocabulary, and the value is for instance the TFIDF weight of the word in the document.</p>
","1","2","64377","14816"
"60712","<blockquote>
  <p>But how do I tell my model that it is actually a good news for Shell.</p>
</blockquote>

<p>You would probably need a complex semantic analysis from which you could build a knowledge graph, from which you could extract the logical implications of one news for other entities not mentioned in the news.</p>

<blockquote>
  <p>P.S. Most important, is there any labelled dataset or any other pre-trained architecture which I can use to calculate the sentiments of financial news?</p>
</blockquote>

<p>You could ask on <a href=""https://opendata.stackexchange.com/"">https://opendata.stackexchange.com/</a>.
As far as I know, really good resources (data and algorithms) for financial applications are very expensive and usually kept secret by the biggest financial institutions.</p>
","0","2","64377","14816"
"60752","<p>I don't know the method you're using but I suspect that what you observe here is a common problem with supervised learning: models tend to favour predictions close to the mean, that is avoid extreme predictions because these are usually more risky (higher loss if it's a mistake). As a consequence the std deviation of the predictions is often significantly smaller than the s.d. of the ground truth.</p>

<p>Afaik there's no perfect solution. Typically you could try to encourage risky predictions a bit more in the loss function, if that's an option with your method. But in most applications it's safer to learn to live with this issue.</p>
","1","2","64377","14816"
"60759","<p>I don't think it's a very good idea, because it boils down to training a model based on the predictions from another model. So no matter how good your model would be, the best it could achieve is the performance obtained by Google Translate. In other words, it will learn the errors made by Google Translate and add its own in the mix. But I guess it's better than nothing, if you don't have any other choice.</p>

<p>I don't know about the legal implications, but last time I checked one had to pay in order to perform a high number of translation queries automatically (beyond a certain number allowed for free).</p>
","0","2","64377","14816"
"60784","<p>I'm not aware of any specific term for this but in general I would call this either:</p>

<ul>
<li><strong>noisy data</strong>, typically in the case where these inconsistencies are due to mistakes in the annotation process (normally the proportion of mistakes is much lower than the proportion of correct instances).</li>
<li><strong>poor data design</strong>, if the way instances/features are defined doesn't match the goal of the ML process (the example provided would fall into this category). </li>
</ul>
","1","2","64377","14816"
"60823","<p>If the size is reasonable (i.e. not too many documents and not too many words in a document), you could try to build a map for each possible itemset, for instance like this:</p>

<pre><code>// Assuming data is an array of size N containing all the documents
// clusters is a map associating each itemset with a set of documents
for i=0 to N-1
  for j=i+1 to N-1
    group = overlap(data[i], data[j])
    add data[i] to the set clusters[group]
    add data[j] to the set clusters[group]
</code></pre>

<p>An alternative version if the number of different values and size of the sets allow it and/or if it's possible to precompute the itemsets of interest:</p>

<pre><code>for i=0 to N-1
  for every subset S of data[i]
    add data[i] to the set clusters[S] 
</code></pre>

<p>(adapted from <a href=""https://datascience.stackexchange.com/a/60609/64377"">https://datascience.stackexchange.com/a/60609/64377</a>)</p>
","0","2","64377","14816"
"60828","<p>You are right: decent journals/conferences are unlikely to accept a paper without a proper evaluation. Moreover the model is handcrafted, which probably means that it's hardly reproducible, right? And also that you can't do cross-validation I guess?</p>

<p>I think one would need pretty strong arguments to justify a contribution which has no scientific validation and cannot be reproduced. The only ways I can think of would be:</p>

<ul>
<li>if the method solves a problem never solved before (or has specific characteristics which make it likely to solve such problem in the future)</li>
<li>if some sort of qualitative analysis demonstrates that the method is much better than state of the art approaches</li>
</ul>
","1","2","64377","14816"
"60852","<p>Since what you want to do is to apply a pre-trained model and use the predictions on the client side, you have two options:</p>

<ol>
<li>Client-side application of the model: the extension would have to download the model file and store it, then it would need to compute the predictions from this model. This would probably require the SVM library used for the training to be available on the client side, this might be an issue. </li>
<li>Server-side application of the model: the extension would have to upload the data to the server, then the server computes the predictions using the model, and sends the predictions back to the client.</li>
</ol>

<p>Usually option 1 is preferable since it doesn't require transfering the input data to the server and then sending back the predictions. It's also safer with respect to the user privacy, their data is not uploaded anywhere. However it requires the client to be able to compute the predictions, so probably requiring the SVM library to be installed and the extension must be able to communicate with it, so it might not be feasible.</p>
","1","2","64377","14816"
"60853","<blockquote>
  <p>If the breeding at one point doesn't result into ""new"" children/chromosomes, the algorithm will run very inefficiently. Assume that all individuals in the population have the same chromosome, then crossovers will result into the same individuals until a mutation occurs in one of the chromosomes</p>
</blockquote>

<p>This is correct, but in theory this case should happen only when the algorithm has already converged, meaning that the chromosome is actually optimal. If this happens by chance before reaching an optimal solution, then it's probably because the size of the population is too low and should be higher in order to maintain diversity and allow mutations from time to time.</p>

<p>For example, if the population size is 100 and the mutation rate is 0.02 then there are in average two mutations for a particular gene at every generation. This maintains a bit of diversity, as the mutated gene is likely to be reproduced at the next generation if it turns out to be beneficial.</p>

<p>My experience with genetic algorithms is that many variants are possible and imho it's a good idea to explore various variants with a dataset, including non-standard parameters such as this idea of ""automatically mutating"", because why not? The danger I can imagine with that is that this could cause too much variation and prevent or slow down the convergence, but testing is the best way to see what happens.</p>
","1","2","64377","14816"
"60925","<p>It's a good sanity check, but the fact it fails means there must be a mistake somewhere:</p>

<ul>
<li><p>Hypothesis 1: mistake in the interpretation of the results. Is the performance improvement significant? If not, it might simply be due to chance. That would mean that none of the models actually uses the random feature, one happens to be slightly better by chance. However the chances that this would happen with 6 distinct classifiers are very low (<span class=""math-container"">$1/2^6$</span> to be precise)</p></li>
<li><p>Hypothesis 2: mistake in the generation of the the random feature. If it actually contributes to the prediction, then there must be a regular pattern so it's not truly random. Manually inspect the models (especially the decision tree one, it's the easiest to interpret) to see what happens. Then try to reproduce the result with a different random sequence and observe the model again: if the model uses the random feature in the same way then it's not really random.</p></li>
<li><p>Hypothesis 3: mistake in the split between training and test set. We always underestimate how easy it is to make a stupid mistake, so my money is on this one ;) More seriously, the fact that the performance is better with the random feature points to this direction: assuming it's truly random and it has mostly distinct values, an overfit model could use it as an id for an instance that was seen in the training set. Try re-sampling and/or cross-validation.</p></li>
</ul>
","3","2","64377","14816"
"60928","<p><em>Important disclaimer: I'm not a statistician and I'm not sure about my interpretation!</em></p>

<p>I also thought at first about the duplicates, but I think the problem might be with this assumption:</p>

<blockquote>
  <p>In my head, the training should be the data points 𝑇, while the test should be 𝑓 (as in 𝑓 is the expectation of the test data). </p>
</blockquote>

<p>Specifically the last part: in principle the test set is made of points from the same distribution as the training data, with the same risk of noise. In other words, the test set <span class=""math-container"">$t$</span> is similar to <span class=""math-container"">$T$</span>: <span class=""math-container"">$t=\{(x_i,g(x_i)):i=1,\dots,m\}$</span> (and not  <span class=""math-container"">$t=\{(x_i,f(x_i)):i=1,\dots,m\}$</span>). </p>

<blockquote>
  <p>If 'test' here means the data 𝑇, we must conclude that 𝑇 contains 𝑥 duplicates: otherwise we could fit a polynomial of degree 𝑛+1 through all the test points and get this to zero.</p>
</blockquote>

<p>Importantly, the test set <span class=""math-container"">$t$</span> is different from the training set <span class=""math-container"">$T$</span>, and the estimated function <span class=""math-container"">$\hat{f}$</span> is based only on the points in <span class=""math-container"">$T$</span>. So this way it makes sense that even a perfect estimate <span class=""math-container"">$\hat{f}=f$</span> might not be able to predict the true (noisy) value for every point <span class=""math-container"">$x\in t$</span>. That could explain the non-zero minimum test MSE.</p>
","1","2","64377","14816"
"60947","<p>Just to add a bit of context to Michael's answer: empirical is a more general term which describes a scientific approach based on observations/experiences, as opposed to theory. </p>

<p>The empirical approach exists for a long time and can be used in many different contexts, including when the process is manual and the observations are not very formal (e.g. feelings in psychology).</p>

<p>Data-driven is a recent term which usually refers to Machine Learning (although not exclusively), i.e. using a formal set of observations in order to build a representation of a population of interest (usually through automated methods). So data-driven can be seen as a formal, usually automatic version of the empirical approach.</p>
","1","2","64377","14816"
"60978","<p>It's not DL but I suggest you start with the following approach: for every question <span class=""math-container"">$Q$</span> and its set of candidate answers <span class=""math-container"">$(A_1,..,A_{10})$</span>, represent each pair <span class=""math-container"">$(Q,A_i)$</span> as an instance with its label 0 or 1. You could start with a few simple features such as:</p>

<ul>
<li>number of words in common</li>
<li>similarity score, e.g. cosine TFIDF</li>
<li>... other indicators of how well question <span class=""math-container"">$Q$</span> and answer <span class=""math-container"">$A_i$</span> match</li>
</ul>

<p>Train a regression model on this (e.g. decision tree, SVM,...). When the model is applied to a new question+answers, it returns a score (mostly between 0 and 1) for each of the 10 answers; finally select the answer which has the highest score.  </p>

<p>You can certainly improve on this idea, e.g. with sentence embeddings.</p>

<p>Note: a simple baseline system based on the same idea would be to select the answer which has the maximum number of words in common with the question.  </p>
","0","2","64377","14816"
"60979","<p>It's indeed a very specific type of relation extraction. Generally relation extraction is much more complex because it's not only about simple sentences <em>Subject Verb Object</em> and not only with the verb <em>is</em>. </p>

<p>It's not clear to me if your examples are representative of the real cases you're dealing with: if yes, you probably don't need full blown relation extraction, basic pattern matching rules will do the trick. </p>
","1","2","64377","14816"
"61020","<p>First, when we as humans interpret <em>""Fyonair is from Fuabalada land""</em> we use our knowledge of <em>""is from""</em> and <em>""X land""</em> to infer that <em>Fyonair</em> is probably a person and <em>Fuabalada land</em> probably a location. Therefore our process is not (at least not completely) unsupervised: we have seen this sentence structure before (""training"") and we use our ""model"" to ""predict"" the two entities. </p>

<p>A good (supervised) NER is trained to recognize this kind of patterns as well. The example that you mention might be a bit too hard, but with something like <em>""Dr Fyonair is the new CEO of Fubalada""</em> a decent NER should be able to recognize that <em>""Dr Fyonair""</em> is a person (thanks to the <em>""Dr""</em>) and that <em>""Fubalada""</em> is a company (thanks to the <em>""CEO of""</em>) even though it has never seen these particular proper names.</p>

<p>So a standard (supervised) NER is supposed to recognize entities it has never seen before, provided it has clues in the sentence about them. Technically it's not really a state of the art NER if it recognizes only the entities that it has seen during training, it's just a string matching program. It's true that NERs make a lot of errors with entities which haven't been seen previously, but that's simply because they are harder to catch in general.</p>
","1","2","64377","14816"
"61094","<p>It's not totally clear to me what is your main question but here are a few points which might help you:</p>

<ul>
<li>Like any regular classifier, a decision tree classifier does not transform the features. It's up to the user if they want to apply any transformation before calling the classifier.</li>
<li>Standard decision tree classifiers can deal with features which have multiple values.</li>
</ul>

<p>Afaik training a decision tree model using a library classifier is not going to help you much understanding how the model is calculated, it's just going to give you the result. You could try to code your own implementation if you want.</p>
","1","2","64377","14816"
"61119","<p>You forgot a few <a href=""https://en.wikipedia.org/wiki/W-shingling"" rel=""nofollow noreferrer"">2-shingles</a> (bigrams but without duplicates) in the second set but  you got the idea right:</p>

<p><span class=""math-container"">$S_1$</span> = { ""the quick"", ""quick brown"", ""brown fox"", ""fox jumps"", ""jumps over"", ""over the"", ""the lazy"", ""lazy dog"" }</p>

<p><span class=""math-container"">$S_2$</span> = { ""jeff typed"", ""typed the"", ""the quick"", ""quick brown"", ""brown dog"", ""dog jumps"", ""jumps over"", ""over the"", ""the lazy"", ""lazy fox"", ""fox by"", ""by mistake"" }</p>

<p>Remark: For this particular example, in each of these two sets every sequence of 2 words appears only once, so there's no need to remove duplicates to obtain the set. In the general case this might be necessary (see <a href=""https://en.wikipedia.org/wiki/W-shingling"" rel=""nofollow noreferrer"">the Wikipedia example</a>). </p>

<p>To calculate Jaccard similarity we need to count:</p>

<ul>
<li>The intersection <span class=""math-container"">$S_1 \cap S_2$</span>, i.e. the 2-shingles in common:  | { ""the quick"", ""quick brown"", ""jumps over"", ""over the"", ""the lazy"" } | = 5</li>
<li>The union <span class=""math-container"">$S_1 \cup S_2$</span>, i.e. all the distinct 2-shingles: | { ""the quick"", ""quick brown"", ""brown fox"", ""fox jumps"", ""jumps over"", ""over the"", ""the lazy"", ""lazy dog"", ""jeff typed"", ""typed the"", ""brown dog"", ""dog jumps"", ""lazy fox"", ""fox by"", ""by mistake"" } = 15</li>
</ul>

<p>Jaccard similarity:</p>

<p><span class=""math-container"">$$\frac{S_1 \cap S_2}{S_1 \cup S_2} = \frac{5}{15} = 0.33$$</span></p>
","1","2","64377","14816"
"61120","<p>The standard option would be to create one feature for each possible pair (variable, category) and use the frequency of this particular pair as value for this particular customer. If the number of times doesn't matter, it could be transformed into a binary feature, i.e. just indicating whether this customer has ever been seen with this category.</p>

<p>In case some categories appear very rarely, it's usually counter-productive to include them so you could have a minimum frequency in order to filter out rare pairs.</p>
","0","2","64377","14816"
"61121","<p>You don't give a lot of details about what you want to do so I'm going to say basic things... hopefully this helps:</p>

<ol>
<li>Check and clean up the data: if you have columns which contain mostly NaN values they're likely useless, so you can discard them. You can also ditch any feature which always contains the same value. </li>
<li>Check the correlation between features: you might have some features which are redundant with each other, remove the ones which are less likely to be informative.</li>
<li>Work with a small subset (rows) first, analyze it and implement a pipeline using this subset. Check from time to time that your pipeline can handle the full dataset, but it's likely that a subset is enough for most of the development. If possible use a few different subsets to cover more cases.</li>
</ol>

<p>Needless to say, keep a backup of the original data ;)</p>
","1","2","64377","14816"
"61150","<p>Data science jobs cover a wide range of different activities so any answer is likely to be subjective. I'm in academia so my knowledge of the job market is limited, but from what I can see:</p>

<ul>
<li>The current context is very favorable to data scientists looking for a job, so anybody with some basic knowledge of ML has a chance. You're already above this level so no worries on this front imho. The trend is reasonably likely to continue like this, but nobody knows the future.</li>
<li>Ideally to increase your chances you should be able to demonstrate that you have some hands-on experience: for instance github projects, participation to a ML competition, this kind of thing.</li>
<li>Don't neglect your maths PhD: you seem to have a lot of free time to learn data science, good for you... But make sure the PhD is your top priority, it's going to be a great asset on your resume even if it's not related to data science. </li>
</ul>
","11","2","64377","14816"
"61207","<blockquote>
  <p>What is the intuition behind larger number of samples are better for statistics?</p>
</blockquote>

<p>This is because the larger the sample, the most likely it is to be a faithful representation of the full population. Formally this is a consequence of the <a href=""https://en.wikipedia.org/wiki/Law_of_large_numbers"" rel=""nofollow noreferrer"">law of large numbers</a>:</p>

<blockquote>
  <p>In probability theory, the law of large numbers (LLN) is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed. </p>
</blockquote>
","0","2","64377","14816"
"61214","<blockquote>
  <p>Is there any good way to estimate the general training speeds of computer hardware?</p>
</blockquote>

<p>Probably not in general, because ""training speed"" means something completely different if you train  a kNN model (virtually no training) or a complex DL model, what kind of data and what kind of task.</p>

<p>However estimations exist for fixed settings, for instance I found <a href=""https://mlperf.org/training-results-0-6/"" rel=""nofollow noreferrer"">this MLPerf Benhchmark</a> or <a href=""https://arxiv.org/pdf/1708.08670.pdf"" rel=""nofollow noreferrer"">this paper</a>. </p>

<p>Btw desktop computers are not ideal for very computer-intensive ML, cluster or cloud computing offer better options.</p>
","1","2","64377","14816"
"61686","<p>Distributional semantics models are based on representing an instance with the words that represent its meaning. Typically if one is interested in representing the meaning of a word <span class=""math-container"">$w_i$</span> then the words that represent its meaning are the ones which appear close to it, e.g. withing a window of N words preceding/following it. </p>

<p>Example where <span class=""math-container"">$w_i$</span> is the target word with a window +/- 2:</p>

<p><span class=""math-container"">$w_1,w_2,...,w_{i-3},\mathbf{w_{i-2},w_{i-1}},w_{i},\mathbf{w_{i+1},w_{i+2}},w_{i+3},...,w_N$</span></p>

<p>So the ""meaning"" of <span class=""math-container"">$w_i$</span> in this instance would be represented by the four words in the context window: <span class=""math-container"">$w_{i-2},w_{i-1},w_{i+1},w_{i+2}$</span>. By collecting occurrences of <span class=""math-container"">$w_i$</span> everywhere in the document one obtains a set of contexts that can be used in different ways. For example one could build a context vector <span class=""math-container"">$a_1,...a_{|V|}$</span> over the vocabulary <span class=""math-container"">$V$</span> where each cell <span class=""math-container"">$a_k$</span> contains the frequency of <span class=""math-container"">$w_k \in V$</span> in the context of the target word. By applying the same process with different words, each word can be represented with its context vector and then various operations can be made on these vectors: similarity measures, clustering, etc.</p>

<p>Generally clustering based on words semantics is done with specific methods such as <a href=""https://en.wikipedia.org/wiki/Latent_semantic_analysis"" rel=""nofollow noreferrer"">Latent Semantics Analysis</a>.</p>

<p>In theory the method can be applied to any unit (e.g. n-grams), but more complex units require more data, more memory and more computing power.</p>
","1","2","64377","14816"
"61687","<p>Your model is <a href=""https://en.wikipedia.org/wiki/Overfitting"" rel=""nofollow noreferrer"">overfit</a>: it learns patterns on the training data which are specific to the training data, hence the big difference in performance on the test data. This is bad, because it means that your model wrongly assumes that some patterns are good indication for the class even though it's just noise. </p>

<p>It's possible that by solving the overfitting problem the performance would improve. However nobody can give you any advice without knowing anything about your actual problem and data. Btw you don't explain why you use accuracy as evaluation measure, it might or might not be optimal for your problem. </p>
","1","2","64377","14816"
"61790","<p>First a warning: is your data anonymized? Even if it is, be extremely careful because medical history is super-sensitive personal information. There are legal requirements about how to handle this kind of data and what you can or can't do with it.</p>

<p>As far as I know there's no python API for cTakes or Metamap. Anyway I guess that such an API would boil down to a system call, so it wouldn't be very good.</p>

<p>As an alternative you could implement your own system using the <a href=""https://www.nlm.nih.gov/research/umls/index.html"" rel=""nofollow noreferrer"">UMLS Metathesaurus</a>, which is a massive list of medical terms grouped by concept (MetaMap extracts UMLS terms/concepts).</p>
","0","2","64377","14816"
"61834","<p>Based on the details given in the comments, it looks like what you need is a binary classifier which predicts whether or not to remove a ""numerical point"". This means that each instance is an individual ""numerical point"", and you would need to annotate a sample of instances to use as a training set.</p>

<p>A simple option is to use a bag of words representation, i.e. represent every instance as a vector over the vocabulary where each cell contains the frequency of the corresponding word (or other variants, e.g. binary or TF-IDF weights). Then many classification algorithms can be used: decision trees, SVM, Naive Bayes, etc.</p>
","1","2","64377","14816"
"61884","<p>As you already understand, the vast majority of the data science work is made with rather high level languages such as Python and R. So it's not a matter of prevalence, it's a matter of which part in the big world of data science you want/can do with your skills and your tools.</p>

<p>Imho inventing new models requires:</p>

<ul>
<li>strong theoretical background in maths and statistics, in-depth knowledge of existing estimation/inference methods </li>
<li>good understanding of computational complexity and (preferably) algorithmic optimization methods.</li>
</ul>

<p>If additionally you implement your models yourself (that's not necessarily the case), that's where you probably need to know low level languages such as C/C++, because computational efficiency is crucial when people are going to use the model with massive datasets which require a lot of computation.</p>
","6","2","64377","14816"
"61923","<blockquote>
  <p>Would that be called as stacked modelling? </p>
</blockquote>

<p>Yes, this is exactly what <a href=""https://en.wikipedia.org/wiki/Ensemble_learning#Stacking"" rel=""nofollow noreferrer"">stacking models</a> means.</p>

<p>I didn't understand what is the role of clustering in your design? </p>

<p>The standard approach consists in using only the predictions of the <span class=""math-container"">$N$</span> individual learners as feature for the meta-model. The training/testing data split is a bit more complex: the training of the meta-model requires predictions from the individual models, so the data could be split like this:</p>

<ul>
<li>training data for individual models</li>
<li>training data for the meta-model (on which individual models are applied)</li>
<li>testing data for the full system </li>
</ul>

<p>Generally it's safer to choose a simple learning algorithm for the meta-model, because there is a higher risk of overfitting (for instance linear regression or majority voting).</p>

<hr>

<p>[Detailed answer to comments below]</p>

<p>The standard setting would be like this:</p>

<ol>
<li>Train <span class=""math-container"">$N$</span> individual models <span class=""math-container"">$m_1,...,m_N$</span> using first-level training set <span class=""math-container"">$T_1$</span></li>
<li>Apply these models in order to obtain <span class=""math-container"">$N$</span> features for the meta-classifier. At this stage it's a bad idea to use instances from <span class=""math-container"">$T_1$</span>, because the individual classifiers have been trained on <span class=""math-container"">$T_1$</span>. so one needs a second set of instances <span class=""math-container"">$T_2$</span>: for every instance <span class=""math-container"">$x\in T_2$</span>, let <span class=""math-container"">$m_1(x),...,m_N(x)$</span> be the predictions resulting from applying classifiers <span class=""math-container"">$m_1,...,m_N$</span>.</li>
<li>Train the meta-classifier model <span class=""math-container"">$M$</span> using <span class=""math-container"">$T_2$</span> as training set, with <span class=""math-container"">$m_1(x),...,m_N(x)$</span> the features for any <span class=""math-container"">$x\in T_2$</span>. </li>
<li>Testing: as usual one needs a fresh set of instances, say <span class=""math-container"">$T_3$</span>. For any instance <span class=""math-container"">$x\in T_3$</span> calculate the prediction by applying <span class=""math-container"">$M$</span> to the <span class=""math-container"">$N$</span> predictions obtained by the individual models, i.e. <span class=""math-container"">$M(m_1(x),...,m_N(x))$</span></li>
</ol>
","3","2","64377","14816"
"61925","<blockquote>
  <p>What are the best practices you apply? What would you recommend?</p>
</blockquote>

<p>My practice is ""kind of structured but quite different every time"", and I don't recommend it ;) </p>

<p>I suspect that I'm not the only one but I don't have any stats.</p>

<p>Thanks for the link to Cookiecutter, this looks interesting! After reading a bit about it, it looks to me like a key criterion is this: <a href=""https://drivendata.github.io/cookiecutter-data-science/#data-is-immutable"" rel=""nofollow noreferrer"">""anyone should be able to reproduce the final products with only the code in src and the data in data/raw.""</a></p>

<p>Based on that I would argue that:</p>

<blockquote>
  <ol start=""2"">
  <li>You have acces to SQL databases and make a no-modification extract -> Still into Raw I guess?</li>
  </ol>
</blockquote>

<p>Yep, because this data cannot be obtained by processing some other part of the raw data.</p>

<blockquote>
  <ol start=""3"">
  <li>Because of very large databases, you create a semi-complex SQL query as base for a feature -> Is this Raw or Interim?</li>
  </ol>
</blockquote>

<p>Assuming this query is run on the raw sql database, the resulting data goes to <code>interim</code>, because it can be re-generated by running the query again.
The query itself should be stored in <code>src/features</code> I think.</p>
","1","2","64377","14816"
"61929","<p>This depends on the exact learning algorithm which is used, but most of the time numerical values are treated... numerically. </p>

<p>It's easy to see with the case of linear regression: let's say a feature has values between 0 and 100, but with a few missing values. Now replace missing values with -999: the coefficient learned for this feature will be completely different and based on  meaningless indications, so the performance will decrease.</p>

<p>This idea might work in the case where the numerical values are 'binned' into several classes, i.e. transformed into ordinal/categorical variables.</p>

<p>But the best way to use such values is to use an algorithm and framework which can deal with missing values in the first place.</p>
","2","2","64377","14816"
"62050","<p>It's very common to have some amount of errors or inconsistencies in a dataset. Sometimes these inconsistencies are not even errors, in some subjective tasks (e.g. translation), annotators may simply not agree on what is the best answer.</p>

<p>What to do with this kind of noise completely depends on the case at hand. If the noise caused by these errors represents a reasonably small proportion of the data, it can safely be ignored: in this case it's up to the learning algorithm to distinguish the relevant patterns from the noise. Otherwise there can be ad-hoc pre-processing implemented to clean up the data. In cases where the subjectivity of the annotator plays an important role, it's useful to have several annotators annotated the same data and check the <a href=""https://en.wikipedia.org/wiki/Inter-rater_reliability"" rel=""nofollow noreferrer"">inter-annotator agreement</a>. This might in turn be used to filter out the least consensual instances, or aggregate the annotations in some way (e.g. majority voting).</p>
","0","2","64377","14816"
"62060","<p>The <a href=""https://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf"" rel=""nofollow noreferrer"">original paper</a> mentions two corpora: CoNLL 2003 (<a href=""https://www.clips.uantwerpen.be/conll2003/ner/"" rel=""nofollow noreferrer"">apparently here now</a>) and the ""CMU Seminar Announcements Task"". However according to the page linked in the question the actual NER was trained on a larger combination of corpora:</p>

<blockquote>
  <p>Our big English NER models were trained on a mixture of CoNLL, MUC-6, MUC-7 and ACE named entity corpora, and as a result the models are fairly robust across domains. </p>
</blockquote>

<p>So it might be difficult to obtain the <em>exact</em> original training data. However most of these corpora were compiled for some shared tasks and should be available online. There are probably more recent ones as well: a quick search ""named entity recognition shared task"" returns many hits. </p>
","3","2","64377","14816"
"62145","<p>As far as I know text anonymization is mostly considered a manual pre-processing step, I'm not aware of any reliable fully automated method. The reliability of the process is usually crucial for legal and ethical reasons, that's why there must be some amount of manual work.</p>

<p>That being said, the process can be made semi-automatically, especially if the scope of the information to be obfuscated is not too large. In your case a <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">NE tagger</a> could probably be applied to capture a large part of the entities. </p>

<p>Once all the entities have been annotated in the original data, it's straightforward to replace them automatically with a placeholder. This can be done while keeping the original and anonymized version aligned (typically using a unique id for every entity).</p>
","2","2","64377","14816"
"62319","<p>I don't use Python so I can't tell you exactly what is going on but I had a quick look at your data:</p>

<p><a href=""https://i.stack.imgur.com/GsuWa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GsuWa.png"" alt=""data points showing relevant features""></a></p>

<p>A few remarks:</p>

<ul>
<li>it looks like the vast majority of the points are created artificially by interpolation. Why not, but that's unlikely to reflect the reality of the price changes: I would expect much more variation/noise in a real dataset about car prices.</li>
<li>there's no need to add so many points to the dataset anyway. With all this data there's not even a need to train a model, since virtually every possible instance is already in the data. </li>
<li>It seems to me that there's something weird with the prediction year: normally the higher the prediction year the more the price decreases right? here year 0 has no decrease at all, year 1 has the highest decrease,..., and year 4 has little to no decrease. That could confuse the model.</li>
<li>because it's mostly an artificial dataset the relation is very simple: a basic regression <code>future_price = past_price * a + b</code> would already give quite good results, and the relation can be learned perfectly when adding the <code>prediction_year</code> feature. At least MLP and random forest should give near perfect results.</li>
<li>From a quick look at the code, I suspect that the problem has to do with the scaling. I'm not sure what is supposed to happen there since I'm not familiar with these functions: it might be that the predicted values need to be ""un-scaled"" at the end maybe? Anyway I don't think scaling it's needed at all here.</li>
</ul>

<hr>

<p>For the record the graph was done with R like this:</p>

<pre><code>library(ggplot2)
d&lt;-read.table('interpolated_dataframe.csv',sep=',',header=TRUE)
ggplot(d,aes(past_price,future_price,colour=factor(prediction_year)))+geom_point()
</code></pre>
","3","2","64377","14816"
"62372","<p>As far as I know, experts systems were popular around the 80s-90s, before the big trend of Machine Learning. They were mostly based on symbolic logic reasoning, as opposed to statistics in ML.</p>

<p>They can probably be considered as one the first stages of ML-based systems: experts systems automate the reasoning steps based on some data, which is a bit similar to ML. However the data was strongly structured (and possibly quite complex) and usually had to be entered manually by an expert. This is the main reason why ML approaches made them obsolete for the most part: ML systems can learn from a vast amount of possibly noisy features with little structure, whereas the cost of preparing the data for experts systems was prohibitive.</p>

<p>Experts systems were indeed a kind of knowledge-based system (probably the first generation), but I think the latter are much more general: knowledge-based system include many different kinds of data base and different methods for processing it.</p>

<p>Note: this answer is based on my general understanding and knowledge of these terms, which is probably correct but not sure ;)</p>
","2","2","64377","14816"
"62377","<p>It depends, there can be different reasons to use CV: </p>

<ul>
<li>if you're using it to evaluate your model only, then it's fine to use the whole training set.</li>
<li>If you're using it to evaluate multiple models and then select the one which performs best (e.g. tuning hyper-parameters), then you shouldn't use the whole training set as you still need to evaluate the result of this tuning step on some unseen data.</li>
</ul>
","1","2","64377","14816"
"62418","<p>One of the main strengths of DL methods is that they can work from raw data, and often perform better this way than traditional methods with carefully crafted features. So it's indeed very tempting to consider traditional feature engineering as obsolete, since it requires more work and often results in lower performance.</p>

<p>However one should be careful before discarding feature engineering in this way:</p>

<ul>
<li>First, as scientists we should be wary of the dynamic nature of technological trends. For example very few ML experts would have bet on neural nets 15-20 years ago as the next big thing. We should take stock of the evolution of ML methods, not blindly adopt the latest technology.</li>
<li>DL methods are computationally expensive and usually require a large amount of data. There are still plenty of applications/problems where more lightweight traditional methods are a better fit.</li>
<li>DL methods are by nature less open to interpreting their results. Interpretability/explainability is already an important issue and is likely to become even more important as applications of ML meet real-life problems: ethical issues (what if a ML system is racist?), legal issues (why did a ML system make a bad decision and who is responsible?). By contrast, some statistical methods such as decision trees offer a very clear explanation of their decisions.</li>
<li>In some cases leaving feature engineering to DL is suboptimal. There have been a few results (in NLP as far as I'm aware) showing that on some specific problems carefully crafted features performs better than DL. I don't know if these are significant or just exceptions to the rule. Subjective interpretation: there might be a risk of ""design laziness"", i.e. counting on DL to do the job instead of properly understanding and structuring the problem.</li>
</ul>
","2","2","64377","14816"
"62442","<blockquote>
  <p>My question is is there a way to aggregate all the frequency distributions for all the fitness values among the 30 runs? so that I can only show a single plot for all the runs.</p>
</blockquote>

<p>Yes absolutely, you can do that with <a href=""https://ggplot2.tidyverse.org/reference/geom_boxplot.html"" rel=""nofollow noreferrer"">boxplots</a> or <a href=""https://ggplot2.tidyverse.org/reference/geom_violin.html"" rel=""nofollow noreferrer"">violin plots</a>, one boxplot (or violin plot) for each fitness value. The height and shape shows the distribution of the values across runs for one fitness value, and allows visual comparison between the different fitness values. </p>
","2","2","64377","14816"
"62505","<p>This problem is related to the following standard problems:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow noreferrer"">topic classification/modeling</a>, which ranges from simple supervised document classification to unsupervised assignment of topics distributions to every document (the advanced option, with Latent Dirichlet Analysis and variants) </li>
<li><a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">sequence labeling</a>, a supervised task which predicts classes for every instance in a sequence, taking into account the order of the instances (e.g. it can leverage the fact that the class of instance <span class=""math-container"">$n$</span> is influenced by the class of instance <span class=""math-container"">$n-1$</span>). </li>
<li><a href=""https://en.wikipedia.org/wiki/Text_segmentation"" rel=""nofollow noreferrer"">text segmentation</a>, more precisely topic segmentation in this case.</li>
</ul>

<p>I think that one can find a lot of good implementations for the first two problems, which are very common. However adapting these to your case and/or combining them will probably be more complex.</p>

<p>If you have a small set of topics and you have (or can have) a reasonable sample of  messages annotated with these topics, I would suggest starting with sequence labeling with <a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">Conditional Random Fields</a>. I think this could give good results given the sequential nature of chat conversations.</p>
","1","2","64377","14816"
"62506","<p>You have textual descriptions, i.e. unstructured data. So you should probably use one of the standard representation methods for text. There are many options including sentence embeddings and this kind of advanced methods, but I'm going to describe the simple traditional option:</p>

<ul>
<li>Each description value can be represented as a vector of feature, one for each word in the full vocabulary (i.e. over all the values for this field).</li>
<li>The value of each feature can be boolean (i.e. does this word appear in this description) or better a TF-IDF weight for the word.</li>
<li>Obviously this would lead to too many features, so one needs to apply select the most relevant ones. This part is very experimental, you might have to try various options to find the right one:

<ul>
<li>Get rid of the <a href=""https://en.wikipedia.org/wiki/Stop_words"" rel=""nofollow noreferrer"">stop words</a> since they provide no semantic information.</li>
<li>Discard all the words which appear only once, and probably also all the words which appear less than some minimum frequency <span class=""math-container"">$N$</span> (try with $N=2,3,4...). The rationale is that rare words are more likely to cause overfitting than to really help any kind of classification.</li>
<li>Beyond that, you could use general feature selection (e.g. information gain) or feature clustering. </li>
</ul></li>
</ul>

<hr>

<p>[ obsolete answer to the first version of the question ]</p>

<p>I would definitely try to normalize these values, because semantically they are numerical and in their original form they are almost useless, no matter the method to categorize them. Making them categorical loses a lot of information, especially the ones which actually provide a number. Since some of the strings used are very vague I would probably try using intervals, i.e. two numeric values for every original input value:</p>

<ul>
<li>three and a half inches, three and 1/2 inches -> min 3.4 - max 3.6</li>
<li>27.6234 inches -> min 27.6234 - max 27.6234</li>
<li>tall -> large range high values </li>
<li>short, kinda short -> large range low values</li>
</ul>

<p>Normally there are not that many ways to give numbers as words, only a few patterns should be enough to capture all the variants. For the other non-standard cases such as ""kinda short"" I would start by looking in the data their distribution: if a value is frequent enough (e.g. probably ""short"", ""tall"") then manually predefined range. Values which are not frequent can be ignored, e.g. replaced with NA (since they are not frequent that shouldn't affect the data too much).</p>
","0","2","64377","14816"
"62545","<p>Text data is at the same time:</p>

<ul>
<li>very structured, because swapping only a few words in a sentence can make it complete gibberish, </li>
<li>and very flexible, because there are usually many ways to express the same idea in a sentence.</li>
</ul>

<p>As a consequence, it's very hard to have a text sample which is representative enough of a ""population"" text, i.e. which covers enough cases for all the possible inputs. But augmentation methods are practically sure to fail, because either they are going to make the text gibberish or just cover minor variations which don't improve the coverage significantly.</p>

<p>That's why a lot of the work in NLP is about experimental design and preprocessing.</p>
","1","2","64377","14816"
"62555","<p>In principle this seems close to a <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">NER</a> task, you could try to annotate a sample and train a <a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">sequence labeling</a> model on it. However this would require quite a lot of work to get it right: annotation, then probably a good bit of trial and error to tune the right combination of features.</p>

<p>In a case like this I would rather go for a few carefully chosen regular expressions, they are likely to perform at about the same level without requiring as much work. </p>
","3","2","64377","14816"
"62567","<p>Sorry, but on the surface, this sounds like a terrible idea to me: if linear regression gives you negative coefficients for some explanatory variables that you think should be positive, then it means that either your data is ""wrong"" (typically noisy or too small) or your intuition is misguided. </p>

<p>I can't see any good reason why one would use a data-driven approach if the goal is to manually force the model in a particular way. This is the equivalent of breaking the thermometer to hide the fever.</p>

<p>I'd suggest the following instead:</p>

<ol>
<li>In general an unexpected outcome is arguably a good thing, in the sense that it tells us something we didn't know about the data. That's a cue to investigate what happens in the data. Linear regression is simple enough to analyze: one can look at the correlation, plot the relation between the variables etc.</li>
<li>If there's really something suspicious going on with some variables, maybe some errors in the data which make them behave in a way they shouldn't, then it's much better to discard them altogether from the model rather than fixing their coefficient, because this way the model won't rely on them at all.</li>
</ol>
","3","2","64377","14816"
"62568","<p>I'm not familiar with scikit but I'm assuming that TfidfVectorizer represents bag of words features right? By this I mean that it treats all the instructions in an instance as a set, i.e. doesn't take into account their sequential order.</p>

<p>I'm also not familiar with compilers but I'm guessing that the order of the instructions could be a relevant indication? I.e. a compiler may generate particular sequences of instructions.</p>

<p>Based on these remarks I would try to represent instances with <a href=""https://en.wikipedia.org/wiki/N-gram"" rel=""nofollow noreferrer"">n-grams</a> of instructions rather than individual instructions. Then you can still use some kind of bag-of-ngrams representation, possibly with TFIDF, but I would start with simple binary or frequency features. A simple feature selection step with something like information gain might be useful.</p>

<p>[edit] N-grams take order into account locally. In a bag of words model, words (or instructions in your case) are considered individually of each other: for instance the sequence <code>push, push, mov</code> is the same as <code>push, mov, push</code>. With bigrams this sequence would be represented as <code>(push,push), (push,mov)</code> whereas the second one is <code>(push,mov), (mov,push)</code>. This means two things:</p>

<ul>
<li>Higher level of detail about the instance, which can help the model capture the relevant indications</li>
<li>More features so higher risk of overfitting (the model taking some random details as indication, which lead to errors on the test set).</li>
</ul>
","0","2","64377","14816"
"62588","<p>In the general case this is impossible because standard learning algorithms require all the instances at once in order to compute the model. They usually work by making calculations based on the set of instances, then only the results of these calculations are stored as a model (for instance a linear regression classifier stores only the final weights which are needed to make a prediction). </p>

<p>It is however possible to train a model incrementally:</p>

<ul>
<li>Very often this is done by simply adding the new instances to the original training set, then re-train a new model on the full set of instances (new and old ones together).</li>
<li>There are also actual <a href=""https://en.wikipedia.org/wiki/Incremental_learning"" rel=""noreferrer"">incremental learning</a> algorithms, including adaptations of some common learning methods. However this is not very common, I'm not aware of these being implemented in the most standard ML libraries that I know of.</li>
</ul>
","5","2","64377","14816"
"62682","<p>I would approach such a problem with a method a bit similar to <a href=""https://en.wikipedia.org/wiki/Record_linkage"" rel=""nofollow noreferrer"">record linkage</a>: trying to match every product description with the most relevant HTS description. The traditional approach would be to use textual similarity measures such as cosine TF-IDF, but many variants can also be considered, e.g. with embeddings or other ways to take semantic similarity into account. At the end the code corresponding to the most similar HTS description is predicted.</p>

<p>Initially this could be done with vectors of words (unigrams), but it's true that comparing vectors of <span class=""math-container"">$n$</span>-grams is likely to be more precise. However <span class=""math-container"">$n$</span>-grams don't work the way you describe: in a case like your example you would have for instance <span class=""math-container"">$n=2$</span> and extract all the sequences of 2 consecutive words. This ""bag of <span class=""math-container"">$n$</span>-grams"" is what the vector represents. Combining different lengths of <span class=""math-container"">$n$</span>-grams is possible but not in the same representation: for example you could measure cosine similarity over 2-grams vectors only, then measure cosine similarity of 3-grams vectors only, and take the mean of the two scores (or even build a regression model using different similarity scores as features).</p>

<p>In case it helps, <a href=""https://datascience.stackexchange.com/a/61119/64377"">here is an example</a> of computing a very simple similarity score with bigrams (one could certainly find better examples online).</p>
","1","2","64377","14816"
"62826","<p>Last time I checked it was not allowed to <strong>store</strong> the contents of the tweets, instead one is supposed to store the tweet id and retrieve the content of the tweet dynamically.</p>

<p>Afaik this is because users are allowed to delete their tweets at any time, and keeping a tweet that they chose to delete would be against Twitter terms of use (and possibly illegal in some jurisdictions). Using the tweet id solves the problem since the content will simply not be available anymore if the tweet was deleted.</p>

<p>Since you plan to write a paper I assume that you're in academia? If yes in case of doubt it's always safer to ask the data protection and/or legal office in your institution. In this case you're using a secondary source (i.e. you're collecting data which already exists) so it should be straightforward (I think).</p>
","1","2","64377","14816"
"62877","<p>High resource languages are languages for which many data resources exist, making possible the development of machine-learning based systems for these languages. English is by far the most well resourced language. West-Europe languages are quite well covered, as well as Japanese and Chinese. Naturally low-resource languages are the opposite, that is languages with none or very few resources available. Unsurprisingly many languages from Africa and other poorest parts of the world are low-resources. But this is also the case for some extinct or near-extinct languages and many local dialects. There are actually many languages which are mostly oral, for which very few written resources exist (let alone resources in electronic format); for some there are written documents but not even something as basic as a dictionary.</p>

<p>There are many different types of resources which are needed in order to train good language-based systems: </p>

<ul>
<li>a high amount of raw text from various genres (type of documents), e.g. books, scientific papers, emails, social media content, etc.</li>
<li>lexical, syntactic and semantic resources such as dictionaries, dependency tree corpora, semantic databases (e.g. WordNet), etc.</li>
<li>task-specific resources such as parallel corpora for machine translation, various kinds of annotated text (e.g. with part-of-speech tags, named entities, etc.)</li>
</ul>

<p>Many types of language resources are costly to produce, this is why the economic inequalities between countries/languages are reflected in the amount (or absence) of language resources. The <a href=""https://universaldependencies.org/"" rel=""nofollow noreferrer"">Universal Dependencies project</a> is an interesting effort to fill this gap.</p>
","4","2","64377","14816"
"62882","<p>My first thought seeing this sample of data was <a href=""https://en.wikipedia.org/wiki/Parallel_text"" rel=""nofollow noreferrer"">parallel corpus</a>, and Yohanes just confirmed that the text columns are translations from each other.</p>

<p>The main thing to do with this kind of data is to train a machine translation model :)</p>
","0","2","64377","14816"
"62904","<p>Chance plays a big role when the data is split. For example maybe the training set contains a particular combination of features, maybe it doesn't; maybe the test set contains a large proportion of regular ""easy"" instances, maybe it doesn't. As a consequence the performance varies depending on the split. </p>

<p>Let's imagine that the performance of your classifier would vary between 0.80 and 0.90:</p>

<blockquote>
  <p>In one approch I just split the dataset into training set and test set</p>
</blockquote>

<p>With this approach you throw the dice only once: maybe you're lucky and the performance will be close to 0.9, or you're not and it will be close to 0.8. </p>

<blockquote>
  <p>while with the other approch I use k fold cross validation. </p>
</blockquote>

<p>With this approach you throw the dice k times, and the performance is the average across these <span class=""math-container"">$k$</span> runs. It's more accurate than the previous one, because by averaging over several runs the performance is more likely to be close to the mean, i.e. the most common case.</p>

<p>Conclusion: k-fold cross-validation isn't meant to increase performance, it's meant to provide a more accurate measure of the performance. </p>
","2","2","64377","14816"
"62905","<p>I think you correctly identified the issue: if your model tries to classify whether an employee stays or leaves, by definition every employee ""stays"" as long as they are an employee in the company. </p>

<p>A possible direction would be to design the response variable as ""does the employee leave within a year?"" (or any specific period of time). This way you can use the records of past employees who have left and past employees who have stayed across time, i.e. you can have several instances corresponding to the same employee at different times. You could add features such as ""had a raise or promotion in the past 2 year"" for instance.</p>
","1","2","64377","14816"
"62935","<p>Neural networks are convenient for automatically building features from the data, but as far as I know this is not compatible with interpretability since we don't know what the features represent. So the only way I'm aware of to obtain predictions that can be explained and analyzed is to use traditional models, and this would certainly require a bit of feature engineering in this case. I would recommend decision trees which are completely transparent in their decision process and can be understood even by non-experts. With this kind of model the idea would be to provide the features which represent the evolution across time, e.g. min/max/average over the last N years, difference current-N, etc., possibly for several values of N.</p>
","1","2","64377","14816"
"62969","<p>I don't know if it's the best nowadays but Mallet includes a good few algorithms for probabilistic learning, in particular sequence labeling methods: <a href=""http://mallet.cs.umass.edu"" rel=""nofollow noreferrer"">http://mallet.cs.umass.edu</a></p>
","1","2","64377","14816"
"62971","<p>I doubt anything so general exists. The closest I can think of would be to exploit resources which exist in the medical domain, in particular <a href=""https://semrep.nlm.nih.gov/"" rel=""nofollow noreferrer"">https://semrep.nlm.nih.gov/</a></p>

<p>Also try asking on <a href=""https://opendata.stackexchange.com/"">https://opendata.stackexchange.com/</a></p>
","0","2","64377","14816"
"63344","<p>As mentioned in other answers, traditionally cosine is used to measure similarity between vectors whereas Levenshtein is used as a string similarity measure, i.e. measuring the distance between sequences of characters.</p>

<p>Nevertheless they both can be used in non-traditional settings and are indeed comparable:</p>

<ul>
<li>the vectors compared with cosine can for instance contain frequencies of characters or characters n-grams, hence making it a string similarity measure</li>
<li>one can replace the sequence of characters with a sequence of strings or a sequence of n-grams, thus making Levenshtein a more general distance measure.</li>
</ul>

<p>The main conceptual difference between Cosine and Levenshtein is that the former assumes a ""bag-of-words"" vector representation, i.e. compares unordered sets, whereas the latter takes into account the order of the elements in the sequences.</p>

<p>In the context of comparing sequences of words many combinations are possible. In case that's what you're looking for you might be interested in this paper: <a href=""https://www.aclweb.org/anthology/C08-1075/"" rel=""nofollow noreferrer"">https://www.aclweb.org/anthology/C08-1075/</a> (full disclosure: I'm one of the authors).</p>
","4","2","64377","14816"
"63345","<p>What you describe is called padding and is indeed used frequently in language modeling. For instance if one represents the sequence ""A B C"" with trigrams:</p>

<pre><code># # A
# A B
A B C
B C #
C # #
</code></pre>

<p>The advantages of padding:</p>

<ul>
<li>it makes every word/symbol appear the same number of times whether it appears in the middle of the sequence or not.</li>
<li>it marks the beginning and end of a sentence/text, so that the model can represent the probability to start/end with a particular word.</li>
</ul>
","2","2","64377","14816"
"63346","<p>Replacing the non-zero values is meant to simplify the matrix, i.e. we only care whether the value is zero or not.</p>

<p>Then you're supposed to calculate the <a href=""https://en.wikipedia.org/wiki/Jaccard_index"" rel=""nofollow noreferrer"">Jaccard distance</a> between pairs of columns. For instance between the first two columns you have <span class=""math-container"">$|a\cap b|=1$</span>, <span class=""math-container"">$|a\cup b|=2$</span> so the distance is 1/2.</p>

<p>After that you're supposed to apply <a href=""https://en.wikipedia.org/wiki/Hierarchical_clustering"" rel=""nofollow noreferrer"">hierarchical clustering</a> using the distance. This means progressively grouping elements (here columns) which are close to each other according to the distance.</p>
","0","2","64377","14816"
"63428","<p>There are different options:</p>

<ul>
<li>Multi-class classification: each tweet can be labelled with several classes</li>
<li>Majority voting: each tweet is labelled with the class chosen most often</li>
<li>Consider tweets with (too many) different labels as ambiguous and remove them from the dataset</li>
<li>...</li>
</ul>

<p>Additionally it's usually useful to calculate the <a href=""https://en.wikipedia.org/wiki/Inter-rater_reliability"" rel=""nofollow noreferrer"">inter-annotator agreement</a>, because this indicates how subjective the task is and therefore how accurate one can expect the predictions to be.</p>
","0","2","64377","14816"
"63502","<p>On the top of my head:</p>

<ul>
<li>with regular clustering techniques you could try to use text-specific distance/similarity measures instead of only considering distinct words as elements. There are hybrid string similarity measures such as SoftTFIDF which take into account character-based and word-based similarity.</li>
<li>use lemmas instead of words in order to facilitate matching of the same concept</li>
<li>for more specific NLP methods you could look at topic modeling and/or word sense induction techniques. The two follow similar ideas in different ways: the former is closer semantically to your case, the latter is meant to work with small context windows similar to your short descriptions size. I'm not sure what is the state of the art nowadays but Latent Semantic Analysis was the standard not so long ago. Afaik these techniques are meant for a quite high amount of data, but they might be worth trying.</li>
</ul>
","1","2","64377","14816"
"63505","<p>That doesn't look at all like a simple dataset for a beginner's project: </p>

<ul>
<li>First if you want to predict future trends you need to look at time series, which is not the easiest type of method. Additionally our data covers only 2 years, it might be too short for good time predictions.</li>
<li>Your dataset is made of various types of attributes so it will probably require some serious preprocessing work (e.g. description string) in order to make the data usable for a particular task.</li>
<li>Start with plotting your data: volume of sales by country, number of customer by country, most common products, this kind of thing.</li>
</ul>
","1","2","64377","14816"
"63543","<p>The first step would be to find how similar a candidate number is against any number in the reference list. I think this is a perfect case for a character-based string similarity measure, typically the <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein edit distance</a>.</p>

<p>In case it's possible to have several matches, there could be a second step which would predict the most likely match, maybe based on the frequencies of the number.</p>
","1","2","64377","14816"
"63569","<blockquote>
  <p>Q1) Should highly correlated features with the target variable be included or removed from classification and regression problems? Is there a better/elegant explanation to this step?</p>
</blockquote>

<p>Actually there's no strong reason either to keep or remove features which have a <strong>low</strong> correlation with the target response, other than reducing the number of features if necessary:</p>

<ul>
<li>It is correct that correlation is often used for feature selection. Feature selection is used for dimensionality reduction purposes, i.e. mostly to avoid overfitting due to having too many features / not enough instances (it's a bit more complex than this but that's the main idea). My point is that there's little to no reason to remove features if the number of features is not a problem, but if it is a problem then it makes sense to keep only the most informative features, and high correlation is an indicator of ""informativeness"" (information gain is another common measure to select features).</li>
<li>In general feature selection methods based on measuring the contribution of <strong>individual</strong> features are used because they are very simple and don't require complex computations. However they are rarely optimal because they don't take into account the complementarity of groups of features together, something that most supervised algorithms can use very well. There are more advanced methods available which can take this into account: the most simple one is a brute-force method which consists in repeatedly measuring the performance (usually with cross-validation) with any possible subset of features... But that can take a lot of time for a large set of features.</li>
</ul>

<p>However features which are highly correlated <em>together</em> (i.e. between features, not with the target response), should usually be removed because they are redundant and some algorithms don't deal very well with those. It's rarely done systematically though, because again this involves a lot of calculations. </p>

<blockquote>
  <p>Q2) How do we know that the dataset is linear when there are multiple variable involved? What does it mean by dataset being linear?</p>
</blockquote>

<p>It's true that correlation measures are based on linearity assumptions, but that's rarely the main issue: as mentioned above it's used as an easy indicator of ""amount of information"" and it's known to be imperfect anyway, so the linearity assumption is not so crucial here. </p>

<p>A dataset would be linear if the response variable can be expressed as a linear equation of the features (i.e. in theory one would obtain near-perfect performance with a linear regression).</p>

<blockquote>
  <p>Q3) How to do feature importance for nonlinear case? </p>
</blockquote>

<p><a href=""https://en.wikipedia.org/wiki/Information_gain_ratio"" rel=""noreferrer"">Information gain</a>, <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""noreferrer"">KL divergence</a>, and probably a few other measures. But using these to select features <strong>individually</strong> is also imperfect.</p>
","8","2","64377","14816"
"63598","<p>Using bigrams and trigrams is likely to generate a high number of features, but with a small dataset the traditional approach would be to reduce the number of features. You could start by removing the least frequent words/n-grams (e.g. less than 3 occurrences), and/or use feature selection with InfoGain. It might not be very accurate but at least you avoid overfitting.  </p>
","0","2","64377","14816"
"63614","<p>That would probably be related to <a href=""https://en.wikipedia.org/wiki/Textual_entailment"" rel=""nofollow noreferrer"">textual entailment</a> and also <a href=""https://en.wikipedia.org/wiki/Relationship_extraction"" rel=""nofollow noreferrer"">relation extraction</a>. </p>

<p>I'm not aware of any specific work but I would check in the biomedical domain, because there are resources such as <a href=""https://semrep.nlm.nih.gov/"" rel=""nofollow noreferrer"">SemRep</a> and I wouldn't be surprised if people tried to use it for similar purposes.</p>
","0","2","64377","14816"
"63641","<p>The question is very broad, so the general answer is: it depends on the specifics of your data and the problem you're trying to solve. The general idea would be to analyze what's happening:</p>

<ul>
<li>Is it actually possible to solve the problem given the indications in the data? Would a human expert do better than 50-60% accuracy given only the information in the data? If yes which clues would they use, and are these potential clues directly available as features to the learning algorithm?</li>
<li>Ratio between size of data and size of features: if there are not enough instances, the algorithm doesn't have enough to generalize from. If there are too many features, the algorithm is likely to overfit (i.e. consider things which happen by chance in the data as significant patterns). And of course if there are not enough features the algorithm doesn't have the indications necessary to make accurate predictions.</li>
</ul>

<p>Common things to try:</p>

<ul>
<li>remove features (here words) which appear very rarely, because they can't help and can cause overfitting.</li>
<li>if the problem seems to be that words are not informative enough, try with bigrams or even trigrams... but be careful to avoid overfitting, the number of features is likely to increase a lot.</li>
<li>use weights (e.g. TFIDF) to help the learning algorithm focus on the relevant features.</li>
<li>depending on the task, more sophisticated methods could be relevant: topic modeling, disambiguation techniques, syntax...</li>
</ul>
","0","2","64377","14816"
"63660","<p>I would strongly advise <em>against</em> doing anything like this: a features which is semantically discrete should be typed as such. There's nothing to be gained about casting categorical values into real numbers:</p>

<ul>
<li>It obfuscates the meaning to a human analyzer</li>
<li>For categorical variables which are not ordinal (i.e. have no natural order), it introduces a serious bias for learning algorithms (note: most attributes in the dataset mentioned in the question are ordinal).</li>
<li>Real number binary representation can lead to approximations, thus possible errors when used as labels and compared for strict equality (something that a categorical variable should support)</li>
<li>It becomes impossible to fix errors in the data</li>
</ul>

<p>Any conversion in the data should always rely on reasons which are specific to the problem/method to be used, not on some agnostic technical transformation. There's no universal recipe for encoding features (categorical or other), one has to understand what they represent and how an algorithm could use them in order to determine the best representation.</p>
","2","2","64377","14816"
"63686","<p>Interesting problem... </p>

<p>If I understand correctly you'd like to obtain clusters of states which have similar patterns/proportions of symptom+diagnosis, right? </p>

<p>If yes, I would suggest you reorganize the data so that one instance represents a state, with its features being the frequency of each pair (symptom, diagnosis). Based on this representation you could cluster states which have similar prevalence for a pair.</p>

<p>The disadvantage of this idea is that it considers pairs (symptom, diagnosis) as distinct even if only one of the two is different. Of course the same process could be done by considering only symptoms or only diagnosis. There are probably better approaches but this simple one might already provide some insight.</p>
","1","2","64377","14816"
"63702","<blockquote>
  <p>As you know MATLAB plots GA result with two curves, one for the best values and other to show the mean values and when this two curves touch each others it means algorithm has been converged.</p>
</blockquote>

<p>More accurately when the two curves touch each other it means that all the individuals in the population perform exactly the same, because that's the only way for the best and the mean to be equal (and usually this happens because all the individuals are identical). That can happen only by chance and/or if the mutation probability is very small, because as soon as there is an individual with a mutation it's unlikely that this individual performance will be equal to the best.</p>

<p>To me in a plot like yours successful convergence was reached around 60-70 iterations. The residual difference is due to the random mutations.</p>
","2","2","64377","14816"
"63703","<blockquote>
  <p>What is the proper way to assign weights? Should I assign weights on the full dataset and then split into train and test?</p>
</blockquote>

<p>No, your test set doesn't have to be weighted and shouldn't be, because it should reflect the real distribution of the data. So you should split first and weight only the instances in the training set, since this is during the training stage that these weights are taken into account.</p>
","1","2","64377","14816"
"63803","<p>As far as I know the word ""lexicon"" is mostly used for a simple list of words (or terms), I would say it's quite rare to use it for describing a list of patterns/rules. ""gazetteer"" and ""dictionary"" would be a bit more general in my opinion, for instance one can have a ""dictionary of rules"" which associates specific patterns with actions. But overall I agree with your impression that these terms are often used interchangeably, I'm not aware of any standard definition. </p>
","1","2","64377","14816"
"63824","<p>Since topic modeling is unsupervised, it's not usually evaluated against labeled data. Instead people devise measures which evaluate the clusters, typically based on how much the most probable words for a topic are semantically relevant.</p>

<p>You might find data ideas in this paper: <a href=""https://www.cs.cornell.edu/~laurejt/papers/authorless-tms-2018.pdf"" rel=""nofollow noreferrer"">https://www.cs.cornell.edu/~laurejt/papers/authorless-tms-2018.pdf</a></p>
","0","2","64377","14816"
"63870","<p>Adding to two good answers, here is a non statistical testing option: <a href=""https://en.wikipedia.org/wiki/Bhattacharyya_distance"" rel=""nofollow noreferrer"">Bhattacharyya distance</a></p>
","2","2","64377","14816"
"63886","<p>Yes, it's common to consider that the prediction is made of multiple answers (typically top N most relevant answers) and use a performance measure based on that.</p>

<p>Currently you're treating the problem as a classification problem but logically this is more like a recommendation problem or an information retrieval problem (like results from a search engine). Usually for this kind of problem the gold answer would also consists of a list of several items, but apparently your dataset contains a single answer for every instance.</p>

<hr>

<p><em>Answer to comment: a couple of papers using some top N performance measures (note: it's just a quick selection based on the keyword ""information retrieval"")</em></p>

<ul>
<li><a href=""https://www.microsoft.com/en-us/research/publication/letor-benchmark-collection-research-learning-rank-information-retrieval/"" rel=""nofollow noreferrer"">https://www.microsoft.com/en-us/research/publication/letor-benchmark-collection-research-learning-rank-information-retrieval/</a></li>
<li><a href=""http://informationr.net/ir/18-2/paper582.html"" rel=""nofollow noreferrer"">http://informationr.net/ir/18-2/paper582.html</a></li>
</ul>

<p>The <a href=""https://en.wikipedia.org/wiki/Conference_and_Labs_of_the_Evaluation_Forum"" rel=""nofollow noreferrer"">CLEF series of Shared Tasks</a> have proposed many datasets and evaluation measures across the years, it's probably a good source for resources and papers... if you have a bit of time to explore it ;)</p>
","2","2","64377","14816"
"63889","<p>As far as I know you don't have a lot of options, you're probably stuck with heuristics:</p>

<ul>
<li>Capital letters</li>
<li>Regular expressions (e.g. for dates)</li>
<li>List of predefined entities (e.g. from Wikipedia) stored in a dictionary</li>
</ul>
","2","2","64377","14816"
"63935","<p>You can email the authors to ask them if they could share their code with you, but maybe they can't for IP reasons or don't want to share it.</p>

<p>Papers like these are not unusual in experimental research. <em>In theory</em> you should be able to reproduce their system following the explanations in the paper.</p>

<p>However there are other tools available for biomedical NER: <a href=""https://metamap.nlm.nih.gov/"" rel=""nofollow noreferrer"">MetaMap</a>, <a href=""https://ctakes.apache.org/"" rel=""nofollow noreferrer"">cTakes</a>.</p>
","1","2","64377","14816"
"64005","<p>The difference between discriminative and generative models is a common question which has some very good answers on Cross Validated, in particular <a href=""https://stats.stackexchange.com/q/12421/250483"">here</a> and <a href=""https://stats.stackexchange.com/q/4689/250483"">here</a>.</p>

<blockquote>
  <p>But the question is why; infact HOW? How does joint probability give rise to new data meanwhile conditional probability just works on current dataset?</p>
</blockquote>

<p>The word ""generative"" in generative models doesn't mean that the model generates actual new data additionally to the dataset. It refers to the nature of the theoretical model, in the sense that the generative approach <strong>assumes</strong> that any sample of data is generated from some distribution, and it tries to <strong>estimate</strong> this distribution. Once the distribution is estimated the model could be used to actually generate instances following this distribution, but:</p>

<ul>
<li>usually that's not the goal, the goal typically being to predict the probability of a new instance using the model (inference)</li>
<li>normally this data should not be reused for estimation (i.e. training), since it is artificial data. Additionally it would be pointless since the best that can be achieved is to re-estimate the same model.</li>
<li>naturally the model obtained from estimation is only as good as the assumptions made for its design, and there are usually many such assumptions. </li>
</ul>
","2","2","64377","14816"
"64015","<p><a href=""https://wordnet.princeton.edu/"" rel=""nofollow noreferrer"">WordNet</a> is certainly an interesting resource to explore for this task. It might not cover all your vocabulary but I can't think of any other way to capture fine-grained semantic relationship between words.</p>
","1","2","64377","14816"
"64016","<p>What the authors call sentiment bias is the tendency for such systems to give strong results, either positive or negative, as opposed to neutral. Very often sentences or documents are more or less sentiment-neutral, but accumulating the positive or negative weights associated to individual words makes it more likely to result in a non-zero ""sentiment value"".</p>

<p>Very simple example:</p>

<pre><code>I am comparing deep learning and lexicon/rule-based models for sentiment analysis.
               +1   +1                   -1         +1         +1
</code></pre>

<p>Individually words such as ""deep"", ""learning, ""model"", ""sentiment"" can be considered positive; ""rule"" can be considered negative. As a result your sentence would receive a strong positive score of +3, even though it's actually neutral.</p>
","1","2","64377","14816"
"64045","<p>Named Entity Recognition (NER) would extract names of people, organizations and such. Example:</p>

<pre><code>""Penalty missed! Bad penalty by &lt;person&gt;Felipe Brisola&lt;/person&gt;  - &lt;organization&gt;Riga FC&lt;/organization&gt; -  shot with right foot is very close to the goal. &lt;person&gt;Felipe Brisola&lt;/person&gt; should be disappointed.""
</code></pre>

<p>So it could be helpful for the ""person"" field, but probably not for the rest. Note that you could also train a system similar to NER in order to predict other fields, but it would require a good amount of annotated data and it's not sure to work well.</p>
","2","2","64377","14816"
"64046","<p>You probably need to re-design the problem in a way which takes time into account (time series), and maybe use a specific model for that. </p>

<p>Currently each of your instances is for a single month, so the algorithm tries to predict the number of passengers based only on this individual month and year. Logically the main information that the system would need is the number of passengers in the past few months for this airport, and it doesn't have it.</p>

<p>A simple way to start would be to format the data so that one instance contains features providing information about the last N months.</p>
","0","2","64377","14816"
"64060","<p>A few simple ideas to observe whether there are any patterns:</p>

<ul>
<li>Plotting the days with seizure/without seizure together with the days when medication is taken across time. It's quite general but it might show some patterns.  </li>
<li>for each seizure, difference with the date of last seizure, i.e. number of days without a seizure. Plotting this value (simple plot of the number of days over time) might show whether there is any regularity and whether there is an evolution in the frequency.</li>
<li>for each seizure, difference with the date of last medication. Plotting the <em>distribution</em> of this number (i.e. how often it's 1,2,3,...) might show if there is a direct relation between the two, e.g. if the number of days is generally more than 10 then it's likely that the effect of the medication wears down within 10 days. The medication might also have a delayed effect, which would be visible in the distribution.</li>
<li>In case there is a long period of data available, you can calculate the average (for instance of days without seizure) over the last N days and plot the evolution of this average across time.</li>
</ul>

<p>Probably you and your wife can think about other things which could be relevant. The general idea is to represent a potential parameter numerically and then observe how this value behaves.</p>
","1","2","64377","14816"
"64112","<p>It's indeed a quite large variation, but nothing alarming since 9 out of 10 folds are within the range 0.8 to 0.9. </p>

<p>There are many possible factors: yes a fold can be easier than another one by chance, but it also indicates that the training process is a bit unstable. Increasing the number of instances and/or reducing the number of features often helps reducing the variation.</p>
","1","2","64377","14816"
"64139","<p>Since you always have a fixed number of days, I think your approach is good. In order to help the learning algorithm you might consider adding some statistics as features for every instance, for example:</p>

<ul>
<li>mean of the last N days</li>
<li>difference dayN-day(N-1) (evolution)</li>
<li>...</li>
</ul>

<p>Of course this can work only if there is actually a dependency between the features and the predicted speed.</p>
","1","2","64377","14816"
"64212","<p>The key term for your problem is <strong>low resource languages</strong>. I'm not sure whether there is a standard approach but you could find papers about what people have done before in similar cases, potential software tools/data repositories which could help, etc.</p>

<p>You might also be interested in the Universal Dependencies project: <a href=""https://universaldependencies.org/"" rel=""nofollow noreferrer"">https://universaldependencies.org/</a></p>

<hr>

<p>There are many things that can be done related to Universal Dependencies (UD):</p>

<ul>
<li>use the existing resources to analyze/parse some text data. As far as I know the standard tool would be <a href=""https://ufal.mff.cuni.cz/udpipe"" rel=""nofollow noreferrer"">UDPipe</a> (python libraries <a href=""https://pypi.org/project/ufal.udpipe/"" rel=""nofollow noreferrer"">here</a>, <a href=""https://pypi.org/project/spacy-udpipe/"" rel=""nofollow noreferrer"">here</a>, maybe others...)</li>
<li>train a new dependency parser. for instance I found <a href=""https://github.com/UppsalaNLP/uuparser"" rel=""nofollow noreferrer"">this repository</a>.</li>
<li>start a treebank for a new language: <a href=""https://universaldependencies.org/how_to_start.html"" rel=""nofollow noreferrer"">https://universaldependencies.org/how_to_start.html</a>. Warning: this is probably a lot of work if you start from scratch! and there's no or little ML involved in the creation of the data itself.</li>
</ul>

<p>The focus of UD resources is on dependencies but imho the main interest is that it provides resources which can be used for all the standard NLP tasks: sentence or word segmentation, POS tagging and lemmatization, etc.</p>
","0","2","64377","14816"
"64371","<p>In general, training your own classifier is likely to perform better but it's going to take more time and effort:</p>

<ul>
<li>Probably the NLTK system was trained on some generic data which might be very different from your target text. Since any supervised system assumes the same distribution between the training and test set, it's always better to train on a sample of your target data. </li>
<li>By implementing your own method you can adapt it more specifically to your use case.</li>
</ul>

<p>So it's a trade-off between the level of quality and the time you're ready to spend on annotating your own data and tuning your own classifier.</p>
","0","2","64377","14816"
"64385","<p>There is no formal definition for the concept of relevance, because relevance depends completely on the context and is therefore highly subjective. This is why the best way (some might say the <em>only</em> way) to evaluate relevance is to actually ask users what is relevant for them.</p>

<p>For any ML-based task, one needs to design a proper <strong>evaluation</strong> framework in order to  control and measure the quality of the results. Naturally the evaluation method should be chosen so that it reflects as much as possible the level of quality with respect to the goal of the task, i.e. what one would intuitively expect from it. Evaluation metrics are almost always simplified indicators of this ""level of quality"", so what matters is how well they correlate with what a user would expect from the system: sometimes even a perfectly standard evaluation measure might not to be suited to the goal of the task.</p>

<p>My point is that evaluation is a matter of <strong>analysis and design</strong>. There are infinitely many options, but the point is to select the most appropriate one for the job. Here are some of these options:</p>

<ul>
<li>The ideal case is to have annotated data (e.g. user feedback) directly suited to the data and the task: then it's just a matter of counting how often the prediction is correct.</li>
<li>It's common to evaluate a system against <em>another</em> annotated dataset X, assuming that the task is similar enough so if the system works well on X then it will work well on the real dataset.</li>
<li>Another less than ideal way is to evaluate against the predictions of another reference system X: in this case X is considered the gold standard, so there is no way for the tested system to perform better than X.</li>
<li>Indirect evaluation: if there is another task being performed at a later stage with the predictions, and this task can be evaluated more easily than the IR task itself.</li>
<li>Heuristics: that would be the least reliable kind of evaluation, but it's better than nothing. It ranges from simply counting the number of words in common between the query and top N results to developing complex methods using third-party resources.</li>
</ul>
","4","2","64377","14816"
"64386","<p>In principle there's nothing wrong with that, since every instance in the test set is predicted individually. You will have a 4 x 3 confusion matrix, because the model might predict some false positives on the fourth class. Of course you won't be able to know if the model can correctly identify a true instance from the missing class.</p>

<p>It depends what is the goal: </p>

<ul>
<li>If the model is meant to be able to predict any of the 4 classes, then it should be trained on the 4 classes and it would be preferable to also test it on the 4 classes, but testing it only on 3 should already gives a good indication of its performance.</li>
<li>If the model only needs to predict 3 classes ever, then the instances of the 4th class should be removed from the training set since they just make things more complex.</li>
</ul>
","1","2","64377","14816"
"64387","<p>To the extent possible you should try to evaluate based on your data rather than some ad-hoc measure. As you rightly noticed, there is a real risk that the ad-hoc measure would just confirm the predictions of the model, since it uses a somewhat similar method.</p>

<p>I would suggest that you split your data between a training set and test set (or even better use cross-validation), and indeed use top-K accuracy (or something similar) to evaluate <em>on the test set</em>. That would be the safe option for a proper evaluation, and then you could try to see if your ad-hoc measure correlates with it: if it does, then you have evidence that in the future it can be be used instead of a test set.</p>

<p>Side note: your instances don't contain any negative evidence such as resumes <em>rejected</em> by an employer. In case you could obtain this kind of data, it could probably improve the predictions.</p>
","1","2","64377","14816"
"64446","<p>From OP's comment:</p>

<blockquote>
  <p>I want to find out if an unlabelled tweet has to categorized as activism or not according to the labelled data I already have (the ones containing activism hashtags)</p>
</blockquote>

<p>This could correspond to a semi-supervised learning setting along the lines of: </p>

<ol>
<li>Train a model on a labelled sample of data, e.g. taking tweets with #activism as positive instances and assuming the others are negative <em>for now</em></li>
<li>Apply the model to the rest of the data (unlabelled instances)</li>
</ol>

<p>In order to maximize the accuracy this process can be iterated in the following way:
take the instances predicted as <em>very likely positive</em> and the ones <em>very likely negative</em> as a new training set, and repeat the process until convergence (i.e. very little change in the predictions). </p>

<p>Btw there are example of the one-class learning approach (which is different) for the problem of authorship verification, which has some similarities to this one.</p>
","1","2","64377","14816"
"64652","<blockquote>
  <p>Given that background, how feasible would it be to apply machine learning here? What important limitations/considerations should be taken during this process? Can machine learning address the problem of PII in this case? Why or why not?</p>
</blockquote>

<p>I think that ML <strong>can be used to speed up the process</strong> (by annotating the most obvious types of PII), but <strong>ML cannot be trusted to reliably redact any and all occurrences of PII</strong>. Two main reasons why:</p>

<ul>
<li>Even for the reasonably simple task of Named Entity Recognition (NER), where state of the art ML perform very well, realistically the performance is never 100%. </li>
<li>NER is not sufficient to remove any PII, and to the best of my knowledge there is no safe automatic method to do so. Consider the following example: ""my girlfriend has a butterfly tatoo on her ankle"" -> this is a PII, even though there is no named entity: if the text contains other specific details about this person, crossing this information with other sources can lead to identification of the author.</li>
</ul>

<p>If the goal is to satisfy some legal/contractual requirements, there is no other way than to have human annotators do it, possibly with the help of ML (and even human annotators will need very specific guidelines).</p>
","0","2","64377","14816"
"64715","<p>What you describe touches on several tasks in NLP:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Semantic_role_labeling"" rel=""nofollow noreferrer"">semantic role labeling</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Relationship_extraction"" rel=""nofollow noreferrer"">relationship extraction</a> </li>
<li>paraphrase detection and <a href=""https://en.wikipedia.org/wiki/Textual_entailment"" rel=""nofollow noreferrer"">textual entaiment</a></li>
</ul>

<p>I might not be aware of the recent developments on these tasks, but as far as I know this is a very complex problem for which so far there is no satisfactory solution.</p>
","0","2","64377","14816"
"64716","<blockquote>
  <p>Maybe I'm having trouble formulating the inherent difference between NLP and NLU, when do we draw the line between the two?</p>
</blockquote>

<p>There is a confusion here: NLP is the whole domain of AI which deals with natural language. It includes virtually any task related to processing language data (usually mostly written data, but that's not the point). Topic modeling is one of these tasks.</p>

<p>NLU is the problem of Natural Language Understanding, which is usually considered as one of the main goals of NLP. If anything, NLU is a problem that NLP tries to solve, i.e. a sub-topic in the large area of NLP.</p>

<p>Also notice that using words embeddings can improve things, but it doesn't solve all the difficulties related to semantics, far from it.</p>

<hr>

<p><em>[edit]</em> The scope of NLU is not strictly defined: in the broadest possible definition, it would include anything vaguely related to extracting meaning from text, and in this very generous sense topic modeling would have a connection to it with or without embeddings (and so would a lot of other NLP tasks). <a href=""https://en.wikipedia.org/wiki/Natural-language_understanding#Scope_and_context"" rel=""noreferrer"">Wikipedia</a> says:</p>

<blockquote>
  <p>The umbrella term ""natural-language understanding"" can be applied to a diverse set of computer applications, ranging from small, relatively simple tasks such as short commands issued to robots, to highly complex endeavors such as the full comprehension of newspaper articles or poetry passages. Many real world applications fall between the two extremes, for instance text classification for the automatic analysis of emails and their routing to a suitable department in a corporation does not require in depth understanding of the text.</p>
</blockquote>

<p>But the most commonly accepted definition of NLU is stricter, it would only consider tasks which directly involve the interpretation of text in a quite complex setting. The typical example is the ""virtual assistant"" such as Amazon Alexa, OK Google, Apple's Siri. In this sense topic modeling is simply a completely different task, no matter the ""degree of understanding"".</p>
","7","2","64377","14816"
"64733","<p>It's an interesting problem, but I think the answer might be disappointing:</p>

<p>Usually problems about maximizing recall are considered in the context of a trade-off with precision, i.e. the goal is to sacrifice some precision by predicting more positives, whether true (TP) or false (FP). Usually in a binary problem this is possible because we focus on one class of importance and consider the other as irrelevant, so its performance doesn't matter.</p>

<p>Here we have a binary problem where we want to maximize recall <em>for both classes</em>, so we cannot sacrifice one for the sake of the other: any gain in recall for a class is likely to cause a loss in recall for the other class, since predicting more instances as positive for class A would mean more negative instances for class B, and conversely. If we were talking micro-recall we could still use the higher importance in proportion of class A, but with macro-recall we can't. Note that this is a typical case where accuracy could be used, since it would give the same weight to both classes and would be a much simpler metric.</p>

<p>So the only way to improve macro-recall is to increase true positives. Let's look at the options:</p>

<blockquote>
  <p>1) A: 10*N observations, B: N observations</p>
</blockquote>

<p>Pro: performance for class A is maximized. Cons: class B proportionally disadvantaged, so possible loss in its performance.</p>

<blockquote>
  <p>2) A: 5*N observations, B: N observations</p>
</blockquote>

<p>Cons: class A a bit less advantaged; pro: class B a bit less disadvantaged.</p>

<blockquote>
  <p>3) A: N observations, B: N observations</p>
</blockquote>

<p>Pro: maximum performance for class B; cons: class A underperforms.</p>

<p>Actually the best option is probably to use all of N*10 instances for A and repeat 10 times the N instances for B, so that:</p>

<ul>
<li>the learning method can benefit from all the available training data for A</li>
<li>class B is not proportionally disadvantaged.</li>
</ul>

<p>But my guess is that it's unlikely to make a very big difference anyway. When it's a matter of increasing true positives, it's usually the features and the ML method which can have an impact.</p>
","1","2","64377","14816"
"64809","<blockquote>
  <p>The data set comprises people's activity on Office 365 and my goal is to predict whether the person is experiencing pressure or not using machine learning algorithms</p>
</blockquote>

<p>Very often in data science the problem or the data (or both) is not well defined from the start. Thus the first task is often to <strong>design</strong> a properly defined ML problem out of the vague initial problem. This is a really important step if one wants to achieve anything useful: ML is not magical, it's a tool which needs to be used in the right way in order to provide interesting results. </p>

<p>According to your description, the initial problem seems ill-defined: for a goal such as predicting whether somebody experiences pressure, there's no clear way to represent ""pressure experienced"" other than asking the user how much pressure they feel. <strong>If your data doesn't contain any such information from the user and you can't obtain it in any other way, then it's likely that the problem cannot be solved at all</strong>. As a rule of thumb, if a human cannot find indications in the data which helps answering the question, it's likely that a ML system cannot either.</p>

<p>So your first task is to determine if there's anything in the data that can represent ""pressure experienced by the user"". For example, maybe a stressed user tends to click everywhere randomly? Then maybe you could try to find in your data if there is a particular pattern or ""clicking frequency"" and calculate a new column which represents the ""stress level"" based on this. But there is a high risk that such an assumption is not very reliable, and this would influence your results.</p>
","2","2","64377","14816"
"64810","<p>Very interesting question, it's always hard to prove a negative.
I have a vague idea but I really don't know if it's worth anything or even applicable to this problem, so please take it with a grain of salt! </p>

<p>The idea is to use randomness and multiple samples in order to compare the result of predicting from random noise vs. actual data X: if the result from X isn't significantly better than from random noise, then you have proved that X doesn't have any predictive power. Of course this relies on the assumption that the model used to predict is sensible enough.</p>

<p>I have seen methods using this idea but I don't remember the details unfortunately. The only directions I can give are these:</p>

<ul>
<li>the <a href=""https://www.tandfonline.com/doi/abs/10.1080/0013838X.2012.668794"" rel=""nofollow noreferrer"">impostor method for authorship verification</a>, which is based on a somewhat similar idea</li>
<li>I've seen some works using <a href=""https://en.wikipedia.org/wiki/Binomial_test"" rel=""nofollow noreferrer"">the binomial test</a> to formally test whether the outcome from a method X is significantly different from a random baseline.</li>
</ul>

<p>Sorry for the lack of details, I hope this helps.</p>
","1","2","64377","14816"
"64830","<p>Python tells you that the data you give for the column ""predictions"" is not 1-dimensional (i.e. it's not a flat list). And indeed <code>preds</code> is not a 1-dimensional array, what you want to give is the corresponding labels that you collected in <code>predictions</code>.</p>

<p>So Instead of this: </p>

<pre><code>results=pd.DataFrame({""image ID"":filenames,
                  ""Predictions"":preds})
</code></pre>

<p>you probably want:</p>

<pre><code>results=pd.DataFrame({""image ID"":filenames,
                  ""Predictions"":predictions})
</code></pre>
","4","2","64377","14816"
"64833","<p>It seems to me that there's a bit of confusion between these two steps:</p>

<ol>
<li>Designing the problem and preparing the data accordingly</li>
<li>Applying ML methods to the data (feature selection, etc.)</li>
</ol>

<p>The two steps must be distinct: any aggregation must take place during step 1 so that at the end of step 1 the dataset is fixed. </p>

<p>In your example in step 1 you decided to predict the total volume of sale across  stores given the week and price per unit. Note that you could also choose other options:</p>

<ul>
<li>average volume of sale by store as target</li>
<li>keep one row for each store: depending on the goal, sometimes it's ok to have inconsistencies in the data, regression can deal with these.</li>
<li>you could decide that an instance for a store spans over the N past weeks and provide the sales for these weeks, the target being the week after that (that would probably help btw). </li>
<li>...</li>
</ul>

<p>And of course you can do this as many times as needed for studying different problems, each time obtaining a different version of the data which represents a particular problem. It's only once you have formally defined your problem and formatted your data for it that the ML part starts. For instance stepwise regression (or any other technique) cannot work if the data is modified/aggregated during the steps.</p>
","0","2","64377","14816"
"64854","<p>Yes but there are two issues:</p>

<ul>
<li>Is it really useful? If the prediction is just added to the features on which it's based, it's unlikely to improve performance. However there are cases where this is useful, e.g. <a href=""https://en.wikipedia.org/wiki/Ensemble_learning#Stacking"" rel=""nofollow noreferrer"">stacked learners</a>.</li>
<li>You need to split the training set into two parts t1 and t2:

<ul>
<li>t1 is the regular training set used to produce the first model</li>
<li>t2 is the data used to predict the new ""feature"" and train the second model with this new feature.</li>
</ul></li>
</ul>

<p>It would be a bad idea to use the same training set for the two models because the predictions used in the second model would be obtained on the training set, so they would be unrealistically good.</p>
","2","2","64377","14816"
"64947","<p><em>[edit thanks to comment] I'm assuming this is a binary classifier, since normally a multi-class classifier would not be evaluated with precision/recall (it would require micro/macro precision/recall).</em></p>

<p>Yes, that should be enough:  </p>

<ul>
<li>accuracy = 92.7%: </li>
</ul>

<p><span class=""math-container"">$$\frac{TP+TN}{110}=0.927 \rightarrow TP+TN=101.97$$</span></p>

<p>This means we have 102 correct predictions, so <span class=""math-container"">$FP+FN= 8$</span> incorrect predictions (since <span class=""math-container"">$TP+FP+TN+FN=110$</span>).</p>

<ul>
<li>precision = 96.9%:</li>
</ul>

<p><span class=""math-container"">$$\frac{TP}{TP+FP}=0.969 \rightarrow TP=31.258\times FP$$</span></p>

<ul>
<li>recall = 95%:</li>
</ul>

<p><span class=""math-container"">$$\frac{TP}{TP+FN}=0.950 \rightarrow TP=19 \times FN$$</span></p>

<p>This gives us:</p>

<p><span class=""math-container"">$$\frac{TP}{31.258}+\frac{TP}{19}=8 \rightarrow TP = 94.6$$</span></p>

<p>let's assume that means 95 true positive instances, so we get:</p>

<ul>
<li><span class=""math-container"">$FP = 3$</span></li>
<li><span class=""math-container"">$FN = 5$</span></li>
<li><span class=""math-container"">$TN = 7$</span></li>
</ul>
","2","2","64377","14816"
"65036","<p>I'm not expert in this but as far as I know the proper way to test for equality modulo floating point imprecision is to compare the differences of the two values, i.e. instead of:</p>

<pre><code>trunc(a) == trunc(b) 
</code></pre>

<p>one would do:</p>

<pre><code>abs(a-b) &lt;= epsilon
</code></pre>

<p>where <code>epsilon</code> is the constant which represents an acceptable difference, e.g. <span class=""math-container"">$10^{-6}$</span>. Of course this requires to read the two values and calculate the difference, instead of simply comparing one against the other.</p>
","2","2","64377","14816"
"65167","<p><em>[edited, I misread the question in the first version]</em></p>

<p>The fact that the label is determined from a combination of values from this feature is not a problem in itself: if it makes sense, it's always better to give the best indicators to the learning algorithm. So the only questions are:</p>

<ul>
<li>whether it makes sense for your problem to have the feature provided as input for any new instance: if yes, then there's no reason to remove it.</li>
<li>whether it's useful to apply ML to your problem: if the label can be determined directly from a single feature, it's simply not useful to train a model.</li>
</ul>

<p>You mention that the label is based on information from the previous/next two records. Keep in mind that the model needs to predict its target for any individual instance as input, unless you're using a sequential model (for instance with times series).</p>
","2","2","64377","14816"
"65169","<blockquote>
  <p>Basically what I am trying to know is should we feed variables which we are confident that they will impact the outcome into the model?</p>
  
  <p>Or is the model to help us know something which we aren't aware of .</p>
</blockquote>

<p>It depends, there's no general rule. The goal is of course to predict the target variable as well as possible, i.e. to maximize the performance. So the question is to find the right balance between:</p>

<ul>
<li>Giving only a few features which are known to help predict the target variables. In this case we make things as easy as possible for the learning algorithm, so that it can perform optimally with the features provided. Disadvantage: maybe there are other more subtle indications that would have helped the algorithm make better predictions.</li>
<li>Feeding the learning algorithm with a very high number of features. In this case we let the algorithm find the relevant indications itself, hoping that it will catch even subtle clues that a human expert would easily miss. Disadvantage: too many features are susceptible of causing two problems: overfitting, when the model relies on very specific patterns which happened by chance in the training set; and redundancy, which can also decrease the performance with some algorithms.</li>
</ul>

<p>The data-driven answer is to try various combinations of features on the spectrum between these two options, and evaluate the corresponding models on a validation set (preferably with cross-validation in order minimize the effect of chance). This can be done manually or using feature selection/extraction methods. </p>

<p>[added for clarity:] when selecting the best option between different sets of feature (or different algorithms as well), one needs to use a validation set which is distinct from the test set (and also from the training set of course), otherwise there is again a risk of overestimating the performance.</p>
","3","2","64377","14816"
"65216","<p><em>Disclaimer: I'm not at all expert about deploying big ML systems in production. This answer is only based on my experience with many different ML problems and datasets</em></p>

<p>My humble advice would be <strong>not to try to design the format of the data before having a quite precise idea of what kind of ML process is going to be applied</strong>. There is no ""one size fits all"" in ML and there's a real risk that by starting with the format of the data, you will end up with something which turns out to be completely inappropriate for the task.</p>

<p>Start with local experiments instead:</p>

<ul>
<li>Use a small subset of the data at first.</li>
<li>Design some simple problems of the same kind as the real ones you plan to do eventually. </li>
<li>Vary as many aspects as possible of the experiments: preprocessing, learning algorithms, parameters, size of the data, etc. Move progressively to more realistic tasks and amount of data.</li>
<li>Evaluate the advantages/disadvantage of the different methods/setups, then select a range of target setups</li>
<li>Finally design everything including the data format based on these target setups.</li>
</ul>

<p>Following this logic during the experimental stage you can just export your data in any format convenient for whatever framework you're testing. it's only at the end of the experimental stage that you design the production system, e.g. SQL server etc.</p>
","1","2","64377","14816"
"65250","<p><a href=""https://en.wikipedia.org/wiki/Question_answering"" rel=""nofollow noreferrer"">Question answering (QA)</a> is a complex problem and an active field of research. There are probably some academic prototypes around, but I doubt there's any general-purpose ready-to-use QA library. However there are probably state of the art implementations for closed QA, i.e. QA restricted to a specific domain (I'm not aware of any specific library though).</p>

<p><a href=""https://en.wikipedia.org/wiki/Paraphrasing_(computational_linguistics)"" rel=""nofollow noreferrer"">Paraphrasing</a> is a related but different problem, and also an active research question. </p>

<p>Extracting keywords is a much more standard task and is an important part of traditional <a href=""https://en.wikipedia.org/wiki/Information_retrieval"" rel=""nofollow noreferrer"">Information Retrieval</a> methods.</p>
","1","2","64377","14816"
"65251","<p>The ranking of the answers is part of the ML process, i.e. a system should be <em>trained</em> to rank the answers according to their relevance. Heuristic measures such as the ones mentioned in your question may offer decent approximations, but as you noticed they are very limited. </p>

<p>You may be interested in datasets and methods used in shared tasks about QA, for instance <a href=""https://mrqa.github.io/shared"" rel=""nofollow noreferrer"">https://mrqa.github.io/shared</a>.</p>
","1","2","64377","14816"
"65670","<blockquote>
  <p>My doubt is related to the following question: Do we use those two measures only if we are testing a IR system or not?</p>
</blockquote>

<p>Technically the answer is no because precision and recall are used to evaluate <em>not only IR systems</em> but also many other tasks. However your question seems to be specific to IR so I'll assume that it's actually about the <strong>distinction between testing and evaluation</strong>:</p>

<ul>
<li>Testing a ML system consists in predicting the target variable for a set of instances given as input (in the case of supervised learning a ""model"" obtained from a previous stage of training is required as input as well). At this stage we don't know whether the predictions are correct or not.</li>
<li>Evaluation is the process of assessing the quality of the predictions: it's done after obtaining the predictions from the testing stage, and it requires some form of ""gold standard"", i.e. data which says what is the correct answer for every instance.</li>
</ul>

<p>In IR, the testing stage happens every time the system is run to find relevant documents based on a query. </p>

<ul>
<li>Naturally one wants at first to make sure that the system works properly and returns actually relevant documents, so the system needs to be evaluated, for instance with precision and recall using a dataset containing some queries and their relevant documents (gold standard).</li>
<li>Once the quality is evaluated, the goal is to use the IR system (testing) without evaluating the results every time. Of course there's no evaluation so the performance measures (precision and recall) are not used.</li>
</ul>
","2","2","64377","14816"
"65727","<p>The model depends entirely on the training data: if you train with some data which has only PrdName as label, the model knows only this label and can predict only this label. You need to provide as much training data as possible, containing all the possible labels.</p>

<p>For the record, NER are usually trained with thousands of sentences in order to account for the diversity of the cases where a NE can appear.</p>
","0","2","64377","14816"
"65760","<p>Just a couple thoughts:</p>

<ul>
<li>It looks like these ""regimes"" could be represented as a <a href=""https://en.wikipedia.org/wiki/Latent_variable"" rel=""nofollow noreferrer"">latent variable</a>:  you could probably design a bayesian model in which the OLS model depends on the value of this latent variable. This means that the model would still be trained only with the observed features, but would internally predict the value of the regime and this value would determine the parameters of the OLS model. </li>
<li>A more direct approach for this kind of case-by-case setting would be to use decision trees (or random forests), since they can handle independent models in different branches. However I'm not sure how to make decision trees and linear regression work together (or if it's possible at all).</li>
</ul>
","1","2","64377","14816"
"65860","<p>From a theoretical perspective, I think this question is a variant of ""given a problem, is there a way to determine the absolute best learning algorithm for it?"". Why? Because as OP correctly suggests, if there was a way to represent all the possible models/priors, then assuming that an infinite stream of data is provided we would be able to <em>eventually</em> determine the absolute best learning algorithm. Unfortunately the <a href=""https://en.wikipedia.org/wiki/No_free_lunch_theorem"" rel=""nofollow noreferrer"">No Free Lunch theorem</a> is a theoretical result which states that there can be no such ""absolute best"" learning method. (to be honest personally I'm out of my depth with this kind of thing, but Wikipedia gives a nice short explanation of <a href=""https://en.wikipedia.org/wiki/No_free_lunch_theorem#Implications_for_computing_and_for_the_scientific_method"" rel=""nofollow noreferrer"">the implications of the theorem for machine learning</a>.)</p>

<p>From a practical perspective, this is a matter of design, i.e. how one chooses to represent the problem. It's easy to overlook how much simplification is made when one represents a problem. In the example proposed by OP, one could imagine an infinity of ""alternative worlds"": maybe the word ""father"" doesn't have its usual meaning; maybe it happens in a future where people can have any number of parents; etc. My point is: whenever a problem is formally stated, a vast number of assumptions are made anyway, most of them unconsciously. It's not only a matter of cost (it's important, of course), it's a matter of ""targeting"" the exact problem: if we tried to leave the space of possibilities completely open, then we cannot even start to solve the problem, in the same way that people have to agree on a common language in order to understand each other. Thus choosing a specific space of priors/models for a particular problem is akin to fixing the language of the problem, and when doing it it's important to take into account the limitations/assumptions associated with it.</p>
","1","2","64377","14816"
"65866","<p>You need an ontology or any form of third-party data which describes the relationships between fields and sub-fields. You could use resources such as Wikipedia categories or standard library classifications for instance. There are probably other options for scientific fields.</p>
","1","2","64377","14816"
"65870","<blockquote>
  <p>Can we Create a model using (bi lstm crf) where we only want to predict 1 entity.?</p>
</blockquote>

<p>[edited] Yes, the trick I would use is to train a regular model and predict the top 10 most likely predictions for each sentence. This will give you the different possible labelings ordered by probability. The idea is to select the first labeling which contains a single entity, i.e. the most likely option after eliminating the predictions with several entities.</p>

<blockquote>
  <p>In CRF States of the neighbors affect the current prediction so predicting 1 entity per sentence seems difficult specially with CRF?</p>
</blockquote>

<p>There's a confusion here: this is true but the ""neighbors"" are not other entities, they are the other words of the sentence. This means that if a word belongs to an entity then the next word is more likely to belong to an entity as well (i.e. to be part of the same entity). So using CRF doesn't increase the probability to find several <em>distinct</em> entities in a sentence. However if all the target entities are single words then CRFs might not be needed (I'm not sure whether there would any better alternative though).</p>

<blockquote>
  <p>if i Cannot achieve this with CRF can I use Bert to train a model having 1 entity per model?</p>
</blockquote>

<p>I don't know the answer to this one but I doubt it's needed.</p>
","2","2","64377","14816"
"65924","<p>Simple precision/recall/F-score is perfectly suitable for imbalanced data. It should be computed on the minority class of course.</p>

<ul>
<li>precision says how often the system is correct when predicting an instance in the minority class.</li>
<li>recall says how often the system detects an instance which belongs to the minority class (this is usually the hardest part with imbalanced data)</li>
<li>As usual F-score can be used as a ""summary metric"" of both precision and recall.</li>
</ul>

<p>ROC and AUC (which are also based on precision/recall) can be used only with a soft classifier, i.e. when the system predicts a numerical value instead of a binary label.</p>

<p>Macro/micro/weighted average (usually over F-score) are irrelevant: averaging over the two classes would take into account the majority class which is very easy to predict, so it would mask the useful information about the minority class.</p>
","1","2","64377","14816"
"66105","<p>A direct way to find the words which are the most representative of a class is to calculate the probability of the class given a word:</p>

<p><span class=""math-container"">$$p(c|w)=\frac{\#\{\ d\ |\  label(d)=c\ \land w\in d\}}{\#\{\ d\ |\ w\in d\ \}}$$</span></p>

<p>Ranking the words according to their probability <span class=""math-container"">$p(c|w)$</span> gives:</p>

<ul>
<li>highest values: the most correlated words for the class</li>
<li>lowest values: the least correlated words for the class</li>
</ul>

<p>Remark: with this method it's safer to filter out the least frequent words (e.g. remove the words with frequency lower than 3), because these are likely to happen by chance so they are not really representative.</p>
","1","2","64377","14816"
"66144","<blockquote>
  <p>When unsupervised learning is more beneficial in comparison with supervised learning even the labeling are existed? </p>
</blockquote>

<p>I would say that there are two main cases:</p>

<ul>
<li>The task is semantically more meaningful as an unsupervised task. For instance let's consider a collection of books which have been annotated with topics: if the goal is to classify new books into the same pre-existing categories, then it makes sense to use a supervised setting. However if the goal is to discover new patterns of similarity that might not be intuitively easy to notice for a human annotator, then unsupervised topic modeling makes more sense.</li>
<li>The annotations are available for some subset of data (e.g. for evaluation purposes) but will not be available later in production (to some extent it's also a case of what the goal is, but here for technical reasons). </li>
</ul>

<blockquote>
  <p>If there is no labeling the unsupervised learning is better than supervised learning but in some cases even the labeling targets are available, the supervised learning approach works betters? What about conditions of these cases? </p>
</blockquote>

<p>The problem here is what ""better"" means, i.e. how the task is evaluated. If the task is evaluated against the pre-existing labels, in theory the unsupervised version cannot work better than the supervised one since the supervised one has access to more information. It's possible that a particularly unsuitable supervised method would perform worse than a well chosen unsupervised one, but that's not a fair comparison and it's very unlikely in practice.</p>

<p>In general the two are not comparable because the tasks are fundamentally different: in a supervised setting one wants to find patterns related to some information which is known beforehand (the labels), whereas in an unsupervised setting one wants to discover unknown patterns.</p>

<blockquote>
  <p>Can we say that if there is no clear dependency between variables unsupervised learning works better?</p>
</blockquote>

<p>I don't think so because:</p>

<ul>
<li>Some supervised algorithms are very good at optimizing whatever little amount of information is available in the data. Even with ""no clear dependency"" visible according to standard measures, some algorithms can combine the features optimally to minimize errors.</li>
<li>In the case of strictly no dependency at all between the features and the label, the unsupervised method might perform better in the sense that it might find meaningful patterns, but these won't be related to the labels at all (since there's no dependency). So we go back to the question of what ""better"" means with respect to the task: if the task was to discover unknown patterns then sure unsupervised is ""better"", but this has nothing to do with the labels and a supervised method would make no sense here. If the task was about predicting the labels, the unsupervised approach is just as bad as the supervised one (and there's a serious flaw in the design of the task!). </li>
</ul>
","1","2","64377","14816"
"66145","<p>I'd suggest looking at <a href=""https://en.wikipedia.org/wiki/Hierarchical_clustering"" rel=""nofollow noreferrer"">hierarchical clustering</a>:</p>

<ul>
<li>It's simple so you could implement and tune your own version</li>
<li>It lets you decide at which level you want to stop grouping elements together, so you could have a maximum distance.</li>
</ul>

<p>Be careful however that this approach can sometimes lead to unexpected/non-intuitive clusters.</p>
","2","2","64377","14816"
"66211","<p>Yes that's correct, but assuming that you follow the exact same methodology you will obtain exactly the same performance at the end, so there's no advantage.</p>

<p>Keep in mind that the problem with class imbalance is not that one class is harder to identify than the other, but rather that it's harder to properly separate the two classes.</p>

<p>[edit] It would be a different story when using <a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">one-class classification</a>. I'm not sure if it makes sense in this case but maybe it could be something worth trying.</p>
","1","2","64377","14816"
"66230","<p>something like this might work:</p>

<pre><code>library(plyr)

# initialize All_Data around here

dlply(All_Data, 'Identifier', function(dataSubset) {
  g &lt;- ggplot(dataSubset) + geom_point(mapping =aes(SampleDate, Total.Result)) + ylim(0,20000)
  file_name &lt;- paste0(""Scatter_"", unique(dataSubset$Identifier), "".tiff"", sep="""")
  ggsave(file_name,g)
})
</code></pre>

<p>(I didn't test it)</p>
","3","2","64377","14816"
"66233","<p>I don't think there's anything close to doing this:</p>

<ul>
<li>It would be very hard to even define the task objectively, as different humans wouldn't agree about what is credible or not.</li>
<li>It would require a complex system to represent reliable background knowledge... and again people wouldn't agree what should be considered ""reliable"" or not.</li>
<li>Generally the state of the art in NLP is still far from solving tasks related to Natural Language Understanding in a satisfying way. Juding the credibility of a text requires not only a real understanding of the text but also an ability to reason at a higher level. It's not clear whether this level of AI can ever be reached. </li>
</ul>

<p>If you find a package which pretends to achieve this task, try to apply it to its own documentation because it's not credible ;)</p>
","3","2","64377","14816"
"66234","<p>This task looks similar to what is called text segmentation, in particular <a href=""https://en.wikipedia.org/wiki/Text_segmentation#Topic_segmentation"" rel=""nofollow noreferrer"">topic segmentation</a>. I don't know any python package to do it but apparently Google gives a good few results for ""semantic text segmentation python"" (I'm not sure that this is the best phrase, you might want to try variations).</p>

<p>Note: this is still an active NLP research topic as far as I know. I don't know how fast reliable python packages are written and maybe there are some for this, but I wouldn't be too surprised if there were only research prototypes available at this stage.</p>
","1","2","64377","14816"
"66297","<blockquote>
  <p>So, is it alright to replace missing values with the mean calculated from values that include some that have once replaced other missing values?</p>
</blockquote>

<p>There's no law against it :)</p>

<p>But by replacing missing values everywhere in many different ways you're probably damaging the reliability of your dataset at some point, and that could in turn mess up your experiment.</p>

<p>You don't give any detail about the task or the data so I assume you chose this approach on purpose, but just in case let me remind you of other options:</p>

<ul>
<li>you could discard features for which most values are missing, unless they happen to be very important for the task</li>
<li>you could discard instances which have many/some missing values, especially if you have a very large dataset anyway</li>
<li>you could use a ML method able to deal with missing values by itself</li>
</ul>
","1","2","64377","14816"
"66420","<p>This is a very ambitious project. First it's important to realize that ML cannot really solve this kind of problem in general, it can only help detect the posts which are likely fake news (see for example <a href=""https://datascience.stackexchange.com/q/66201/64377"">this other post</a> about measuring credibility, i.e. the same question seen the other way around).</p>

<p>Assuming you work on the text of the message (I'm not competent about images), the first step would be to collect a corpus of data from all your sources and manually label all of it as fake or not (you could also decide to use a score of ""fakeness"" for instance). From there you could train a model: at first I would suggest something simple like Naive Bayes (it's the traditional model used for spam detection).</p>

<p>In any case it's important to keep in mind that what the model would learn is not whether the input text is a fake news or not, it would only learn to recognize the tell tale signs: for instance fake news tend to use scary words whereas real news tend to be worded more neutrally. But of course there can real news using scary words and fake news using neutral language, so it's not going to be perfect. In order to really reach the goal the only way would be to have the <em>potential</em> fake news checked by humans, who would be able to give a more informed judgement. </p>

<p>The last part of the question is more a legal question. As far as I know it's often very hard on social media to discover who is really the author, only the police can investigate this but they do it only in very serious cases.</p>
","8","2","64377","14816"
"66676","<p>Answer based on the comment: </p>

<blockquote>
  <p>Intention here is to reduce the error by all means and considerations</p>
</blockquote>

<ul>
<li>Choice of model: First let's address the obvious assumption: linear regression is a model which requires the response variable to be expressed as a linear combination of the independent variables. In order to improve performance in general one must make sure that these constraints are satisfied. If not it's often worth looking into other models or finding a way to ""linearize"" the data.</li>
<li>Data cleaning: depending on the size of the data, linear regression can be very sensitive to outliers. If it makes sense for the problem, outliers can be discarded in order to improve the quality of the model. Of course one shouldn't remove points just because it decreases the error, this has to be done sensibly with respect to the characteristics of the task.</li>
<li>Feature engineering: it's worth analyzing/experimenting which of the independent variables actually help obtaining a good model. For instance redundant variables may decrease performance. It's also possible that variables can be expressed differently or transformed in a way which improves performance.</li>
</ul>
","0","2","64377","14816"
"66678","<p>I'm not sure that I understand every part of the process but there is one clear issue with it: because the CV is applied in the inner loop, there is a serious risk of overfitting the model with respect to the other parameters (feature subset, model type, sampling technique). Depending on the goal, this is not necessarily wrong but it's important to interpret the results accordingly. For instance in this case the results will show how different subsets of features perform on the data, but this difference in performance across subsets shouldn't be considered reliable: it's possible that a particular subset happens to be better than another by chance.</p>

<p>It's quite a complex setting and I assume that there are efficiency constraints to take into account. If possible the most reliable results would be obtained by doing several stages of CV (or other techniques, e.g. <a href=""https://en.wikipedia.org/wiki/Bootstrap_aggregating"" rel=""nofollow noreferrer"">bagging</a>) using different subsets of data. For instance you could run the whole process a few times, each time using a different random subset of instances: in this way you can average performance and see whether a particular subset of features is constantly better than another (for example, same idea for other parameters).</p>

<hr>

<p><em>[edited] Disclaimer: I don't know if there is any standard way to proceed with a complex multi-level setting like this, my advice is based only on the experience I had with a few broadly similar cases.</em></p>

<p>Generally the idea is that every choice to make can be considered as an hyper-parameter, including the subset of features, the type of model, the architecture, the sampling technique. Therefore I think that ideally one would cross-validate at every level, i.e. put every loop level in a function and call this function k times with a different subset of data. It would look like something like this:</p>

<pre><code>train1, val1 = splitDataset()
iterate each featureSubset:
    train2, val2 = splitData(train1)
    resultTrain2 = kfoldCV(processLevel2, train2)
    resultLevel2 = apply(resultTrain2, val2)
resultLevel1 = apply(resultLevel2, val1)

processLevel2:
    iterate each modelType:
        train3, val3 = splitData(train2)
        resultTrain3 = kfoldCV(processLevel3, train3)
        ...
</code></pre>

<p>remark: I'm not 100% sure about the algorithm, maybe I over-complicated it. I think it gives the general idea though.</p>

<p>But of course following this logic the computational complexity becomes way too high, so you will probably have to take a few shortcuts. One thing I've tried successfully in the past is to use genetic learning in order to optimize different parameters at the same time: that would mean having different ""genes"" which represent the different parameters (feature subset, model type, etc.), each with its set of values, and then run the genetic process which is supposed to converge to a set of optimal values for all the parameters (I was using CV every time a particular combination of parameters is evaluated). But again I don't know if it's a very orthodox method :)</p>

<hr>

<p>[edit2]</p>

<p>After more thought I think I would try to do something like this:</p>

<pre><code>innerData, valOuter = splitDataset() // keep large amount for validation set, say between 20-40%
train, valInner = splitDataset(innerData)
iterate each featureSubset:
    iterate each modelType:
        model = manuallySearchGoodArchitecture(featureSubset, modelType, train, val)

        iterate each samplingTechnique:
            train_temporary = applySampling(samplingTechnique)
            HPs = HPOptimization(model, train_temporary, valInner)
            result =  k-FOldCV(model, HPs, samplingTechnique, train)
        end
    end
end

bestHPCombinations = select top N HPCombinations from first stage
for each HPCombination in bestHPCombinations
     model = train(innerData)
     result = apply(model, valOuter)
end
</code></pre>

<p>It's simpler: the idea is just to re-evaluate the results from the first stage on some fresh data in order to avoid overfiting (here HPCombination includes featuresSubset, modelType, etc.). The second stage could also include CV or bagging for more reliable results. But in general I think this option would be reasonably reliable, since it's unlikely that the best models from the first stage would also be the best models in the second stage by chance only. </p>
","1","2","64377","14816"
"66687","<p>Yes, the performance can vary a lot using feature engineering.</p>

<p>Example: suppose a dataset where the response variable <span class=""math-container"">$y$</span>  is true if <span class=""math-container"">$x$</span> is odd.</p>

<pre><code>x    y

346  F
13   T
178  F
64   F
987  T
...
</code></pre>

<p>Most learning models will fail to identify the pattern and will perform poorly, usually falling back to always predicting the majority class. However simply adding a feature <span class=""math-container"">$x \% 2$</span> to the data will allow any model to perform perfectly.</p>

<p>Of course this a toy example, but the point is that a single well chosen feature can drastically change the performance. Naturally the increase in performance totally depends on the data and the nature of the features added.</p>
","3","2","64377","14816"
"66688","<p>Maybe somebody will have a better idea but the default method would be to generate a set of names, then ask a few annotators to label them as good or bad (possibly scoring them from 1 very bad to 5 very good), and finally train a supervised model to recognize the good from the bad ones. This approach would also give you the opportunity to check the inter-annotator agreement, i.e. assess how subjective the choice good vs. bad is. In general a subjective task is difficult (or impossible) to do with an unsupervised metric.</p>
","1","2","64377","14816"
"66689","<blockquote>
  <p>Is there any better way to validate this piece of python code. ?</p>
</blockquote>

<p>No, because any automatic method would be equivalent to creating another learner, and there would be no way to know if it's good or bad at the job. So unless you find another annotated dataset (i.e. a set of resumes with labels indicating whether or not they switched jobs in the past year), you must create your own annotated set. </p>
","1","2","64377","14816"
"66709","<p>You might be interested in resources built around <a href=""https://en.wikipedia.org/wiki/TimeML"" rel=""nofollow noreferrer"">TimeML</a>, I think there are some corpora and specific parsers specialized in extracting the time details of events. I don't remember any specifics but I tried to google ""timeml extract time"" and found a few related resources, that might give you at least some inspiration about how people have dealth with similar problems.</p>

<p>If you don't find anything that suits your need, in general the best approach would be to train a custom NER using your own annotated dataset with ""start-time"", ""end-time"", ""neither"" labels.</p>
","0","2","64377","14816"
"66728","<p>That would depend on the exact goal of the task and the specifics of the dataset, but in general I would say that it's always better to use the information specifically provided with the data if it's relevant for the task. In this case the rating for the product is indeed very likely to reflect the sentiment of the text, so I would go with it. Notice that you could also do both and compare the cases where the predicted sentiment differs from the one derived from the rating.</p>

<p>Given that the ratings are provided as 1-5 scores I would also consider the option of treating the task as a regression problem, instead of the standard binary classification setting.</p>
","1","2","64377","14816"
"66778","<p>Don't know if this is what you need but I know of the <a href=""https://github.com/Accenture/AmpliGraph/"" rel=""nofollow noreferrer"">Ampligraph</a> library:</p>
<blockquote>
<p>Python <a href=""https://docs.ampligraph.org"" rel=""nofollow noreferrer"">library</a> for Representation Learning on Knowledge Graphs</p>
</blockquote>
","2","2","64377","14816"
"66844","<blockquote>
  <p>When I undersample and train my classifier on a balanced dataset and test on a balanced dataset, the results are pretty ok</p>
</blockquote>

<p>It's not surprising that the results are good since the job is easier in this case. It's actually a mistake to test on the artificially balanced dataset, since it's not a fair evaluation of how the system will perform with real data.</p>

<blockquote>
  <p>Should I train the model on the original imbalanced dataset if I want it to perform well on real world test data that is also imbalanced.</p>
</blockquote>

<p>Both training on the original dataset or the balanced dataset are valid methods, choosing between the two options is a matter of design and performance. It's often a good idea to try both and then pick the one which performs better than the other on the real imbalanced dataset.</p>
","1","2","64377","14816"
"66851","<p>That would be a <a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">sequence labeling</a> task, the most common type is <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition</a>, you'll find many examples about it but you can train a custom system with your data. The traditional method is <a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">Conditional Random Fields</a>, there are a good few libraries available.</p>

<p>Side note: usually a single CRF model is used to do both detecting and labeling at once (your steps 1 and 2).</p>
","1","2","64377","14816"
"66894","<p>It's a matter of data quality so it depends how the dataset was built: </p>

<ul>
<li>Either these instances are meaningful, i.e. it makes sense that an observation would have zeros for all the features and that it would happen that often.</li>
<li>Or these are the result of an error, typically the complete absence of measurement for these observations.</li>
</ul>

<p>Naturally one wants to keep the instances in the data in the former case but (usually) not in the later case, because the values don't represent an actual data point so they would introduce a massive bias for the clustering algorithm.</p>
","3","2","64377","14816"
"66900","<p>You certainly need to add at least one other variable representing the time of year, because from your graph it's clear that the fare can't be predicted accurately using only the time until departure: for the same day you can have many points representing different fares. That makes sense, since the fares are going to be very different depending if the flight occurs during holiday season or not. There might also be other variables of interest but since it's always the same flight I can't think of any.</p>

<p>I would suggest plotting a graph where x is the time of year and the time before departure is represented for instance with colour or gradient, that should make things clearer.</p>
","0","2","64377","14816"
"67058","<p>There can be no objective answer to this question. Obviously the more one understands the better, but the field of ML is vast, quite specialized and ranges from very theoretical to very applied research, so it's perfectly reasonable to publish in ML without a strong background in maths. </p>

<p>A better way to estimate your own ability to publish papers in a particular area or journal is to study recent papers published in this area/journal. You should be able not only to understand them but to redo the reasoning: </p>

<ol>
<li>Understand the problem and the solution proposed by the authors</li>
<li>starting from the initial problem, how would you solve it? Can you think of alternatives to the authors' solution? </li>
<li>Can you find limitations to their approach and improve on it?</li>
</ol>

<p>If you reach step 3 then congratulations: you are ready to publish your own research!</p>
","2","2","64377","14816"
"67059","<p>This is a standard problem with distance/similarity measures between texts of different length. I'm not aware of any standard way to solve it, but in your case I would simply try to remove any email shorter than a certain length from the training set (you can experiment with different thresholds). This would hopefully force the centroids to be more specific, the goal being that none of them can easily attract all the instances.</p>
","1","2","64377","14816"
"67068","<p>Like any preprocessing step, feature selection must be carried out using the training data, i.e. the process of selecting which features to include can only depend on the instances of the training set. </p>

<p>Once the selection has been made, i.e. the set of features is fixed, the test data has to be formatted with the exact same features. This step is sometimes called ""applying feature selection"" but it's an abuse of language: it's only about preparing the test data with the features which were previously selected during the training stage.</p>

<p>Applying feature selection on the test data is a mistake because the training depends on it, so that would mean that the model ""has seen"" the instances of the test set and therefore invalidate the results on the test set.</p>
","3","2","64377","14816"
"67073","<p>Weights across different types of models are not always comparable, so
I think that it would make more sense to do this kind of comparison not across different types of model but within a single type of model varying:</p>

<ul>
<li>the hyper-parameters (if any),</li>
<li>the set of instances (e.g. selecting different subsets randomly),</li>
<li>the set of features.</li>
</ul>

<p>I'd recommend in particular varying the instances: if the weight of a feature tends to change a lot depending on the training set it's a sign of overfitting.</p>

<p>If the goal is to compare the importance of some specific features for different models, it's probably more reliable to directly evaluate how the model performs with/without the feature(s).</p>
","3","2","64377","14816"
"67085","<p>There's probably a bit of interpretation involved in the question, here is my take on it. </p>

<p>For the sake of clarity let me start with my definition of the concept: overfitting is when a model takes into account patterns which are present in the training data by chance, i.e. the model assumes that these patterns are characteristic of the distribution even though they're not.</p>

<blockquote>
  <p>Is it true to argue that no effective learning has taken place here?</p>
</blockquote>

<p>No it would be wrong to say that: the fact that a model overfits does not mean that no effective learning happened at all. In fact it's often the case that a model successfully acquires the patterns that it's supposed to capture (let's call this effective learning), <em>but also</em> acquires patterns that it shouldn't (overfitting). With any complex data it would even be very rare that no overfitting happens at all, and in fact very often it's hard to say exactly where is the line between effective learning and overfitting. </p>

<p>However people usually talk about overfitting when it's actually in excess, that is when the model generalizes too much on the ""chance patterns"" and not enough on the patterns which actually characterize the distribution. A performance which is significantly lower on the test set than the training set is a typical sign of such ""excess overfit"". So in this sense (excessive) overfitting is when the ""chance patterns"" cause the model to be suboptimal. But even a suboptimal model may have learned some relevant patterns.</p>

<blockquote>
  <p>If so, can one conclude from this that a better model with better accuracy in test exists that does not overfit?</p>
</blockquote>

<p>Not really: as said above overfitting can make the model perform poorly, so naturally it's usually the case that a non-overfit model performs better than an overfit one. However there's no guarantee that a better performance can be reached just by getting rid of the overfitting: as an extreme example, if the features are mostly random and/or not correlated with the response variables then it's very likely that model will overfit, but the performance would be terrible without overfitting anyway.</p>

<p>In the case described by OP I would say that it's always worth trying to avoid (excess) overfitting: first of course because it can only improve performance, but also because at a more general level it means that the model is not very reliable. If later the model is applied ""in production"" to a large set of instances which happen not to have this particular ""chance pattern"" which was in the training data, the model is going to go very wrong and it will be too late to detect it.</p>
","3","2","64377","14816"
"67135","<p>There's no simple answer to this question. As far as I know in general the choice depends mostly on the type of classification:</p>

<ul>
<li>Bag of Words (usually with tf-idf weights) is a simple but quite efficient representation for classification based on the text topic or similar, assuming the classes are reasonably distinct from each other.</li>
<li>Word embeddings are a more advanced option for semantic-based classification. They can handle more subtle semantic relations but require being trained on a large training corpus. Using pre-defined embeddings can be a solution but then there's the risk that the original training data isn't perfectly suitable for the dataset.</li>
<li>N-grams models can be used in many different ways but are often chosen when the classification involves syntax and/or writing style. Note that the higher the value <span class=""math-container"">$n$</span>, the larger the training corpus needs to be, this can also be taken into account in the choice.</li>
</ul>

<blockquote>
  <p>I might have around 40 categories and then around a same number of sub-categories upto 4 levels.</p>
</blockquote>

<p>It depends on the data but 40 classes is already a very challenging classification task. For the sake of simplicity let's assume a uniform distribution over classes: a random baseline accuracy would be 1/40 =  2.5%. Of course it depends on the data and a good classifier will do better than that, but don't expect too much... </p>

<p>Now 4 levels of 40 sub-categories means 40^4 = 2.5 millions classes! Even assuming you have enough data (say around 10 instances by class in average, that is 25 millions instances!), it's very unlikely that a classifier will be able to predict anything useful from such a huge amount of classes.</p>
","2","2","64377","14816"
"67136","<p>According to your definition (consecutive, order matters, max +/-2 difference), it's not a fuzzy matching case. It's just a minor variant of searching a subsequence:</p>

<pre><code>for i=0 to len(source)-len(test) {
  j=0
  while (j&lt;len(test)) &amp;&amp; (abs(source[i+j]-test[j]) &lt;= 2) {
    j++
  if (j == len(test)) { 
     // match found
  }
}
</code></pre>

<p>This is the simple version, in case efficiency is an issue there is a more efficient version using dynamic programming.</p>
","1","2","64377","14816"
"67297","<p>In theory it's of course possible to reach perfect performance: if the algorithm can find what it needs in the features to correctly distinguish between classes (or clusters), then it will perform perfectly. </p>

<p>In reality however it's very rare that performance is perfect, because:</p>

<ul>
<li>Text data is noisy and extremely diverse</li>
<li>Most of the time when there is a way to obtain perfect performance, there is also a simple heuristic which can do the same job more efficiently than using ML. Basically ML is used precisely <em>because</em> the task is hard and/or the data is complex, so it's not surprising that there are errors.</li>
</ul>

<p>In the case of your problem, I notice that you have labels for your data but you use an unsupervised topic modeling method, right? If this is the case you might want to try using a supervised method, since the system would have more clues to find the right answer. Also you use accuracy for evaluation, so be careful: accuracy can be misleading since it doesn't give any detail about the different classes. </p>
","2","2","64377","14816"
"67299","<p>I'm not sure if that's what you need but this library seems pretty similar to your problem: <a href=""https://github.com/Accenture/AmpliGraph/"" rel=""nofollow noreferrer"">https://github.com/Accenture/AmpliGraph/</a></p>
","0","2","64377","14816"
"67300","<p>Decision trees are supervised methods, so they need to be trained on some annotated data. Thus the general idea is the same as for any text classification: given a set of documents (for instance represented as TFIDF vectors) together with their labels, the algorithm will calculate which how much each word correlates with a particular label. </p>

<p>For instance it might find that the word ""excellent"" often appears in documents labeled as positive, whereas the word ""terrible"" mostly appears in negative documents. By combining all such observations it builds a model able to assign a label to any document.</p>
","1","2","64377","14816"
"67302","<p>Yes, I think that's a sound approach and a good way to compare different systems. </p>

<p>A ROC curve comparison is usually more informative than the raw performance scores, but it's still quite general. In case you want to observe even more detail, you could also try to look at specific groups of instances. One way to do that is to count for every instance how many systems correctly classify it: an instance almost always correctly classified is ""easy"", and conversely an instance which is almost always misclassified is ""hard"". It's often interesting to look at what happens specifically for the ""hard"" instances with the different systems. You could take a subset of ""hard"" instances and calculate the performance or ROC curve only on those, in order to distinguish more precisely the best systems. </p>

<p>For the record, if it makes sense for your task you might also want to consider more flexible scoring methods for text spans: currently it seems that your evaluation considers an answer correct only of the exact span is predicted. You could consider counting the fact that a span is partially correct, for instance by counting the number of tokens correctly annotated.</p>
","1","2","64377","14816"
"67304","<p>Paraphrase detection is still a very active and very challenging research area, so it's unlikely that there are full-fledged standard libraries for this task since there is still no clear ""best solution"" to this problem.</p>

<p>In order to build a corpus you might want to look at how shared tasks/competitions have done it before. I know at least of <a href=""http://alt.qcri.org/semeval2015/task1/"" rel=""nofollow noreferrer"">SemEval</a> which often proposes tasks related to paraphrases (there might be others). I haven't checked but usually the overview paper of the task (e.g. <a href=""https://www.aclweb.org/anthology/S15-2001/"" rel=""nofollow noreferrer"">here</a>) explains how the corpus was built and presents the main approaches submitted by participants.</p>
","2","2","64377","14816"
"67329","<p>Your problem is indeed a typical <a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">one-class classification problem</a>, and as far as I know one-class SVM is usually a good option for that. </p>

<p>I think you should investigate what causes the poor performance:</p>

<ul>
<li>Evaluating with accuracy is probably not informative enough, you would need to find out at least whether the errors tend to be mostly false positive or false negatives, thus using precision/recall.</li>
<li>You could look at what is happening at the level of features: I would expect some words specific to sports to be assigned a strong weight by SVM for instance. It could also be a problem with the dimensionality being too high, maybe you need to remove stop words or filter out rare words, etc.</li>
</ul>
","1","2","64377","14816"
"67367","<p>As Benj said, there's no general answer since it depends not only on the algorithm but also a lot on the data. It's easy to find examples where the exact same size of data with the same algorithm performs terrible in one case and perfectly in the other.</p>

<p>Given a particular dataset and a particular algorithm, there are experimental methods which can help determine the relationship between data size and performance:</p>

<ul>
<li>Ablation study: train a model using say 10%, 20%, 30%... 100% of the training data and evaluate (preferably with cross-validation) for each subset, then plot the performance at each stage. The evolution of the performance across various sizes shows how much gain in performance is made with each step of additional data, and by extrapolation one can roughly predict how much more would be gained with more data. </li>
<li>Features: the complexity of the data depends a lot on the number and diversity of the features, so in order to get a full picture of the relationship between data size and performance it's important to also study how different subsets of features perform. It's possible that a certain size of data gives poor performance with a large set of features, but the same set of instances with less features performs perfectly well. </li>
</ul>
","1","2","64377","14816"
"67369","<p>There is a quite detailed comparison with references here: <a href=""https://towardsdatascience.com/a-tale-of-two-macro-f1s-8811ddcf8f04"" rel=""nofollow noreferrer"">https://towardsdatascience.com/a-tale-of-two-macro-f1s-8811ddcf8f04</a></p>

<p>Basically the two definitions are used and both can be considered valid. For the sake of clarity I would recommend mentioning which definition you are using when you report your results.</p>
","2","2","64377","14816"
"67372","<blockquote>
  <p>Naïve Bayes is robust to irrelevant features. What does this mean? Can anyone give an example how does the irrelevant features cancels out and what are the irrelevant features?</p>
</blockquote>

<p>Imagine a classifier for sentiment analysis. For a strongly positive word like <span class=""math-container"">$w=great$</span>, the conditional probability <span class=""math-container"">$p(w|pos)$</span> is going to be quite high whereas <span class=""math-container"">$p(w|neg)$</span> is going to be quite low, so the posterior <span class=""math-container"">$p(pos|d)$</span> for a document <span class=""math-container"">$d$</span> containing this word is likely to be much higher than <span class=""math-container"">$p(neg|d)$</span>.</p>

<p>Now what happens with a neutral word <span class=""math-container"">$w=today$</span>? Neither <span class=""math-container"">$p(w|pos)$</span> or <span class=""math-container"">$p(w|neg)$</span> is going to be much higher than the other. So all other things being equal, the difference between the two posterior probabilities is not going to depend much on this word compared to other more relevant words, for instance ""great"".</p>

<blockquote>
  <p>It is optimal if the independence assumption holds. Can anyone give an example of independence assumption not holding? I think it would be related to presence of words like Hong Kong, United Kingdom etc in a sentence.</p>
</blockquote>

<p>In practice the independence assumption almost never holds with real data. For example words in a text actually depend on each other, that's how they make sense together in a sentence. This is true for entities like ""Hong Kong"" but also for virtually any sentence. For instance ""I love chocolate but you hate it"" doesn't mean that same as ""You love chocolate but I hate it"", or ""it chocolate you hate love but"" which means nothing. NB will treat all these variants the same way: basically the model assumes independence for the purpose of making things simpler and easier to compute, and it turns out that it works quite well in general, despite the massive simplification.</p>
","1","2","64377","14816"
"67384","<p><em>Disclaimer: you need to ask a legal expert: software patents is a very complex and specialized domain, so don't rely on strangers on the internet ;)</em></p>

<p>As far as I know some algorithmic methods are patentable, but there are conditions to satisfy, in particular about the novelty of the method. You would probably have to prove the novelty by doing a thorough literature review, it's probably not enough to show that the software leads to efficiency improvements.</p>
","1","2","64377","14816"
"67412","<p>Something like this for instance:</p>

<pre><code>library(plyr)
ddply(data,'region',function(x) {mean(x$age)})
</code></pre>
","0","2","64377","14816"
"67413","<p>It's probably a quite subjective matter, here is my opinion:</p>

<ul>
<li>I would say that this method makes a lot of sense if the ML model uses a number of features, something which would be hard to factor properly in a standard statistical test (especially if some kind of feature selection is involved).</li>
<li>Also in favor of the method, statistical significance tests are usually interpreted in a binary way (typically p-value higher or lower than 0.05) even though the underlying p-value is continuous: there's very little different between a p-value of 0.049 and 0.51 but the two will be interpreted in a completely opposite way.</li>
<li>However I agree with you that statistical significance tests are much more grounded in statistical theory, with a clear interpretation especially about levels of confidence.</li>
</ul>

<p>Overall I can see an interest of using such a method from a practical viewpoint, but it is indeed limited in terms of theoretical interpretation and reliability.</p>

<p>Disclaimer: I'm not a statistician, I might be biased towards practical ML methods ;)</p>
","0","2","64377","14816"
"67531","<p>Here are a few general thoughts about your project:</p>

<p>As far as I understand, you're trying to extract very specific information from a semi-structured database using free-form natural language queries, correct? If yes it's important that you realize that this is a quite ambitious project, reaching a decent stage of quality is probably going to take a lot of work, and the performance is unlikely to be perfect.</p>

<p>Apparently numerical values and units are important pieces of information for matching the query. In this case you should probably implement a special process for those, because standard text processing is not going to work very well.</p>

<ul>
<li>Detection: if there is only a small number of possible ways in which these values are written, it's probably more efficient to use ad-hoc regular expressions. If not, you could try to train a custom <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">NER</a> model.</li>
<li><p>Representation: that's the tricky part imho. For general text vectors are fine, but with vectors it's difficult/impossible to represent special values like those. Given that you have a semi-structured database, you might want to try a more semantic representation adapted to your data: that could involve techniques from <a href=""https://en.wikipedia.org/wiki/Semantic_role_labeling"" rel=""nofollow noreferrer"">semantic role labeling</a>, <a href=""https://en.wikipedia.org/wiki/Relationship_extraction"" rel=""nofollow noreferrer"">relationship extraction</a>, etc.</p></li>
<li><p>Matching: the advantage of a semantic representation is that you could convert the query to a semantic representation and then apply a detailed matching procedure suited to your data (in particular you can compare corresponding numerical values and use a threshold, or return the difference to represent how far they are).</p></li>
</ul>
","2","2","64377","14816"
"67535","<p>I don't think your question deserves so many downvotes, but I can understand (well guess at least) why ML experts don't see it as a legitimate question:</p>

<ul>
<li>It's a bit naive: while ML has concepts such as reinforcement, reward, cost etc., and of course these terms are derived from their respective meaning in the general language, but in the ML context these have a specific formal meaning. In particular they are not meant to be assigned hard-coded values like you suggest.</li>
<li>In the same idea, ML requires a precise, formal representation of concepts. What would be an action in this context? How would you represent an agent/person, and the interactions between them?</li>
<li>Currently as far as I can see what you propose is a kind of simulation involving predefined mechanisms, there's no actual data involved. The only way to evaluate this would be to compare against real cases whether the simulation ends with the same results, and that would also be difficult to formalize properly.</li>
</ul>

<p>In my opinion why not? It could be an interesting idea, but there are a lot of details to clarify, and I suspect that psychological disorders are not the easiest thing to represent accurately in a formal setting.</p>
","1","2","64377","14816"
"67547","<p>According to your description you can only use similarities between descriptions, and since there's no labelled data it has to be unsupervised. </p>

<ul>
<li>Option 1: heuristic (i.e. ad-hoc unsupervised method). Based on your knowledge of the specifics of the data, implement a function which returns a score representing how similar two descriptions are. For example a basic matching could simply count the number of common items between the two descriptions divided by the number of items in the longest description. For a query description return the N top similar matches after comparing it to every potential match. Naturally this can be improved in many ways.</li>
<li>Option 2: represent each description as a vector and use any generic similarity measure over pairs of vectors (e.g. cosine).  The representation can be some kind of direct representation, e.g. some kind of TF-IDF vector, or a more sophisticated  representation as a tree or graph embedding.</li>
</ul>
","2","2","64377","14816"
"67608","<p>In general tidy data is great... but it can quickly become unreasonably large. This is the main reason why I usually try to refactor my data in a tidy format as late as possible in the process. </p>

<p>Example: imagine a dataset containing <span class=""math-container"">$N$</span> instances, with columns <code>feature1</code> ... <code>featureX</code> and <code>result1</code>... <code>resultY</code>, where the <code>result?</code> columns represent some value based on several methods/parameters. The tidy version would have columns <code>feature1</code> ... <code>featureX</code> plus <code>method</code> (which takes as values <code>result1</code>,..,<code>resultsY</code>) and of course <code>result</code> for the result value. The tidy version would contain <span class=""math-container"">$N \times Y$</span> instances, i.e. for each instance it repeats the values of the <span class=""math-container"">$X$</span> features <span class=""math-container"">$Y$</span> times. If <span class=""math-container"">$X$</span> is large the dataset is going to be very large in memory (and also stored as a file).</p>

<p>Naturally this gets even worse when you have for example several steps involving various parameters, multiplying the instances every time is not always feasible.</p>
","2","2","64377","14816"
"67653","<blockquote>
  <p>First question; is it possible to pass an entire data frame as a variable to a model?</p>
</blockquote>

<p>No, each feature must be a single value. In other words you could provide the data frame as a vector containing all the values, assuming the size is fixed: each column would correspond to a specific cell in the original data frame.</p>

<p>But I think an even better option in your case is to look into methods which take (chronological) sequences into account. <a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">Conditional Random Fields</a> might be a good option, assuming you need to predict the target variables for the whole sequence? </p>
","2","2","64377","14816"
"67728","<p>Clearly this problem doesn't always have a unique solution, but if you are interested in finding one possible solution you could try a simple genetic algorithm simulation:</p>

<ul>
<li>Each individual gene represents an item from the list of all possible items. </li>
<li>Each gene/item is assigned a price randomly at first (gene expression)</li>
<li>When a mutation is applied to a gene/item, its price is slightly modified randomly.</li>
<li>A crossover causes a ""child gene"" to take as value the mean of its two ""parents genes"".</li>
</ul>

<p>This setting means that every individual in a population consists of all the items being assigned a particular price. At each generation each individual/assignment is evaluated by applying the prices assignment to the actual data and then measuring the error compared to the actual prices. Finally the top N individuals/assignments which perform the best are selected as parents for the next generation. Eventually the population should converge to realistic prices assignments.</p>

<p>I think this is a perfect case for a genetic algorithm because the evaluation of a potential price assignment is a very simple calculation, so there is no major efficiency issue repeating the process over many generations (as opposed to many problems where evaluation is prohibitively expensive).</p>
","0","2","64377","14816"
"67729","<p>Indeed it's often a good idea to remove boolean features which are very rare, but the problem is that choosing a threshold by intuition is not necessarily optimal. Whenever possible the optimal value should be determined experimentally, and typically that should be possible for efficient methods such as log regression or SVM. The idea is simply to consider a range of values as threshold and run a grid search using a subset of the training set. The threshold is exactly like an hyper-parameter for the learning method. </p>
","1","2","64377","14816"
"67730","<p>I think that the most natural way to design this problem is indeed with missing values, and of course that means using a learning algorithm which can deal with these. Additionally to NB I think decision trees (at least some implementations) can handle missing value as well.</p>
","1","2","64377","14816"
"67731","<p>As far as I understand (disclaimer: I'm not very familiar with Python) your approach is correct: the selected hyper-parameters are tested on the hold out test set which is different from the training set, this way there's no data leakage and you can evaluate the true performance of your model before applying it to the test set.</p>

<p>For analysis purposes it could be useful to compare the performance of the best model on X (training set) and X_test (hold out) in order to check for overfitting.</p>

<p>Note that in a case like this where you directly select the best hyper-parameters I would consider it acceptable to skip the testing on the hold out set, however in this case you wouldn't know the true performance of your model (so for instance you wouldn't be able to check if it's overfit). To be clear: I don't think you should do this, it's just a remark to show the difference with/without hold out set. </p>
","1","2","64377","14816"
"67750","<p>That depends what you want to show and how much information fits in the graph. Typically you can think of:</p>

<ul>
<li>Simply using a different colour for every dataset, but 20 is probably too many and that will make the graph hard to read.</li>
<li>Using boxplots (or violin plots) at every instant, where the boxplot represents the 20 values of the dataset.</li>
</ul>
","1","2","64377","14816"
"67769","<p>Using NER (more generally sequence labeling) means classifying every token in the sentence, so if the goal is only to label every sentence there's no strong need for it in your case.</p>

<p>However NER might be more appropriate in case the order of the words is important, because sequence labeling models take it into account whereas traditional text classification methods often use a ""bag of words"" representation (order doesn't matter).</p>

<p>To some extent it also depends whether the labels are always related to a particular term in the sentence: if yes, then NER might be better at locating these terms (this is related to the point about order). If no, then classifying at the level of sentences is likely to perform better.</p>
","3","2","64377","14816"
"67923","<p>For comparing two rankings <a href=""https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"" rel=""nofollow noreferrer"">Spearman's rank correlation</a> is a good measure. It's probably worth a try, but since your gold truth appears to be binary I would think that <a href=""https://stats.stackexchange.com/questions/95391/what-is-the-definition-of-top-n-accuracy"">top-N accuracy</a> (or some variant of it) would be more appropriate (advantage: easy to interpret). You could also consider using the <a href=""https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve"" rel=""nofollow noreferrer"">Area Under the Curve (AUC)</a>, using the predicted rank as variable threshold (less intuitive to but doesn't require choosing any top N).</p>
","1","2","64377","14816"
"67926","<p>It's a quite complex problem and there might be better options for the design (I'm thinking maybe something more specific to times series)... </p>

<p>However before that there's a more obvious problem to solve: it seems that you are calculating accuracy on the ""Glucose_t"" numeric value, right? If yes this is incorrect and that would explain your terrible results: </p>

<ul>
<li>Accuracy is an evaluation measure for classification tasks, not for regression tasks: accuracy simply checks for each instance whether the predicted value (which is supposed to be a categorical value) is the same as the gold true value, and then divides the number of correct cases by the total. Naturally it's very hard to predict the exact true value in the case of numbers: if the true value is 183 but the algorithm predicts 184, then accuracy would count that as incorrect even though it's very close. That would explain why your accuracy is very low. </li>
<li>Typical regression evaluation measures are <a href=""https://en.wikipedia.org/wiki/Mean_absolute_error"" rel=""nofollow noreferrer"">mean absolute error</a> and <a href=""https://en.wikipedia.org/wiki/Mean_squared_error"" rel=""nofollow noreferrer"">mean squared errors</a>. These measures (and their variants) are designed to calculate how far the predicted numerical value is from the true value. If you use these keep in mind that it's an error score, so the lower the value the better.</li>
<li>If you want to use accuracy, it would make sense to use it on your ""event"" value (which is derived from your ""glucose_t"" prediction I assume?): in this case you have 3 classes (categorical values), and I bet the accuracy will be much better. Note that accuracy can be biased by class imbalance, so don't forget to check the confusion matrix.</li>
</ul>
","1","2","64377","14816"
"67927","<p>It's a supervised classification problem: you're trying to predict the destination (class) based on some categorical features (input columns). I would suggest starting with some simple algorithms such as decision trees or Naive Bayes.</p>

<p>However I'm guessing that logistical shipments can evolve over time: maybe a shipper business grows with country X but decreases with country Y, etc. If this is relevant, it might make sense to look into more advanced methods which could take the chronological evolution into account (time series).</p>
","1","2","64377","14816"
"67992","<blockquote>
  <p>Anyways is this problem I am trying to solve fit into the general NLP ML space? </p>
</blockquote>

<p>Generally speaking, feeding the source data in bulk to a ML system is unlikely to give the kind of structured output you expect. It's likely that you would have to somehow guide the process in the direction of what you want to obtain, and this might take a lot of time and effort (depending on the requirements).</p>

<p>That being said, there are indeed NLP methods which are meant to extract specific pieces of information from text, and it usually works quite well with domain-specific data (provided it's done correctly, caveats apply). I'm just going to list a few typical tasks which can be done:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition</a> would be the most common and probably the most simple, since there are many existing libraries. Most libraries use a pre-trained model, but it's likely to give much better results when it's trained on the kind of data it's applied to (of course that usually means manually annotating your own training set).</li>
<li><a href=""https://en.wikipedia.org/wiki/Document_classification"" rel=""nofollow noreferrer"">Text classification</a> can be used to automatically assign documents to a category (class) among a set of predefined categories. This is supervised so you would also need a training set containing labelled documents. Again there is a number of algorithms and libraries available.</li>
<li>Simple information retrieval methods based on measuring semantic similarity (see e.g <a href=""https://datascience.stackexchange.com/questions/12872/how-can-i-get-a-measure-of-the-semantic-similarity-of-words"">here</a>) between terms and documents can be used to search e.g. documents relevant to a term.</li>
<li><a href=""https://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow noreferrer"">Topic modeling</a> is an unsupervised approach which groups similar documents together (clustering). Since it's unsupervised it doesn't require any training data, but on the other hand what the algorithm finds as ""topics"" (groups) is often different than what a human would expect. </li>
<li><a href=""https://en.wikipedia.org/wiki/Relationship_extraction"" rel=""nofollow noreferrer"">Extracting relations between concepts</a> (typically between named entities) is a more advanced task which usually requires more work in order to capture the specifics of the job. I'm not aware of any general library for that.</li>
</ul>

<p>Overall there are many things possible, but the first step would be to try to design the system precisely, typically using some of the existing tasks as building blocks. </p>

<blockquote>
  <p>Is it fine to include a specific document in multiple categories? For example an instance in AWS can be in category ec2 and a general category Computing unit</p>
</blockquote>

<p>Yes it's fine, but if you want a ML classifier to be able to predict several classes you will need to use <a href=""https://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow noreferrer"">multi-label classification</a> (the ""standard"" is single label).</p>

<blockquote>
  <p>Is the starting point to this is to build a corpus? I tried it out nltk categorized corpus builder.</p>
</blockquote>

<p>I'd recommend building a corpus only once you have a clear idea of how you're going to use it. Also it's usually an experimental process with lots of back and forth, so try to progress iteratively rather than starting with strong assumptions/decisions which might later turn out not relevant.</p>
","2","2","64377","14816"
"67999","<p>Simple: </p>

<ol>
<li>Write a function which returns TRUE if the values in the input array are always increasing, FALSE otherwise. </li>
<li>Optional: give the value returned by the function as feature to the classifier. The training requires only two examples, one true and one false.</li>
</ol>

<p>Sorry I'm messing with you :)</p>

<p>Seriously, the point is: there's no interest at all to train a ML model for a problem which can easily be solved with an efficient deterministic algorithm. Use ML for problems which cannot be solved (or not as efficiently) without it.</p>
","0","2","64377","14816"
"68054","<blockquote>
  <p>Does the validation score's standard deviation have a correlation with overfitting / error </p>
</blockquote>

<p>Yes definitely: a high variance shows that the model is not stable across different training sets, which indicates a high risk of overfitting.</p>

<blockquote>
  <p>and can this be used in my scoring? </p>
</blockquote>

<p>Using the std dev directly in the scoring itself, I'm not sure. I would consider this more like a kind of qualitative indication. However if two different models have  similar performance but one with higher variance than the other, it's usually a good idea to select the latter.</p>

<blockquote>
  <p>currently the stdev is around 5% that seems ALOT.</p>
</blockquote>

<p>It always depends on the specifics of the data, but yes I'd say that 5% is quite high.</p>

<blockquote>
  <p>I propose that if I reduce the stdev of the validation scores of each k fold then this should decrease variance on out of sample predictions? </p>
</blockquote>

<p>I don't understand what you mean here.</p>

<p>In general the standard way to reduce overfitting is to increase the ratio between number of instances and number/complexity of the features:</p>

<ul>
<li>Add more instances in the training set if possible (but it's rarely possible)</li>
<li>Apply feature selection to reduce the number of features</li>
<li>Simplify the most complex features; that depends on the task, but typically removing values which appear very rarely can help.</li>
</ul>
","3","2","64377","14816"
"68111","<p>Experimentally: using cross-validation on a subset of your training data, compute the performance of every option that you want to consider. Then select the best option and train the final model using this option.</p>

<hr>

<pre><code>// different settings for hyper-parameters, 
// for instance different pruning criteria:
hpSet = { hp1, hp2, ...}  

trainSet, testSet = split(data)

for each hp in hpSet:
    // run cross-validation over 'train' using hyper-parameter 'hp' 
    // and store resulting performance
    perf[hp] = runCV(k, trainSet, hp)

bestHP = pick maximum hp in 'perf'
model = train(trainSet, bestHP)
perf = test(model, testSet)
</code></pre>
","1","2","64377","14816"
"68174","<p>First, I wouldn't use the word ""noisy"" here because if you know which instances are ""wrong"" then these are not noise, they are negative examples. In my opinion ""noisy"" is when positive and negative cases are mixed together in a way that makes it difficult (or impossible) to distinguish between them. I think this matters because you're more likely to find similar use cases and relevant methods using this terminology.</p>

<p>I don't have a precise method to suggest but I would check the state of the art in machine translation: it's also a sequence-to-sequence task in which there are potential positive/negative cases. In particular there has been some work done in MT quality estimation, where the goal is to predict the quality of a translation for a sentence. This might be related because it's about labeling or quantifying how good a translation is, and I would assume that there are works which re-use labelled/scored translations (including potentially wrong ones) in order to obtain a better model. Unfortunately I don't have any pointers since I haven't followed the field recently.</p>
","3","2","64377","14816"
"68233","<p>There are two issues:</p>

<ul>
<li>Linear regression might be simple to calculate, but it's not sure that the relation between your variables is actually linear. If not, sometimes this simplification is ok because there's not too much variation, but sometime it's completely wrong.</li>
<li>There's no general minimum number of instances valid for every case. Generally the more complex the relation one wants to represent, the more instances one needs.</li>
</ul>

<p>I'd suggest you start by plotting your data: visualizing the relation between the variables should tell you whether linear regression is a good option, and seeing how scattered your points are should tell you whether you have enough instances.</p>
","1","2","64377","14816"
"68308","<p>I'm assuming you're asking about the intuition behind Naive Bayes (NB). For the sake of clarity I'm considering only categorical features. Gaussian NB is simply an application of NB to numerical features (assumed to be normally distributed).</p>

<p>During training every <span class=""math-container"">$p(f_i|C_k)$</span> is calculated by counting how often the feature value <span class=""math-container"">$f_i$</span> is associated with class <span class=""math-container"">$C_k$</span> among all the other possible features values associated with <span class=""math-container"">$C_k$</span>:</p>

<ul>
<li>This is done by counting how often <span class=""math-container"">$f_i$</span> and <span class=""math-container"">$C_k$</span> appear together <em>across all the instances</em>. That's how NB generalizes: the fact that a feature appears with a particular class is just an example, but the fact it appears proportionally more often with class A than class B forms a pattern.</li>
<li>The probability <span class=""math-container"">$p(f_i|C_k)$</span> represents how important <span class=""math-container"">$f_i$</span> is <em>within class <span class=""math-container"">$C_k$</span></em>.  </li>
</ul>

<p>When predicting the class for a new instance:</p>

<ul>
<li>NB ""weighs all the pros and cons"" for this instance to be predicted as <span class=""math-container"">$C_k$</span> by combining all the <span class=""math-container"">$p(f_i|C_k)$</span> corresponding to this instance (in the sense that some of the probabilities <span class=""math-container"">$p(f_i|C_k)$</span> are low and some are high, so their product reflects the combination of ""pros"" and ""cons"" indications). </li>
<li>But even if <span class=""math-container"">$p(f_i|A) &gt; p(f_i|B)$</span>, it doesn't imply that <span class=""math-container"">$f_i$</span> is a strong indication of class <span class=""math-container"">$A$</span>, because maybe this is due to class <span class=""math-container"">$A$</span> being less frequent than class <span class=""math-container"">$B$</span>. This is taken into account in the prior <span class=""math-container"">$p(C_k)$</span>, which gives more importance to a frequent class than a rare one (this is the basis of the Bayes theorem).</li>
</ul>

<p>These last two points show how NB uses the ""knowledge"" of the trained model to make a prediction about any unknown instance.</p>
","0","2","64377","14816"
"68413","<p>A way to speed up this process is to preprocess the large dataset, the goal being to store the documents from A in a way which avoids a lot of useless comparisons.</p>

<ul>
<li>Store each document from A in an inverted index <span class=""math-container"">$m$</span>, so that for any word <span class=""math-container"">$w$</span> <span class=""math-container"">$m[w]$</span> is the list of all documents in A which contain word <span class=""math-container"">$w$</span> (note that a document can appear several times in this data structure).</li>
<li>When comparing a new query against <span class=""math-container"">$A$</span>, instead of iterating through all documents in <span class=""math-container"">$A$</span> just compare against the subsets which have at least one word in common, i.e. <span class=""math-container"">$m[w]$</span> for every word <span class=""math-container"">$w$</span> in the query.</li>
</ul>

<p>Couple remarks:</p>

<ul>
<li>Normally stop words would be excluded from the keys since they appear everywhere and they are not relevant for matching.</li>
<li>The key doesn't have to be a single word, it could also be an n-gram (or several n-grams) or even several words in case it fits in memory.</li>
</ul>

<p>This kind of problem is frequent in the task of <a href=""https://en.wikipedia.org/wiki/Record_linkage"" rel=""nofollow noreferrer"">record linkage</a>.</p>
","4","2","64377","14816"
"68431","<p>It simply depends how you want to reward/penalize tasks completed ahead/late and whether you need the score to be normalized.</p>

<p>For instance:</p>

<ul>
<li>0 for late</li>
<li>.5 for on-time</li>
<li>1 for early</li>
</ul>

<p>Then divide by the total number of tasks assigned to the employee. That gives you a score between 0 and 100%:</p>

<pre><code>| Emp.Name | Ahead(%) | OnTime(%)  | Deviation(%) |  Score  |
-------------------------------------------------------------
|   Tom    |    5     |    55      |      40      |  32.5%  |
|  Edwin   |   100    |     0      |       0      | 100.0%  |
|   Paul   |    0     |    100     |       0      |  50.0%  |
</code></pre>
","2","2","64377","14816"
"68432","<p>I'm afraid your data is probably too complex and specific for somebody else to understand exactly what's going on.</p>

<p>The only idea I can suggest is to try to analyze manually the errors that your model makes: </p>

<ul>
<li>Are there any patterns, like a kind of errors which happens quite often? For instance a country which tends to be overestimated, a month which tends to be under-estimated, this kind of thing.</li>
<li>If yes try to investigate why: for instance is there some additional feature that could help the model make better predictions for these cases?</li>
</ul>
","0","2","64377","14816"
"68463","<p>I'm not entirely sure but it looks like <a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">sequence labeling</a> might be what you need:</p>

<ul>
<li>Sequences of varying length </li>
<li>Supervised: you would need to train a model with a sample of sequences annotated with a label at every step (not sure that this is your use case?)</li>
<li>Can handle any number of features</li>
</ul>

<p><a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">Conditional Random Fields</a> is the state of the art method, there are many libraries
available.</p>
","1","2","64377","14816"
"68495","<p>Rank all your instances by their output value. Then for each instance (or for each different output value, if many instances have the same value), calculate the performance (precision, recall, f-score) considering that the threshold is the output value for this instance. When this is done for all the instances, you can plot the graph of f-score as a function of the threshold for a nice visual result, or simply pick the threshold corresponding to the maximum performance.</p>

<p>For the record, this is also how a <a href=""https://en.wikipedia.org/wiki/Receiver_operating_characteristic"" rel=""nofollow noreferrer"">ROC curve</a> can be built manually.</p>

<p>Btw your optimal threshold is certainly lower than 0.5, since your precision is 1.0 at 0.5 but you can probably still increase recall.</p>
","1","2","64377","14816"
"68567","<p>BLEU scores are based on comparing the translation to evaluate against a gold-standard translation. In general the gold-standard translation is the same source sentence translated by a professional translator, so in theory a professional human translation should always receive the maximum score of 1 (BLEU scores are normalized between 0 and 1).</p>

<p>However it's important to keep in mind that:</p>

<ul>
<li>Even professional translators don't always agree on what is the ""correct"" translation, so there's no perfect evaluation method.</li>
<li>There can be multiple valid translations for the same sentences. This can be taken into account in the BLEU score, but most of the time BLEU scores are calculated using a single translation. As a consequence it's possible that a perfectly good translation gets a low score.</li>
<li>BLEU scores are based on counting the number of n-grams in common between the predicted translation and the gold-standard. It's a quite good proxy for translation quality, but it's also very basic. In particular it cannot take the meaning into account.</li>
</ul>
","5","2","64377","14816"
"68586","<p>The approach you propose is ok but it can be improved in my opinion:</p>

<ol>
<li>Find a corpus (or several corpora), chosen to be representative of the kind data you expect to process eventually. Ideally it would be directly a sample of the target data. </li>
<li>Label <em>all the sentences in the corpus</em> or a random subset. This is because you need to preserve the distribution as much as possible, especially the proportion of positive/negative instances. If you start from the set of filtered sentences you'll have two problems: 

<ul>
<li>a proportion of positive cases higher than in regular text, and this is likely to make your model over-predict positive cases;</li>
<li>sentences containing only the selected trigger words, and this will cause your model to predict as negative any sentence which doesn't have a trigger word (that's a problem since your list of trigger words can't be exhaustive).</li>
</ul></li>
</ol>

<p>Ideally you would manually label all your corpus, but that's probably not realistic. That's why you could try to use your idea of trigger words in a slightly different way in order to label all the instances efficiently:</p>

<ol>
<li>Filter the sentences which contain any trigger word and label these; this is your initial training set</li>
<li>Use a <a href=""https://en.wikipedia.org/wiki/Semi-supervised_learning"" rel=""nofollow noreferrer"">semi-supervised</a> approach to label the rest of the sentences (maybe you could consider <a href=""https://en.wikipedia.org/wiki/Active_learning_(machine_learning)"" rel=""nofollow noreferrer"">active learning</a>).</li>
</ol>
","1","2","64377","14816"
"68648","<p>Machine Learning is useful for problems which can't be solved (or not efficiently) with a deterministic method. The problem you propose is extremely simple since it doesn't even require any calculation, so there is no point using ML for it.</p>

<blockquote>
  <p>Could you please give a toy example. We just have y and don't have x and z</p>
</blockquote>

<p>Suppose </p>

<p><span class=""math-container"">$y=\begin{bmatrix}1 &amp; 2\\
3 &amp; 4\\
5 &amp; 6\\
7 &amp; 8\\
\end{bmatrix}$</span></p>

<p>An easy solution is to define <span class=""math-container"">$x$</span> and <span class=""math-container"">$z$</span> as follows: </p>

<p><span class=""math-container"">$x=y=\begin{bmatrix}1 &amp; 2\\
3 &amp; 4\\
5 &amp; 6\\
7 &amp; 8\\
\end{bmatrix}$</span> and  <span class=""math-container"">$z=\begin{bmatrix}1 &amp; 0\\
0 &amp; 1\\
\end{bmatrix}$</span></p>
","1","2","64377","14816"
"68649","<p>I don't think it's worth doing any kind of significance test for such a small sample.</p>

<h1>option 1: pure categorical values</h1>

<p>You could just count the number of identical ratings between the two ratings and divide by the total number of players</p>

<h1>option 2: take the order into account: weak &lt; moderate &lt; strong</h1>

<p>Define a similarity measure such as:</p>

<ul>
<li>sim(X, X) = 1</li>
<li>sim(weak,moderate) = 0.5</li>
<li>sim(moderate,strong) = 0.5</li>
<li>sim(weak,strong) = 0</li>
</ul>

<p>Then use the same process as option 1 except that for every player you add sim(ratingA, ratingB), and divide by the total. This will also give you a normalized score between 0 and 1 which is a bit more precise (the value will be higher or equal than in option 1). </p>
","0","2","64377","14816"
"68671","<p>You're probably aiming too high: a research topic doesn't have to lead to a major breakthrough, and very often it's impossible to know what it leads to before doing it. A Master thesis is not very long so you need to find a research topic which is feasible within the time frame. A common mistake by Master students is to spend too much time on implementation, then botching the analysis and/or writing a poor quality dissertation because there's not enough time left.</p>

<ul>
<li>Discuss with your advisor, they have the experience and they know what is a good topic.</li>
<li>Read a few random papers in your domain. Why? Because if you read only the top quality papers (yes, of course these are important to read), you can get the impression that you need to achieve the same level of quality. But that's not the case: these top quality papers are 1 in a million and are often done by people who have years of experience behind them. A ""regular"" research contribution is often just a small improvement or a decent analysis on a specific problem.</li>
<li>Typical process: just pick a general topic/problem you're interested in, then study the state of the art thoroughly (write your state of the art section at this stage), reproduce a few experiments and analyze the results: what can be observed? what are the limitations? what would be a reasonable idea to overcome one of these limitations? Then you implement and evaluate this idea.</li>
</ul>

<p>Keep in mind that in research what matters is not the complexity of the topic or even the performance, it's the rationale and the method (and of course originality of the work, but even that doesn't matter that much at Master level).</p>
","6","2","64377","14816"
"68768","<p>The goals of all these methodological guidelines is to avoid <a href=""https://en.wikipedia.org/wiki/Leakage_(machine_learning)"" rel=""nofollow noreferrer"">data leakage</a>. </p>

<p>Example: let's imagine we want to classify short messages (e.g. tweets). When inspecting the data we find various kinds of smileys: <code>:-)</code>, <code>:|</code>, <code>:-/</code>... At preprocessing stage we replace all smileys found in the data with a special token like <code>&lt;smiley&gt;</code> (or something more specific).</p>

<ul>
<li>If the detection/replacement is done on the whole data, every occurrence of a smiley in the test set is replaced with <code>&lt;smiley&gt;</code>.</li>
<li>If the detection/replacement is done on the training set, even after preprocessing there might be a few smileys left in the test, because some uncommon ones didn't appear in the training set.</li>
</ul>

<p>In the first case there is data leakage: we fixed some issues in the test set even though this wouldn't have been possible with actual fresh data (here the variants of smiley that were not seen in the training set). In the second case the test set is ""imperfect"", i.e. it's exactly as if it was made of ""fresh"" unseen data, therefore the evaluation will be more realistic.</p>

<p>This example shows why it's always safer to separate the data first, design the preprocessing steps on the training data, then apply exactly the exact same preprocessing steps to the test data.</p>

<p>In practice there can be cases where it's more convenient to apply some general preprocessing to the whole data. The decision depends on the task and the data: sometimes the risk of data leakage is so small that it can be neglected. However it's crucial to keep in mind that even the design of the preprocessing can be a source of data leakage.</p>
","2","2","64377","14816"
"68769","<p>Measuring the <a href=""https://en.wikipedia.org/wiki/Inter-rater_reliability"" rel=""nofollow noreferrer"">inter-annotator agreement</a> is one of the ways to measure the reliability of the annotations.</p>

<p>(I assume that there are also other methods but I'm not that knowledgeable about the topic)</p>
","1","2","64377","14816"
"68827","<p><strong>Whether it's removed before/after splitting</strong> doesn't matter: the variable will not appear in any part of the data so it cannot cause any data leakage anyway.</p>

<p>However what might matter (at least in theory) is <strong>from investigating which part of the data</strong> was the decision made. Any preprocessing and design decision should be made based on the training set only in order to avoid any risk of data leakage (see <a href=""https://datascience.stackexchange.com/a/68768/64377"">example</a>). In practice it's not really a problem in the case you mention.</p>
","1","2","64377","14816"
"68891","<p>If I understand correctly (not sure), it looks to me like you don't need a model which can predict <em>at any time</em>, you just need:</p>

<ul>
<li>a model which predicts the ETA at any stage given information about the past stages. The easiest way to do that is probably to just train a different model for each stage, since the number of stages is fixed.</li>
<li>Then between two stages the ETA can be updated in a deterministic way: if the last stage was passed at time <span class=""math-container"">$t$</span> and the predicted ETA was say 10mn, then at time <span class=""math-container"">$t'$</span> the ETA is just 10mn - <span class=""math-container"">$(t'-t)$</span>.</li>
</ul>
","0","2","64377","14816"
"68904","<p>Disclaimer: I'm not familiar with AUC/ROC with multiclass or multi-label tasks myself.</p>

<ul>
<li>According to <a href=""https://stats.stackexchange.com/q/2151/250483"">this question and its answers</a>, the case of multiclass classification doesn't seem that simple. I would be very cautious about simply averaging values across classes, because the properties of AUC/ROC would problably not hold in general.</li>
<li>That being said, in case the method mentioned for multiclass is considered sufficient, then there's no reason not to use the same for multi-label: counting each instance N times for N classes is already what you do (or should do) in the multiclass case: the instances not relevant for a class are actually counted as true negatives. ROC curves are by nature built for a binary classification task, which means that every instance is classified as exactly one of the four possibilities True/False Positive/negative.</li>
</ul>
","0","2","64377","14816"
"69073","<p>The problem is not really ""new text"", since by definition any classification model for text is meant to be applied to some new text. The problem is <em>out of vocabulary words (OOV)</em>: the model will not be able to represent words that it didn't see in the training data.</p>

<p>The most simple way (and probably the most standard way) to deal with OOV in the test data is to completely remove them before representing the text as features.</p>

<p>Naturally OOV words can be a serious problem, especially in data such as Twitter where vocabulary evolves fast. But this issue is not related to using TF-IDF or not: any model trained at a certain point in time can only take into account the vocabulary in the training data, it cannot guess how future words are going to behave with respect to the class. The only solution for that is to use some form of re-training, for instance semi-supervised learning.</p>
","1","2","64377","14816"
"69106","<p>Decision trees can handle conditions based on several features by design: if the model determines that the feature ""at home"" is important for the prediction, it will create a node based on this condition close to the root of the tree. By construction other features will appear as conjuncts, e.g. ""if at-home==true and featureX==valueX and ... then predict ..."" (that's just how a decision tree works).</p>

<p>In case you consider that this particular feature is so important that everything else should depend on it, another option is to train two distinct models: one for ""at home"" and one for ""away"". The disadvantage is that a model can only use the instances relevant for its case, so there is less training data for each of them.</p>
","1","2","64377","14816"
"69165","<p>Unfortunately I don't think a generative model could prevent from leaking private information from the original dataset.</p>

<p>Like any other kind of model, the generative model is based on the values obtained from the training data. The idea of using such a model in ""generation mode"" is indeed interesting since it would make it difficult to reverse-engineer the instances it generates back to the real individuals. It would be difficult sure, but not impossible: by re-connecting pieces of information together or exploiting rare (distinctive) cases, somebody could acquire at least partial personal information from the instances.</p>

<p>Additionally the design of the generative model itself introduces a huge bias in the data: the distribution of the instances would be modeled after this design, which might or might not accurately represent the real distribution. This issue significantly lowers the interest in exploiting the generated instances, since it's essentially artificial data.</p>

<p>For the record I think I've seen research about using distributed ML methods in order to overcome the privacy issue. As far as I understand the idea is to keep every individual/institution in control of their data, allowing specific automatized methods to read it in some kind of safe way.</p>
","7","2","64377","14816"
"69312","<p>It's difficult to answer precisely without knowing the data and the task. </p>

<p>Assuming it's a single column of values with no order involved, it boils down to finding the optimal threshold to separate regular values vs. anomalies. Given that you know which ones are the anomalies (labelled data), the problem can be treated as a binary classification task: a value is either regular (negative) or an anomaly (positive).</p>

<p>The idea is simply to evaluate the performance for every possible threshold, i.e. count the number of True/False Positive/Negative for every value. Let's say you take the set <span class=""math-container"">$S$</span> of all the distinct values in your column. For every value <span class=""math-container"">$t \in S$</span> you calculate the confusion matrix (i.e. how many TP/FP/TN/FN) obtained by predicting every value <span class=""math-container"">$x$</span> as positive if if <span class=""math-container"">$x&gt;t$</span>, negative otherwise.</p>

<p>From there you can plot a ROC curve and/or find the optimal threshold according to some evaluation measure (typically the one which maximizes the F1-score).</p>

<p>I assume that there are libraries which do that automatically, but it's simple enough so personally I would do my own code for this kind of thing.</p>
","2","2","64377","14816"
"69320","<p>You'll probably find pointers in the literature searching for ""offensive text detection"". There are many variants/overlaps with related tasks such as detecting bullying. There are probably also annotated datasets around, in case you want to use these as training data.</p>
","2","2","64377","14816"
"69334","<p>The simple option is to implement it as a text search, preferably enriched with <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">tf-idf weights</a> and/or ignoring stop words: in theory all the possible topics (documents) are encoded as tf-idf vectors. When a query is provided it's also encoded as a tf-idf vector, then compared against all the documents vectors, typically with <a href=""https://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow noreferrer"">cosine similarity</a>. The results are then ordered by the similarity score.</p>

<p>In practice various efficiency techniques can be used (the most obvious one is not to represent the vectors as sparse vectors).</p>

<p>There are obviously more advanced techniques, but it can get pretty complex so I would start with this simple option first and then consider improvements later.</p>
","1","2","64377","14816"
"69335","<p>In case you have a lot of cases where the same brand is described in different ways, e.g. ""K.F.C"", ""KFC"", ""Kentucky Fried Chicken"", then it might be worth using clusters based on textual similarity measures. This kind of problem is similar to <a href=""https://en.wikipedia.org/wiki/Record_linkage"" rel=""nofollow noreferrer"">record linkage</a>.</p>

<p>But if this is not the case, then it would be a bad idea to try to merge brands based on their names since you would end up with groups of brands which have nothing in common except some part of their name.</p>

<p>At a more general level, it looks to me like the design of your task is flawed: why do you need a unique instance for every seller? A seller can have multiple brands, each from a different source, right? That would mean that there are multiple sources for each seller, how do you plan to deal with that? Are you sure you don't want to predict the source for a particular pair seller + brand instead?</p>
","1","2","64377","14816"
"69337","<p>If you assume that the documents returned for a query already share a common ""topic"", then what's the point of using topic modeling? You already have a kind of ""topical subset"" of the documents.</p>

<p>However if the goal is to clean up the results so that documents which are ""topically similar"" don't appear together at the top, i.e. to favor ""topical diversity"" in the top resulting documents, then this could be done in the following way:</p>

<ul>
<li>train a topic model for all the documents offline. You obtain a vector of posterior probabilities p(T|D) for every topic T and document D.</li>
<li>for every query, modify the scoring method so that there is for instance a minimal topic distance between documents ranked at the top (this part might be tricky to get right but it's feasible).</li>
</ul>

<p>Incidentally, this approach is also more realistic because it would be quite inefficient to train a full topic model for every particular query.</p>
","0","2","64377","14816"
"69377","<p>You can train a regular sequence labeling model (typically CRF) where one of the features is the rule-based predicted label: its value is the actual label when known or a special <code>unknown</code> value otherwise. Given that the model can take into account dependencies between labels (as specified in the parameters) and that the rule-based feature always gives the label except if <code>unknown</code>, the model should be able learn: </p>

<ul>
<li>in which cases it should ""trust"" the rule-based feature (when value is not <code>unknown</code>)</li>
<li>to predict the missing labels by exploiting both the features and the known labels.</li>
</ul>

<p>Note that in some rare cases the model might still predict a wrong label even if the rule-based label is provided. That would happen if the probability of the sequence is maximized in this way, but it's unlikely to happen if the training data is representative of the distribution.</p>
","1","2","64377","14816"
"69540","<p>This is very unusual according to my experience, and I agree that it's difficult to interpret.</p>

<p>There is a single value for either precision or recall for a particular label, but  since these tables are presented as confusion matrices the values cannot be precision/recall.</p>

<p>I notice that the matrices show percentages which sum to 100 across each row for the ""recall"" one and sum to 100 across each column for the ""precision"" one. Based on this observation <strong>my guess</strong> is that the values show:</p>

<ul>
<li>in the ""recall"" table, the percentage of instances predicted with class X (column) among true instances of class Y (row). Example: 12.23% of of instances where the true label is ""carry"" are labelled as ""walk"".</li>
<li>in the ""precision"" table, the percentage of instances which are truly class X (row) among instances predicted as class Y (column). Example: 6.87% of the instances predicted as ""walk"" actually belong to class ""carry"".</li>
</ul>

<p>In my opinion this kind of non-standard representation should be avoided unless there's a really good reason. In this case a regular confusion matrix would have been clearer.</p>
","1","2","64377","14816"
"69971","<p>I don't know the model you use, but I would suggest the perhaps old-fashioned feature engineering approach: </p>

<ul>
<li>In each set of transactions, calculate the duration between two consecutive transactions for every pair of consecutive transactions (i.e. difference between the dates). Then add the standard deviation of these durations as a a feature, so that a very regular transaction set should get a near zero value.</li>
<li>Same idea with the amount of the transaction: the standard deviation will be zero if the amount never changes.</li>
<li>Additional ideas:

<ul>
<li>number of distinct amounts across all transactions</li>
<li>number of distinct duration values between transactions in days, weeks, months, years. </li>
<li>standard deviation of the sum by day/week/month/year</li>
</ul></li>
</ul>
","0","2","64377","14816"
"69973","<p>For synonyms I would directly use <a href=""https://wordnet.princeton.edu/"" rel=""nofollow noreferrer"">WordNet</a>.</p>

<p>[added] For contextually similar words the traditional approach is to extract a context vector for every target word:</p>

<ol>
<li>for every occurrence of a target word extract the words within a -/+ N window (e.g. N=5).</li>
<li>for every target word aggregate all its context words in a single context vector over the whole vocabulary.</li>
</ol>

<p>Finally once a context vector has been calculated for every target word a similarity measure can be used, for example cosine. That means for every target word, compare its vector against any other candidate. </p>

<p>The same approach can be used with word embeddings instead of context vectors.</p>
","1","2","64377","14816"
"69974","<p>I would try some kind of <a href=""https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search"" rel=""nofollow noreferrer"">grid search</a> or a <a href=""https://en.wikipedia.org/wiki/Evolutionary_algorithm"" rel=""nofollow noreferrer"">genetic algorithm</a>.</p>
","2","2","64377","14816"
"69975","<blockquote>
  <p>how can I trust of my results?</p>
</blockquote>

<p>You probably shouldn't trust your results, because the large variation is likely caused by overfitting. Basically your model is not very reliable.</p>

<p>My guess is that you have either too many features or not enough instances, or any combination of these two issues.</p>

<blockquote>
  <p>I am not sure If it's correct to take the best result on the test set, or maybe I should consider an average of the results</p>
</blockquote>

<p>It is definitely not correct to take the best result. The average result is more representative of the true performance, but you should also provide the standard deviation.</p>

<blockquote>
  <p>Second, would not it be better to use CV on all data, instead of just splitting at the beginning the dataset in test and train just one time?</p>
</blockquote>

<p>Yes, that's a good idea: as far as I understand, currently you're just manually running the program several times and obtaining different performance each time. You can indeed use Cross-validation instead.</p>

<blockquote>
  <p>I read about random_state but the problem is that the results depend on which value I use.. example random_state = 2 --> accuracy 0.6 (on test set) random_state = 6 --> accuracy 0.79 (on test set) so basically this does not resolve my problem. How can I validate my model if I dont know which one to use?</p>
</blockquote>

<p>That's normal: the split function separates the training and test set randomly according to a random sequence. Setting the random seed to a particular value guarantees the exact same sequence every time, but you don't want that since it makes the splitting non-random. </p>

<p>Instead you should work on reducing the variation. Usually it's not possible to add instances, so you should probably try to reduce the number of features, or simplify them.</p>
","0","2","64377","14816"
"70052","<p>Sure you could do that, but why not try to predict the rating directly? </p>

<p>Unless you have a specific need for the 3 categories positive, neutral, negative, there's no reason not to use the rating provided since it also represents the sentiment of the review. </p>

<p>Notice that this would be a regression task since you predict a number (as opposed to a classification task where one predicts a categorical variable).</p>
","2","2","64377","14816"
"70053","<blockquote>
  <p>Is there a heuristic formula or a study of some sort that can guide me regarding the best</p>
</blockquote>

<p>Maybe, but I don't know any. My approach is to run a few experiments and observe the speed of convergence with different values for the size of the population, typically in the range 50 to 500. My experience is that usually this parameter doesn't have a big impact (especially compared to rate of mutation/crossover) so I tend to stay on the low side for efficiency reasons.</p>

<p>It's not technically required to specify the number of generations provided there is a criterion to check convergence, either manually or programmatically. I think it's more important to have such a criterion, because there's a always a risk that the predefined number of generations won't be sufficient.</p>
","2","2","64377","14816"
"70128","<p>It looks like a good case for <a href=""https://en.wikipedia.org/wiki/Analysis_of_variance"" rel=""nofollow noreferrer"">ANOVA</a>, which can be seen as a generalization of <a href=""https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test"" rel=""nofollow noreferrer"">Student t-test</a> for more than two groups.</p>

<p>For visualizing the differences I'd suggest using boxplots (or violin plots) for every group: it's more informative than only plotting the mean which is sensitive to outliers.</p>
","1","2","64377","14816"
"71405","<p>It's not possible to obtain a single f1-score when there are several classes, probably you are using <strong>micro</strong> or <strong>macro</strong> f1-score?</p>

<p>First don't use the micro F1-score, since it gives the same weight to every instance and therefore gives a lot of weight to the ""X"" class.</p>

<p>Macro F1-score should be closer to what you want to evaluate, since it gives every class equal weight. If you want you could even calculate the macro F1-score over only classes A,B,C (i.e. excluding X), but that seems unnecessary to me.</p>
","0","2","64377","14816"
"71485","<p>I think there are three very different kinds of contributions that can be made:</p>

<ul>
<li>Professional: many countries and organizations (typically <a href=""https://ec.europa.eu/info/news/coronavirus-eu-mobilises-eur10-million-for-research-2020-jan-31_en"" rel=""nofollow noreferrer"">the EU</a>, also many NGOs) have issued emergency calls for proposals, some of them involve data science research either about the disease itself or for dealing with its consequences. Participating to these projects usually requires a team of researchers but conditions vary.</li>
<li>Citizen Science: participating to a research project as a non-professional, usually through a platform such as <a href=""https://scistarter.org/"" rel=""nofollow noreferrer"">SciStarter</a> or <a href=""https://www.zooniverse.org/"" rel=""nofollow noreferrer"">Zooniverse</a>. Some of these projects are in the medical domain but I haven't seen any specifically related to the coronavirus yet.</li>
<li>Doing your own project and publishing the results. There are many datasets available (see e.g. <a href=""https://opendata.stackexchange.com/questions/16185/covid19-coronavirus-case-data-count-of-cases-and-deaths"">this question on OpenDataSE</a>).</li>
</ul>
","4","2","64377","14816"
"71504","<p>Evaluation is a crucial part of any serious ML project, but when it comes to evaluation choices there is no perfect answer. Generally speaking one evaluates a system in order to know how well it performs (i.e. how reliable are its predictions), usually to know which level of quality to expect when used in production (but not only). However the evaluation results are only useful to some extent:</p>

<ul>
<li>the evaluation method/measures should be chosen so that it actually represents ""quality"" for the target task. In general this is imperfect because no evaluation score can fully represent the diversity of a specific task (and that's assuming good evaluation choices)   </li>
<li>the test data should be representative of the production data, and in reality it rarely is from the same distribution. </li>
<li>There's almost always a chance factor that one tries to eliminate using cross-validation or other methods, but again that's imperfect.</li>
</ul>

<p>Besides these unavoidable simplifications, an evaluation score can be interpreted for exactly what it is. For example a precision score of 80% for class X means that one should expect an instance predicted X as truly of class X in 80% of the cases <em>in average</em>. Predicting a specific probability for a particular prediction requires either using a probabilistic model or devising a specific model which predicts a confidence score/probability... But in both cases the model could still can be wrong about the probability it predicts! </p>
","2","2","64377","14816"
"71608","<p>In general this doesn't work well, since it's almost unavoidable that the classifier won't be able to distinguish all the categories from each other. I'd suggest tying to reduce the number of categories (for instance discard the least common ones). </p>

<p>In any case I'm not aware of any specific model to deal with a high number of classes, it's regular text classification. I'd suggest to start with a robust method such as decision trees, but there are many options.</p>
","2","2","64377","14816"
"71691","<p>I would call this bad feature engineering, I'm afraid: as the designer of a ML system, one is supposed to analyze their data and find the best way to make the ML system perform as well as possible. In this case by adding a simple feature <code>x % 2</code> for every instance the decision tree can perform perfectly.</p>

<p>[added] Even in the case of a more complex pattern, if there are such ""clusters"" of numerical values then there must be a logical explanation why this happens, i.e. something which depends on the task that an expert in this problem can analyze and understand. In most real cases this implies that there are some hidden/intermediate variables, and designing the system so that it represents these variables is key. In other words, the numeric variable is not directly semantically relevant for predicting the response variable, because the assumption when using numeric values is that their order matters (here the numeric value behaves more like a categorical variable).</p>
","5","2","64377","14816"
"71862","<p>As you correctly observed, there is no assumption that an observation must be defined by a single key (variable). In this example an observation must be defined by a pair country + year, hence the correct tidy version in table1. It's not a complex case at all, this is very common and sometimes with more than two variables.</p>

<p>In general ""tidying"" a dataset increases the number of rows and often decreases the number of columns. A way to see that your second table is not tidy is that it would require new columns for every new year added to the dataset. As you noticed another indication is simply that it requires variables values in the columns names, which is a very bad design idea in general.</p>

<p>This being said, tidy data isn't a magical solution to every data design problem: it tends to demultiply the number of rows to an extent which makes it impractical in some cases.</p>
","0","2","64377","14816"
"73078","<p>In my opinion there's no correct answer between the two choices, and you are right about the arguments. </p>

<p>I would argue for a balanced compromise between the two: have a proportion of A (say 20%) labelled by multiple annotators (say at least 3), but the rest of A can be labelled by a single annotator. You could refine the proportion/number of annotators progressively based on the results: for instance if after the first batch one observes that annotators disagreement is very high, then it might be useful to increase the two parameters. This way you can still evaluate the level of disagreement and it effect on performance, while also maximizing the amount of data annotated.</p>

<p>An alternative is to use some form of active learning. For example, first use method 2 (each instance labelled once), then use cross-validation to tran a model and apply it to the corresponding test set. At the end of the process, the misclassified instances are the ""hard ones"" which need to be re-annotated by multiple annotators. This kind of process can be used iteratively to identify the most ""ambiguous"" instances. </p>
","0","2","64377","14816"
"73152","<p>Assuming the server is as at least as powerful as the first local machine, you should obtain the same time or shorter duration by request. If not, there's a problem worth investigating. </p>

<p>Then to improve further it could be useful to design the server side architecture so that:</p>

<ol>
<li>the model is pre-loaded in memory, not read from file every time (huge bottleneck).</li>
<li>there are multiple instances of the program running in parallel so as to distribute the load.</li>
</ol>
","0","2","64377","14816"
"73180","<p>Traditional methods don't have such a limit: Naive Bayes, SVM, decision trees...</p>

<p>Also see <a href=""https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification"">https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification</a></p>
","0","2","64377","14816"
"73181","<blockquote>
  <p>is it even possible to ""un-learn"" a single training example without retraining on the new data?</p>
</blockquote>

<p>To the best of my knowledge, the answer is no except in some very special cases.</p>

<p>The most obvious exception that comes to mind is <a href=""https://en.wikipedia.org/wiki/Instance-based_learning"" rel=""nofollow noreferrer"">instance-based learning</a>, such as kNN: since the ""model"" itself consists only of the set of training instances, it's straightforward to remove an instance.</p>

<p>In general, supervised ML relies on generalizing patterns based on the instances from the training set. Any non-trivial model consists of multiple such patterns, with every pattern potentially resulting from a different subset of instances. Even if there was a way to trace which instance participated to which pattern (that would be extremely inefficient), removing any pattern would probably cause the model to fail.</p>
","2","2","64377","14816"
"73524","<p>The simple answer is to use a sample of equal size from every book, or even better to randomly extract several samples of equal size from every book and then use the mean across samples.</p>

<blockquote>
  <p>I tried simply dividing the number of unique words in each book by the number of total words in each book</p>
</blockquote>

<p>This is known as the type/token ratio, the simplest way to measure <a href=""https://en.wikipedia.org/wiki/Lexical_density"" rel=""nofollow noreferrer"">lexical density</a>. I think it makes perfect sense in the case you describe, as far as I know usually it's not too biased. </p>
","0","2","64377","14816"
"73809","<p>There are many manual text annotation tools available, but you will probably have to search around in order to find the one which suits your precise needs.</p>
<p>Here are a few pointers:</p>
<ul>
<li><a href=""https://mpqa.cs.pitt.edu/annotation/gate_basics/"" rel=""nofollow noreferrer"">Gate</a></li>
<li><a href=""http://www.odbms.org/2017/06/text-annotation-tools/"" rel=""nofollow noreferrer"">Text annotation tools</a></li>
<li><a href=""https://github.com/doccano/doccano"" rel=""nofollow noreferrer"">Doccano</a></li>
<li><a href=""https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbz130/5670958"" rel=""nofollow noreferrer"">a recent review</a></li>
</ul>
","1","2","64377","14816"
"73888","<p>Generally your approach looks good to me. Here are a few comments/suggestions:</p>

<ul>
<li>You didn't mention how the set of labelled pairs is obtained. This part can be tricky when doing record linkage among a large set of N addresses, since it's very difficult to manually annotate all the N*N pairs. Bootstrapping is a common approach afaik. </li>
<li>The way the data is obtained may also have consequences about the proportion of positive/negative cases. In general the proportion positive cases is very low, and this can cause the usual problems of class imbalance. It's important not to train the model with a positive/negative ratio completely different from the one in the test set/in production.</li>
<li>It's often useful to diversify the similarity measure: instead of using only Jaro-Winkler, you can think of cosine-TFIDF, Levenshtein edit distance, and a lot of variants. In the same logic it's useful to vary the levels of comparison, e.g. characters, characters bigrams/trigrams, words.</li>
</ul>
","1","2","64377","14816"
"73982","<p>As far as I understand this theorem (not much), it doesn't really say anything about the relation between <span class=""math-container"">$n$</span> (the size of the training set) and the error rate. The main relation that it claims is between the error on the training set and the true error, assuming a minimum size on the size of the training set and that the training set is drawn from the true distribution.</p>

<p>I don't think it's possible to give an absolute upper bound on the number of samples needed in the training set for at least the following reasons: </p>

<ul>
<li>The success of the learning depends on which instances are in the training set, i.e. one needs a sample representative enough so that the model correctly captures the distribution. For instance, in theory it could happen by chance that all the <span class=""math-container"">$n$</span> instances have the same value for feature <span class=""math-container"">$x$</span>, so the model would never know that feature <span class=""math-container"">$x$</span> can have another value. In general there's no way to be sure that the training set contains all the diversity required for a successful model in the instances. </li>
<li>There's also the question of what the learning algorithm does exactly: if one wants to prove that a particular DT learning algorithm (say C4.5) can learn a particular model with less than <span class=""math-container"">$n$</span> instances (assuming these <span class=""math-container"">$n$</span> instances are sufficient), the proof must involve the particular properties of the algorithm itself. Otherwise it's obvious that the claim is false: if one chooses an algorithm which randomly builds a tree, it's clear that this cannot lead to the right model.</li>
</ul>

<p>I think it's more feasible and more common to provide an <em>experimental</em> upper bound: run many cross-validation experiments, varying the training set size and possibly other parameters. Then based on this one can estimate the probability of reaching the correct model given the size and other parameters.</p>
","-1","2","64377","14816"
"74038","<p>The training of a linear regression model is very basic, it simply assigns coefficients to every feature according to what is seen in the training set. As a consequence it is very sensitive to outliers and overfitting indeed: for instance if a feature appears in the test set with a value unseen in the training set, it can throw the predicted value far from the target range. The fact that the training set doesn't contain any such value doesn't matter, since it's caused by the coefficients of the features.</p>

<p>In your experiment the difference is likely due to chance, not to the size of the training set (i.e. the distribution is simply different between the experiments). It's likely that if you repeat the 200 rows experiment with different random splits some of them are going to produce wrong results as well.</p>

<p>The more features there are, the less likely the training set is going to cover the whole range of possibilities. So reducing the number of feature would minimize the risk of overfitting.</p>
","0","2","64377","14816"
"74039","<p>I assume you're training a CRF model, or some other variant of sequence labeling, right? </p>

<p>I'm not especially familiar with Mallet but it's very unlikely that any model would let you add features at testing stage: the trained model and the test data must have exactly the same features, no more no less.</p>

<p>Apparently the features you want to add consist of a similarity score with a few particular categories, I assume this score is calculated independently during pre-processing? If yes this would usually be represented like this in the data:</p>

<pre><code>&lt;token&gt;   &lt;PLACE_LIKE&gt; &lt;CITY_LIKE&gt; &lt;COUNTRY_LIKE&gt; &lt;label&gt;
KFC       1.0          0.0         0.0            PLACE
Bangalore 0.0          1.0         0.0            CITY
INDIA     0.0          0.0         1.0            COUNTRY
</code></pre>

<p>Note that this format with token + N features must be followed in both the training and test data. Therefore you must apply the same pre-processing steps for both.</p>
","1","2","64377","14816"
"74116","<p>It depends on how sophisticated you want the system to be:</p>

<ul>
<li>the most basic way is to compare the user answer with the gold answer using a simple string similarity measure, such as the overlap coefficient. Basically it just counts the words in common, and there would be a minimum threshold to count the answer as correct (e.g. 80% words in common). It's not very good because a small typo is enough to make the score wrong and it gives the same importance to every word.</li>
<li>The same idea but with TF-IDF weights, typically with cosine similarity. This requires a corpus on which to calculate the IDF weights (which reflect the importance of the words in general).</li>
<li>Still based on string similarity measures but more advanced: a hybrid similarity measure which combines character-level similarity between words (e.g. Jaro, Levenshtein edit distance) and similarity across words. Soft-TFIDF is a common example. Disadvantage: can be tricky to properly adapt to the task.</li>
<li>Beyond that there are a lot of fancy options: using semantic similarity with WordNet (synonyms), words embeddings, etc.</li>
</ul>

<p>Note: fyi this is not related to the task called Question Answering, which is about a computer-generated answer to a question.</p>
","0","2","64377","14816"
"74142","<p>The main reason is that in many cases (but not always) the model obtains enough evidence to make the right decision just from knowing which words appear and don't appear in the document (possibly also using their frequency, but this is not always needed either).</p>

<p>Let's take the textbook example of topic detection from news documents. A 'sports' article is likely to contain at least a few words which are unambiguously related to sports, and the same holds for many topic as long as the topics are sufficiently distinct.</p>

<p>In general tasks which are related to the general semantics of the text work reasonably well with unigrams (single words, unordered) as features, whether with NB or other methods. It's different for tasks which require taking syntax into account, or which require a deeper understanding of the semantics.</p>
","0","2","64377","14816"
"74186","<p>If your data always looks like this, there is little reason to use sequence labeling: every token belongs to an entity, so it's just a matter of correctly separating the entities and classifying them. But since the entities are already separated by line breaks, there's no need to train a model to separate them. So in the end you just have to classify the entities by category, and this doesn't require sequence labeling. But even for that, from your example it looks like the skills vs. education entities are already separated, so in the end I'm not sure what you want the model to learn?</p>
","1","2","64377","14816"
"74243","<p>In terms of evaluation, the best you can do with a very small amount of data is repeating <span class=""math-container"">$k$</span>-fold cross-validation many times (i.e. very large <span class=""math-container"">$k$</span>), and consider the whole distribution of scores as the performance (in particular take into account the variance across folds).</p>

<p>It's going to be difficult anyway to obtain a reliable measure of performance with such a small dataset. Two options come to mind:</p>

<ul>
<li>obtain more instances, possibly by using some interpolation method to generate artificial data (but it's not as good as real data).</li>
<li>make the model less complex by reducing the number of features, as this is likely to reduce the variance in the performance.</li>
</ul>
","1","2","64377","14816"
"74464","<p>Obtaining a dataset is an important part of defining a ML problem but it's not the only one. Typically this involves the following steps:</p>

<ol>
<li>Define the goal of the problem. Example: predict AZT level of tolerance among AIDS patients.</li>
<li>Obtain appropriate data for the problem.</li>
<li>Design the formal setting of the experiment: 

<ul>
<li>what kind of problem is it (e.g. classification)</li>
<li>what is the target variable and what are the features in the data</li>
<li>how to evaluate the quality of the results (performance measure, e.g.f1-score)</li>
<li>Experimental setup: ML method(s), use of cross-validation etc.</li>
</ul></li>
</ol>
","1","2","64377","14816"
"74538","<p>This problem is called <a href=""https://en.wikipedia.org/wiki/Record_linkage"" rel=""nofollow noreferrer"">record linkage</a>, there are various techniques which can be used, usually involving some distance measure between record and/or <a href=""https://en.wikipedia.org/wiki/Approximate_string_matching"" rel=""nofollow noreferrer"">approximate string matching</a> between string fields.</p>

<p>Fyi it's a quite complex problem, especially if quality deduplication is expected and the volume of data is high.</p>
","0","2","64377","14816"
"74558","<p>In my opinion you should make the range as large as possible (to a reasonable extent) for the first random initialization. </p>

<p>The genetic algorithm will converge to the appropriate range eventually, but giving it a narrow range could result in a sub-optimal solution because the algorithm doesn't have any way to reach a better solution. The only downside of a large range is that it might take a bit longer (more generations) to converge.</p>

<p>So I would suggest you keep a completely random initialization of the values, for instance in the range [-10,10].</p>
","1","2","64377","14816"
"74600","<p>You should definitely <em>not</em> use one hot encoding with values which represent numbers, as this removes the natural order between your intervals.</p>

<p>So these values should be represented as numbers:</p>

<ul>
<li>Either with the average of the limits indeed</li>
<li>Or a simple integer encoding of the intervals, e.g. tumor sizes 0-4, 5-9, 10-14,... would be represented as 0,1,2,... </li>
</ul>
","1","2","64377","14816"
"74662","<p>I'm not sure if it meets all your criteria (mostly because I'm not sure I understand all your criteria!), but you could have a look at <a href=""https://archive.mpi.nl/tla/elan"" rel=""nofollow noreferrer"">ELAN</a>:</p>

<blockquote>
  <p>Description: With ELAN a user can add an unlimited number of textual
  annotations to audio and/or video recordings. An annotation can be a
  sentence, word or gloss, a comment, translation or a description of
  any feature observed in the media. Annotations can be created on
  multiple layers, called tiers. Tiers can be hierarchically
  interconnected. An annotation can either be time-aligned to the media
  or it can refer to other existing annotations. The content of
  annotations consists of Unicode text and annotation documents are
  stored in an XML format (EAF).</p>
</blockquote>

<p>See also the <a href=""https://en.wikipedia.org/wiki/ELAN_software"" rel=""nofollow noreferrer"">Wikipedia page</a> about ELAN. There is also a <a href=""https://en.wikipedia.org/wiki/Computer-assisted_qualitative_data_analysis_software"" rel=""nofollow noreferrer"">Wikipedia list of related software</a>, I don't know if this is relevant.</p>
","2","2","64377","14816"
"74669","<p>Let's assume you want a sample of size <span class=""math-container"">$N$</span> where variable <span class=""math-container"">$V$</span> follows its distribution in the new dataset:</p>

<ol>
<li>Draw <span class=""math-container"">$N$</span> instances from the new dataset. Let <span class=""math-container"">$A= [v_1,..,v_N ]$</span> the list of <span class=""math-container"">$N$</span>  values of <span class=""math-container"">$V$</span> corresponding to these instances.</li>
<li>For every <strong>distinct</strong> value <span class=""math-container"">$v \in A$</span>:

<ul>
<li>Select the subset <span class=""math-container"">$S$</span> of instances in the old dataset which have <span class=""math-container"">$v$</span> as a value for <span class=""math-container"">$V$</span>,</li>
<li>let <span class=""math-container"">$\#v$</span> be the frequency of <span class=""math-container"">$v$</span> in <span class=""math-container"">$A$</span>: draw <span class=""math-container"">$\#v$</span> instances from the subset <span class=""math-container"">$S$</span></li>
</ul></li>
</ol>

<p>At the end of this process you have obtained <span class=""math-container"">$N$</span> instance from the old dataset which follow the distribution of <span class=""math-container"">$V$</span> in the new dataset.</p>
","0","2","64377","14816"
"74672","<p>The rationale for stacking learners is to combine the strengths of the individual learners. </p>

<ul>
<li>On the one hand your idea makes sense: if learners have different strengths, adding some features might help the meta-model detect when to give more importance to a particular learner. It's totally possible that overall the meta-model will work better this way in some cases.</li>
<li>On the other hand, generalizing this idea would often defeat the purpose of stacking: 

<ul>
<li>if we know in advance that a particular model is particularly good in some specific identified cases, then it's likely optimal to switch entirely to this model in such cases (no stacking).</li>
<li>if we don't have any particular knowledge, then this would require to add many/all of the original features to the meta-model, just in case they help. But by adding features it's likely that (1) the model will try to use the features directly, hence collapsing into an individual learner itself, and (2) the model will be more complex, therefore more prone to overfitting and likely not to make the best use of the individual learners' output.</li>
</ul></li>
</ul>

<p>To sum up: the stacking approach relies on ""making things simple"" for the meta-model, so that it only has to make a call about the answers of the individual learners. This way the meta-model can ""focus"" solely on optimally use these answers. The more features we add to it, the more risk that it will not to its job correctly.</p>
","1","2","64377","14816"
"74701","<p>You probably did:</p>

<pre><code>install.packages(ggplot)
</code></pre>

<p>This is a common mistake: when installing a package you need quotes, otherwise R interprets the name of the package as a string and since the string isn't defined you get an ""object not found"" error. The correct version is:</p>

<pre><code>install.packages(""ggplot"")
</code></pre>

<p>(with single or double quotes, it doesn't matter)</p>

<p>Note: I would normally use the ggplot<strong>2</strong> package, but maybe ggplot redirects to the same package:</p>

<pre><code>install.packages(""ggplot2"")
</code></pre>
","1","2","64377","14816"
"74727","<p>There are different ways to address the task that you describe:</p>

<ul>
<li>If the goal is simply to predict the author among a set of predefined authors, then this is not a clustering task (unsupervised) but a classification task (supervised). This implies that you would split the data between training and test set (or use cross-validation), train a model using the training set then apply it the test set, and finally evaluate the performance on the test set by comparing the predicted author with the true author for every text. Typically the performance is aggregated as f1-score for every author, then macro/micro f1-score.</li>
<li>If the goal is some kind of experiment to see whether a clustering algorithm can correctly identify the groups by author (spoiler: it certainly can't, at least not well), then you're doing unsupervised learning with an external evaluation. I'm not aware of any standard evaluation method for this context (there might be). I assume that you would need a matching method between the clusters ids and the authors (e.g. assign each author to the cluster which has the most instances by this author), then you can apply the regular classification evaluation.</li>
</ul>

<p>Two additional points:</p>

<ul>
<li>The general problem of distinguishing texts by their author is called stylometry. There are many variants, such as open or closed classification, author verification, author profiling, etc. It's an active area of NLP research (which means there's a lot of literature about it), and state of the art methods are quite different from regular classification/clustering. It doesn't meant that regular methods don't work at all, but usually not as well as specialized methods.</li>
<li>Features: TF-IDF words vectors are used for tasks where the ""topic"" of the text matters, for instance to classify articles about sports, politics, etc. There are better options for the features when the task is about writing style: 

<ul>
<li>no focus on rare words (as IDF does), instead rare words should usually be discarded (because they cause massive overfitting) </li>
<li>words are not necessarily the right unit to consider. Character trigrams/4-grams have been proved to work better.</li>
<li>tokenizing can be tricky, it's often considered safer not to tokenize at all so that the author's punctuation habits are preserved as features.</li>
</ul></li>
</ul>
","1","2","64377","14816"
"74805","<blockquote>
  <p>Do you think this result is right?</p>
</blockquote>

<p>Depends what you mean by ""right""... the results seem reasonable, I don't see any obvious sign of mistake in the process.</p>

<blockquote>
  <p>Can you explain what would you understand by looking at this result please? </p>
</blockquote>

<p>I observe that you don't have any data for classes 1 and 5, so technically it's a 3-classes problem. </p>

<ul>
<li>First with 3 classes the random baseline accuracy would be 0.33, 0.45 is better so your model does better than this (that's the bare minimum). </li>
<li>However according to the confusion matrix class 3 has 92 instances out of a total of 172, which means that a basic majority class learner always predicting class 3 would get 52% accuracy (if my calculation is correct). So 45% is not very good.</li>
</ul>
","1","2","64377","14816"
"74946","<p>Maybe relying on set notation would work?</p>

<p><span class=""math-container"">$P(X_k \in s_k)$</span> where:</p>

<ul>
<li><span class=""math-container"">$s_k = \{ x_k \}$</span> if <span class=""math-container"">$X_k$</span> is discrete </li>
<li><span class=""math-container"">$s_k = [ x_k-\epsilon , x_k+\epsilon]$</span> if <span class=""math-container"">$X_k$</span> is continuous</li>
</ul>
","0","2","64377","14816"
"74959","<p><a href=""https://en.wikipedia.org/wiki/Semi-supervised_learning"" rel=""nofollow noreferrer"">Semi-supervised learning</a> and in particular <a href=""https://en.wikipedia.org/wiki/Active_learning_(machine_learning)"" rel=""nofollow noreferrer"">active learning</a> could be considered in cases like this:</p>

<ul>
<li>The general semi-supervised setting consists in training a model from an initially small training set by applying it iteratively to unlabelled instances. There are various methods to minimize the risk of training the model on wrongly classified instances.</li>
<li>Active learning is a variant of semi-supervised learning where the model queries the human expert for annotations, but the instances are carefully selected in order to minimize the amount of human labor.</li>
<li>There is also bootstrapping, where one would focus on the positive instances: apply the original model to the unlabelled data, the manually annotate only the instances which are predicted as positive (useful only in cases where the positive class is much smaller than the negative one).</li>
</ul>
","1","2","64377","14816"
"75114","<p>It looks like you try everything but didn't design the system so that it does what you need it to do. In this task I don't see any reason to use things like LDA for instance. In my opinion this is a typical case for training a custom NE system which extracts specifically the targets you want. The first step is to annotate a subset of your data, for example like this:</p>

<pre><code>Please   _
book     _
a        _
cab      _
from     _
airport  FROM_B
to       _
hauz     TO_B
khaas    TO_I
at       _
3        TIME_B
PM       TIME_I
</code></pre>

<p>A NE model is trained from such annotated data. Here I proposed an option with labels by category plus B for Begin, I for Inside, but there can be many variants.</p>

<p>Once the model is trained, applying to any unlabelled text should directly give you the target information.</p>
","1","2","64377","14816"
"75303","<p>This corresponds to an NLP task called <a href=""https://en.wikipedia.org/wiki/Paraphrasing_(computational_linguistics)"" rel=""nofollow noreferrer"">paraphrase detection</a>. It's an active area of research, as far as I know there's no ready-to-use system able to perform this task very well, but there are probably a good few methods and prototypes around. A quick search gives these links for example:</p>

<ul>
<li><a href=""https://aclweb.org/aclwiki/Paraphrase_Identification_(State_of_the_art)"" rel=""nofollow noreferrer"">https://aclweb.org/aclwiki/Paraphrase_Identification_(State_of_the_art)</a></li>
<li><a href=""https://arxiv.org/abs/1712.02820"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1712.02820</a></li>
</ul>
","1","2","64377","14816"
"75336","<p>First, let's be clear about the fact that all these measures are only for evaluating binary classification tasks. </p>

<p>The way to understand the differences is to look at examples where the number of instances is (very) different in the two classes, either the true classes (gold) or predicted classes.</p>

<p>For instance imagine a task to detect cities names among the words in a text. It's not very common, so in your test set you may have 1000 words, only 5 of them are cities names (positive). Now imagine two systems: </p>

<ul>
<li>Dummy system A which always says ""negative"" for any word</li>
<li>Real system B (e.g. which works with a dictionary of cities names). Let's say that B misses 2 real cities and mistakenly identifies 8 other words as cities.</li>
</ul>

<p>System A gets an accuracy of 995/1000 = 99.5%, even though it does nothing. System B has 990/1000=99.0%. It looks like A is better, that's why accuracy rarely gives the full picture.</p>

<p>Precision represents how correct a system is in its <em>positive</em> predictions: system A always says negative so it has 0% precision. System B has 3/11 = 27%.</p>

<p>Recall represents the proportion of <em>true positive</em> instances which are retrieved by a system: system A doesn't retrieve anything so it has 0% recall. System B has 
3/5 = 60%.</p>

<p>F1-score is a way to have a single value which represents the harmonic mean of the precision and recall. It's used as a ""summary"" of these two values, which is convenient when one needs to order different systems by their performance.</p>

<p>The choice of an evaluation measure depends on the task: for instance, if predicting a FN has life-threatening consequences (e.g. cancer detection), then recall is crucial. If on the contrary it's very important to avoid FP cases, then precision makes more sense (say for instance if an automatic missile system would mistaken identify a commercial flight as a threat).
The most common case though is certainly F1-score (or more generally F<span class=""math-container"">$\alpha$</span>-score), which is suited to most binary classification tasks.</p>
","1","2","64377","14816"
"75472","<p>You could look at <a href=""https://en.wikipedia.org/wiki/String_metric"" rel=""nofollow noreferrer"">string similarity measures</a> and <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">TFIDF</a> (usually with cosine). If you want a measure which works at both levels of words and sentences, there are more advanced options such as SoftTFIDF.</p>
","0","2","64377","14816"
"75576","<p>I'm not familiar with sklearn NB methods but it's likely that these are actually the predicted probabilities. NB is known to often assign extreme probabilities, i.e. close to either 0 or 1. This issue implies that the probabilities predicted by NB are not really meaningful, i.e. they cannot (usually) be interpreted as ""there is an x % chance that this instance belongs to this class"" for example.</p>
","0","2","64377","14816"
"75673","<p>It's a complex problem, the standard method is to train a model which represents this semantic similarity function using a large text corpus (i.e. not only the tags themselves), by considering that the meaning of a word is provided by its context words.</p>

<p>A more direct approach is to use a pre-trained word embeddings vectors that can be compared directly to find their similarity.</p>

<p>A similar idea is to use <a href=""https://wordnet.princeton.edu/"" rel=""nofollow noreferrer"">WordNet</a>, which is a database of words with their semantic similarity to each other.</p>
","0","2","64377","14816"
"75676","<p>It depends what you mean by ""can be used"": any regression algorithm can be used, the question is how reliably it would perform. You can compare different algorithms experimentally (if you have a dataset). </p>

<hr>

<p>[Updated after question edited]</p>

<p>In general the way to use ML with this kind of setting is to train a classification model based only on the categorical features. Depending on the type of algorithm, the combination of features might not always be a weighted sum, and the result label may or may not be based on a cutoff point. In order to have a cutoff point (thus a numerical prediction), the method must be a <em>soft classification</em> method. Alternatively a regression model could be trained for predicting the numerical value.</p>

<p>So that leaves you with many options: </p>

<ul>
<li>soft classification: linear/logistic regression, Naive Bayes, ...</li>
<li>regression: linear/logistic regression, SVM, decision trees, ... </li>
</ul>

<p>Note: technically the probability doesn't represent ""how far from the cutoff point"", it represents the probability of the instance being positive (p=1).  </p>
","1","2","64377","14816"
"75679","<p>I would suggest using a genetic algorithm of some kind. The idea is to assign hypothetical costs to each item, then check how well the hypothesis matches the data you have. An individual represents an ""hypothesis"", i.e. assignment of costs: starting from random hypotheses, the genetic algorithm might be able to converge to a solution. </p>

<p>I gave the following more detailed answer to <a href=""https://datascience.stackexchange.com/q/67725/64377"">a similar problem</a> a while back:</p>

<blockquote>
  <p>Clearly this problem doesn't always have a unique solution, but if you
  are interested in finding one possible solution you could try a simple
  genetic algorithm simulation:</p>
  
  <ul>
  <li>Each individual gene represents an item from the list of all possible items. </li>
  <li>Each gene/item is assigned a price randomly at first (gene expression)</li>
  <li>When a mutation is applied to a gene/item, its price is slightly modified randomly.</li>
  <li>A crossover causes a ""child gene"" to take as value the mean of its two ""parents genes"".</li>
  </ul>
  
  <p>This setting means that every individual in a population consists of
  all the items being assigned a particular price. At each generation
  each individual/assignment is evaluated by applying the prices
  assignment to the actual data and then measuring the error compared to
  the actual prices. Finally the top N individuals/assignments which
  perform the best are selected as parents for the next generation.
  Eventually the population should converge to realistic prices
  assignments.</p>
  
  <p>I think this is a perfect case for a genetic algorithm because the
  evaluation of a potential price assignment is a very simple
  calculation, so there is no major efficiency issue repeating the
  process over many generations (as opposed to many problems where
  evaluation is prohibitively expensive).</p>
</blockquote>
","2","2","64377","14816"
"75687","<p>The distribution of a variable is an <strong>abstract concept</strong> which represents how the variable is ""distributed"", that is it represents the chances that the variable has any particular value.</p>

<p>For example if the variable is the outcome of a regular dice, then any of the values 1 to 6 has the same chances to appear (1/6). This is a uniform distribution. If you actually do the experiment yourself of throwing a dice 100 times, you are going to observe that the frequency (i.e. the count) of every value is around 100/6 = 16.6, but not exactly.</p>

<p>Now say you want to represent visually the result of your experiment, i.e. the <strong>observed distribution</strong> of your sample. The most standard way to do that is to draw a <strong>histogram</strong>: as you know, you write your 6 values on the X axis and for each of them draw a bar as high as its frequency (or probability) on the Y axis. The histogram is a <strong>visual representation</strong> of the distribution: it shows for every value the chances that it appears, and it's visually useful in order to observe the ""shape"" of the distribution. For instance if the distribution is normal, the histogram has this typical bell shape. But there are many other kinds of distribution as well.</p>
","1","2","64377","14816"
"75800","<p>Your question doesn't describe exactly what you did so it's impossible to tell you what happens, but anyway relying on R saving the workspace is not the best option to save your model. It would be much safer to actually save the model to a file, as described in <a href=""https://stackoverflow.com/q/5118074/891919"">this question</a> for instance:</p>

<blockquote>
<pre><code>&gt; set.seed(345)
&gt; df &lt;- data.frame(x = rnorm(20))
&gt; df &lt;- transform(df, y = 5 + (2.3 * x) + rnorm(20))
&gt; ## model
&gt; m1 &lt;- lm(y ~ x, data = df)
&gt; ## save this model
&gt; save(m1, file = ""my_model1.rda"")
&gt; 
&gt; ## a month later, new observations are available: 
&gt; newdf &lt;- data.frame(x = rnorm(20))
&gt; ## load the model
&gt; load(""my_model1.rda"")
&gt; ## predict for the new `x`s in `newdf`
&gt; predict(m1, newdata = newdf)
        1         2         3         4         5         6 
6.1370366 6.5631503 2.9808845 5.2464261 4.6651015 3.4475255 
        7         8         9        10        11        12 
6.7961764 5.3592901 3.3691800 9.2506653 4.7562096 3.9067537 
       13        14        15        16        17        18 
2.0423691 2.4764664 3.7308918 6.9999064 2.0081902 0.3256407 
       19        20 
5.4247548 2.6906722
</code></pre>
</blockquote>
","1","2","64377","14816"
"75802","<p>It's impossible <em>in general</em>, simply because a particular value or range for feature A might correspond to class 'good' if feature B has a certain value/range but correspond to class 'bad' otherwise. In other words, the features are inter-dependent so there's no way to be sure that a certain range for a particular feature is always associated with a particular class.</p>

<p>That being said, it's possible to simplify the problem and assume that the features are independent: that's exactly what <a href=""https://en.wikipedia.org/wiki/Naive_Bayes_classifier"" rel=""nofollow noreferrer"">Naive Bayes classification</a> does. So if you train a NB classifier and look at the estimated probabilities for every feature, you should obtain more or less the information you're looking for.</p>

<p>Another option which takes into account the dependency between variables is to train a simple decision tree model: by looking at the conditions in the tree you should see which combinations of features/ranges lead to which class.</p>
","2","2","64377","14816"
"75829","<p>There's <a href=""https://stats.stackexchange.com/q/31849/250483"">no clear definition of ""Full Bayes"" as a classifier</a>. Most ""real world"" non-Naive Bayesian classifiers take into account some but not all dependencies between features. That is, they make independence assumptions based on the meaning of the features. </p>

<p>If by ""full Bayesian"" you mean a joint model (as your example suggests), then one of the problems is that such a model doesn't generalize: it just describes the probabilities in the training set, and that implies that it's likely to overfit badly. This is actually why NB works quite well in most cases: yes it makes unrealistic independence assumptions, but this simplification allows the model to capture basic patterns from the data. In other words, the ability of the model to generalize comes from its excessively simplified assumptions.</p>

<p>Note: as far as I can tell, your example is well chosen and you should see a big difference between NB and a joint model: NB should perform no better than a random baseline while the joint model should obtain near perfect accuracy. There's probably a mistake somewhere if you don't obtain these results. But while this is a good toy example, it cannot help you understand the advantage of the NB assumptions.</p>
","0","2","64377","14816"
"75830","<p>In theory decision trees (and random forests) are able to deal with missing values in the data. But whether a particular implementation of the algorithm allows this (and how to use it with this implementation) depends on the specific package. </p>
","1","2","64377","14816"
"75834","<p>There's no ""most suitable"" way, but there might be one which works better with your data. The only way to know that would be to try all of them.</p>

<p>In case choosing the number of topics is an issue, you might be interested in using the non-parametric extension of LDA for topic modeling, which doesn't require you to specify the number of topics: this is called <a href=""https://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process"" rel=""nofollow noreferrer"">Hierarchical Dirichlet Processes</a>, see for instance <a href=""https://towardsdatascience.com/dont-be-afraid-of-nonparametric-topic-models-d259c237a840"" rel=""nofollow noreferrer"">this introduction</a>.</p>
","1","2","64377","14816"
"75849","<p>The goal is to predict the current stage for one patient, so for the classification part you must have a single instance for every patient. So the question is how to transform your features repeated for every visit into a fixed number of features.</p>

<p>The standard option is to use expert knowledge to engineer the features. For example there might be some features which are only relevant at a certain point in time and no longer relevant later, so for these you can just keep the latest value. There might be some features for which the evolution across time is significant, so for example the average increase in the past N months might make sense. The idea is to ""summarize"" any number of visits into a fixed number of features.</p>

<p>A more advanced option would be to ""vectorialize"" your features, i.e. generate some kind of embedding using some unsupervised deep learning method (but that's not my area of expertise).</p>
","0","2","64377","14816"
"75891","<p>It looks like a very difficult problem, since there are many possible classes and very little information in the features to distinguish them. For the record, the reverse problem of estimating the travel time based on the route would probably be more feasible. </p>

<p>So you can't expect great performance on a problem like this, the goal will be to design the problem in a way which makes things as simple as possible for the classifier to do a decent enough job. Here are some suggestions:</p>

<ol>
<li>Start with training a model specific to a pair BSTN,ASTN. </li>
<li>Discard the least likely routes, i.e. routes which are rarely used for the pair BSTN,ASTN (for instance routes with frequency lower than 10).</li>
<li>Inspect the data to see if the features allow a distinction between the (main) classes. For instance you can plot the distribution of BSEC, TFtime, Ttime for different routes: if the distributions are close there's little chance the classifier will succeed. You can also train a decision tree and inspect it manually so see what happens. </li>
</ol>
","1","2","64377","14816"
"75944","<p>The problem is a bit vague but here a couple ideas:</p>

<ul>
<li>Just use the Y variables as features and predict the X variables. The most basic option is to consider every Xi as independent of the others, and train an independent model for each of them. A more advanced approach is to train a joint model which predicts all the Xi together (i.e. the class would be for instance ""1,1,0""), but this probably requires a lot more data.</li>
<li>In the idea of optimization techniques, there might be some way to use genetic learning in order to obtain the optimal X values given the Y values. However I don't see how to design the population with many different Y combinations, but maybe it's possible.</li>
</ul>
","0","2","64377","14816"
"75946","<blockquote>
  <p>Should we modify the features in such a way that we would use the difference between the two teams, instead of two values? </p>
</blockquote>

<p>Yes, you should definitely design your features ""to help"" the ML algorithm, this is called <a href=""https://en.wikipedia.org/wiki/Feature_engineering"" rel=""nofollow noreferrer"">feature engineering</a>. Be careful though that there can be many different options for representing the contrast in ""average possession"": you can think of the difference indeed, but also a ratio or many other arithmetic combinations.</p>

<blockquote>
  <p>On the other hand some algorithms might be good at finding this interactions?</p>
</blockquote>

<p>Most ML handle take into account interactions between features <em>to some extent</em>, but it depends what kind of interaction and which ML algorithm. </p>
","0","2","64377","14816"
"75950","<p>Practically everything related to statistics (including Machine Learning) has to do with studying <strong>chance</strong>, i.e. trying to determine to what extent an observation is due to chance or not. </p>

<p>For example one might want to know whether a drug actually helps with a particular disease or not. If we observe that one patient improves after taking the drug, there's not enough evidence to conclude since many other factors could have caused the improvement. This is why one needs a very strict protocol in order to obtain a <strong>statistically meaningful</strong> observation (two groups of patients, placebo etc.).
A reasonably high number of observations is needed, otherwise it's impossible to distinguish the effect of ""chance"" (any other factor) and the real effect of the drug.</p>

<blockquote>
  <p>a) Is there any systematic/mathematical/theoretical approach that can tell me anything less than N% is too little to impact/influence the output?</p>
</blockquote>

<p>The standard method for knowing whether an observation is due to chance or not is to use the appropriate <a href=""https://en.wikipedia.org/wiki/Statistical_significance"" rel=""nofollow noreferrer"">statistical significance test</a>. There are many of them and they depend on what exactly is being tested. </p>

<blockquote>
  <p>b) How do you decide which items are too little to impact output. Do you go with your judgement which is a subjective approach?</p>
</blockquote>

<p>In ML it's common to take a more experimental approach, for example trying with/without an observation or feature and then evaluate which versions works better. Of course it helps to have an intuition of what is more likely to work. In general including extremely rare observations is a bad idea because it's likely to cause overfit, i.e. when the model ""learns"" something which is actually due to chance.</p>

<hr>

<p>[added following OP's update]</p>

<p>In this case this is a resource allocation problem, I don't think  statistical significance is relevant here. Assuming that you want to optimize the use of manual labor based on how often a drug is used, i.e. the only thing to maximize is the sum of the frequencies of the drugs being labelled, then it's simple: rank all the drugs by their frequency in descending order, then proceed with manual annotation following this order. This way you're sure that the drugs which account for more patients are done first, so whenever manual annotation stops the largest possible amount has been labelled.</p>
","7","2","64377","14816"
"75991","<p>The example you describe looks more like an optimization problem, possibly related to <a href=""https://towardsdatascience.com/what-is-operations-research-1541fb6f4963"" rel=""nofollow noreferrer"">operations research</a> but I'm not sure.</p>

<p>Naturally ML can be used to approximate the optimal solution(s) for this kind of problem. For example <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow noreferrer"">genetic algorithms</a> are a common way to find the ""approximately best"" solution to an optimization problem.</p>

<p>This kind of problem can also sometimes be formalized as a regression problem, depending on the exact constraints to satisfy.</p>
","1","2","64377","14816"
"75993","<p>The simplest way is to build a matrix representing how often two items are bough together: <span class=""math-container"">$M[i,j]$</span> is the count of how many times items <span class=""math-container"">$i$</span> and <span class=""math-container"">$j$</span> are bought together (note that you need to fill only a diagonal matrix, since it's symmetric).</p>

<p>From there you can calculate whatever you need: most frequent <span class=""math-container"">$B$</span> item bought with a given item <span class=""math-container"">$A$</span>, top most frequent pairs of items in general, etc.</p>

<p>You could also consider the matrix as a graph representation, and use algorithms specific to graphs.</p>
","1","2","64377","14816"
"76047","<p>It's very likely that the authors assume that the <em>spam</em> class is positive, whereas you intuitively associated the <em>ham</em> class with positive. Both options make sense in my opinion:</p>

<ul>
<li>the former interpretation is based on the idea that the goal of the task is to detect the <em>spam</em> emails, seen as the class of interest.</li>
<li>the later interpretation considers that the <em>ham</em> emails are the ""good ones"", the ones that we want, hence the ""positive"" class.</li>
</ul>

<p>There's no error when one reads the paragraph with the authors' interpretation in mind. This confusion illustrates why one should always clearly define which class is defined as positive in a binary classification problem :)</p>
","0","2","64377","14816"
"76067","<p>In my opinion the validation set should follow the original imbalanced distribution: the goal is ultimately to apply the model to the real distribution so the hyper-parameters should be chosen to maximize performance for this distribution. </p>

<p>But since I'm not completely sure I'd suggest trying both options, and adopt the one which gives the best performance on the test set.</p>
","2","2","64377","14816"
"76165","<p>You definitely want the comparison to be based on the test set:</p>
<ul>
<li>Evaluating on the training set doesn't make sense for all the usual reasons.</li>
<li>Especially in the case of different features, comparing the performance on the training set could be badly misleading: if one of the model overfits, its performance on the training set will appear better but its real performance (on the test set) is likely worse.</li>
</ul>
<p>Note that it might make sense to study what happens on the training set (e.g. to measure overfitting), but that cannot be the real evaluation used for comparing the models.</p>
","0","2","64377","14816"
"76187","<p>I don't think there is any way of doing that with decision trees, because that's not how decision trees work: the predicted label is not the result of some linear combination of the features. Instead you can look at the actual decision tree that the model represents and see which features have been used to classify a particular instance.</p>
","0","2","64377","14816"
"76240","<p>In order to make the model take into account the two columns <em>and</em> distinguish whether a word is from the title or the article, you can generate the two sets of features independently and then concatenate the two vectors of features. This way the learning algorithm can assign different weights to a particular word depending on whether it comes from the title or the article.</p>
<p>However there is a disadvantage in doing that: depending on how many instances, how many words etc. there are, increasing the number of features might make the task harder for the model (for instance cause overfitting). It's probably worth testing the two options:</p>
<ul>
<li>concatenate the text from the two columns then generate the features, i.e. no distinction between the columns but simpler job for the classifier</li>
<li>generate the features independently then concatenate the two sets of features (as above).</li>
</ul>
","2","2","64377","14816"
"76260","<p>A Naive Bayes model consists of the probabilities <span class=""math-container"">$P(X_i|Class)$</span> for every feature <span class=""math-container"">$X_i$</span> and every label <span class=""math-container"">$Class$</span>. So by looking at the parameters of the model one can see <em>how important a particular feature is for a particular class</em>. the opposite could be calculated as well: <span class=""math-container"">$P(Class|X_i)$</span> represents the distribution of the classes given a feature.</p>
<p>Now at the level of individual instances it's not so clear what would be the &quot;effect&quot; of a particular feature: For every class the posterior probability is:</p>
<p><span class=""math-container"">$$P(Class| X_1,..,X_n) = \frac{P(Class)\prod_i P(X_i|Class)}{P(X_1,..,X_n)}$$</span></p>
<p>You can easily order the features by how much they contribute to the prediction, i.e. the class which obtains the maximum posterior probability (for instance obtain the top 3 features). However you cannot quantify precisely the effect of each feature, because the prediction is not a linear combination of the features.</p>
<hr />
<p>[Details added following comments]</p>
<p>Due to the NB assumption that features are independent, we have:</p>
<p><span class=""math-container"">$P(Class|X_1,..,X_n) = \prod_i P(X_i|Class)$</span></p>
<p><span class=""math-container"">$P(Class|X_1,..,X_n) = P(X_1|Class) * P(X_2|Class) * .. * P(X_n|Class)$</span></p>
<p>From the conditional definition:</p>
<p><span class=""math-container"">$P(Class|X_1,..,X_n) = P(Class,X_1,..,X_n) / P(X_1,..,X_n)$</span></p>
<p>which gives:</p>
<p><span class=""math-container"">$P(Class,X_1,..,X_n) = P(Class) * P(Class|X_1,..,X_n)$</span>
<span class=""math-container"">$P(Class,X_1,..,X_n) = P(Class) * P(X_1|Class) * P(X_2|Class) * .. * P(X_n|Class)$</span></p>
<p>Now we use the marginal to calculate <span class=""math-container"">$P(X_1,..,X_n)$</span>:</p>
<p><span class=""math-container"">$P(X_1,..,X_n) = \sum_j P(Class_j,X_1,..,X_n)$</span>
<span class=""math-container"">$P(X_1,..,X_n)  = P(Class_1,X_1,..,X_n) + .. + P(Class_n,X_1,..,X_n)$</span></p>
<p>So at the end we have <span class=""math-container"">$P(Class,X_1,..,X_n)$</span> and <span class=""math-container"">$P(X_1,..,X_n)$</span>, so we can calculate:</p>
<p><span class=""math-container"">$P(Class|X_1,..,X_n) = P(Class,X_1,..,X_n) / P(X_1,..,X_n)$</span></p>
<p>Note that if you do all these steps you should obtain the same probability for <span class=""math-container"">$P(Class|X_1,..,X_n)$</span> as the one returned by the function <code>predict_proba</code>.</p>
<p><strong>Caution</strong>: the functions  <code>feature_log_prob_</code> and <code>class_log_prior_</code> don't give you the probability directly, they give you the logarithm of the prob. So you need to apply exponential in order to get back the probability.</p>
","1","2","64377","14816"
"76323","<p>What you need is simply a <a href=""https://en.wikipedia.org/wiki/Language_model"" rel=""nofollow noreferrer"">language model</a>. This is a very common task so you should be able to find code and data easily. <a href=""https://datascience.stackexchange.com/q/38540/64377"">This question</a> gives some pointers for Python (be careful, the accepted answer is incorrect according to the two other answers).</p>
<p>Applying the language model to a sentence gives you a probability (or a perplexity score,  which works the opposite way), so you have to define a threshold in order to classify as real language or not.</p>
","2","2","64377","14816"
"76325","<p>As far as I know LDA is the state of the art approach for topic modeling, but I'm not following the field very closely. So I would say that it's pretty safe to use LDA, and my guess is that the different approaches are likely to give similar results overall.</p>
<p>In case you want to try different methods, the question of evaluating topic models is quite complex. <a href=""https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"" rel=""nofollow noreferrer"">This article</a> might help.</p>
<p>Side note: there is a non-parametric variant (no need to choose the number of topics) of LDA called <a href=""https://stats.stackexchange.com/questions/135736/hierarchical-dirichlet-processes-in-topic-modeling"">Hierarchical Dirichlet Processes</a>.</p>
","1","2","64377","14816"
"76350","<p>Any ML model assumes a particular representation of the data, and connects bits of information only within this representation.</p>
<p>For example a linear regression model assumes a linear relation between the features and the target variable. A Naive Bayes model assumes that the features are independent of each other. And these are only the most obvious kind of simplifications made by ML models.</p>
<p>Naturally this results in different models representing data differently. Any semantic interpretation based on some model outcome is potentially biased by the assumptions made by the model.</p>
","1","2","64377","14816"
"76360","<p>No, there's no &quot;adequate&quot; number of levels for a categorical variable.</p>
<p>The choice to simplify the data by discarding some levels (for example by using a default category, as you propose) depends on what the goal is (and also number of instances, etc.). Very often this choice is made experimentally, that is by trying different methods (e.g. different thresholds) and observing which one gives the best performance: here you could do a program which tries the different proportions as threshold, then train and test the resulting model for every value, and finally plot the performance for every value.</p>
","2","2","64377","14816"
"76364","<p>Yes it makes sense, a moving average makes the curve &quot;smoother&quot; in the sense that it's less sensitive to short variations. This usually makes it easier to observe the general tendency.</p>
<p>You could also try different time periods for the average, e.g. 10 days or 15 days.</p>
<p>It looks to me like there's a moderate increase trend in your data, but the variations are important and the time window is short, so it's too early to be sure. You could apply linear regression to confirm the increase.</p>
","3","2","64377","14816"
"76372","<p>As explained in the post you link, it depends how you select the features: if it doesn't involve the target variable, then it's probably fine. I'll assume the most common case, that is that the selection relies on the target variable.
There are two parts in this problem:</p>
<ol>
<li><strong>About the ensemble method for feature selection:</strong> let's say a particular feature is selected 90% of the time. This means it's not selected 10% of the time, so if you were using a single training set it would have a 10% chance not to have this feature selected. So in theory your model is likely to be better than a &quot;regular model&quot;, because it includes a few features which wouldn't have been selected in the &quot;regular model&quot;. Note however that this method could also have negative side effects, because the features are not selected as a whole but individually (i.e. it might not be the optimal subset of features).</li>
<li><strong>About the risk of data leakage:</strong> by definition, applying this method on the whole data means using information from the test set, so a potential bias in the evaluation. It's true that the ensemble method decreases the risk of selecting a feature by chance, but for every feature there's a 10% chance that it wouldn't have been selected in the &quot;regular model&quot;. Since this selection is based partly on the test set, you can't be certain that the evaluation is reliable.</li>
</ol>
<p>Assuming your goal is to use cross-validation and that the feature selection process is computationally expensive, I can think of two ways to do this properly:</p>
<ul>
<li>Select a random N% of the data (where N is the size of the training set, e.g. 90% if using 10 fold CV), do the feature selection on this data, then use this predefined set of features every time, independently from the training set. The idea here is that the selected set of features cannot take advantage of a selection on the whole data, so it's not particularly  optimized for the test set or any CV split. That should be enough to obtain a fair evaluation, although technically there is still data leakage in this way.</li>
<li>Do an additional split in your data: with the training set you apply exactly the process that you described for feature selection, then run CV on it. After this, apply the final model to the test set. For example take 80% of the data for the training set, do feature selection and CV <em>based only on this training set</em>, and after that use the 20% left of unseen instances as test set. There's no data leakage in this way, but the final evaluation comes from a single test set, not from CV (here the CV stage can be used to study performance variation or tune hyper-parameters).</li>
</ul>
","0","2","64377","14816"
"76431","<p><a href=""https://shiny.rstudio.com/"" rel=""nofollow noreferrer"">Shiny</a> could be an interesting option: it's an R library which lets the programmer generate interactive web pages from an R program.</p>
<ul>
<li>All the R libraries for data manipulation/visualization can be used (e.g. the great <a href=""https://ggplot2.tidyverse.org/"" rel=""nofollow noreferrer"">ggplot2</a> library for graphs)</li>
<li>The interactive pages are rendered with very little effort required on the programmer side</li>
<li>Very flexible, allows quite sophisticated visualizations (see the <a href=""https://shiny.rstudio.com/gallery/"" rel=""nofollow noreferrer"">gallery</a>) and the result looks very professional imho.</li>
<li>It requires the Shiny server to run on the site, it can be installed quite easily in my experience. There is a commercial version but <a href=""https://rstudio.com/products/shiny/shiny-server/"" rel=""nofollow noreferrer"">the free version</a> doesn't have any important limitation, as far as I can tell.</li>
</ul>
<p>Drawbacks:</p>
<ul>
<li>I'm not sure I would recommend this to somebody who doesn't have any experience with R, it could be a quite steep learning curve to learn the language just for using Shiny.</li>
<li>It might not scale well with a high amount of traffic, depending on the application and the memory resources.</li>
</ul>
","1","2","64377","14816"
"76539","<p>Imho with only 2 months of data one big problem you will have is seasonality: pollution depends a lot on temperature and wind conditions, so in order to predict pollution level accurately you would need at the very least one year of data, preferably several past years. I don't think you can predict properly pollution level two months in advance with only two months of training data.</p>
<p>Maybe you could try to make the problem broader: use a dataset including multiple cities with several years of data, then you can try to forecast pollution level in a particular location using features representing local conditions such as density, temperature, etc.</p>
","2","2","64377","14816"
"76556","<p>This is called an <strong>open-class</strong> text classification problem, it's used in particular for some author identification problems.
I don't have any recent pointers but from a quick search I found this article: <a href=""https://www.aclweb.org/anthology/N16-1061.pdf"" rel=""nofollow noreferrer"">https://www.aclweb.org/anthology/N16-1061.pdf</a></p>
<p>In the field of author classification there is a similar problem called author verification, which can be treated as a <a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">one-class classification</a> problem. You could consider using it in this way:</p>
<ol>
<li>one-class classification between &quot;known classes&quot; vs. others</li>
<li>regular classification between the known classes</li>
</ol>
","2","2","64377","14816"
"76591","<p>Additionally to the major issues described in other answers and comments, the technological limitations of current AI methods are absolutely insufficient for such high level of abstraction in causality reasoning.</p>
<p>Let's assume this would be operationalized using on a corpus of scientific articles from various domains (this part is ok) using Natural Language Processing (NLP) methods. The state of the art in NLP doesn't even reach satisfying levels of performance in detecting direct causality between two sentences, a task that humans can do quite well. This means that the most basic step required for this problem is not even doable, let alone complex reasoning across different domains.</p>
","1","2","64377","14816"
"76610","<p>I'm not familiar with LDA, but as far as I know you're not really changing the &quot;model&quot; (i.e. the way to measure impact) between the two versions, what you're changing is the features: in the 2nd version, instead of looking at whether the value of the feature impacts Y, you look at whether <strong>the log of the value</strong> of the feature impacts Y.
The first version is of course the most natural way to look at features, the second is common but usually this is used when we already know that the distribution of the feature (or the relation between the feature and the response variable) makes it relevant.</p>
","1","2","64377","14816"
"76632","<p>I don't think that's a very good idea: the goal is not to make the model predict a more extreme polarity when the tweet relates to the company.</p>
<p>Instead you might want to consider oversampling the few instances of this specific company. For instance if you have 100 company-specific tweets and 1000 general tweets in your training set, you could duplicate the company-specific ones 10 times in order to give the specific tweets have a higher weight in the data. If possible you should tune the parameter of how many times to duplicate in order to obtain the optimal value.</p>
","1","2","64377","14816"
"76714","<p>I don't think there's any obviously best option. I'd suggest trying a few reasonable solutions, evaluate on a development set and then pick the one which performs best. Don't forget to try with the original data as it is, resampling doesn't always work better.</p>
<blockquote>
<p>Upsample the non-target class? Downsample the target class?</p>
</blockquote>
<p>These two options are very likely to give the same result (assuming you use the same proportion).</p>
<blockquote>
<p>Reframe the problem as &quot;not drama&quot; to maintain underbalance for every label?</p>
</blockquote>
<p>For the classifier it would be exactly the same.</p>
","0","2","64377","14816"
"76761","<ul>
<li>The X axis is the number of instances in the training set, so this plot is a data ablation study: it shows what happens for different amount of training data.</li>
<li>The Y axis is an error score, so lower value means better performance.</li>
<li>In the leftmost part of the graph, the fact that the error is zero on the training set until around 6000 instances points to overfitting, and the very large difference of the error between the training and validation confirms this.</li>
<li>In the right half of the graph the difference in performance starts to decrease and the performance on the validation set seems to be come stable. The fact that the training error becomes higher than zero is good: it means that the model starts generalizing instead of just recording every detail of the data. Yet the difference is still important, so there is still a high amount of overfitting.</li>
</ul>
","11","2","64377","14816"
"76769","<p>It simply depends what you count as your object of interest: from your description the unit can be either document or annotation. Your method describes using the document as unit, it's fine as long as the tests you plan to do are compatible with this.</p>
<p>Another option is to use the annotation as unit: in this case you would pick 60% of the 44k annotations every time, so you would have a mix of annotations from multiple documents. Depending what you test exactly, this might be an issue, in particular I don't see how you would count False Negative cases in this way.</p>
<p>Since you have text documents of varying size (I assume), you could also consider different options: sentence, paragraph, block of N sentences, etc.</p>
","0","2","64377","14816"
"76808","<p>Similarly to NB or kNN, the DT and SVM algorithms work with <strong>the features</strong> which are provided as input. So whenever ML is applied to text it's important to understand how the unstructured text is transformed into structured data, i.e. how text instances are represented with features.</p>
<p>There are many options, but traditionally a document is represented as as a vector over the full vocabulary. A very simple version of this is a boolean vector: a cell <span class=""math-container"">$v_i$</span> contains 1 if the word <span class=""math-container"">$w_i$</span> occurs in the document and 0 otherwise. The DT training will generate the tree the usual way, so in this case the conditions at the nodes will be <code>v_i == 1</code>, representing whether the word <span class=""math-container"">$w_i$</span> is present or not. If the values in the vector are say TFIDF weights, the conditions might look like <code>v_i &gt; 3.5</code> for instance. Similarly for SVM: the algorithm will find the optimal way to separate the instances in a multi-dimensional space: each dimension actually represents a single word, but the algorithm itself doesn't know (and doesn't care) about that.</p>
","2","2","64377","14816"
"76817","<p>Topic modelling is an unsupervised task, so by definition there is no gold-standard label. The task is a kind of clustering, i.e. it tries to group together documents with similar topics, but it doesn't label the groups.</p>
<p>Instead people usually use the words which are the most associated with a topic by the model as a kind of description for the topic.</p>
","0","2","64377","14816"
"76898","<p>Generally models can be compared by evaluating them. It depends on the specific application, for instance if the goal is to predict the next element(s) in a sequence then one can take a subset of the data as test set and see how accurately the model predicts the next element.</p>
","0","2","64377","14816"
"76912","<p>Maybe you could put the sentences in a dataframe and used <code>renderTable</code>, something like this:</p>
<pre><code>server &lt;- function(input, output, session) {
  sentences &lt;- reactive({
    data.frame(sentences=make_sentences(input<span class=""math-container"">$inputString))
  })
  output$</span>prediction &lt;- renderTable(sentences())
}
</code></pre>
<hr />
<p>[edit]</p>
<p>In order to only have them line by line something like this might work:</p>
<pre><code>server &lt;- function(input, output, session) {
  sentences &lt;- reactive({
    paste(make_sentences(input<span class=""math-container"">$inputString),sep='&lt;br/&gt;')
  })
  output$</span>prediction &lt;- renderTable(sentences())
}
</code></pre>
<p>Alternatively you could have a numbered list by using the corresponding HTML tags: insert the whole list inside <code>&lt;ol&gt; ... &lt;/ol&gt;</code> and every sentence in a <code>&lt;li&gt; .. &lt;/li&gt;</code>.</p>
","0","2","64377","14816"
"76916","<p>Imho the &quot;cleanest&quot; option would be to train a probabilistic model on the original categorical target, then obtain the predicted probabilities for every category as the final &quot;predictions&quot;. By &quot;training on the original target&quot; I mean designing each instance as an event, e.g. in order to represent that 7/10 people select category A there would be 7 instances where the target is category A out of 10 instances in total.</p>
<ul>
<li>The most simple option is Naive Bayes, but depending on the data it tends to always predict extreme probabilities, which would defeat the purpose.</li>
<li>An ad hoc Bayesian model could give very good results but it's probably more work to design it, depending on the features.</li>
</ul>
","2","2","64377","14816"
"76981","<p>What you're describing is indeed the traditional approach for building a sentiment analysis system, so I'd say it looks like a reasonable approach to me.</p>
<p>I'm not up to date with the sentiment analysis task at all, but I think it would be worth studying the state of the art for several reasons:</p>
<ul>
<li>There might be more recent, better approaches</li>
<li>There might be datasets in the languages you're interested in, and if there is that could save you <em>a lot</em> of time. Check if there are any shared tasks about this, they often provide annotated datasets.</li>
</ul>
","2","2","64377","14816"
"77008","<ul>
<li>The size of the document on which NER is run shouldn't be a problem at all, a standard NER system scans the document sequentially and just marks any entity it finds.</li>
<li>The size of the entities to find might be more of an issue, because typical NER systems rely on the previous few words to detect the boundaries of an entity. If the entity spans a large sequence of text, it's harder for the system to detect where it ends. In case it actually causes a problem with your data, it might be possible to specify exactly which features to use in the CRF model (this depends on the implementation I guess).</li>
</ul>
","1","2","64377","14816"
"77051","<p>I like  to use the <code>plyr</code> library but there are other ways:</p>
<pre><code>library(plyr)
ddply(mydata, c('Replicate','Node','Day'), nrow)
</code></pre>
<ul>
<li>the <code>dd</code> in <code>ddply</code> means that the input is a <strong>d</strong>ataframe and the output is also a <strong>d</strong>ataframe</li>
<li>the rows are grouped by the values of columns given as second argument</li>
<li>the last argument is the function to apply on every group, in this case <code>nrow</code> to simply count the number of rows in the group.</li>
</ul>
<p>If you want to name the column in the same time you can do:</p>
<pre><code>library(plyr)
ddply(mydata, c('Replicate','Node','Day'), function(groupDF) {
  data.frame(countObservations=nrow(groupDF))
})
</code></pre>
","0","2","64377","14816"
"77104","<blockquote>
<p>Are there certain techniques I can use to reduce the probability to below 0.5 in the stacked model so that it isn't classified the way it currently is?</p>
</blockquote>
<p>It's generally not a good idea to try to bias the classifier in order to deal better with some specific instances, because it's likely to make it weaker in with some (possibly many) other instances. The way to do that would be oversample in the training set the instances which will help predict the target instance correctly, but that's a terrible idea, don't do it! :)</p>
<p>In general it's perfectly normal to have some errors, the data can contain noise or even sometimes annotation errors.</p>
<blockquote>
<p>My other question is if there is even value in making a hybrid model if the accuracy is the same as SVM's?</p>
</blockquote>
<p>If the data is really easy to classify, it's totally possible that stacking learners doesn't improve performance: if the performance of a single model reaches the maximum obtainable on this data, clearly there's nothing to improve.</p>
<p>However it's also possible that the benefit of stacking is not visible in this particular test: in this hypothesis the meta-model is indeed better in general that a single learner, but the test set just doesn't contain any instance on which this can be seen.</p>
<p>A way to check this is to reduce the size of the training set: by making things harder for the classifiers, it's possible that the weaknesses of the individual learners will show up.</p>
","2","2","64377","14816"
"77146","<p>Assuming I understand it correctly, I think your process is ok this way but I'm not sure about step 3 &quot;standardization&quot;:</p>
<ul>
<li>Steps 1 and 2 are ok since this cannot leak any kind of information from the test set to the training set.</li>
<li>If the standardization step involves calculating values (e.g. mean and s.d.) over the whole data (including test set) and then standardizing the whole data using these values, then that could be an issue. I'd suggest moving step 3 after the split, so that standardization is based only on the training set and then applied &quot;as is&quot; to the test set.</li>
</ul>
","3","2","64377","14816"
"77255","<p>Clearly there's no way to have the names of the drugs.</p>
<p>Assuming the relation between the two columns is important, a scatter plot with units prescribed as X and number of patients as Y might work. You could even add the name of the drug for a few isolated points. Transparency/opacity can be used to show the dense areas.</p>
<p>In case the relation between the columns is not important, you could just plot the two distributions (histograms) with different colours on the same graph.</p>
","2","2","64377","14816"
"77322","<p>The simple option is to design your features so that they represent the <strong>distribution</strong> of the values: every feature <span class=""math-container"">$f_i$</span> represents a bin and its value for a particular instance is the frequency of the corresponding range for this instance.</p>
<p>Example: let's consider 10 bins between 0 and 1, i.e. <span class=""math-container"">$f_1=[0,0.1), f_2=[0.1,0.2),..., f_{10}=[0.9,1]$</span>:</p>
<ul>
<li><span class=""math-container"">$x_1=[0.2, 0.25, 0.15, 0.22]$</span> is represented as <span class=""math-container"">$[0,1,3,0,0,0,0,0,0,0]$</span></li>
<li><span class=""math-container"">$x_2 = [0.124, 0.224, 0.215, 0.095]$</span> is represented as <span class=""math-container"">$[1,1,2,0,0,0,0,0,0,0]$</span></li>
<li>...</li>
</ul>
","2","2","64377","14816"
"77618","<p>If nothing else is possible, you could try using a genetic algorithm.</p>
","0","2","64377","14816"
"77639","<p>I'm not sure I understand your problem very well but let's see.
First let me try to formalize the task as a ML problem:</p>
<ul>
<li>Identifying the SMS of interest is a binary classification task. Your &quot;completeness&quot; score seems to correspond to the standard <strong>recall</strong> measure. It is usually a good idea to also look at <strong>precision</strong>, i.e. out of the SMS identified as relevant how many are truly relevant.</li>
<li>You don't really explain the second part of extracting information out of SMS of interest. In general this part might be some kind of sequence labelling problem (similar to Named Entity Recognition).</li>
</ul>
<p>It's important to distinguish these two parts if you're going to use ML.</p>
<p>Now what you are currently doing is a <strong>rule-based</strong> system: you manually identify the patterns one by one, then the system just applies the list of patterns. rule-based systems are considered the ancestor of ML systems: as you noticed, it might be possible to automatize the part of identifying the patterns. Given that you already have labelled examples (training data), you could train a model to recognize the general patterns by itself. The main difficulty is to provide the model with the right features so that it can do the job as correctly as possible.
Note that in general it doesn't really work as a list of templates in the same way as what you do manually.</p>
<p>The question of identifying cases without looking at the whole corpus might be related to ''semi-supervised learning'', where a system iteratively learns to annotate unlabelled data from an small initial set of labelled data.</p>
","1","2","64377","14816"
"77752","<p>In a truly unsupervised setting, the only possibility is to cluster the documents. Typically this would be done with <a href=""https://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow noreferrer"">topic modelling</a>. Then the result could be evaluated by inspecting the words most associated with every topic, and assign a sentiment class to the whole topic/cluster, i.e. all the documents labelled with it.</p>
","-1","2","64377","14816"
"77919","<p>I'm not especially familiar with this but from the example provided we can deduce that:</p>
<ul>
<li>An hypothesis is a partial assignment of values to the features. That is, by &quot;applying the hypothesis&quot; we obtain a subset of instances for which the features satisfy the hypothesis.</li>
<li>An hypothesis is consistent with the data if the target variable (called &quot;concept&quot; apparently, here <code>EnjoySport</code> in the example) has the same value for any instance in the subset obtained by applying it.</li>
</ul>
<p>First case: <span class=""math-container"">$h_1=&lt;?,?,?,Strong,?,?&gt;$</span>. All 4 instances in the data satisfy <span class=""math-container"">$h_1$</span>, so the subset satisfying <span class=""math-container"">$h_1$</span> is the whole data. However the concept <code>EnjoySport</code> can have two values for this subset, so <span class=""math-container"">$h_1$</span> is not consistent.</p>
<p>Second case: <span class=""math-container"">$h_2=&lt;Sunny,Warm,?,Strong,?,?&gt;$</span>. This hypothesis is more precise than <span class=""math-container"">$h_1$</span>: the subset of instances which satisfy <span class=""math-container"">$h_2$</span> is <span class=""math-container"">$\{1,2,4\}$</span>. The concept <code>EnjoySport</code> always have value <code>Yes</code> for every instance in this subset, so <span class=""math-container"">$h_2$</span> is consistent with the data.</p>
<p>Intuitively, the idea is that an hypothesis is consistent with the data if knowing the values specified by the hypothesis gives a 100% certainty about the value of the target variable.</p>
","1","2","64377","14816"
"77936","<blockquote>
<p>How do I test if an event happening in one location has an impact on events happening in another location (dependency)?</p>
</blockquote>
<ul>
<li><a href=""https://en.wikipedia.org/wiki/Pearson_correlation_coefficient"" rel=""nofollow noreferrer"">Pearson correlation</a> between the two columns would already give you a simple indication of whether there is a dependency relation.</li>
<li>A <a href=""https://en.wikipedia.org/wiki/Chi-squared_test"" rel=""nofollow noreferrer""><span class=""math-container"">$\chi$</span>-square test</a> would tell you whether there is a significant difference between an observed variable (e.g. count in one location) and an expected variable (count in the other location). In other words, it can tell you whether the variables are independent or not.</li>
<li>The <a href=""https://en.wikipedia.org/wiki/Conditional_probability"" rel=""nofollow noreferrer"">conditional probability</a> <span class=""math-container"">$p(A|B)$</span> of a variable A given the other variable B tells you how likely the event A is assuming the event B happens. <span class=""math-container"">$A$</span> and <span class=""math-container"">$B$</span> are independent if <span class=""math-container"">$p(A|B)=p(A)$</span> (note that it's unlikely to be exactly equal in the case of a real sample).</li>
</ul>
<blockquote>
<p>I believe that if an event happening in locations B and C are dependant, I should sum the 2 columns together as 1 feature in my model.</p>
</blockquote>
<p>Unless you have a specific reason to do that (e.g. you want to consider a large area which includes locations B and C), this doesn't make a lot of sense:</p>
<ul>
<li>first dependency is not &quot;all or nothing&quot;, two variables can have a certain degree of dependency but it doesn't mean that they follow each other exactly. Therefore you would lose some information by merging them into one feature.</li>
<li>this would make it impossible to predict future events for a specific location, for instance B, if the two values for B and C are combined together.</li>
</ul>
","2","2","64377","14816"
"78020","<p>This is only a partial answer since I'm not familiar with HDBSCAN, hopefully somebody else can provide a more complete answer.</p>
<p>As far as I understand you need to find which cluster in A corresponds to which cluster in B, i.e. an alignment between the clusters labels of A and of B. It's not recommended to match based only on the size, since it could happen by chance that a cluster in A has the same size (or similar size) as a cluster in B.
Since the instances are different, you would have to rely on how the method represents the clusters.</p>
<ul>
<li>For example probabilistic clustering methods represent each cluster as a distribution over the features, so one can use a distance/similarity measure between these distributions.</li>
<li>With k-means one would compare the centroids and match the pairs of clusters for which the distance is the shortest.</li>
</ul>
<p>I'm not familiar with HDBSCAN so I don't know how the clusters are represented inside the model: whatever this is, the idea would be to compare each internal representation of a cluster in A vs. the same in B.</p>
","1","2","64377","14816"
"78101","<p>The problem with your request is that both ways are always possible. For any pair of values x and new_x you can choose to represent the increase either as an addition or a multiplication:</p>
<ul>
<li>additive increment: new_x -x</li>
<li>multiplicative coefficient: new_x / x</li>
</ul>
<p>So you need a criterion to decide which one to choose, for instance a threshold: if x is lower than 100, use addition otherwise use multiplication.
Additionally if any kind of change can happen at anytime it's completely arbitrary, there's no way to represent this in a systematic way.</p>
","0","2","64377","14816"
"78153","<p>Generally the task of recognizing one type of text against &quot;anything else&quot; is a quite difficult problem, since there is so much diversity in text that there cannot be any good representative sample of &quot;anything else&quot;.</p>
<p>Typically this problem is treated as a <a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">one-class classification</a> problem: the idea is for the learning algorithm to capture what represents the positive class only, considering anything else as negative. To my knowledge this is used mostly for author identification and related stylometry tasks. The <a href=""https://pan.webis.de/"" rel=""nofollow noreferrer"">PAN workshop series</a> offer a good deal of state of the arts methods and datasets around these tasks.</p>
<p>It is also possible to frame the problem as binary classification, but then one must be very creative with the negative instances in the training set. Probably the main problem with your current approach is this: your negative instances are only &quot;randomly selected among all other topics of the site&quot;. This means that the classifier knows only texts from the site on which it is trained, so it has no idea what to do with any new text which doesn't look like anything seen in the training data. A method which has been used to increase the diversity of the negative instances is to automatically generate google queries with a few random words which appear in one of the positive instances, then download whatever text Google retrieves as negative instance.</p>
<p>Another issue with binary classification is the distribution of positive/negative instances: if you train a model with 50/50 positive/negative, the model expects that by default there is 50% chance for each. This can cause a huge bias when applied to a test set which contains mostly negative instances, especially if these don't look like the negative instances seen during training.</p>
<p>Finally be careful about the distinction semantic topic vs. writing style, because the features for these two are usually very different: in the former case the stop words are usually removed, the content words (nouns, verbs, adjectives) are important (hence one uses things like TFIDF). In the latter it's the opposite: stop words and punctuation should be kept (because they are good indicators of writing style) whereas content words are removed because they tend to bias the model the topic instead of the style. In stylometry features based on characters n-grams have been shown to perform well... even though it's not very clear why it works!</p>
","0","2","64377","14816"
"78183","<p>You can't.</p>
<p>If the model is trained with 6 features, it means that this model is like a function which requires 6 arguments. For instance the model might calculate the answer like this:</p>
<pre><code>answer = 0 * f1 + 1 * f2 + 0 * f3 + 5*f4 + 0.5*f5 +10*f6
</code></pre>
<p>Obviously there's no way to know the answer of this function without knowing all its arguments.</p>
<p>Another way to look at it: given a model trained with a particular set of features, let's assume that it is possible to apply the model using any subset of these features and still obtain the prediction. This implies that it's possible to remove all the features. Therefore this model is a magic box able to predict reliable information from no information at all. I hope it's obvious that this is not possible.</p>
<p>In order to be able to predict with 3 features, the only way is to train a model with these 3 features.</p>
","1","2","64377","14816"
"78209","<p>As you already know, a precision score (or recall, or f-score) is for a single class, and in the function the argument <code>pos_label</code> says which class.</p>
<p>Now I'm going to guess that when <code>pos_label</code> is not provided and instead <code>average</code> is provided the function probably calculates the metric for every class <em>and then returns the average of these values</em>.</p>
<p>A weighted average can be calculated with any number of classes, and since no pre-defined weights are provided we can reasonably assume that the function takes the proportion of the two classes as weights. So the result of:</p>
<pre><code>metrics.recall_score(test_sentiments,predicted_sentiments,average='weighted')
</code></pre>
<p>is probably the weigthed average (by proportion of instances) of:</p>
<pre><code>metrics.recall_score(test_sentiments,predicted_sentiments,pos_label='positive')
metrics.recall_score(test_sentiments,predicted_sentiments,pos_label='negative')
</code></pre>
<p>If I'm not mistaken, this is equivalent to the micro-average recall.</p>
","1","2","64377","14816"
"78260","<p>For somebody new to ML you're starting with a quite complex problem!</p>
<p>Your first job is to formalize the problem: what exactly do you want to predict and from which information?</p>
<ul>
<li>Do you want to predict whenever a specific signal will occur, or whenever any one of several specific signals occurs, or detect any unusual pattern...? In the first two cases your task is supervised classification, detecting unusual patterns would probably be unsupervised outlier detection.</li>
<li>In the case of supervised classification, which seems more compatible with your description, you're going to use the specific signal to detect as target variable: your features are made of a sequence of data until time <span class=""math-container"">$t$</span>, and the target variable is 0 or 1 depending on whether the signal occurs in the next 30 mns.</li>
</ul>
<p>There's a lot of good advice that  you can use in the related question you mention. You could consider <a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">sequence labeling</a> methods, but there are certainly a lot of other options.</p>
<p>I'm not so sure you will need dimensionality reduction in your case: the signals you're using as features are binary so your features are much less complex than in the related question.</p>
","1","2","64377","14816"
"78347","<p>There is a bit of confusion, I'm afraid:</p>
<ul>
<li>The definition you propose for uncertainty doesn't really represent the concept of uncertainty: if <span class=""math-container"">$p_i$</span> is the probability that <span class=""math-container"">$x$</span> belongs to category <span class=""math-container"">$a_i$</span>, then <span class=""math-container"">$1-p_i$</span> is just the probability that <span class=""math-container"">$x$</span> <em>doesn't belong</em> to category <span class=""math-container"">$a_i$</span>.</li>
<li>Yes, if the classifier assigns a very high <span class=""math-container"">$p_i$</span>, say 0.99, it is supposed to mean that the classifier is very confident in its prediction. But it's also the case for a very low probability: if <span class=""math-container"">$p_i=0.01$</span>, the classifier is very confident that <span class=""math-container"">$x$</span> doesn't belong to <span class=""math-container"">$a_i$</span>. According to your definition, the uncertainty in this case would be very high (0.99) even though the classifier is very confident, so your definition is inconsistent.</li>
</ul>
<p>Now the main problem: whatever you call a confidence measure based on the probability predicted by the classifier, <strong>it's not reliable</strong>. A prediction is at best an informed decision of the classifier given the data it has seen in the training set and the features of instance. But it could be a random classifier, or a majority-class classifier: in these cases the probability it &quot;predicts&quot; is arbitrary. Imagine you are a teacher and one of your students says &quot;the answer of x=2+2 is x=5&quot;, I'm 100% sure&quot;. The fact that the student is &quot;100% sure&quot; doesn't make them right, same thing for the classifier. In other words, any reliable measure of uncertainty involves the gold-standard answer, so it's usually part of the evaluation process. That's not to say that the predicted probability is useless, but in general <strong>it has no direct link to accuracy</strong>, and it would be a mistake to interpret it in this way.</p>
<p>Interpretability (or explainability) is a completely different matter: the general idea is to know whether the answer predicted by a classifier can be understood by a human. Typically traditional models like Naive Bayes or Decision Tree models are more directly interpretable (at least with not too many features) than deep NN models.</p>
","1","2","64377","14816"
"78382","<p>Here are a few ideas:</p>
<p>If the number of strings is not too high, you could consider taking a formal approach and use a finite automata determinization algorithm (I'm very rusty about this stuff but I clearly remember that there is such a thing). The idea is to start from a big automaton made of the union of all the strings, then use the algorithm to find the deterministic automaton, which can then be converted to a regular expression.</p>
<p>A more data-science-y idea is to use character-based similarity/distance measures between all the pairs of strings. Then it should be possible to identify outliers, maybe through clustering based on the distance. Typical character-based measures: <a href=""https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance"" rel=""nofollow noreferrer"">Jaro-Winckler</a>, <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein edit distance</a>.</p>
<p>Finally an original (but possibly bad) idea would be to try to train a (character-based) language model on the strings (assuming there are sufficiently many of them). Given an input string, the language model gives you a probability that this string belongs to the &quot;language&quot;, so an outlier could be detected by its low probability.</p>
<hr />
<p><em>[addition following OP's comment]</em></p>
<p><a href=""https://en.wikipedia.org/wiki/Language_model"" rel=""nofollow noreferrer"">Language modeling</a> is normally used for representing the valid sentences in a language, e.g. English, based on the likelihood of sequences of words in this language. It's trained from a large number of correct sentences so that it can estimate the probability of the <span class=""math-container"">$n$</span>-grams of words in this language. This is a common NLP task (<a href=""https://nlpforhackers.io/language-models/"" rel=""nofollow noreferrer"">example</a>) but in your case you would use characters instead of words and strings instead of sentences, so there would be a small adaptation compared to the examples you'll find.</p>
","2","2","64377","14816"
"79610","<p>In general we can't compare the performance of a multiclass classifier with the performance of a binary classifier since the former expresses how good the classifier is at classifying <em>any instance of any class</em>. So if there are <span class=""math-container"">$n_A$</span> samples labelled A, there's only a proportion of <span class=""math-container"">$n_A/n$</span> of the global accuracy of the multiclass classifier which is about A. In particular a multiclass classifier usually tends to favor the largest classes, so if class A happens to be a small proportion of the data then the global performance will not reflect how good it is at classifying A: for example it might have 90% accuracy simply because class B is 90% of the data, this doesn't prove anything about class A. By contrast the performance of the binary classifier is by definition solely about class A.</p>
<p>However if one has access to the detailed evaluation of the multiclass classifier, typically the confusion matrix, then it becomes possible to calculate the performance of the classifier for a single class, say class A. Actually by merging all the B,C,D rows together and all the B,C,D columns together in the confusion matrix one obtains exactly a binary classification confusion matrix, and from that one can calculate a performance which can be compared against another binary classifier. But in this setting the multiclass classifier is at a disadvantage for the reason mentioned above: it also has to deal with the other classes and this can cause it to &quot;sacrifice&quot; a class, whereas the binary classifier doesn't have this issue.</p>
","1","2","64377","14816"
"79625","<p>There are two potential biases:</p>
<ul>
<li>With this automatic method, you might have a few erroneous labels. For example it happens regularly here on DataScienceSE that a user tags a question &quot;python&quot; but actually the question is not specific to python at all. Same thing for the opposite case: for example it's possible that some content contains some python code but doesn't mention &quot;python&quot; anywhere.</li>
<li>The distribution between positive and negative classes is arbitrary. Let's assume you use 50% positive / 50% negative: if later you want to apply your classifier on a new data science website where only 10% of the content is about python, it's likely to predict a lot of false positive cases so the true performance on this data will be much lower than on your test set.</li>
</ul>
<p>It's rare to have a perfect dataset, so realistically in my opinion the first issue is probably acceptable because the noise in the labels should be very limited. The second issue could be a bit more serious but this depends on what is the end application. Keep in mind that a trained model is meant to be applied to the same kind of data as it was trained/tested on.</p>
","0","2","64377","14816"
"79646","<blockquote>
<p>My question is: Does it matter if the minority class is set as the positive or negative label in relation to performance of training a model or affecting a loss function such as cross entropy?</p>
</blockquote>
<p>No it doesn't.</p>
<p>However in binary classification it's customary to call &quot;positive&quot; the main class of interest, so be careful to be clear about which one is positive/negative when/if you present your results to other people.</p>
<p>Also be careful that precision and recall are usually calculated for whatever is called the positive class, so don't inadvertently use the results of the majority class instead of the one you're interested in.</p>
","1","2","64377","14816"
"79675","<p>If your goal is only to determine whether the level of complexity is the same between these two sources based on these particular features, yes you can simply compare the distributions with a <a href=""https://en.wikipedia.org/wiki/Student%27s_t-test"" rel=""nofollow noreferrer"">Student's t-test</a> if the distributions are normal or a <a href=""https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test"" rel=""nofollow noreferrer"">Wilcoxon test</a> if they are not.</p>
<p>Spoiler alert: it's very likely that they are different. A statistical significance test doesn't give you much information, it's usually much more useful to try to quantify the level of complexity, but it's also much harder. Based on your <a href=""https://linguistics.stackexchange.com/q/36836"">previous linked question</a>, you might be interested to read about text complexity/readability, quite a lot of research has been done on this topic (e.g. <a href=""https://www.aclweb.org/anthology/W16-4123/"" rel=""nofollow noreferrer"">here</a>, <a href=""https://www.aclweb.org/anthology/D18-1289.pdf"" rel=""nofollow noreferrer"">here</a>, or <a href=""https://www.sciencedirect.com/science/article/pii/S1877050918323962"" rel=""nofollow noreferrer"">there</a>, among many others). There are also a few general tutorials apparently (e.g. <a href=""https://towardsdatascience.com/linguistic-complexity-measures-for-text-nlp-e4bf664bd660"" rel=""nofollow noreferrer"">this</a> or <a href=""https://medium.com/glose-team/how-to-evaluate-text-readability-with-nlp-9c04bd3f46a2"" rel=""nofollow noreferrer"">that</a>). I also know that there have been a number of readability/complexity metrics proposed in the literature, but I don't have references.</p>
<p>Note that looking at the features from the text is unlikely to be very useful on its own. Probably you will need either some kind of corpus annotated by complexity, or to use a metric which has been proved to correlate with sentence complexity.</p>
","0","2","64377","14816"
"79714","<p>This output represents the dendogram as a tree. The innermost parentheses represent the deepest parts of the tree. For instance the top (root) of the tree start with the pair K5 and a subtree, then this subtree is made of another subtree and K4, and so on.</p>
<p>If we ignore the numerical values (distances I assume?) we have this:</p>
<pre><code>(K_5,
  (
    (K_1,
       (
         K_2,K_3
       )
       
    )
    K_4
  )
)
</code></pre>
<p>Which represents this tree:</p>
<pre><code> --------------------
 |                  |
 |            -------------
K_5           |           |
           -------       K_4
           |     |
          K_1  -----
               |   |
              K_2 K_3
</code></pre>
<p>Then it can be converted to the desired format:</p>
<pre><code>[K_1 , K_2 , K_3 , K_4 , K_5]
[K_1 , K_2 , K_3 , K_4] [K_5]
[K_1 , K_2 , K_3] [K_4] [K_5]
[K_1] [K_2 , K_3] [K_4] [K_5]
[K_1] [K_2] [K_3] [K_4] [K_5]
</code></pre>
","0","2","64377","14816"
"79731","<p>In your code you use the Sidebar layout which allows only two columns, instead you should use the grid layout to position your elements where you want them, see <a href=""https://shiny.rstudio.com/articles/layout-guide.html"" rel=""nofollow noreferrer"">https://shiny.rstudio.com/articles/layout-guide.html</a>.</p>
<p>You probably need something like this (not tested):</p>
<pre><code>ui&lt;- shinyUI(fluidPage(
  fluidRow(
    column(3,
      # content left 
    ),
    column(5,
      # content centre
    ),
    column(4,
      # content right
    )
  )
)
</code></pre>
","0","2","64377","14816"
"79774","<p>Assuming the professional has to deal with large datasets and/or computer-intensive tasks (not always the case), the local machine doesn't matter: what matters is access to some kind of high-performance cluster or cloud computing.</p>
","1","2","64377","14816"
"79904","<p>I'm not sure if there is a standard for this kind of case. I think it depends whether the system predicts any of these classes, i.e. whether there are any false positive cases:</p>
<ul>
<li>if yes, then the precision must be zero indeed: out of all the instances predicted positive none was a true positive.</li>
<li>if no, I think it makes more sense to consider the precision as undefined (NaN), since no instance is predicted positive at all (that is, there is a division by zero in the calculation of precision).</li>
</ul>
<p>This is a very borderline case, since it would be very questionable to use a test set which doesn't contain all the classes contained in the training set: such a test set cannot is not fit for the purpose of evaluation.</p>
","0","2","64377","14816"
"79952","<blockquote>
<ol>
<li>Y comes as percent in the format. To put it into the same
dimension as  X I multiplied it with 10. Is that ok from math/data
science perspective?</li>
</ol>
</blockquote>
<p>As far as I can tell there's no reason to do that, and why multiply by 10?</p>
<blockquote>
<ol start=""2"">
<li>5 records is not much but there are a lot
of features. I would like to do multiple linear regression. Do you
think this feasible with this data set? What would be objections and
risks doing that?</li>
</ol>
</blockquote>
<p>The fact that there are lots of features makes it <em>harder</em> to work with few instances, not easier. There is a very high risk of overfitting, that is of the model catching patterns which appear by chance in the features. This leads to predictions being also affected by chance, so bad performance.</p>
<blockquote>
<ol start=""3"">
<li>Would upsampling the dataset help me with
anything here? Or could I just work with the five records?</li>
</ol>
</blockquote>
<p>Upsampling is unlikely to work since it's going to reproduce the patterns in the small dataset, so it's also going to reproduce patterns which appear by chance.</p>
<blockquote>
<ol start=""4"">
<li>With
the strange shape of the dataset especially the low number of records
do you think that sufficient precision can be reached?</li>
</ol>
</blockquote>
<p>It depends what the data represents, if the features happen to be really good predictors for the dependent variable and are not affected by chance, it might work. But these are very optimistic assumptions, in general it's not reasonable to expect good predictions from such a small set of instances.</p>
<blockquote>
<ol start=""5"">
<li>How could
I calculated the maximum possible precision/discriminative power
possible with this dataset? (I am looking for strong arguments why
they should give me access to the complete dataset)</li>
</ol>
</blockquote>
<p>In general I would suggest doing a leave-one-out experiment: use 4 instances as training set, 1 instance as test set, repeat 5 times with a different instance as test set every time. Measuring the average performance should give you an idea how far off the predictions are going to be (you could use a very simple evaluation measure such as mean absolute error).</p>
<p>However what you have is actually a time series apparently, so it might be worth looking at methods which take time evolution into account.</p>
","0","2","64377","14816"
"79963","<blockquote>
<p>what is the best mathematical aproach to dynamically find the best possible threshold value above which i can say that the definition is correct and below which you need to have a look if the classes are correct</p>
</blockquote>
<p>First, in general it's clear that there's no way to be certain that an answer is correct or incorrect: for instance if for some reason all the classifiers return the same wrong answer, then the threshold condition is satisfied but the answer is wrong.</p>
<p>Now the only way to find an <strong>optimal</strong> threshold to decide whether an answer is acceptable is to use a set of labelled examples in order to evaluate the answers of the classifiers.
Why? Because the correctness of the answers cannot be determined by any mathematical formula.</p>
<p>So the goal of the game is to assess to what extent this set of classifiers can be trusted to give a correct answer, and that depends on the threshold. So the threshold is a <strong>parameter</strong> of the predicting system, and this parameter can be estimated (tuned) based on some validation data. The basic method consists in trying all the possible values for the parameter, evaluate the performance in every case and pick the value which obtains the highest performance. Note that there are many  possible evaluation measures for such a scenario: the measure should be chosen carefully depending on the goal of the system (e.g. is it better to have one false positive answer or false negative?).</p>
","0","2","64377","14816"
"79999","<p>A good starting point is to look at the evaluation measures used in the NER shared tasks: <a href=""https://nlpprogress.com/english/named_entity_recognition.html"" rel=""nofollow noreferrer"">https://nlpprogress.com/english/named_entity_recognition.html</a>.</p>
<p>Generally the F1-score can be used for one specific class, but there are different options regarding what is counted as an instance:</p>
<ul>
<li>every occurrence of the full NE. In this case any difference between the predicted and the gold is counted as false, even if it's only one token difference.</li>
<li>every token in an entity. In this case a partially matched entity counts as &quot;partially correct&quot;: if a word is predicted outside instead of inside, it's a false negative and conversely.</li>
<li>Other variants:
<ul>
<li>count only unique entities, in order to observe the diversity of the entities recognized.</li>
<li>count only entities which didn't appear in the training set, to observe the generalization power.</li>
</ul>
</li>
</ul>
<p>(writing this from memory, I could miss something)</p>
","1","2","64377","14816"
"80039","<p>It's not a single TFIDF score on its own which makes classification possible, the TFIDF scores are used inside a vector to represent a full document: for every single word <span class=""math-container"">$w_i$</span> in the vocabulary, the <span class=""math-container"">$i$</span>th value in the vector contains the corresponding TFIDF score. By using this representation for every document in a collection (the same index always corresponds to the same word), one obtains a big set of vectors (instances), each containing <span class=""math-container"">$N$</span> TFIDF scores (features).</p>
<p>Assuming we have some training data (labelled documents), we can use any supervised method to learn a model, for instance Naive Bayes, Decision Trees, SVM, etc. These algorithms differ from each other but they are all able to take into account <em>all the features</em> for a document in order to predict a label. So in the example you give maybe the word &quot;mobile&quot; only helps the algorithm eliminate the categories &quot;sports&quot; and &quot;literature&quot;, but maybe some other words (or absence of other words) is going to help the algorithm decide between categories &quot;Business&quot; and &quot;Tech&quot;.</p>
","0","2","64377","14816"
"80070","<p>This seems to correspond to <a href=""https://en.wikipedia.org/wiki/Entity_linking"" rel=""nofollow noreferrer"">entity linking</a> or possibly <a href=""https://www.aclweb.org/anthology/W19-2801/"" rel=""nofollow noreferrer"">named entity coreference</a>. You might find some datasets <a href=""https://nlpprogress.com/english/entity_linking.html"" rel=""nofollow noreferrer"">here</a>.</p>
","0","2","64377","14816"
"80083","<p>Generally there are two main options for multi-label classifications:</p>
<ul>
<li>A binary model for every class, in this case the labels are supposed to be independent of each other.</li>
<li>A joint model which learns all the classes together, so at training stage the class represents the three values for the labels, for instance:
<ul>
<li>class &quot;NNN&quot; means no label at all</li>
<li>class &quot;NNY&quot; means no for question, no for complain and yes for complements</li>
<li>class &quot;NYN&quot; means no for question, yes for complain and no for complements</li>
<li>...</li>
<li>class &quot;YYY&quot; means yes for question, yes for complain and yes for complements</li>
</ul>
</li>
</ul>
<p>In theory there are <span class=""math-container"">$2^3$</span> possible classes for 3 labels, but in practice there might be less combinations in the data (as suggested in the question). If the labels depend on each other (for instance if it's unlikely to have a document which is both complain and complements), a joint model is more appropriate. However a joint model might need more instances to be trained properly, since it has more work to do than a binary model.</p>
","0","2","64377","14816"
"80193","<p>The <span class=""math-container"">$k$</span>-fold cross-validation (CV) process (method 2) actually does the same thing as method 1, but it repeats the steps on the training and validation sets <span class=""math-container"">$k$</span> times. So with CV the performance is averaged across the <span class=""math-container"">$k$</span> runs before selecting the best hyper-parameter values. This makes the performance and value selection more reliable in general, since there is less risk to obtain the best result by chance. However it takes much longer (since repeating <span class=""math-container"">$k$</span> times), so if the training process is long it's not always practical to use CV.</p>
","3","2","64377","14816"
"80330","<p>I assume that currently you're training a binary classification model, right?</p>
<p>You could try training a regression model which predicts the score between 0 and 100. It would be better to have some examples in your training data which are between 10 and 90, because that would make the model learn the distribution of the scores. But even if you don't have this kind of instances, it's possible (not sure though) that the model will predict instances in this range in some cases.</p>
","1","2","64377","14816"
"80397","<p>If you only want the average daily consumption over the whole period of time, you can simply calculate the difference between the last and first reading and divide by the total number of days.</p>
<p>As far as I understand your explanation:</p>
<ul>
<li>your method 1 would not give you the true average over the period, unless you multiply each individual average by a weight corresponding to the number of days in the period.</li>
<li>your method 2 gives the true average, since it would represent every day individually (if I understand correctly)</li>
</ul>
<p>However if you want to be able to observe the variations across time while smoothing some of the irregularities, you could:</p>
<ol>
<li>calculate the daily average for every day like in method 2.</li>
<li>calculate a rolling average over a fixed number of days for every day. This is not perfect if there are long periods with no reading, but it should show the general trend better.</li>
<li>Plot a graph based on rolling average.</li>
</ol>
","0","2","64377","14816"
"80425","<p>You're right, the encoding step itself can be a source of data leakage and normally it should be done inside the CV loop using only the current training set, as you describe.</p>
<p>The reason is indeed the one you mention in the comment: if there is a class label or a feature category which doesn't appear by chance in a particular training set during CV, the model is not supposed to know that this class/category even exists.</p>
<p>In general I would think that this issue can only decrease the performance on the test set, so it's probably not as serious as other kinds of data leakage. Still, it's definitely a cleaner experimental design to encode using only the training set.</p>
<p>A closely related issue in NLP is when the system is not designed to deal with out-of-vocabulary (OOV) words: if all the words in both the training and test set are encoded (same mistake), then it wrongly looks as if any text can be fully encoded, potentially causing to bad surprises later.</p>
<p>That being said, it's usually a good idea to discard rare features or label values, and if this is done then the result should be the same using either the proper method or the sloppy one.</p>
","1","2","64377","14816"
"80427","<p>This is a <a href=""https://en.wikipedia.org/wiki/Stylometry"" rel=""nofollow noreferrer"">stylometry</a> task, more exactly some form of author verification/identification task. In case you want to dig deeper, the <a href=""https://pan.webis.de/"" rel=""nofollow noreferrer"">PAN workshop series</a> is a good source of datasets and methods.</p>
<p>About your method:</p>
<ul>
<li>Your intuition is correct about selecting the most frequent words, in particular stop words: writing style is better characterized by the patterns in frequent grammatical constructs than by the choice of content words. However depending on the size of the data you might want to be more flexible about the number of words: if you have enough data, you should probably take more than the top 100 frequent words. If you don't, well... it might not work very well. Note also that some successful methods don't use the top frequent words but rather a range of middle frequency words.</li>
<li>Using TFIDF is controversial for style identification. It's often done simply with the term frequency instead.</li>
</ul>
<p>Other features you could consider:</p>
<ul>
<li>words n-grams, typically bigrams or even trigrams if there is enough data.</li>
<li>characters n-grams (usually trigrams) have the surprising property to be very robust features for style detection. If you use these do not apply any tokenizer or remove punctuation signs.</li>
</ul>
","1","2","64377","14816"
"80477","<p>I know of a good few NLP regression tasks, but these are usually <em>not</em> about a predicting an extrinsic value from the semantic description, but rather predict some numerical property of the text itself:</p>
<ul>
<li>MT quality estimation: predict the quality of a translated sentence, usually as a score. Data available at the <a href=""http://statmt.org/wmt18/quality-estimation-task.html"" rel=""nofollow noreferrer"">WMT QE Shared Task series</a>. There are probably other similar datasets about predicting the quality of a sentence in general.</li>
<li>Author profiling: predicting the age of the author of a text. Some datasets exist in particular through <a href=""https://pan.webis.de/"" rel=""nofollow noreferrer"">the PAN workshop series</a>.</li>
<li>Predicting a rating based on a comment - there's data everywhere on social media, Amazon etc. You could even use StackExchange posts and predict votes :)</li>
</ul>
<p>I'm not aware of any data similar to your example (btw it would be a very challenging task). I know that there are some works on detecting fraud from the text of an insurance claim, so maybe there are datasets that you could use in this area.</p>
","1","2","64377","14816"
"80534","<p>It's perhaps a bit naive but the first idea that comes to mind is to simply count the number of parameters which have to be estimated during training: the more values need to be estimated, the more complex the model is, since the hypothesis space is larger. For example a linear model needs only <span class=""math-container"">$n+1$</span> parameters (with <span class=""math-container"">$n$</span> the number of features), whereas the number of parameters in an ensemble model needs is the sum of the numbers of parameters for every learner, so it's likely to be higher. This idea could be refined to take  into account the range of values of a parameter.</p>
<p>As a very rough approximation, one could simply calculate the size of the object which represents the model in python (assuming the representation of the model is space-efficient, it might not always be the case).</p>
","2","2","64377","14816"
"80581","<p>Ok, let's be clear:</p>
<ul>
<li>When we say that evaluation should never be done on the training set, it means that <strong>the real performance of the model</strong> can only be estimated on a separate test set.</li>
<li>It's totally fine to calculate the performance of a system on the training data, and it's often useful (e.g. to avoid overfitting). Of course the obtained result <strong>does not represent in any way the real performance of the system</strong>, so it's important to make sure that there's no confusion by mentioning it clearly.</li>
</ul>
","0","2","64377","14816"
"80594","<p>The way to calculate <span class=""math-container"">$p(x)$</span> is indeed:</p>
<p><span class=""math-container"">$$p(x) = \sum_k p(C_k) \ p(x| C_k)$$</span></p>
<p>Since in general one needs to calculate <span class=""math-container"">$p(C_k,x)$</span> (numerator) for every <span class=""math-container"">$k$</span>, it's simple enough to sum all these <span class=""math-container"">$k$</span> values. It would be incorrect to use the product, indeed.</p>
<blockquote>
<p>Lastly, I understand we don't actually need the denominator to find our probabilities but am asking out of curiosity.</p>
</blockquote>
<p>Calculating the marginal <span class=""math-container"">$p(x)$</span> is not needed in order to find the most likely class <span class=""math-container"">$C_k$</span> because:</p>
<p><span class=""math-container"">$$argmax_k(\{ p(C_k|x)  \}) = argmax_k(\{ p(C_k,x)  \})$$</span></p>
<p>However it's actually needed to find the posterior probability <span class=""math-container"">$p(C_k | x)$</span>, that's why it's often useful to calculate the denominator <span class=""math-container"">$p(x)$</span> in order to obtain <span class=""math-container"">$p(C_k | x)$</span>, especially if one wants to output the actual probabilities.</p>
","1","2","64377","14816"
"80671","<p>You could start with plotting the relation between any relevant pair of variables, typically with a simple scatter plot and possibly using color to represent a third variable.</p>
<p>Pearson correlation coefficient is a simple but useful measure of association between two variables. By calculating it between two variables on the whole data and then on subsets based on specific conditions (e.g. third column is A) you can observe the effect of the condition, i.e. see if it increases or decreases the correlation level.</p>
","1","2","64377","14816"
"80704","<p>I'm not aware of any standard representation which increases the importance of document-frequent words, but IDF can simply be reverted: instead of the usual</p>
<p><span class=""math-container"">$$idf(w,D)=\log\left(\frac{N}{|d\in D\ |\ w \in d|}\right)$$</span></p>
<p>you could use the following:</p>
<p><span class=""math-container"">$$revidf(w,D)=\log\left(\frac{N}{|d\in D\ |\ w \notin d|}\right)$$</span></p>
<p>However for the task you describe I would be tempted to try some more advanced feature engineering, typically by using features which represent how close the distribution of words in the current document is from the average distribution.</p>
","1","2","64377","14816"
"80705","<blockquote>
<p>If I merge on dates, I'd have multiple repeated rows with fires occurring at the same time in different places, would that be the best way?</p>
</blockquote>
<p>Probably not, since you don't want to lose the location information. You should probably find a way to map the latitude/longitude to borough/county between the two datasets, so that you obtain a semantically consistent dataset (list of fires by both date/time and location).</p>
<blockquote>
<p>The problem I see is that the locations where there was no fire would not be represented, so I'd have to add a bunch of blank rows for all locations where there was no fire to balance the set.</p>
</blockquote>
<p>This depends what you plan to do with your data, but given how your first dataset is currently structured (list of fires) it doesn't make a lot of sense to add locations where there was no fire. You might want to create a dataset which lists for every place and every time whether there was a fire or not, for example.</p>
","0","2","64377","14816"
"80811","<p>The general principle of ensemble learning is indeed to rely on the diversity of the individual learners rather than their performance. Therefore it's ok to include models which perform well by chance (typically because they're overfit), since if there is sufficient diversity across the models it's very unlikely that two models would be overfit <em>in the same way</em>, i.e. that they would wrongly predict the same instances by chance. So if most models perform reasonably well in general through different mechanisms, it is expected that for any given instance most models will predict the right answer and only a few will be wrong.</p>
<p>The risk of using models which have a quite high correlation together is to defeat the purpose of ensemble learning and obtain a performance similar to the best individual learner.</p>
<p>Note also that the risk of overfitting is quite low if the individual learners have been tested with CV. It could also be worth checking the variance of the performance across CV runs (high variance indicating potential instability), but even that is not so useful in my experience: it's really the diversity of the learners which makes ensemble learning work optimally.</p>
<p>But as usual a lot depends on the actual data/task, so I'd suggest properly testing a few different approaches.</p>
","0","2","64377","14816"
"80824","<p>Formally the problem of topic modelling is a clustering problem: given a collection of text documents, group together the documents which are topically similar.</p>
<p>So technically it can indeed be done with a TF-IDF representation of documents as follows:</p>
<ol>
<li>Collect the global vocabulary across all the documents and calculate the IDF for every word.</li>
<li>Represent every document as a TF-IDF vector the usual way: for every word, obtain the term frequency in the document (TF) then multiply by the global IDF for this word (IDF). Note that every vector must represent the document over the global vocabulary.</li>
<li>Use any clustering method over the vector representations of the documents: <a href=""https://en.wikipedia.org/wiki/K-means_clustering"" rel=""nofollow noreferrer"">K-means</a>, <a href=""https://en.wikipedia.org/wiki/Hierarchical_clustering"" rel=""nofollow noreferrer"">hierarchical clustering</a>, etc.</li>
</ol>
<p>Note that this method is unlikely to be as good as state of the art methods for topic modelling.</p>
","3","2","64377","14816"
"80988","<blockquote>
<p>If my specified model is predicting equally well (say in terms of classification accuracy) on two unrelated datasets; can I assume/conclude that the two datasets follow the same distribution? The parameters of the predictive model remain exactly the same in both the cases.</p>
</blockquote>
<p>No you cannot, because there is no general equivalence between the two datasets having the same distribution and their performance being identical. There is only a one-way implication: <strong>if</strong> the two datasets have the same distribution, <strong>then</strong> their performance is identical.</p>
<p>There are actually many ways in which the datasets could have the same performance without following the same distribution, for instance if the design of the model doesn't properly represent the data, or simply by chance.</p>
<p>So the exact same performance is only <strong>compatible with the hypothesis</strong> that the datasets have the same distribution, but it doesn't prove it.</p>
<p>As far as I know, proving that two datasets have the same distribution with a complex set of features is a difficult problem. Additionally if the original dataset is small it's difficult to know how representative it is of the true distribution.</p>
","0","2","64377","14816"
"80997","<p>As you certainly know, Machine Translation (MT) is a very challenging and useful task in the domain of Natural Language Processing (NLP). As such it is a very specialized research domain but also a very active area of research, and a very competitive one (in particular due to commercial applications, obviously).</p>
<p>So there's a massive amount of research being done and a massive amount of resources, but in order to gain real expertise in MT one needs to acquire quite a lot of background knowledge. Let's be clear: a beginner level in data science is not sufficient to understand state of the art MT. Typically one needs not only a good level in statistics and programming, but also knowledge of the recent progress in MT: the old statistical MT approach (e.g. <a href=""http://www.statmt.org/moses/"" rel=""nofollow noreferrer"">Moses</a>) has been replaced with  better Neural MT approaches.</p>
<p>A slightly less ambitious objective would be to study the limitations of current MT systems, since this doesn't require understanding how they work. Note that even simply training a state of the art model using existing software is not trivial, and requires quite a lot of computational resources. I'd suggest looking at the resources and papers published at the <a href=""http://www.statmt.org/wmt20/"" rel=""nofollow noreferrer"">Workshop on Machine Translation</a> (check also the previous years). Note also that there are many sub-tasks related to MT to look at:</p>
<ul>
<li>model design</li>
<li>evaluation metrics</li>
<li>building training corpora</li>
<li>quality estimation</li>
<li>post-editing</li>
</ul>
<p>The WMT Shared Tasks offer datasets for these different sub-tasks. Reading the overview paper for a task is a good way to get an idea of what it is and how it's done.</p>
","1","2","64377","14816"
"81080","<p>I agree with Nicholas' answer, a few more thoughts:</p>
<ul>
<li>you could use a standard English tokenizer (e.g. nltk, Spacy), if only to see how they process hyphenated words. Similarly you could check how it's done in a pre-tokenized dataset, but be aware that the tokenization conventions followed might differ from one dataset to the other.</li>
<li>Imho the choice depends on the task/application and to some extent on the size of the data: if the data is large, option 1 is probably preferable because there's a chance the same hyphenated word can appear several times. However if the data is small then option 2 is better since it will allow partial match with individual tokens.</li>
<li>option 3 is an interesting compromise between options 1 and 2, but it has the disadvantage of slightly messing with the words distribution.</li>
</ul>
","1","2","64377","14816"
"81114","<p>First, about interpreting these confusion matrices: the sum of every row is 1, which implies that every value is a conditional probability <code>p( predicted label | true label )</code>, i.e. the probability of a given true label to be a particular predicted label. Example: the top left cell in both matrices is 0.01, which means that when the true label is 5 the probability that the system predicts label 1 is 1%.</p>
<p>The two confusion matrices show the results of the predictions <em>by two different systems</em>. This means that in general there is not one which is correct and the other which makes errors, just two different ways to predict labels which might lead to different types of errors.</p>
<ul>
<li>The diagonal shows the True Positive cases, i.e. cases where the predicted label is the same as the true label (this is important since these are the &quot;correct&quot; cases). The probabilities show very little difference between the two systems on the diagonal, the top one being slightly better for labels 3 and 4 and the bottom one slightly better for label 5.</li>
<li>In general it's important not to look only at the diagonal especially for ordinal values, because if the predicted value is far from the true value it's a more serious error than if it's close to it. Example: if the true value is 4 it's better for a system to predict 3 than 1 (both are errors but the latter is worse). However here again there is very little difference between the two systems, they appear to have a very similar behaviour.</li>
</ul>
<p>In order to quantitatively summarize and compare the performance of the two systems, confusion matrices are too complex. Typically one would use an appropriate evaluation measure, for instance micro or macro f-score (classification evaluation) or Mean Absolute Error (regression evaluation).</p>
","1","2","64377","14816"
"81265","<p>This would be done with a controlled experiment, similar to clinical tests:</p>
<ul>
<li>Randomly assign customers into to groups 1 and 2. Group 1 is given the attractive offer, group 2 is not (control group).</li>
<li>It's important to avoid any external bias, so for example ideally nobody in the company should know whether a customer is in group 1 or group 2. The goal is to make sure that the <em>only</em> difference between the two groups is the offer.</li>
</ul>
<p>After a pre-determined duration (e.g. 3 months), collect the results (how many customers in group X have left/stayed). Then a simple <a href=""https://en.wikipedia.org/wiki/Chi-squared_test"" rel=""nofollow noreferrer"">Chi-square test</a> can be performed on the 2x2 contingency table in order to assess whether there's a significantly different rate of leaving between the two groups. Since the offer is the only possible explanation, this proves that the offer causes the difference.</p>
","1","2","64377","14816"
"81298","<p>I'm not sure I completely get the idea, but it looks to me like what you're actually interested in is the sentiment of a word <em>in a particular context</em>: a content word like &quot;car&quot; might not carry a stable sentiment by itself, but its usage in a specific context might.</p>
<p>So I'd suggest a method like this: for any target word you extract either the sentence or a context window, i.e. N words on the left and N words on the right of the target word. Then you could use predefined sentiment analysis tools to extract a sentiment value for this instance. From there you could:</p>
<ul>
<li>measure the mean sentiment for a word by averaging over the instances</li>
<li>compare the distribution of sentiment or average sentiment for two different words</li>
</ul>
","0","2","64377","14816"
"81299","<p>k-fold CV is meant to evaluate the model. Once the evaluation is done and one is ready to move to deployment, there's no point using CV anymore: the method has been tested and validated, so one can reasonably assume that from now on applying the same method to the same kind of data will lead to the same level of performance. Thus the usual process is:</p>
<ol>
<li>Train a final model on the full dataset (no CV, no testing)</li>
<li>Apply the model to new instances</li>
</ol>
","1","2","64377","14816"
"81331","<p>Your model certainly overfits. It's likely that the main issue is the inclusion in the features of words which appear very rarely (especially those which appear only once in the corpus):</p>
<ul>
<li>Words which appear only once don't help classification at all, if only because they can never be matched again. More generally, words which appear rarely are more likely to appear by chance, so using them as features causes overfitting.</li>
<li>Naive Bayes is very sensitive to overfitting since it considers all the features independently of each other.</li>
<li>It's also quite likely that the final number of features (words) is too high with respect to the number of instances. A low ratio instances/words causes overfitting.</li>
</ul>
<p>The solution is to filter out words which occur less than <span class=""math-container"">$N$</span> times in the data. You should try with several values of <span class=""math-container"">$N$</span>, starting with <span class=""math-container"">$N=2$</span>.</p>
<p>Another issue: in your current process the data is preprocessed before splitting between training and test set, this can cause data leakage. Note that filtering out words of low frequency should be done using the training data only, and then just selecting the same words on the test set (ignoring any other word).</p>
","4","2","64377","14816"
"81439","<p>There's quite a lot of features for the number of instances, so it's indeed likely that there's some overfitting happening.</p>
<p>I'd suggest these options:</p>
<ul>
<li>Forcing the decision trees to be less complex by setting the <code>max_depth</code> parameter to a low value, maybe around 3 or 4. Run the experiment with a range of values (e.g. from 3 to 10) and observe the changes in performance (preferably use a validation set, so that when the best parameter is found you can do the final evaluation on a different test set).</li>
<li>Reducing the number of features: remove rare words (i.e. those which appear less than <span class=""math-container"">$N$</span> times) and/or use some feature selection method.</li>
</ul>
","2","2","64377","14816"
"81446","<p>There is a very important difference in Machine Learning (ML) between <a href=""https://en.wikipedia.org/wiki/Supervised_learning"" rel=""nofollow noreferrer"">supervised methods</a> and <a href=""https://en.wikipedia.org/wiki/Unsupervised_learning"" rel=""nofollow noreferrer"">unsupervised methods</a>:</p>
<ul>
<li>Supervised learning consists in training a model with some labelled data in order to make the final model able to predict the label on some new (unlabelled) data. This means that the task is designed by choosing exactly what what one wants to predict. For example the task of predicting the author of a text is completely different from predicting the topic of a text, even if the text might be the same.
<ul>
<li>Note: A &quot;label&quot; is for categorical data, and the task is called classification in this case. The same principle can be applied to numerical values, in which case this is a regression task.</li>
</ul>
</li>
<li>Unsupervised learning consists in detecting patterns in the data with no additional information than the data itself. This means that there's no specific &quot;label&quot; to predict. Very often unsupervised learning is some form of clustering, i.e. grouping instances by similarity. Data mining methods usually belong to this category. In general there is no need for separate training and testing steps with unsupervised methods. However one might need to tune some parameters or sometimes to evaluate the model with some annotated data, so this would require separating training and testing again.</li>
</ul>
","1","2","64377","14816"
"81481","<p>I'm still not 100% sure about the setting, but based on OP's comments I understand that there is no hyperparameter tuning, so there's a single method being trained in two different ways. So if my understanding is correct:</p>
<ul>
<li>In option 1 the training data is used for CV training/testing, then the model which corresponds to the best CV run is selected and applied to the unseen test set. This would be an unusual way to use CV, since normally CV is used only for evaluation, not for extracting one of the models. Unsurprisingly the performance of the model on the unseen test data is lower than during CV, because the maximum performance during CV is likely due to chance.</li>
<li>Option 2 is just regular CV evaluation for a single model, so I would use this result.</li>
</ul>
<p>However there is an inconsistency between the results obtained: if in option 1 the average CV accuracy is 91.5, there's no logical reason why it's 92.5 in option 2 (there's slightly more data but it's unlikely to improve that much).</p>
","1","2","64377","14816"
"81482","<p>First it's not a matter of duration, it's a matter of number of data points that can be collected: if there are only a few users every day, it's going to take much longer to collect enough data points than if there are millions.</p>
<p>Now how many data points do you need? There's no simple answer since it depends what exactly you are going to test and how big the difference turns out to be between the two cases.</p>
<p>A good workaround is to start with some hypothetical scenarios: imagine for instance 100, 1000 or 10000 users and an outcome with a large, a medium or a small difference. Run the test in all these different scenarios and check when a significant difference is obtained. Based on this, choose a target number of users which would be sufficient to obtain a clear result in most of the scenarios.</p>
<p>Finally the duration can be approximated based on the average number of daily users.</p>
","1","2","64377","14816"
"81560","<p>A very simple example is the use of relative frequency instead of absolute frequency, for instance in text classification: if the features represent the raw absolute frequency of every word, then large documents have higher values than small documents. In this case the learning algorithm won't be able to distinguish between a word used 10 times in a small 50 words document (so this word is important for the document) vs. the same word used 10 times but in a large 1000 words document (not very important for the document).</p>
<p>Using relative frequency instead (i.e. dividing by the size of the document) solves this problem. It's an example of row-wise normalization, there are many others.</p>
","-1","2","64377","14816"
"81588","<p>From an NLP perspective, the main issue is to identify and extract the terms/relations of interest from the text.</p>
<p>It may be easy if the sentences always appear like these simple examples: one can use pattern matching <code>&quot;the &lt;entity&gt; is &lt;property1&gt; [and &lt;property2&gt;]&quot;</code> (no need for ML).</p>
<p>However with general text it's a quite complex problem which would probably require training an ad-hoc model from annotated data. This would generally involve the tasks of <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition</a> and <a href=""https://en.wikipedia.org/wiki/Relationship_extraction"" rel=""nofollow noreferrer"">Relation Extraction</a>.</p>
","0","2","64377","14816"
"81590","<p>(started as a comment but it turned out to be longer than expected)</p>
<p>With a dataset like this a simple barplot could be very insightful: on the X axis the days of the week, on the Y axis the frequency, with two bars (spam/not spam using different color) for each day. A slightly more advanced version: two boxplots, one for weekdays the other for weekends. A boxplot is kind of overkill for only 5 (mon-fri) and 2 (sat-sun) values but it's easy to do and shows the big picture.</p>
<p>In order to test whether any difference (e.g. weekdays vs weekends) is significant I think this is a good case for a chi square test.</p>
","3","2","64377","14816"
"81711","<p>In the first code you split the data <code>X</code> randomly with this line:</p>
<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)
</code></pre>
<p>Then after training the model you correctly apply it to the test set, which is 20% of the instances:</p>
<pre><code>y_pred = xg.predict(X_test)
</code></pre>
<p>Whereas in the second code you apply the model to the full data <code>X</code> instead of only to the test set:</p>
<pre><code>X_ = data.predict(X)
</code></pre>
<p>So in this second code you are testing on the whole dataset which includes the 80% training set. It is of course incorrect to test on the training set, this is why you obtain artificially high performance: the model &quot;is cheating&quot; since it has seen 80% of the instances during training.</p>
<hr />
<p><em>[answer to comment]</em></p>
<p>There are at least two possibilities to split the data properly and use the saved model only on the test data:</p>
<ul>
<li>option 1: split the data before running the training code into two different files, one for the training set and one for the test set. The training code should be provided only the training data as input, it doesn't perform any splitting and just outputs the model. The testing code takes the model and the test data as input and outputs the predictions (or performance).</li>
<li>option 2: apply exactly the same splitting in the testing code as in the training code. This can be done thanks to the <code>random_state</code> parameter in the call to <code>train_test_split</code>: by using the same value for this parameter the function will always separate the data exactly the same way. Then of course use only the test set for predicting and evaluating (ignore the training set part of the data).</li>
</ul>
<p>Option 2 is much easier to implement given your current code (you just have to copy the <code>train_test_split</code> call). Option 1 is more general and makes the ML design cleaner, but it's less convenient.</p>
","2","2","64377","14816"
"81802","<blockquote>
<p>Following discussion with Erwan: one of his previous answer partially has answered my question. However I would like to understand the following. One needs to have a corpus, then label news/tweets in fake/not fake, then run the model. But how the algorithm works on texts and takes relevant words or features for detecting fake news?</p>
</blockquote>
<p>First, let me emphasize that the concept of &quot;fake news&quot; is very vague and subjective. This fact alone is a red flag for any rational data scientist, since if humans don't always agree what is the correct answer it's going to be difficult to know whether a program gives the correct answer or not.</p>
<p>Now let's assume one has some news data annotated as fake or not. We can represent every news item as a vector of features (for example as a simple one-hot encoding vector of words), and for every document provide these features together with the &quot;fake or not&quot; boolean label to a supervised ML algorithm.</p>
<p>The supervised ML algorithm doesn't know and doesn't care about the semantic task, that is it doesn't have any idea what the label represents. It's job is only to find the best way to predict a given label (whatever this represents) from the given features. Typically it does this by measuring statistical links between features and labels, for instance using the fact that a particular word is often associated with a particular label in the training set.</p>
<p>It can do this part very well, but the rest is up to humans: are the features really good indications for the label? are the labels really representative of the task? Is the amount of training data sufficient? Is the training data diverse enough? Is the ML algorithm the right choice for this task? How to evaluate and interpret the results? Etc.</p>
<p>The point is: no matter how complex it is, the algorithm doesn't care about the meaning, it just uses the relations that it finds in the data. If for some reason the word &quot;rhinoceros&quot; always appears with the label &quot;fake&quot; in the training data, then when applying the system any news containing the word &quot;rhinoceros&quot; is likely to be predicted as fake. It's easy to see that this can lead to serious errors, depending on the quality/diversity of the data used for training. It's also easy to see that, with a very &quot;sensitive&quot; task such as fake news detection, a good evaluation result with the same kind of data as the training data is not a proof that the system can actually detect fake news in any context, if only because it's impossible to have a fully representative sample of any kind of fake news past, present and future.</p>
<blockquote>
<p>So my question is: If I have a corpus on Trump, would the algorithm be able to detect fake news on Vitamin C, without any words (verb, adj, noun,..) in common between the two dataset, except stopwords?</p>
</blockquote>
<p>As you can guess from my explanation above, it's not sure at all that the algorithm would easily adapt to a new type of data. That being said, normally a good algorithm on this task would probably focus on the subtle cues rather than the topic words, for instance the terms which are meant to trigger an emotional response with the reader. To some extent, these indications might work across topics. But again there's no guarantee that the system works in general, especially since the people who write and disseminate fake news can easily change their techniques if these become easily detected.</p>
","1","2","64377","14816"
"81840","<p>The mistake is not to have read <a href=""https://scikit-plot.readthedocs.io/en/stable/metrics.html"" rel=""nofollow noreferrer"">the documentation</a> ;)</p>
<blockquote>
<p><code>scikitplot.metrics.plot_lift_curve(y_true, y_probas,...)</code></p>
<p>Parameters:</p>
<ul>
<li><code>y_true</code> (array-like, shape (n_samples)) – Ground truth (correct) target values.</li>
<li><code>y_probas</code> (array-like, shape (n_samples, n_classes)) – Prediction probabilities for each class returned by a classifier.</li>
</ul>
</blockquote>
<p>The second parameter <code>y_probas</code> should be a double-dimension array of the predicted probabilities for every class, not a single dimension array of the predicted class.</p>
<p>You can also check the example provided in the documentation which contains:</p>
<pre><code>y_probas = lr.predict_proba(X_test)
</code></pre>
","2","2","64377","14816"
"81879","<p>Individual feature selection methods assign a numerical value to every feature so that features can be ranked according to this value. The calculated value is chosen to represent how much the feature contributes to knowing the label/response variable: common choices are conditional entropy, but also information gain or correlation.</p>
<p>The actual values assigned to the features are not really useful on their own, what matters is the ordering of the features according to this value. Thus the standard method for selecting the features is not to choose a particular threshold on the value, but simply to choose the number of features one wants to obtain.</p>
<p>Example: in a text classification task, there are 1000 documents and a vocabulary of 20000 unique words as candidate features. Using all the words would certainly cause overfitting, so we decide to use only 100 words as features. We can calculate the conditional entropy of every word with respect to the label, and then select the bottom 100 words according to the corresponding ranking as features (the 19900 other words are ignored).</p>
<p>Since individual feature selection is very efficient, it's often possible (and a good idea) to try a range of values as the number of features, and train/test the model for each of these values. This way one can experimentally determine the optimal number of features (the one which maximizes performance on the data). Note that this a form of hyper-parameter tuning, and therefore one has to use a validation set for the tuning stage, and then the final model (with the selected optimal number of features) should be applied on a fresh test set.</p>
","0","2","64377","14816"
"81911","<p>This is regular text classification, but with very little text (only the job title). You could start with a simple one-hot encoding over the words in the job title, then apply your favourite algorithm (e.g. Naive Bayes, Decision Trees, etc). It will probably work better with some form of normalization of the words (at least using the lemma in order to mach variants of the same word).</p>
<p>Word embeddings are probably a good choice too, but my guess is that this would help only if you have a quite high amount of examples.</p>
","1","2","64377","14816"
"81929","<p><a href=""https://en.wikipedia.org/wiki/Dynamic_time_warping"" rel=""nofollow noreferrer"">Dynamic Time Warping</a> measures the similarity between two sequences of values across time. It combines the idea of comparing a pair of points <span class=""math-container"">$(a\in A,b\in B)$</span>  with the idea of finding the optimal alignment of points <span class=""math-container"">$(a,b)$</span> between the two sequence. The alignment algorithm is similar to the edit distance between strings, and can be used to extract a mapping between the sequences.</p>
","0","2","64377","14816"
"82023","<p>Let's clarify a few things first:</p>
<ul>
<li>The <a href=""https://en.wikipedia.org/wiki/Bootstrap_aggregating"" rel=""nofollow noreferrer"">bagging technique</a> is an ensemble method which is not specific to decision trees, it can be applied to any classification method.</li>
<li>It's worth noting that there is another ensemble method specifically for decision trees, it's called <a href=""https://en.wikipedia.org/wiki/Random_forest"" rel=""nofollow noreferrer"">Random Forest</a>. While it's not the same method, it is known to generally improve performance compared to a regular Decision Tree algorithm like C4.5.</li>
</ul>
<p>These techniques exist because they have been proved to improve performance in general, but whether they would improve performance on a specific problem (and by how much) has to be tested.</p>
<p>Also just to be clear: these techniques are already established, so using them wouldn't be an original research contribution in the field of Machine Learning. Their application to a new problem in a specific research domain might be a contribution to this domain, but that depends on the specific context.</p>
","2","2","64377","14816"
"82141","<p>I'm surprised that there is no other tool, especially since usually when there are word tokenizers available there are also sentence tokenizers (the two tasks are often performed at the same time).</p>
<p>In case you need to train your own tokenizer, you could use the <a href=""https://universaldependencies.org/"" rel=""nofollow noreferrer"">Universal Dependencies</a> Hindi corpus.</p>
","0","2","64377","14816"
"82142","<p>Normally if you have enough training data this shouldn't be an issue, because the model will realize that these named entities are not relevant for classifying the text.</p>
<p>Anyway if you want to remove them you would need to first detect them with <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition</a>, there are many implementations. Then you can simply remove them.</p>
<p>Using DL classification as a first option is questionable imho:</p>
<ul>
<li>it usually requires larger training data</li>
<li>it takes much longer to train than traditional models</li>
<li>it's harder to understand what happens when things go wrong</li>
</ul>
<p>I'm old school so I'd recommend starting with simple methods, for instance TFIDF vectors with Naive Bayes or Decision Trees.</p>
","1","2","64377","14816"
"82149","<p>The simple answer is to experiment.</p>
<p>You did a quite detailed analysis of the relations between your features and response variable and that's definitely a good idea, but don't be afraid to experiment with various models, even those which don't seem perfectly adapted to the task. Why? Because the one thing that such an analysis by individual features doesn't show is the patterns which happen with combinations of features, and that's what most ML algorithms are usually good at finding. This is why one often finds surprisingly good results with methods expected to perform poorly.</p>
<p>So my advice is to start with a range of simple methods known to be quite robust, for instance Random Forest, SVM, logistic (or even linear) regression. A good strategy is to start with simple methods and then try to improve on that based on analyzing the results. For example a more advanced approach would be to use ensemble learning: train different types of models and then a meta-model which relies on their predictions.</p>
","0","2","64377","14816"
"82209","<p>The underlying question is: what is a feature, what does it represent, and how can a ML algorithm use it?</p>
<p>A feature is an indicator, it's supposed to help the algorithm predict the response variable. So the <strong>semantic</strong> of the feature is crucial: for instance it's easy to see that a patient's age can be a relevant feature for detecting a particular disease, whereas knowing their last name isn't.</p>
<p>What can a ML algorithm do with a feature? It just compares it: only equality/difference tests (boolean, exactly what happens with one-hot encoding) for categorical features, order tests for numerical features.</p>
<blockquote>
<p>But can it treat a fixed-size vector as a whole feature to learn?</p>
</blockquote>
<p>Per my last point, the algorithm needs a way to compare the &quot;whole vector&quot;. As far as I know this can only happen:</p>
<ul>
<li>either by comparing the whole vector to any possible value it might take: this is a boolean test, we go back to the one-hot encoding case.</li>
<li>or by comparing every individual cell independently, which is the idea of using the binary representation described by OP:</li>
</ul>
<blockquote>
<p>Or we can use binary to represent the 128-bit string, which only contributes to 128 dimensions. I don't know if the traditional ML models (not CNN) such as random forrests are able to learn the dependence of the 128 features.</p>
</blockquote>
<p>In this case the problem is about the semantic of these individual cells: what do they represent and can they help predict the response variable?</p>
<p>Take for instance a spam binary classifier: traditionally with one-hot encoding every feature represents whether a particular word appears in the text document. This makes sense, because knowing whether the document contains for example the word &quot;viagra&quot; or not gives an indication about whether it's spam or not. Now if you just use the binary representation of the word &quot;viagra&quot;, the feature <span class=""math-container"">$i$</span> represents whether the <span class=""math-container"">$i^{th}$</span> bit is 1 in the ASCII or UTF8 binary representation of the word. Quite clearly this is not a relevant indicator to decide whether the document is spam or not (btw this can be tested with correlation or other measures). So sure one can technically provide the individual bits as features, however the predictions will be near random because the model doesn't have relevant indicators as features.</p>
<p>Important note: I focused on the semantic issue so I didn't even talk about the problem of whether order matters, but this would also be an issue.</p>
","0","2","64377","14816"
"82297","<p>A function which &quot;explains the labelling&quot; has to map any value from the input set <span class=""math-container"">$X$</span> to a value in the output set <span class=""math-container"">$Y$</span>.
Let's try a more simple example with <span class=""math-container"">$X=\{a,b,c\}$</span> and <span class=""math-container"">$Y=\{0,1\}$</span>. The possible functions are:</p>
<ul>
<li><span class=""math-container"">$a \mapsto 0, b \mapsto 0, c \mapsto 0$</span></li>
<li><span class=""math-container"">$a \mapsto 0, b \mapsto 0, c \mapsto 1$</span></li>
<li><span class=""math-container"">$a \mapsto 0, b \mapsto 1, c \mapsto 0$</span></li>
<li><span class=""math-container"">$a \mapsto 0, b \mapsto 1, c \mapsto 1$</span></li>
<li><span class=""math-container"">$a \mapsto 1, b \mapsto 0, c \mapsto 0$</span></li>
<li><span class=""math-container"">$a \mapsto 1, b \mapsto 0, c \mapsto 1$</span></li>
<li><span class=""math-container"">$a \mapsto 1, b \mapsto 1, c \mapsto 0$</span></li>
<li><span class=""math-container"">$a \mapsto 1, b \mapsto 1, c \mapsto 1$</span></li>
</ul>
<p>One may notice that we need to enumerate every possible assignment of the 3 input variables, because each of them is a different function. This can be represented as a tree where each node represents the choice for an input variable to be assigned any of the output variable, so a node has <span class=""math-container"">$|Y|$</span> children and a depth of <span class=""math-container"">$|X|$</span>. The number of leaves in such a tree is <span class=""math-container"">$|Y|^{|X|}$</span>. In my above example this is <span class=""math-container"">$2^3=8$</span>. In the question example this would be <span class=""math-container"">$2^{16}$</span>.</p>
","1","2","64377","14816"
"82306","<p><em>Fair warning, this is just an intuition and I'm not really expert in this kind of question. Nice question anyway :)</em></p>
<p>Theoretical models of learning like PAC are meant to be used to prove learnability results. Therefore what matters is not only that the definition corresponds to the intuition of what &quot;learning&quot; means, but also that it's actually technically possible to prove anything with this definition. I suspect that this is why (or one of the reasons why) the PAC definition is constrained to the particular class of functions that the algorithm deals with, since it makes it possible to theoretically determine what is the true best function in this class. This is needed to prove (or disprove) that the algorithm can always return it.</p>
<p>I also suspect that in the universe of all possible functions, the optimal function could always be some kind of infinite non-generalizing function which assigns exactly the true <span class=""math-container"">$Y$</span> for any <span class=""math-container"">$X$</span> (at least to the extent possible, i.e. when <span class=""math-container"">$X\neq X' \Rightarrow Y\neq Y'$</span>). In this perspective restricting the class of target functions forces the algorithm to <strong>generalize</strong>, since it must represent the data in the finite space of the parameters of the class of functions.</p>
<p>This could explain why the definition proposed by OP is not usable:
if I'm not mistaken, there's simply no way to prove any learnability result with it.</p>
","2","2","64377","14816"
"82309","<p>The idea of first classifying whether to fold, to call or to raise (btw you could consider 3 labels here) is reasonable, then you could have a regression task for the amount to bet/raise if this move is predicted.</p>
<p>Another option is to restrict the range of predicted values: if the linear regression predicts a value lower or equal X, consider it equal to zero. Anyway this is necessary if only to take care of the rules of the game: usually the bet must be at least equal to the big blind, so if the linear regression gives an amount lower than that it has to be rounded up or down.</p>
<p>Side note: this can be a fun ML project, but I wouldn't use this kind of algorithm with any real money game for at least two reasons:</p>
<ol>
<li>afaik bots are illegal with most poker sites</li>
<li>linear regression is very basic, it's very unlikely that the algorithm will play optimally</li>
<li>an experienced opponent will very easily notice the pattern and outplay the algorithm</li>
<li>Given the diversity of the situations, you need a massive amount of poker hands to train the model in order to obtain anything minimally reliable.</li>
</ol>
","0","2","64377","14816"
"82326","<p>In addition to etiennedm's answer:</p>
<p>Informally, this is the equivalent of making a student take a test while giving them all the answers.</p>
<p>The goal of evaluating a model on an unseen test set is to measure the ability of the model to predict on different instances, in the same way that a school test is supposed to check if the student can solve new problems based on the knowledge they acquired. If the student is provided with all the answers, the result of the test will only show that they can read and copy the answer, and this is not very useful. Same thing for a program, it's pointless to prove that it can store and copy answers, and there's no need for Machine Learning to achieve that.</p>
<p>Evaluating on an unseen test set is meant to measure how well the model <strong>generalizes</strong> (i.e. actually &quot;learns something&quot;) from what it has seen in the training data to answer new problems. Using the training instances as test set completely defeats this purpose, since the model doesn't need to generalize anything to answer these instances.</p>
","1","2","64377","14816"
"82327","<blockquote>
<p>Is this a fair assumption?</p>
</blockquote>
<p>No: Named Entity Recognition (NER) is a specific task which consists in detecting <a href=""https://en.wikipedia.org/wiki/Named_entity"" rel=""nofollow noreferrer"">named entities</a>. The more general term for this kind of task in Machine Learning is <a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">sequence labelling</a>, because it's not only about classifying words but annotating a sequence of instances in which order matters (e.g. words).</p>
<p>It's true that NER is certainly the most famous task of this kind, but there are other important ones, for instance <a href=""https://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""nofollow noreferrer"">Part-Of-Speech (POS) tagging</a>.</p>
","1","2","64377","14816"
"82344","<p>In general this is a difficult problem, it's about the problem of <a href=""https://en.wikipedia.org/wiki/Natural-language_understanding"" rel=""nofollow noreferrer"">Natural Language Understanding</a> which is far from being solved.</p>
<p>The advanced option requires a full <a href=""https://en.wikipedia.org/wiki/Parsing"" rel=""nofollow noreferrer"">syntactic parsing</a> of the sentence, ideally followed by some kind of semantic representation of the sentence, for example by <a href=""https://en.wikipedia.org/wiki/Relationship_extraction"" rel=""nofollow noreferrer"">extracting relations</a>. As far as I know this is rarely used because these steps will often cause as many errors as they solve.</p>
<p>Some more reasonable heuristics can be considered, for instance detecting specific negation words and either including this information as feature or modifying the original features accordingly (e.g. when a negation is detected replace the token &quot;happy&quot; with &quot;not(happy)&quot; in the features).</p>
<p>Note that it's unlikely to be perfect anyway, due to the usual obstacles: <a href=""https://en.wikipedia.org/wiki/Hedge_(linguistics)"" rel=""nofollow noreferrer"">hedging</a> (&quot;I would assume that it was quite good&quot;), sarcasm (&quot;Sure, I was very happy with the terrible service&quot;), metaphors (&quot;I was feeling like a fish in water&quot;), etc.</p>
","1","2","64377","14816"
"82487","<p>There's a good chance that it's a sign of overfitting: the fact that the importance of the features is not stable can be considered as an indication that the model itself is not stable, and this typically happens when it doesn't have enough information in the data to be sure how to use the features. As a result minor variations in the features or data cause the model to change a lot, because it uses features which occur by chance. A way to investigate is to reduce the number of features: if the model becomes more stable this way, it confirms overfitting (and the performance on the test set should be the same or better).</p>
<p>[edited] There is also a possibility that the new features are really useful for the model, causing it to use the whole set of features in a very different way since it can take advantage of new combinations of features. For instance, suppose we have a model which predicts a disease based on features which represent the patient's symptoms, then we add features which represent the patient's age and gender. Let's assume that having a particular symptom at a particular age is a strong indicator of the disease, so this would mean that this symptom feature was not very useful on its own, but becomes more important with the additional feature.
In such a case I would probably expect to observe a significant improvement in performance on the test set when the new features are added.</p>
","1","2","64377","14816"
"82488","<blockquote>
<p>I want to have a model that, for example, predicts Joseph but does not consider it wrong because it is one of the two possible answers.</p>
</blockquote>
<p>It seems that you are confusing prediction and evaluation: whether an answer is correct or not is a matter of evaluation, and the evaluation method doesn't have to follow the same rules as the predicting system.</p>
<p>For example the system can be designed to always predict a single name, and the evaluation measure can be based on whether the predicted name belongs to a set of correct answers or not. This would technically be simple multiclass classification, since there's a single class predicted. However you would have to implement your own evaluation method, because the standard one will consider correct only if the prediction is exactly the true class.</p>
<p>You could also decide that the system can predict any combination of names, that would be multi-label classification. But here again you could decide any specific evaluation method, for instance count a prediction as correct as long as it contains at least one of the true answers, or contains only names which belong to the true answer, etc.</p>
<p>Note that in theory any multi-label problem can be transformed into a multiclass problem simply by considering any possible set of classes as a single class (e.g. &quot;Mike+Michael&quot; as one class). In general it's a matter of number of values, since if every possible combination can happen in the data there would be <span class=""math-container"">$2^N$</span> classes as a result. I don't think this is relevant in your case, since this would defeat the idea of a flexible evaluation method.</p>
","0","2","64377","14816"
"82511","<p>I think there are (at least) two parts to take into account in evaluating such a model:</p>
<ul>
<li>Whether the generated text correctly relate to the input topic</li>
<li>Whether the generated text is grammatically and semantically acceptable</li>
</ul>
<p>In my opinion the first kind of evaluation could reasonably be done with an automatic method such as the one you propose. Note that cosine scores should not be interpreted absolutely: you should probably compute cosine similarity with a random sample of topics, and normally one expects the similarity to be much higher with the input topic than any other. You could also think of other variants, for instance training topic models on the generated text together with a sample of documents from various known topics, then check that the generated text belongs to the target topic (i.e. it should be grouped with the documents known to belong to this topic).</p>
<p>For the second kind of evaluation, it would be difficult and unreliable to use an automatic method. As far as I know the only reliable way would be to ask human annotators to assess whether the text is grammatically correct and whether its content makes sense. If you're going to do that you might as well ask them to annotate to what extent the text is related to the topic.</p>
<hr />
<p><em>[added following comment]</em></p>
<p>if you check whether the generated text is similar to the topic only by computing similarity with this target topic, what you obtain is for instance an average cosine score. Then you would probably select a threshold: for instance if the similarity is higher than 0.5 then consider that the text is indeed related to the topic. But there are two problems with this option:</p>
<ul>
<li>In some cases the average similarity will be lower than the threshold even though the text is correctly related to the topic. This could happen for example with a very &quot;broad&quot; topic which covers a large vocabulary.</li>
<li>On the contrary you might have cases where the average similarity is higher than the threshold, but actually comparing to another topic would give an even higher similarity value.</li>
</ul>
<p>These issues are due to interpreting the similarity score &quot;absolutely&quot;, as opposed to interpreting it relatively to other similarity scores. Instead you can calculate the similarity not only against the target topic but also against other topics, and then just check that the target topic is the most similar topic (or at least one of the top similar). This way
:</p>
<ul>
<li>The target similarity score may be low, as long as it's higher than the other topics</li>
<li>you can detect the case where another topic happens to have higher similarity than the target topic</li>
</ul>
","1","2","64377","14816"
"82525","<p>A confusion matrix is indeed a very useful way to analyze the results of your experiment. It provides the exact number (or percentage) of instances with true class X predicted as class Y for all the possible classes. As such it gives a detailed picture of what the system classifies correctly or not.</p>
<p>But a confusion matrix is a bit <em>too detailed</em> if one wants to summarize the performance of the classifier as just one single value. This is useful especially when one wants to compare two different classifiers, since there's no general way to compare two confusion matrices. That's why people often use <em>evaluation measures</em>: for binary classification, the most common ones are:</p>
<ul>
<li>Accuracy, which is simply the number of correct predictions divided by the total number of instances.</li>
<li>F-score, which itself is the harmonic mean of precision and recall. Quite ironically, F-score gives a more accurate picture of performance than accuracy because it takes into account the different types of possible errors.</li>
</ul>
","4","2","64377","14816"
"82634","<p>I agree with your assumption, the vector space is the same so I don't see any major problem with this approach.
Still this approach might cause some more subtle bias, depending on the differences between the models (sets of terms, number of clusters). I could imagine the following problems happening:</p>
<ul>
<li>if there is a big difference in number of clusters between models, a model which has more clusters is more likely to contain the closest match, simply because it has more centroids. This might favour the most precise clusters (this might actually be a good thing, depends).</li>
<li>if there are many models sometimes there might be many close centroids across the models, and this would probably make the selection of the closest among them almost random: the exact position of a centroid is significant with respect to other centroids within the same model, not necessarily with respect to other centroids outside the model.</li>
</ul>
","1","2","64377","14816"
"82699","<blockquote>
<p>Input - A Target Goal (e.g. Go To Cinema Hall)</p>
<p>Output - A sequence of steps (e.g. Go To Bus Stop -&gt; Onboard A Bus -&gt; Drop at Location X -&gt; Walk 100 meters.</p>
</blockquote>
<p>I assume that there's no other information available to the model, such as geographical information about the possible locations (i.e. the training data is made only of examples objective + sequence). As far as I can tell this implies that there's no way to use similarity between two objectives, since there's no way to know if similar places are in the same area.</p>
<p>Under this definition, the objective is a single categorical variable. So there's just nothing to learn except mapping an objective with its corresponding sequence, no need for ML at all: just store the list which maps any objective in the training data to its corresponding sequence in a map (dictionary).</p>
","0","2","64377","14816"
"82741","<blockquote>
<p>if we don't know <span class=""math-container"">$F(x)$</span> for unseen data, how does a decision tree minimize this error?</p>
</blockquote>
<p>Every supervised ML method relies on the assumption that the test data (any unseen data) follows the same distribution as the training data (note that this is not specific to Decision Trees). In fact both the training data and the test data are assumed to be sampled from the true population data. As a consequence <span class=""math-container"">$F(x)$</span> is assumed to be the same for the training data and the test (unseen) data.</p>
<p>If one uses a trained model on some unseen data which is not distributed like the training data, the results are simply unpredictable and the performance is likely to drop.</p>
<blockquote>
<p>Why do we estimate the error for unseen data using the error observed in the training data?</p>
</blockquote>
<p>You seem to suggest to use the &quot;unseen data&quot; in the training process. You would indeed get better results on the &quot;unseen data&quot; if you optimized on it, but then you would lose the point of having a portion of data set apart. &quot;Unseed data&quot; is necessary to estimate how good your model will perform on data never seen before. If you don't keep some data set apart you may have a better model but you have no way of estimating how good it will be when put into production.</p>
","6","2","64377","14816"
"82751","<p>What you are proposing is a <a href=""https://en.wikipedia.org/wiki/Heuristic_(computer_science)"" rel=""nofollow noreferrer"">heuristic</a> method, because you define the rules manually in advance. From a Machine Learning (ML) point of view the &quot;training&quot; is the part where you observe some data and decide which rules to apply, and the &quot;testing&quot; is when you run a program which applies these rules to obtain a predicted label. As you correctly understood, the testing part should be applied to a test set made of unseen instances. The instances in the test set should also be manually labelled (preferably before performing the testing in order to avoid any bias), so that you can evaluate your method (i.e. calculate the performance).</p>
<p>Technically you're not using any ML approach here, since there is no part where you automatically train a model. However heuristics can be useful, in particular they are sometimes used as a baseline to compare ML models against.</p>
<hr />
<p>[addition following comment]</p>
<blockquote>
<p>I think most of common pre-processing approach requires to convert text into lower case, but a word, taken in different contest, can have a different weight.</p>
</blockquote>
<p>This is true for a lot of tasks in NLP (Natural Language Processing) but not all of them. For example for tasks related to capturing an author's writing style (stylometry) one wouldn't usually preprocess text this way. The choice of the representation of the text as features depends on the task so the choice is part of the design, there's no universal method.</p>
<blockquote>
<p>how to train a model which can 'learn' to consider important upper case words and punctuation?</p>
</blockquote>
<p>In traditional ML (i.e. statistical ML, as opposed to Deep Learning), this question is related to feature engineering, i.e. finding the best way to represent an instance (with features) in relation with the task: if you think it makes sense for your task to have specific features to represent these things, you just add them: for instance you can add a boolean feature which is true if the instance contains at least one uppercase word, a numeric feature which represents the number of punctuation signs in the instance, etc.</p>
<p>Recent ML packages propose standard ways to represent text instances as features and it's often very convenient, but it's important to keep in mind that it's not the only way. Additionally nowadays Deep Learning methods offer ways to bypass feature engineering so there's a bit of a tendency to forget about it, but imho it's an important part of the design, if only to understand how the model works.</p>
","2","2","64377","14816"
"82791","<p>Your calculation is correct, but you forgot to ask yourself one question: why should we consider &quot;red&quot; as the positive class? Precision and recall can be calculated for every class (i.e. considering the current class as positive), as opposed to accuracy.</p>
<p>So if we take &quot;blue&quot; as positive we get:</p>
<ul>
<li>precision = NaN (because there's 0 predicted positive)</li>
<li>recall = 0 / 2 = 0</li>
</ul>
<p>Usually after that one would calculate micro- and/or macro-precision/recall:</p>
<ul>
<li>macro-precision = NaN because the precision is NaN for blue</li>
<li>micro-precision = totalTP / totalTP+totalFP = 8/10</li>
<li>macro-recall = ( 1 + 0 ) / 2 = 0.5</li>
<li>micro-recall = totalTP / totalTP+totalFN = 8/10</li>
</ul>
<p>By definition, micro-performance favours the majority class so it's still high. However macro-performance would clearly show the problem that accuracy cannot show.</p>
","1","2","64377","14816"
"82798","<p>In the question you mention that you need <span class=""math-container"">$n *p$</span> points distributed according to the input distribution, I'm going to simplify by just defining <span class=""math-container"">$N=n*p$</span> as the number of points to sample.</p>
<p>I assume that you have the input distribution in a way so that you could plot a histogram with any number of bins. This means that for any interval <span class=""math-container"">$[a,b]$</span> you can obtain the probability of a point to belong to this interval.</p>
<ol>
<li>Define a bin width parameter, for instance <span class=""math-container"">$\epsilon=0.001$</span>. Calculate the number of bins <span class=""math-container"">$n_b$</span>: divide the length of the range of values (here around 2 according to your graph) by <span class=""math-container"">$\epsilon$</span>. In your case bin <span class=""math-container"">$B_i$</span> represents the interval <span class=""math-container"">$[i*\epsilon,(i+1)*\epsilon]$</span> (with <span class=""math-container"">$0\leq i&lt; n_b$</span>)</li>
<li>Obtain the probability <span class=""math-container"">$p_i$</span> for every bin <span class=""math-container"">$B_i$</span> according to the input distribution, then simply calculate the number of points in this bin: <span class=""math-container"">$x_i=N * p_i $</span>. You can pick the mean of the interval <span class=""math-container"">$B_i$</span> as sampled value.</li>
</ol>
","0","2","64377","14816"
"82839","<p>Generally lower recall means that the system is too strict, i.e. it predicts an instance as positive only when it has clear indications in the features that it's indeed a positive. As a consequence, it misses the true positive instances for which the indications are not so clear.</p>
<p>But when looking at macro-recall it's more complex: it depends primarily on how many classes there are and their distribution. In case of imbalance a very common problem is that the system predicts the majority class too often, causing a low recall for the minority class(es) and therefore a lower macro-recall. The only way to check for this is to look at the performance of individual classes.</p>
","3","2","64377","14816"
"82841","<p>Disclaimer: I can only answer for the NLP part since I'm no expert for image processing.</p>
<p>I assume that text-based image retrieval is the task of finding the image (or the part of an image) which corresponds to a short text which exclusively describes the object. Practically it means that any content word (i.e. excluding grammatical words like determiners) in the text refers directly to the object: &quot;a bike&quot;, &quot;a black cat&quot;, &quot;the red car&quot;, etc. For a ML process it means that there's nothing to analyze in the text, every word can directly be associated with a characteristic of the image.</p>
<p>By contrast Natural Language object retrieval involves analyzing the text. For instance &quot;the cat on the left of the picture&quot; is different than &quot;the picture on the left of the cat&quot;, even though the words are the same. Additionally there can be different ways to refer to the same object: &quot;the book at the left of the shelf&quot; may be the same as &quot;the leftmost book&quot; or &quot;the book next to the green book&quot;. There are usually many ways to express the same meaning with language, and that makes the task much more complex. Additionally I would assume that mapping positional descriptions to the image characteristics can be tricky: &quot;the man behind the tree&quot; or &quot;the second bridge&quot; in a 2D image requires the model to &quot;understand&quot; depth. In a picture with two dogs, &quot;the small dog&quot; requires the model to &quot;understand&quot; size relation between objects. Humans intuitively know how to interpret these sentences, but for a machine <a href=""https://en.wikipedia.org/wiki/Natural-language_understanding"" rel=""nofollow noreferrer"">Natural Language Understanding</a> hasn't been solved yet (it might never be).</p>
","0","2","64377","14816"
"83964","<p>&quot;fuzzy match&quot; is a vague term for many different kind of <a href=""https://en.wikipedia.org/wiki/String_metric"" rel=""nofollow noreferrer"">string similarity measures</a>. There are two main categories:</p>
<ul>
<li>Sequence-based methods, such as <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein edit distance</a> (and variants), <a href=""https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance"" rel=""nofollow noreferrer"">Jaro-Winkler</a></li>
<li>Bag-of-something methods, such as <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">Cosine-TFIDF</a>, <a href=""https://en.wikipedia.org/wiki/Overlap_coefficient"" rel=""nofollow noreferrer"">Overlap</a>, <a href=""https://en.wikipedia.org/wiki/Jaccard_index"" rel=""nofollow noreferrer"">Jaccard</a>, ...</li>
</ul>
<p>Traditionally the first category is used at the level of characters (e.g. to match &quot;abcdefg&quot; with &quot;abdfeg&quot;) and the second at the level of words (e.g. to match &quot;the management of the team&quot; with &quot;the team management&quot;), however there is no technical restrictions to keep it this way. The main difference between the two categories is whether or not they take the order of the units (e.g. words or characters) into account. In fact there are also some hybrid measures such as SoftTFIDF which combine both approaches.</p>
<p>Conclusion:</p>
<ul>
<li>For the problem of word order, a simple bag-of-words measure such as cosine or Jaccard would perfectly match two profiles.</li>
<li>If more flexibility is needed, Soft-TFIDF or other variants would work.</li>
</ul>
<p>The last part of the question is about selecting a standard string. In general there's no way to know what should be the standard string from the different variants themselves. A simple approach would be to pick the most common variant.</p>
","0","2","64377","14816"
"83990","<p>Don't be scared :)</p>
<p>It's normal not to understand everything at first, Machine Learning (ML) is a vast domain and nobody knows everything about it.</p>
<p>My advice: pace yourself, don't try to learn everything at once. start from the most simple concepts and methods and understand them really well before going to more advanced topics. There are plenty of resources available for starting with ML, including <a href=""https://datascience.stackexchange.com/q/26449/64377"">maths courses for people with a Computer Science background</a>. Generally it's recommended to have a decent level in linear algebra, statistics and probabilities.</p>
<p>But also don't underestimate the general concepts which are specific to ML, it's important to have a really good understanding of concepts such as supervised vs. unsupervised, classification vs. regression, training/testing/evaluation, etc.</p>
<p>Finally keep in mind that ML (and data science in general) is a highly experimental domain, so hands-on experience is key: reproduce a tutorial or code for a problem that you like, and play with it: see what happens if you change the parameters, the data, the evaluation measure...</p>
","1","2","64377","14816"
"84023","<p>A few ideas:</p>
<ul>
<li>As mentioned by @weareglenn in general there is no way to know if the performance obtained on some data is good or bad, unless we know the performance of other systems which have been applied to the same task and dataset. So yes, your results are &quot;acceptable&quot; (at least it does the minimum job of beating the random baseline). However given that that your approach is quite basic (no offense!), its reasonably likely that the performance could be improved. but that's just an educated guess, and there's no way to know by how much it can be improved.</li>
<li>To me the level of imbalance is not that bad. Given the low recall on the minority class (fake news) you could try to oversample it if you want to increase recall, but be aware that this is likely to decrease precision (i.e. increase False Positive errors = class 0 predicted as 1). In my opinion you don't have to, unless for your task you must minimize False Negative errors.</li>
<li>You could try a lot of things with the features, and I'm quite confident that there is room for improvement at this level:
<ul>
<li>First as mentioned by @weareglenn you should try without removing punctuation, maybe even without removing stop words.</li>
<li>Then you could play with the frequency: very often excluding the words with a low frequency in the global training vocabulary allows the model to generalize better (i.e. it avoids overfitting). Try with different minimum frequency threshold: 2,3,4,... (depends how large is your data).</li>
<li>More advanced: use feature selection, preferably with a method such as genetic learning, but it might take time because it will redo the training+test process many times. Individual feature selection (e.g. with information gain or conditional entropy) might work, but it's rarely very good.</li>
<li>If you want to go very advanced, you could even borrow methods from automatic stylometry, i.e. methods used to identify the style of a document/author (the <a href=""https://pan.webis.de/"" rel=""nofollow noreferrer"">PAN shared tasks</a> is a good source of data/systems). Some use quite complex methods and features which could be relevant for identifying fake news. A simple thing I like to try is to use characters n-grams as features, it's sometimes surprisingly effective. You could also imagine using more advanced linguistic features: lemmas, Part-Of-Speech (POS) tags.</li>
</ul>
</li>
<li>You didn't mention Decision Trees in your methods, I would definitely give it a try (random forests for the ensemble method version).</li>
</ul>
","1","2","64377","14816"
"84024","<p>What you're proposing is some very simple form of ensemble learning.
You need to have at least a sample of labelled data in order to evaluate any method. Using this labelled data you can:</p>
<ul>
<li>evaluate each of the three methods on their own</li>
<li>evaluate your idea of averaging the 3 methods predictions</li>
<li>if you have enough labelled data, you could even train a model which combines their predictions optimally (this would be full-fledged <a href=""https://en.wikipedia.org/wiki/Ensemble_learning#Stacking"" rel=""nofollow noreferrer"">stacked generalization</a>)</li>
</ul>
","1","2","64377","14816"
"84071","<p>Usually the point of a rolling window is to calculate and use <a href=""https://en.wikipedia.org/wiki/Moving_average"" rel=""nofollow noreferrer"">a moving average</a>:</p>
<blockquote>
<p>A moving average is commonly used with time series data to smooth out short-term fluctuations and highlight longer-term trends or cycles.</p>
</blockquote>
<p>By definition the only meaningful aggregation for this kind of purpose is the mean: the idea is to represent the same type of value as the original series while correcting noisy variations.</p>
<p>Anything else is not the standard use case for a rolling window, so the answer to questions 2 and 3 would completely depend on the application.</p>
","1","2","64377","14816"
"84101","<p>I agree that the main potential issue is bias due to a particular group of instances being over-represented in the missing values. For example it's possible that the type of company is missing in cases where it's unknown or ambiguous, and this might correspond to a particular company profile (e.g. smaller, more recent, ...). To me the measures you propose to ensure that the remaining data is representative look good (of course this depends on the specifics of the data).</p>
<p>Another important point to think about is what the model is going to be used for: if it's going to be used to predict something from unseen instances, chances are that some of these future instances will also have missing values. In this case the model might be less useful if it cannot deal with them.</p>
","1","2","64377","14816"
"84131","<p>The Jaccard coefficient (or Jaccard similarity) is defined on two <strong>sets</strong> <span class=""math-container"">$A$</span> and <span class=""math-container"">$B$</span>:</p>
<p><span class=""math-container"">$$
J(A,B) = {{|A \cap B|}\over{|A \cup B|}} = {{|A \cap B|}\over{|A| + |B| - |A \cap B|}}
$$</span></p>
<p>There is a single definition for this coefficient, but note that Jaccard is a general similarity measure, it is not specifically designed as an evaluation measure. So assuming one wants to use it as an evaluation measure, they need to decide (design) how. More precisely confusion may arise from the following questions:</p>
<ul>
<li>Which pair of sets does one want to compare?</li>
<li>Which data structure does one use to represent a set?</li>
</ul>
<p>Let's start with the second question: a binary vector is a common and convenient way to represent (encode) a set. Given a set of possible elements (universe) <span class=""math-container"">$U=\{x_1, ...,x_n\}$</span>, any subset <span class=""math-container"">$s\subseteq U$</span> can be represented as a binary vector of length <span class=""math-container"">$n$</span> where the <span class=""math-container"">$i^{th}$</span> boolean is 1 if <span class=""math-container"">$x_i\in s$</span>, 0 otherwise. It follows from this representation that the Jaccard coefficient is the number of indexes where both vectors contain 1 divided by the the number of indexes where at least one of the two vectors contain 1.</p>
<p>Example from <a href=""https://scikit-learn.org/stable/modules/model_evaluation.html#jaccard-similarity-score"" rel=""nofollow noreferrer"">scikit documentation</a>:</p>
<pre><code>&gt;&gt;&gt; y_true = np.array([[0, 1, 1],
...                    [1, 1, 0]])
&gt;&gt;&gt; y_pred = np.array([[1, 1, 1],
...                    [1, 0, 0]])
&gt;&gt;&gt; jaccard_score(y_true[0], y_pred[0])
0.6666...
</code></pre>
<p>The vectors compared are <code>[0, 1, 1]</code> and <code>[1, 1, 1]</code> (meaning for instance sets <span class=""math-container"">$\{b,c\}$</span> and <span class=""math-container"">$\{a,b,c\}$</span>), so the result is 2/3.</p>
<p><a href=""https://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow noreferrer"">Multi-label classification</a> is when an instance can be assigned any number of classes. In the linked scikit example they apply the Jaccard coefficient to the two sets of classes for every instance (optionally aggregating the results across instances). This is an example of a design choice as mentioned in my first question above. <em>Note: in the case of the multiclass problem, I don't even understand how they obtain this result for the third value so I can't comment about it.</em></p>
<p>The formula <span class=""math-container"">$TP / (TP + FP + FN)$</span> is another example of what does one choose as sets. This formula only makes sense for a binary classification task: TP is the intersection of true and predicted positive instances, <span class=""math-container"">$TP+FP+FN$</span> is the union of all positive cases (whether true or predicted). This is why it doesn't work with integer vectors, since these cannot represent the result of binary classification.</p>
<p>Conclusion: one must define which sets they want to compare. The most simple way with integer vectors is to consider them as sets of integers, in this case they could be represented as binary vectors as follows:</p>
<pre><code>[0, 1, 2, 5, 6, 8, 9] -&gt; [1,1,1,0,0,1,1,0,1,1]
[0, 2, 3, 4, 5, 7, 9] -&gt; [1,0,1,1,1,1,0,1,0,1]
</code></pre>
<p>But going through the binary representation is not a must, it's only needed if one wants to use a function which takes this format as input. One could as well define their own function using a set as data structure for instance.</p>
","1","2","64377","14816"
"84344","<p>The limited size is not especially an issue. However the normal way to do that is to use a supervised classification method (for instance decision trees, but there are many options), and this means training a model from a large enough set of annotated instances.</p>
<p>If obtaining a training set is not possible, you could try some kind of similarity-based approach by comparing every instance against a set of words representing the topic. This is unlikely to work as well as a supervised method trained specifically with this kind of data. Also you would have to evaluate the validity of the predicted topics on an annotated test set anyway.</p>
","1","2","64377","14816"
"84345","<p>I agree with German C M, there is some structure in your data even though it's not fully structured. So the first task is to transform the data into features that can be exploited by ML. This is typical feature engineering: the idea is to try to organize the different types of elements in the data in a way susceptible to provide useful indications to the algorithm. Many learning algorithms can deal with missing values, so the absence of a particular type of information is not necessarily an issue.  Of course it's difficult to give precise advice since this stage requires expert knowledge.</p>
<p>Note that technically there are methods which take as input such variable length sequences, but it's highly unlikely that it would work well if the algorithm has to guess everything by itself.</p>
","0","2","64377","14816"
"84389","<p>The <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor"" rel=""nofollow noreferrer"">documentation</a> says:</p>
<blockquote>
<p><strong>random_state : int, RandomState instance, default=None</strong></p>
<p>Controls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to &quot;best&quot;. When max_features &lt; n_features, the algorithm will select max_features at random at each split before finding the best split among them. But the best found split may vary across different runs, even if max_features=n_features. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer.</p>
</blockquote>
<p>So:</p>
<blockquote>
<p>it takes different variables to build tree each time?</p>
</blockquote>
<p>The randomness comes from picking different variables when building the tree, indeed.</p>
<blockquote>
<p>Shouldn't there be only few top variables which should be used to split and should throw me similar trees everytime?</p>
</blockquote>
<p>Not necessarily, this depends on the parameters and the data:</p>
<ul>
<li>If an int is given as value for the parameter <code>random_state</code>, a particular random state is used every time so the tree is fixed, even though it's not necessarily the only one possible. Let's assume that the default value <code>None</code> is provided.</li>
<li>If the parameter <code>max_features</code> is anything other than <code>None</code> or <code>auto</code>, then a random subset of the features is selected before every split. Of course, this means that a particular variable might not be selected, so this might cause differences at every run. Let's assume that the default value <code>None</code> is provided.</li>
<li>If all the features can be used at every run, there's no randomness anymore in the selection <em>before the split</em>. But even if the default value <code>best</code> is provided for <code>splitter</code>, it would be wrong to assume that there is a single best variable
at each node: it's possible that two (or more) different variables are tied, i.e. they would provide an equal improvement in the criterion chosen (by default <code>mse</code>).</li>
</ul>
<p>Conclusion: assuming you used the default <code>None</code> for <code>max_features</code>, the fact that you obtain different trees at every run means that there are frequently ties between variables with your data. If there are only minor differences in the nodes at the bottom of the tree then it's not significant, but if there are many changes including close to the root of the tree then it means that the model is unstable. The latter case is usually a bad sign, possibly none of the features is really helpful to predict the value and the model overfits.</p>
<blockquote>
<p>Also, if I use integer for random_state and run the decision tree, it gives me a different tree for each random_state setting. Which tree should be selected in case of so many trees?</p>
</blockquote>
<p>The possibility to select a random state is not intended to select a particular tree, it's intended to make the experiment reproducible. So in my opinion you shouldn't select one of the trees like this. Instead of selecting an arbitrary tree, you could try to investigate why the model is unstable and fix it if possible. The first step is to actually evaluate the model(s) on some test data: if there is a problem such as overfitting, it will be visible in the performance.</p>
","1","2","64377","14816"
"84417","<p>I didn't watch the linked video but based on your explanations: yes, your understanding is correct.</p>
<p>A common confusion is to assume that cross-validation is similar to a regular training stage and therefore produces a model. This assumption is wrong: CV includes repeated training/testing for <em>the purpose of evaluating the method/parameters</em>. From this understanding it follows that:</p>
<blockquote>
<p>for doing model training using k fold CV, we re-train on the entire dataset after the end of the CV loop and that is the final model.</p>
</blockquote>
<p>Yes, since we want to obtain the final model as accurate as possible so we should use all the data. In this case the CV has been used to calculate a good estimate of the performance.</p>
<blockquote>
<p>We do not select any model from inside the CV loop if the idea of doing CV training is to check the accuracy of the ML algorithm on the entire dataset.</p>
</blockquote>
<p>Correct, otherwise there's no point using CV.</p>
<blockquote>
<p>However, if we have multiple ML algorithms say random forest, neural network, SVM inside the CV loop then we select the algorithm with the highest accuracy.</p>
</blockquote>
<p>Any case where multiple methods and/or parameters are being evaluated is a bit more complex than the regular case of a single method: evaluating multiple systems is by itself an additional layer of training, in the sense that we select some parameters (typically the best model) based on the data. This means that the selection itself is based on the whole data used in the CV stage, so the CV performance of the best model is akin to a performance obtained on a training set. This is why one needs another test set (or nested CV) in order to obtain the final performance of the model. An intuitive way to understand this is to imagine evaluating say millions of models with CV: the only way to know if the best performance is due to chance or not is to evaluate the corresponding model on some fresh test set.</p>
<p>Note: the case of combining the outputs of all the models is a different story, since this boils down to a single meta-model.</p>
","1","2","64377","14816"
"84433","<blockquote>
<p>Can I conclude that the error at my node is 60 +-13 i.e my values in this particular sample split ranges from 60-13 to 60+13.</p>
</blockquote>
<p>No you cannot, because the actual error values depend on the data. For example you might have 1 instance with error 41.11 and 9 instances with error 0:</p>
<p><span class=""math-container"">$$MSE=\frac{41.11^2+0^2+...+0^2}{10}=169$$</span></p>
<p>This example shows that the only guarantee you have for an individual absolute error is that it's lower or equal than <span class=""math-container"">$\sqrt{MSE \times n}$</span>.</p>
<p>The MAE (mean absolute error) is easier to interpret: it's literally the mean of the absolute value of the error.</p>
","1","2","64377","14816"
"84434","<p>It's impossible to answer this question in general, because the answer strongly depends on the content of the data. More precisely it depends if the relations between the features and the target class in the training data are sufficient for the parameters of the model to be estimated as accurately as possible. In the most simple cases a few instances might be enough, for example a linear regression model with one single feature would need only two &quot;perfect&quot; instances. Practically the following factors have a strong impact on the number of instances needed:</p>
<ul>
<li>the number of parameters to estimate and the complexity of the model: a more fine-grained model needs more detailed information, i.e. more instances</li>
<li>the number of classes, because there are more possible combinations for the model to learn and because it usually implies a higher number of parameters as well (more complex model)</li>
<li>the number of features, for the same reason</li>
<li>the amount of noise in the data, because finding the general patterns is more difficult if there are lots of inconsistencies/errors in the data, so statistically more instances are needed to distinguish the effect of chance from the real patterns.</li>
</ul>
<p>So the only way to check how much data is needed for a particular task and dataset is to do an <a href=""https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence)"" rel=""nofollow noreferrer"">ablation study</a>, i.e. a series of experiments in which a model is trained every time with a different number of instances. For example if one has 1000 instances, they could try to train a model with 100, 200, 300,...,1000 instances. Then by plotting the performance of every model one can observe whether the curve becomes stable at some point: if yes, this point shows how many instances are needed. If not (i.e. the performance keeps increasing significantly), then more than 1000 instances are needed.</p>
<blockquote>
<p>I think the size may affect the accuracy/precision of a classifier, but I am not sure 100% of that.</p>
</blockquote>
<p>Definitely.</p>
","2","2","64377","14816"
"84449","<p>I'm quite pessimistic about this: ideally one would run the actual task (or a proxy for the task, like a simplified version of the task) with the 100 CPUs. Here the only option I can think of is to:</p>
<ol>
<li>use the noisy measurements of the 10-20 CPUs as the basis of the data for the actual task</li>
<li>try to map these measurements to the 8 benchmarks, so that we can predict the performance of the 100 CPUs based on their results with the 8 benchmarks.</li>
</ol>
<p>I'm pessimistic because:</p>
<ul>
<li>the measurements on the tasks are noisy,</li>
<li>there's no guarantee that these measurements can be reliably linked with the 8 benchmarks,</li>
<li>and no guarantee either that the 8 benchmarks would provide a good estimate on the task.</li>
</ul>
<p>That being said, technically you could imagine a design with supervised learning, the idea being to predict performance on the task (target variable) from the results of the 8 benchmarks (features). The training data would consist of the results of the 10-20 CPUs for which you have both results (8 benchmarks + performance on the task). I would suggest trying this with decision tree regression because it provides you with an interpretable model (white box solution).</p>
","0","2","64377","14816"
"84483","<p>First be careful, looking only at accuracy in a multiclass problem can be misleading: with almost 75% of the data in the majority class, a dummy model which always predict the majority class achieves almost 75%. Measuring performance with micro or macro F1-score would be more informative.</p>
<p>Now about designing your experiments: currently you seem to be trying various methods at random, including sampling techniques and classification algorithms. Why not, but in this way you rely entirely on luck to improve performance. In particular what strikes me is that you don't mention anything about the task or the features (btw it's probably the reasons why some people downvoted the question). The type and nature of the features, their number and their relation to the class can be important to understand why certain methods work and others don't. There might be some feature engineering to do. In particular using feature selection methods sometimes brings great improvement. It would also be useful to get an idea of the performance obtained with simple methods (like decision trees, SVM, logistic regression). Finally you could investigate in more detail which kind of cases which get misclassified and/or study how stable the model is with respect to varying the number of instances or features.</p>
","1","2","64377","14816"
"84537","<p>Apparently <a href=""https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf"" rel=""nofollow noreferrer"">the paper introducing the dataset</a> mentions a list of &quot;online appendices&quot; (table 1 p 363) which seems to contain details about the categories. However I wasn't able to find these appendices in the <a href=""https://www.jmlr.org/papers/volume5/lewis04a/"" rel=""nofollow noreferrer"">additional material</a>.</p>
<hr />
<p>[edit] Since it appears that there is no existing source for the original list of topics, I think your best bet is to rely on an acronym dictionary. Based on <a href=""https://www.acronymfinder.com"" rel=""nofollow noreferrer"">Acronym Finder</a>, given that the data is mostly about economic news, I think the most likely meaning for the three topics you mention are:</p>
<ul>
<li>DLR: dollar</li>
<li>GDP: Gross Domestic Product</li>
<li>BOP: Balance Of Payments</li>
</ul>
<p>You could check whether these are correct by looking at a few documents labelled with these topics in the dataset.</p>
","1","2","64377","14816"
"84555","<p>My approach would be quite similar to yours with a baseline, maybe just a bit more general:</p>
<p>Assuming it's practical (i.e. the training process is not too costly), you could train/test multiple models with various number of features, e.g. 10%, 20%...,100% of the features. For every subset you use an appropriate feature selection method, preferably something like genetic algorithm but it might be too costly. If it's not possible a simple individual feature selection method, e.g. using information gain, but it's likely not to be optimal. Once all the models have been trained, tested and evaluated, a plot of the performance as a function of the number of features should (hopefully) show a curve which increases less and less but never reaches the plateau. If it does reach the plateau before your chosen number of features, it means that you could actually cut down the number features.</p>
","1","2","64377","14816"
"84572","<p>The problem described is not a more general version of Named Entity Recognition, it is a different problem called <a href=""https://en.wikipedia.org/wiki/Parsing"" rel=""nofollow noreferrer"">parsing</a>. Parsing consists in extracting the syntactic structure of a text, usually in the goal to better capture its semantics. There are various approaches:</p>
<ul>
<li><a href=""https://en.wikipedia.org/wiki/Shallow_parsing"" rel=""nofollow noreferrer"">Shallow parsing</a> only identifies the constituents of the sentences (based on your example this could be sufficient in your case)</li>
<li><a href=""https://en.wikipedia.org/wiki/Statistical_parsing"" rel=""nofollow noreferrer"">Statistical parsing</a> and in particular <a href=""https://nlpprogress.com/english/dependency_parsing.html"" rel=""nofollow noreferrer"">Dependency parsing</a> represent the full structure of the sentence, including the links between its constituents.</li>
</ul>
<p>There are various libraries and datasets for parsing: one of the most famous is probably the <a href=""https://nlp.stanford.edu/software/lex-parser.shtml"" rel=""nofollow noreferrer"">Stanford parser</a>, but there are many others often included in NLP toolkits such as <a href=""https://opennlp.apache.org/"" rel=""nofollow noreferrer"">OpenNLP</a>. The <a href=""https://universaldependencies.org/"" rel=""nofollow noreferrer"">Universal Dependencies</a> project is a vast multilingual collection of annotated text which can be used to train parsers.</p>
<p><a href=""https://en.wikipedia.org/wiki/Semantic_role_labeling"" rel=""nofollow noreferrer"">Semantic Role Labeling</a> (SRL) is a closely related task which consists in identifying the semantic relations between a predicate (verb) and its related constituents (e.g. subject, object).</p>
","1","2","64377","14816"
"84575","<p>Assuming that these results are obtained on a valid test set with no data leakage, these results don't show overfitting because overfitting would cause great performance on the training set but significantly lower perfomance on the test set.</p>
<p>Make sure that your instances between the training and test set are truly distinct: there might be some data leakage, for example if there are duplicate records for the same patient.</p>
<p>Another potential flaw is the gold standard labeling: if the patient has this T2DM  in their medical record, it means that they are already diagnosed right? And since the features are also based on the medical record, it's likely that this medical record contains direct indications about this diagnosis (for example in the drugs prescribed to the patient). There are two interpretations about this:</p>
<ul>
<li>either the task is purposefully defined by this T2DM label, and in this case you can just enjoy the great performance but it's not technically about detecting diabetes in general.</li>
<li>or the goal is to detect patients with diabetes including the ones who are not diagnosed yet, but then it's likely that your gold standard is incorrect for this task.</li>
</ul>
<hr />
<p>[edit following updated question]</p>
<p>Your update clarifies which exact task you're targeting, and it corresponds exactly to my first interpretation above: given that your goal is to predict which instances satisfy this T2DM criterion and that the features contain direct indications about it:</p>
<ul>
<li>I think you're right to keep these features, if a very useful information is available it would be absurd not to exploit it (assuming it's also available in the same form in any future dataset you plan to use, of course)</li>
<li>The very high performance you obtain makes perfect sense for this task, it's not a bug. It simply happens that the task is easy, so the system is able to predict the label very well.</li>
</ul>
<p>However this also means that you could do even better without any ML: currently the ML method gives you around 99% F-score because it doesn't perfectly represent the criterion used for the gold. But since the gold standard label is based entirely on the features, the most direct way to &quot;predict&quot; the label is to apply the criterion. There's no point using a complex ML method to predict an information that you can obtain from the same data more accurately with a deterministic method.</p>
","14","2","64377","14816"
"84590","<p>First, congratulations for thinking to do a qualitative analysis of the results :) I know it should be obvious, but so many people just assume that the system works and don't bother checking their output.</p>
<p>Now, strictly speaking what you're seeing is not a bug. These are errors made by a statistical system. A statistical system is not meant to get everything right, it's only meant to label the input <em>&quot;to the best of its knowledge&quot;</em>, and its knowledge is limited primarily by (1) the data it was trained with and (2) the assumptions made in the design of the model itself.</p>
<p>I don't know the exact characteristics of the systems that you used, but I can make an educated guess about the errors that you mention:</p>
<ul>
<li>&quot;fewer people are dying every day&quot; is likely to be predicted as negative because it contains the word &quot;dying&quot;. Probably there were no (or very few) examples in the training data which contain the word &quot;die&quot; and are labelled positive. As a consequence the system assumes that any sentence containing &quot;die&quot; is likely negative. One may notice that the positive semantics of &quot;fewer people dying&quot; is completely lost on the system, because it focuses on simple clues (individual words), it's not able to parse more complex phrases.</li>
<li>&quot;The audience here in the hall has promised to remain silent.&quot; would be a similar case: the word &quot;silent&quot; or perhaps the two words &quot;remain silent&quot; likely were found only in negative examples during training, so the system just generalizes wrongly that a sentence containing these words is negative.</li>
<li>The sarcastic &quot;Oh really?!&quot; is an even more complex concept for the system to properly identify. The task of <a href=""https://towardsdatascience.com/sarcasm-detection-with-nlp-cbff1723f69a"" rel=""nofollow noreferrer"">sarcasm detection</a> is studied on its own because it's such a difficult task for a machine. I don't follow this field closely so I could be wrong, but I don't think the task has reached any satisfying level of maturity yet, let alone been integrated with standard sentiment analysis systems.</li>
</ul>
<p>Nonetheless these errors don't mean that the results are useless. If you annotate manually a random sample and evaluate the performance of the system on this sample, hopefully you'll see that overall the system performs decently. That's what is expected of a statistical system: it's not reliable on an individual basis, but normally it's doing a good job in average.</p>
<p>More generally, all these errors show that the problem of <a href=""https://en.wikipedia.org/wiki/Natural-language_understanding"" rel=""nofollow noreferrer"">Natural Language Understanding</a> is far from being solved yet... and it might never be. The good news is that there's still a lot of interesting problems to solve for NLP scientists ;)</p>
","2","2","64377","14816"
"84617","<p>In general it depends on the exact method used to select instances and of course on the data. Assuming that the selection is based solely on the uncertainty measure of a single classifier, then by definition the method will prioritize instances predicted with a probability around 50%, i.e. where the classifier is &quot;unsure&quot;. As a consequence an instance predicted with a high probability is unlikely to be selected for annotation. However the iterative training process will make the classifier re-estimate the probability of all the instances, so it's possible that an instance wrongly classified with 90% probability at a particular iteration will later be assigned a lower probability, or even the true class. But overall there's no guarantee: like with any statistical system there can be instances misclassified with a high probability.</p>
<blockquote>
<p>The rationale behind my question is: does adding more samples to the training set always improve the performance of a classifier?</p>
</blockquote>
<p>In active learning, the performance depends more on how many instances end up being manually annotated than on the size of the unlabelled sample. But as usual the performance strongly depends on the data itself.</p>
","2","2","64377","14816"
"84672","<p>There is another reason: words which don't appear in the dictionary. Of course a dictionary approach will correctly stem all the forms which are known in the dictionary, and depending on the language this may indeed lead to better accuracy. However the dictionary approach cannot do anything about unknown words, whereas a generic stemmer can try to apply its generic rules. This can be particularly important with texts which are either very domain-specific (e.g. medicine), which often contain technical words which are not in a general dictionary, or recent user-generated texts such as social media posts where people may use neologisms or words borrowed and sometimes transformed from another language.</p>
","3","2","64377","14816"
"84688","<p>From the example I assume that an instance corresponds to a user, and you have both full sequences of the mouse and keyboard as features for predicting the user. I can think of two options for using these features in the same model:</p>
<ul>
<li>With feature engineering, find a way to represent both sequences as a fixed array of features. For example you might have features such as average typing speed average mouse speed, number of mouse movements, number of times each key is pressed, etc.</li>
<li>Similar idea but in a more DL approach: find a way to represent both sequences as embeddings (there are methods for word embeddings, sentence embeddings, graph embeddings...)</li>
</ul>
<p>In my opinion the main issue is the variable length of the sequences, not the fact that they are not aligned (the alignment would matter if the target variable was for one element in the sequence).</p>
","1","2","64377","14816"
"84743","<p>I have no knowledge at all about this kind of problem, but logically I would say:</p>
<blockquote>
<p>Should I remove the operations where there were no products shown (even though some of them there were clicks on products)?Example (should discard this columns?):</p>
</blockquote>
<p>Yes, because otherwise you could in theory end up with a rate higher than 1, this wouldn't make sense. However this raises the question of what exactly is represented: since some clicks can be counted when there is no product, it looks like the number of clicks is over-estimated in the data since the clicks are supposed to be on products.</p>
<p>Also note that it's the rows which would be removed in this case, not the columns.</p>
<blockquote>
<p>My other doubt is wether should I sum the clicks and the products shown and then divide one on other OR should I do the CTR for each operation:</p>
</blockquote>
<p>This issue means that either the data doesn't have the proper format to apply the definition (strictly speaking the data should have one row for every product shown, and the number of clicks should be either 0 or 1), or that the instructions for applying the CTR formula with this specific data are under-specified. I would say that the first option is closer to the original formula since it doesn't require to do a mean over individual rates.</p>
","0","2","64377","14816"
"84870","<blockquote>
<p>Can someone explain why the huge gap</p>
</blockquote>
<p>It simply means that there's a quite high variance depending which random set of instances is picked. How many times do you re-sample the instances in the bagging process? Probably increasing the number of runs will decrease the variance. As mentioned in a comment, the most common reason for variance in performance is a sample which is too small (and/or a number of features/classes which is too high). It's likely to cause your models to overfit.</p>
<blockquote>
<p>and should I just use the accuracy with highest value in my research paper or takes the average with random seed=None?</p>
</blockquote>
<p><strong>Never ever use the highest performance across random runs</strong>, this is cherry-picking and it doesn't reflect the true performance. The possibility to select a random seed is for reproducibility purposes, and selecting the one which gives the best results is the opposite of the principle of reproducibility.</p>
<p>Since you're using bagging, you should decrease the variance (that's the whole point) by increasing the number of runs. If you can't do that for any reason, then don't use bagging: simply repeat the regular process splitting-training-evaluating <span class=""math-container"">$N$</span> times (with a fixed proportion training/testing data) or use cross-validation, and report the average performance (preferably report the variance as well, e.g. standard deviation).</p>
","1","2","64377","14816"
"84872","<p>Interesting problem which potentially involves many aspects of ML, here are a few thoughts:</p>
<ul>
<li>At first sight I thought that this looks more like an optimization problem, not a regular classification problem. In this case I would suggest to maybe try things like <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow noreferrer"">genetic learning</a>, because it can find an optimal assignment for individual elements which maximizes a global cost or reward function.</li>
<li>However I suspect that the problem is not entirely well defined: <em>&quot;but some that we classified as junk sold for so much in the auction that it wasn't worth sending any to junk.&quot;</em>: I'm clueless about the business model but wouldn't it damage the reputation of the company if they were trying to sell all the junk cars at an auction? If I'm right then it matters that only a small proportion of cars are sent to the auction, and then the problem might be related to <a href=""https://en.wikipedia.org/wiki/Resource_allocation#Algorithms"" rel=""nofollow noreferrer"">resource allocation</a> (which is a specific kind of optimization problem).</li>
<li>Another way to look at it is that the vast majority of the cars are junk. Thus it might be relevant to see the problem as <a href=""https://en.wikipedia.org/wiki/Anomaly_detection"" rel=""nofollow noreferrer"">anomaly detection</a>, in the sense that one tries to pick the rare cases out of a sea of regular cases. The same idea could be implemented as <a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">one-class classification</a>, the idea being that the model can identify all the common cases (junk) and anything else is potentially valuable.</li>
</ul>
<p>Finally it's certainly worth investigating in the data if the valuable cars can actually be found from the features: would a human expert with only the information in the features be able to correctly classify a car? I could imagine that for instance if a particular car is valuable because it was used in a famous movie, it doesn't help to just know its model and mileage.  It would also be useful to check the relation between how common a car model is and its junk/valuable status, this could be an important indicator to take into account in the model via a feature. In the most simple case, it might even be possible to detect potentially valuable cars just by looking at this indicator... in which case there's no need for ML at all.</p>
","0","2","64377","14816"
"84992","<p>It's of course technically possible to calculate macro (or micro) average performance with only two classes, but there's no need for it. Normally one specifies which of the two classes is the positive one (usually the minority class), and then regular precision, recall and F-score can be used.</p>
<p>Commonly there is a majority and a minority class, and naturally the majority class is easier to predict for the classifier. That's why the minority class is usually chosen as the positive class: by choosing the most difficult class to predict, the performance value represents more precisely the real ability of the classifier. As a consequence, the macro-average performance is often better than the performance on the positive class, since the former includes the &quot;easy&quot; class.</p>
","0","2","64377","14816"
"85043","<p>One way or another any data can be represented in a table or even in a big binary string, since after all the physical memory of a computer is just one big binary sequence. But the question is whether the table format adequately represents the semantics of any data, and the answer is definitely no: while there are tabular representations for <a href=""https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)"" rel=""noreferrer"">graphs</a>, text, images or videos, these representations simplify and/or make assumptions about the nature of the data. Virtually any representation is a simplification of the data it represents, but some representations are more faithful to the original data than others.</p>
","8","2","64377","14816"
"85087","<blockquote>
<p>Is smoothing in NLP ngram done on test data or train data?</p>
</blockquote>
<p>In short: both.</p>
<p><a href=""https://en.wikipedia.org/wiki/N-gram#Bias-versus-variance_trade-off"" rel=""nofollow noreferrer"">Smoothing</a> consists in slightly modifying the estimated probability of an n-gram, so the calculation (for instance <a href=""https://en.wikipedia.org/wiki/Additive_smoothing"" rel=""nofollow noreferrer"">add-one smoothing</a>) must be done at the training stage since that's when the probabilities of the model are estimated.</p>
<p>But smoothing usually also involves differences at the testing stage, in particular for assigning a probability to unknown n-grams instead of 0.</p>
","0","2","64377","14816"
"85183","<blockquote>
<p>does it make sense to use micro and macro precision in binary classification problems when classes are imbalanced?</p>
</blockquote>
<p>In general micro- and macro-average performance are not relevant in binary classification, whether the classes are balanced or not. Their value can be especially misleading if there is a strong imbalance, because it takes into account both the minority class (harder for the classifier) and the majority class (easier):</p>
<ul>
<li>By definition micro-average gives more weight to the majority class, so the micro-average performance can be high even if the classifier does a terrible job at distinguishing the two classes.</li>
<li>The macro-average is not biased towards any of the two classes, still it's uselessly complex, it makes it harder to understand what's going on than simple performance on the positive class, which is normally the minority one (because that's the challenging one).</li>
</ul>
<p>Of course there can be cases where it makes sense not to follow this standard evaluation setting, it's always a matter of choosing the appropriate way to evaluate a particular task.</p>
<p>The below example illustrates why micro- and macro-average are confusing in a standard case of imbalance:</p>
<pre><code>              true A   true B
predicted A     90        9
predicted B      0        1
</code></pre>
<ul>
<li>For A: precision = 0.91, recall = 1, f1-score = 0.95</li>
<li>For B: precision = 1, recall = 0.1, f1-score = 0.18</li>
<li>micro-average: precision = 0.91, recall = 0.91, f1-score = 0.91</li>
<li>macro-average: precision = 0.95, recall = 0.55, f1-score = 0.70</li>
</ul>
<p>Assuming we don't know anything else than the selected performance measure, this classifier:</p>
<ul>
<li>performs almost perfectly according to the performance of the majority class A,</li>
<li>performs very well according to micro-average,</li>
<li>performs decently according to macro-average,</li>
<li>performs terribly according to the performance of the minority class B.</li>
</ul>
<p>Looking at the confusion table, it's clear that the classifier doesn't do a good job at distinguishing the two classes. So the most &quot;honest&quot; performance measure is the last one, i.e. the non-averaged performance on the minority class.</p>
","1","2","64377","14816"
"85213","<p>I read the thread but didn't analyze the data.</p>
<p>It's very difficult to answer this question in any conclusive way: assuming the graphs are correct, interpreting them is a highly risky/subjective game because there's so many hidden factors: the way votes are collected at different times, in different places which have different population density, under different state laws and procedures...</p>
<p>I would simply emphasize that in statistics the word &quot;anomaly&quot; only describes data points which deviate from the norm (regular pattern). Note that this is a quite vague notion (how far from the norm is an anomaly?), and more importantly that <strong>statistical analysis by itself does not explain the reason why anomalies happen</strong>, it can only detect them. The explanation must rely on what is usually called &quot;expert knowledge&quot;, i.e. indications which are not present in the data itself, obtained by human analysis of how the data was obtained.</p>
<p>The linked analysis is quite interpretative and possibly biased:</p>
<ul>
<li>The <em>&quot;slight drift from D to R&quot;</em> in almost all states is <em>&quot;likely due to outlying rural areas having more R votes. These outlying areas take longer to ship their ballots to the polling centers.&quot;</em> This explanation is based on expert knowledge, with no way for a non-expert to check its validity.</li>
<li>The Wisconsin shift is explained by <em>&quot;Around 3am Wisconsin time, a fresh batch of 169k new absentee ballots arrived. They were supposed to stop accepting new ballots, but eh, whatever I guess.&quot;</em> One may note again the use of external knowledge (and a suggestion that some illegal stuff happened)</li>
<li>The explanation for this case looks like forensics analysis: <em>&quot;quite possibly bc additional ballots were added to the batch, either through backdating or ballot manufacturing or software tampering. This of this being kind of analogous to carbon-14 dating, but for ballot batch authenticity.&quot;</em> Yet all of this is hypothetical, there could be other explanations.</li>
<li>About the Pennsylvania shift: <em>&quot;But then as counting continues, the D to R ratio in mail-in ballots inexplicably begin &quot;increasing&quot;. Again, this should not happen, and it is observed almost nowhere else in the country, because all of the ballots are randomly shuffled...&quot;</em> Saying that this shift is &quot;inexplicable&quot; is interpretative: one just doesn't have an explanation, that doesn't mean there isn't one. <em>&quot;the ballots are randomly shuffled&quot;</em>: not as far as I know: the ballots are collected by county and different counties can have a different distribution.</li>
</ul>
<ul>
<li>...</li>
</ul>
<p>My point is: there's no way to know if this analysis is correct just from the data, most of the author's conclusion are based on external explanations.</p>
","0","2","64377","14816"
"85225","<p>The names of the classes don't matter, you might as well call them class A and class B. In binary classification the typical choice is to evaluate using precision, recall and F1-score. There are other options, but that depends on the task.</p>
<p>Assuming you choose F1-score, the choice of which class you select as the &quot;positive&quot; class for evaluation also depends on the task. Usually it's recommended to use the minority class because it's the most challenging one for the classifier.</p>
<p>The only problem here is the possible confusion of calling a class &quot;negative&quot; and calculating F1-score using it as the &quot;positive&quot; class, but that's just a naming issue. You could easily add this point to the explanations, or avoid any confusion by calling your classes A and B for instance.</p>
","0","2","64377","14816"
"85264","<p>My interpretation is that the data can be described as the following three groups, by decreasing order of the prediction to be positive:</p>
<ul>
<li>a large number of positive instances which can &quot;easily&quot; be predicted as positive, with no false positive</li>
<li>then a large number of negative instances</li>
<li>then a smaller number of positive instances, <strong>which are predicted as less likely to be positive than the negative instances</strong>.</li>
</ul>
<p>For instance it might look like this:</p>
<pre><code>&lt;predicted probability&gt; &lt;true class&gt;
       1                     P
      0.9                    P
      ...                   ...
      0.4                    P

      0.38                   N
      ...                   ...
      0.14                   N

      0.11                   P
      ...                   ...
      0.05                   P
</code></pre>
<p>This kind of configuration would explain the three stages which are clearly visible. There might be other ways this would happen, but I can't think of anything very different than this explanation (I could be wrong about that).</p>
<p>Note that this is why both precision and recall increase with this third group: since the threshold has already passed all the true negative instances (i.e. all the possible FP errors have been made), adding these true positive cases increases recall (more TP cases) without decreasing precision (no more FP). It's indeed very unusual, but not impossible.</p>
<p>What is interesting here is to investigate why this third group of positive instances are totally misclassified: they're not only confused with the negative (the common case), they are predicted as &quot;more negative&quot; than the true negative instances. This is very strange: it's as if there are two very distinct groups of positive instances that the classifier cannot predict on the same side of the positive/negative continuum. But it's encouraging that the classifier can quite well distinguish the three groups from each other, it means that it's probably something which can be fixed.</p>
","3","2","64377","14816"
"85269","<p>It think it's a reasonable approach, but currently it seems that you have no way to check whether the new labels are correct or not. I think you should at least check that the new labels don't introduce more errors than they solve.</p>
<p>Ideally you would re-annotate a random sample of instances, keeping both the old (possibly erroneous) labels and the new ones. Then you can use this sample as a test set and evaluate the two following points:</p>
<ul>
<li>most/all instances for which the new label is the same as the old label should be predicted with this label (otherwise it means your method changes correct labels)</li>
<li>most instances for which the new label is different from the old label should be predicted with the new label (otherwise it means your method doesn't fix the wrong labels)</li>
</ul>
<p>The problem with this approach is that you need to annotate a large sample, since you need a reasonable number of wrong labels which are only present in 1% of the data.</p>
<p>If re-annotating a large sample is not possible, you could try a kind of boostrapping approach: run your method, then take a sample of instances which are predicted as different from the old label. Among these labels changes, count how many are correct. This approach requires less manual annotation effort since you don't need a large random sample, however it would miss the cases of wrong label which is not changed by the classifier.</p>
","2","2","64377","14816"
"85271","<p>It looks to me like what you propose makes sense, but there has been some research done around these questions of time representation already. I'd suggest you check the state of the art in this domain, if only not to reinvent the wheel or miss important cases.</p>
<p>I'm not very knowledgeable about it but I can at least point you to <a href=""http://www.timeml.org/"" rel=""nofollow noreferrer"">TimeML</a> and the <a href=""https://scholar.google.com/scholar?hl=en&amp;q=timeml"" rel=""nofollow noreferrer"">related publications</a>. There are certainly other recent works building on TimeML, for example <a href=""https://www.aclweb.org/anthology/W18-4709.pdf"" rel=""nofollow noreferrer"">this one</a> (disclaimer: I know the author).</p>
","1","2","64377","14816"
"85272","<p>If an information is never available for one of the classes, it's not a usable indication. So it seems to me that the login name is simply irrelevant for the task, so it shouldn't be included as a feature.</p>
<p>Mixing different sources of data can be ok in some cases, but only if the different datasets provide consistent features and are generally representative of the distribution you would expect (another important problem). It looks like you have no username nor timestamp in the &quot;bad&quot; one, do you? if so it's impossible to use these features.</p>
<p>No you cannot give priority to any feature. This wouldn't make sense, since the point of ML is to let the algorithm find patterns in the data. If you already know how the decision should be made then you should simply write a regular deterministic program (this is called a heuristic).</p>
<p>Don't forget that you can decide to add any feature that you think could be relevant:</p>
<ul>
<li>you could have a boolean feature indicating whether a login name is known or unknown, instead of setting the value to 0. But be careful that by definition in your external dataset they are all unknown whereas they are all known in your &quot;good&quot; dataset, that's an clear construction bias.</li>
<li>if you have a timestamp you could think of many other features, for instance number of repeated attempts in the past N seconds, regular time patterns for login, etc.</li>
</ul>
","0","2","64377","14816"
"85312","<p><a href=""https://en.wikipedia.org/wiki/Dynamic_time_warping"" rel=""nofollow noreferrer"">Dynamic Time Warping</a> might be what you're looking for: it measures similarity between two time series based on the optimal alignment between the two sequences. For example point <span class=""math-container"">$i$</span> in sequence 1 might be better aligned with point <span class=""math-container"">$i+3$</span> in sequence 2 based on the evolution of the sequences (as opposed to Euclidean distance which would always compare <span class=""math-container"">$i$</span> in seq. 1 with <span class=""math-container"">$i$</span> in seq. 2). The alignment method is inspired by the <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein edit distance</a> method, which can be implemented efficiently with dynamic programming.</p>
<p>There are many <a href=""https://en.wikipedia.org/wiki/Dynamic_time_warping#Open-source_software"" rel=""nofollow noreferrer"">software implementations</a> available.</p>
","1","2","64377","14816"
"85399","<p>Technically your problem is not about a variable number of features, since you can have a finite list of all the possible features. The standard case is just to use all these features, even if only a few of them are &quot;active&quot; for a particular instance (your first option). If the number of features is too high, then you need <a href=""https://en.wikipedia.org/wiki/Dimensionality_reduction"" rel=""nofollow noreferrer"">dimensionality reduction</a>.</p>
<p>The advanced option is to use a <a href=""https://en.wikipedia.org/wiki/Feature_extraction"" rel=""nofollow noreferrer"">feature extraction</a> method: an unsupervised method which &quot;groups&quot; similar features together, the goal being to reduce dimensionality while preserving the information contained in the features. By definition this method modifies the set of features: the &quot;new features&quot; are not directly interpretable, as opposed to the original ones.</p>
<p>Note that the other way to reduce dimensionality is <a href=""https://en.wikipedia.org/wiki/Feature_selection"" rel=""nofollow noreferrer"">feature selection</a>: these methods don't modify the features, but they discard the least informative ones. Feature selection is supervised (as opposed to feature extraction) because informativeness is measured with respect to the response variable.</p>
","0","2","64377","14816"
"85475","<p>I think it's always a good idea to start simple, so I'd simply suggest to try with all the features, including the different levels of &quot;nesting&quot; so around 2k apparently. Given that the dataset is large, I don't see any obstacle to trying this way. For the same reason I would start with a very simple model like Decision Trees or SVM, which have the additional advantage that they're fast to train. This could be a first step which provides you with a decent baseline, at least.</p>
<p>If the number of features turns is an issue for a more advanced option, I think this is a good case for using <a href=""https://en.wikipedia.org/wiki/Feature_extraction"" rel=""nofollow noreferrer"">feature extraction</a> (for example PCA): this would reduce the number of features and also merge features which represent the same information.</p>
","0","2","64377","14816"
"85493","<blockquote>
<ol>
<li>Shouldn't an index have only one value in the feature axis?</li>
</ol>
</blockquote>
<p>Yes, that's correct. On the graph given as example this is not visible because there are too many row indexes (50000). As a consequence it's impossible to distinguish a particular index from its neighbors, but if the X axis was stretched long enough one would see a single feature value for every index.</p>
<blockquote>
<ol start=""2"">
<li>One horizontal line should mean that the feature values for all indexes have been uniformized, not randomized?</li>
</ol>
</blockquote>
<p>I think there could be two different confusions here:</p>
<ul>
<li>An horizontal line means that <em>a single feature value</em> is distributed uniformly across the indexes, which is equivalent to saying that <em>the indexes</em> are random for this feature value. In other words, the chance that this feature value appears at a particular index is the same as at any other index. This is what the author means: the order (indexes) is random for any feature value.</li>
<li>The values for all the features have not been uniformized, this can be seen from the fact that vertically the density of the points is different around the middle (say 0.4-0.6) and the extremes (say 0-0.2 and 0.8-1). Of course this would be more visible with a standard histogram, which would show a kind of peak in the middle but with two high bars at the extremes for 0 and 1 (it can be seen from the continuous lines for these two features values that they appear much more frequently).</li>
</ul>
<p>One may also note on this graph that there is some kind of underlying discrete distribution of the values: very clearly for values 0 and 1, but also from all the white horizontal lines which show that some values seldom exist in the data.</p>
","1","2","64377","14816"
"85501","<p>I think this problem could be represented as a <a href=""https://en.wikipedia.org/wiki/Markov_chain"" rel=""nofollow noreferrer"">Markov Chain</a>.</p>
<p>Maybe a <a href=""https://en.wikipedia.org/wiki/Markov_model"" rel=""nofollow noreferrer"">Markov model</a> (or Markov Random Fields) could be used to estimate the probabilities. Normally it's easier than with the more standard <a href=""https://en.wikipedia.org/wiki/Hidden_Markov_model"" rel=""nofollow noreferrer"">Hidden Markov Model</a> since there is no hidden state.</p>
<p>My suggestion would be to estimate the probabilities of the model (parameters) from the data, then it should be easier to discover the patterns based on these probabilities.</p>
","0","2","64377","14816"
"85517","<p>There seems to be a mistake in your method:</p>
<blockquote>
<p>I read about the use of downsampling and upsampling, so I applied them before training and testing the dataset.</p>
</blockquote>
<p>It's incorrect to change the distribution of the test set. When resampling, the resampling should be applied only on the training set. The goal is to force the model to take into account the two classes, because in case of imbalance the model tends to focus on the majority class. However the true proportion of the class in the &quot;real dataset&quot; is still the same, and the test set should follow this true proportion. Otherwise the performance looks artificially good on the test set, even though the classifier will make more mistakes with real data since it doesn't have the same distribution.</p>
<p>So the performance values that you obtain on a resampled dataset are meaningless, I'm afraid.</p>
<blockquote>
<p>I am considering a publication (not only to include similar analysis), so I would like to check if such results can be considered reliable, despite of the imbalance.</p>
</blockquote>
<p>If you are considering a peer-reviewed publication, you must also make sure that your contribution is original (i.e. new) and brings some advantage over the existing methods. This means that you need to know the state of the art in spam classification (there are <a href=""https://scholar.google.com/scholar?q=survey%20email%20spam%20detection%20OR%20filtering"" rel=""nofollow noreferrer"">a lot of papers already published about this task</a>) and show what your method improves something compared to the existing methods. Ideally this is done by proving that your new method obtains better performance than the state of the arts methods using a benchmark dataset. But it's usually hard to beat state of the art performance on a well-known problem.</p>
","2","2","64377","14816"
"85629","<p>This problem is called <a href=""https://en.wikipedia.org/wiki/Record_linkage"" rel=""nofollow noreferrer"">record linkage</a>. This process usually involves comparing records using an <a href=""https://en.wikipedia.org/wiki/Approximate_string_matching"" rel=""nofollow noreferrer"">approximate string similarity measure</a>.</p>
<p>Here are a few related questions on the site that I'm aware of:</p>
<ul>
<li>About string similarity measures, you might find some info in <a href=""https://datascience.stackexchange.com/q/82923"">these</a> <a href=""https://datascience.stackexchange.com/q/63325"">questions</a>.</li>
<li>About record linkage, I'm aware of <a href=""https://datascience.stackexchange.com/q/68402"">these</a> <a href=""https://datascience.stackexchange.com/q/54570"">questions</a>.</li>
</ul>
<p>(Note that there are certainly many other questions/resources available, I just mentioned the ones I remember)</p>
","0","2","64377","14816"
"85655","<p>There's no algorithm intended specifically for this task, you need to design the process yourself (like for most tasks btw).</p>
<p>Given that the goal would be to use a person's name as an indication, I'd suggest you represent a name as a vector of characters n-grams in the features.</p>
<p>Example with bigrams (<span class=""math-container"">$n=2$</span>):</p>
<pre><code>&quot;Braund&quot; = [ #B, Br, ra, au, un, nd, d# ]
</code></pre>
<p>Intuitively the goal is for the model to find the sequences of letters which are more specific to a nationality. You could try with unigrams, bigrams or trigrams (the higher <span class=""math-container"">$n$</span>, the more data you need for training).</p>
<p>Once the names are represented as features this way, you can train any type of supervised model, for example Decision Tree or Naive Bayes.</p>
","1","2","64377","14816"
"85667","<p>It's a neologism, there's no standard spelling or even meaning.</p>
<p>For the record there are related terms which are more standard: &quot;data representation&quot; (usually low-level) or &quot;<a href=""https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning"" rel=""nofollow noreferrer"">knowledge representation</a>&quot;.</p>
","0","2","64377","14816"
"85701","<p>The simple approach is:</p>
<ul>
<li>To use a learning algorithm which can handle <a href=""https://en.wikipedia.org/wiki/Missing_data"" rel=""nofollow noreferrer"">missing values</a>. You should be careful about whether these missing values occur randomly or not though, as they could cause the model to be biased.</li>
<li>Triplets can be separated into individual features, there's no simple way to process them differently anyway.</li>
</ul>
","0","2","64377","14816"
"85728","<p>This could be seen as a &quot;simple&quot; binary classification problem. I mean the type of problem is &quot;simple&quot;, the task itself certainly isn't... And I'm not even going to mention the serious ethical issues about its potential applications!</p>
<p>First, obviously you need to have an entry in your data for a patient's death. It's not totally clear to me if you have this information? It's important that whenever a patient has died this is reported in the data, otherwise you cannot distinguish the two classes.</p>
<p>So the design could be like this:</p>
<ul>
<li>An instance represents a single patient history at time <span class=""math-container"">$t$</span>, and it is labelled as either alive or dead at <span class=""math-container"">$t+N$</span> days.</li>
<li>This requires refactoring the data. Assuming data spans a period from 0 to <span class=""math-container"">$T$</span>,  you can take multiple points in time <span class=""math-container"">$t$</span> with <span class=""math-container"">$t&lt;T-N$</span> (for instance every month from 0 to <span class=""math-container"">$T-N$</span>). Note that in theory I think that different times <span class=""math-container"">$t$</span> for the same patient can be used in the data, as long as all the instances consistently represent the same duration and their features and labels are calculated accordingly.</li>
<li>Designing the features is certainly the tricky part: of course the features must have values for all the instances, so you cannot rely on specific tests which were done only on some of the patients (well you can, but there is a bias for these features).
<ul>
<li>To be honest I doubt this part can be done reliably: either the features are made of standard homogeneous indicators, but then these indicators are probably poor predictors of death in general; or they contain specialized diagnosis tests
for some patients but then they are not homogeneous across patients, so the model is going to be biased and likely to overfit.</li>
</ul>
</li>
</ul>
<p>Ideally I would recommend splitting between training and test data before even preparing the data in this way, typically by picking a period of time for training data and another for test data.</p>
<p>Once the data is prepared, in theory any binary classification method can be applied. Of course a <a href=""https://en.wikipedia.org/wiki/Probabilistic_classification"" rel=""nofollow noreferrer"">probabilistic classifier</a> can be used to predict a probability, but this can be misleading so be very careful: <strong>the probability itself is a prediction</strong>, it cannot be interpreted as the true chances of the patient to die or not. For example Naive Bayes is known to empirically always give extreme probabilities, i.e. close to 0 or close to 1, and quite often it's completely wrong in its prediction. This means that in general the predicted probability is only a guess, it cannot be used to represent confidence.</p>
<hr />
<p>[edit: example]</p>
<p>Let's say we have:</p>
<ul>
<li>data for years 2000 to 2005</li>
<li>N=1, i.e. we look at whether a patient dies in the next year.</li>
<li>a single indicator, for instance say cholesterol level. Of course in reality you would have many other features.</li>
<li>for every time <span class=""math-container"">$t$</span> in the features we represent the &quot;test value&quot; for the past 2 years to the current year <span class=""math-container"">$t$</span>. This means that we can iterate <span class=""math-container"">$t$</span> from 2002 (2000+2) to 2004 (2005-N)</li>
</ul>
<p>Let's imagine the following data (to simplify I assume the time unit is year):</p>
<pre><code>patientId birthYear year     indicator 
1         1987      2000     26
1         1987      2001     34
1         1987      2002     18
1         1987      2003     43
1         1987      2004     31
1         1987      2005     36
2         1953      2000     47
2         1953      2001     67
2         1953      2002     56
2         1953      2003     69
2         1953      2004     -    DEATH
3         1969      2000     37
3         1969      2001     31
3         1969      2002     25
3         1969      2003     27
3         1969      2004     15
3         1969      2005     -    DEATH
4         1936      2000     41
4         1936      2001     39
4         1936      2002     43
4         1936      2003     43
4         1936      2004     40
4         1936      2005     38
</code></pre>
<p>That would be transformed into this:</p>
<pre><code>patientId yearT age indicatorT-2 indicatorT-1 indicatorT-0   label
1         2002  15  26           34           18             0
1         2003  16  34           18           43             0
1         2004  17  18           43           31             0
2         2002  49  47           67           56             0
2         2003  50  67           56           69             1
3         2002  33  37           31           25             0
3         2003  34  31           25           27             0
3         2004  35  25           27           15             1
4         2002  66  41           39           43             0
4         2003  67  39           43           43             0
4         2004  68  43           43           40             0
</code></pre>
<p>Note that I wrote the first two columns only to show how the data is calculated, these two are not part of the features.</p>
","2","2","64377","14816"
"85766","<blockquote>
<p>I was wondering if i'm doing a GridsearchCV on 10-fold, getting the best parameters, and then using those parameters evaluating the performance on 10-fold - is that &quot;legal&quot; or overfitting? am i suppose to run the best parameters on the entire data ? or can i use 10-fold again?</p>
</blockquote>
<p>I'm pretty sure you won't go to prison for it ;) But it would be incorrect due to <a href=""https://en.wikipedia.org/wiki/Leakage_(machine_learning)"" rel=""nofollow noreferrer"">data leakage</a>: the data that you would use for the final evaluation would be the same which was used for learning the best parameters. So if the best parameters found during tuning (which is a kind of supervised training) happen by chance, the final performance will be artificially high.</p>
<p>The proper way to do that is either:</p>
<ul>
<li>To keep a fresh test set aside, i.e. run the grid search with CV on the training set, then evaluate the final model on the fresh test set.</li>
<li>To use nested CV, i.e. a double CV loop: for each outer CV fold, run the grid-search inner-CV on the training set, then evaluate on the outer CV test set. Needless to say, it's a bit more complex.</li>
</ul>
<blockquote>
<p>also, will LOOCV suppose to give better generalization on the performance part? (not on the gridsearch)?</p>
</blockquote>
<p>Leave one out CV is exactly the same as <span class=""math-container"">$k$</span>-fold CV but with <span class=""math-container"">$k$</span> equal to the number of instances, so in your case it's like 150-fold CV. Advantage: more training data every time, so potentially better model; disadvantage: computational cost higher since the training/testing is repeated 150 times.</p>
","1","2","64377","14816"
"85920","<p>To me it looks like your question is more about expert knowledge than ML approach: the main problem is how to build a sensible score based on the available indicators. I have zero knowledge about this domain, but in general this kind of problem requires a lot of back and forth with domain experts in order to correctly specify the task in detail.</p>
<p>However I have one piece of advice which might help (or not, maybe you already know this) for the evaluation of any system you build for this task: assuming that you obtain a predicted score which is meant to represent the future of the company, you could evaluate how well this score matches &quot;the future&quot; as follows:</p>
<ol>
<li>Split the data at a particular point in time in the past, say 2010</li>
<li>Use only the data before 2010 to train and test a model, i.e. obtain predictions based only on data before 2010</li>
<li>Evaluate how well the predicted scores match the &quot;future&quot; of the companies, i.e. the actual data after 2010. For example a low score should lead to poor indicators, a high score should lead to good indicators (you might need to devise a specific method to correlate the scores with the indicators).</li>
</ol>
","0","2","64377","14816"
"85945","<p>At first sight I don't see the need for ML, you just need a similarity measure which would return a score between two objects:</p>
<ol>
<li>Given the input object, say &quot;12345&quot;, compare it against any known object using the similarity measure. The result is a similarity score for every known object.</li>
<li>Rank the known objects according to their similarity against the input object, or just obtain the object with the highest similarity. for example &quot;12445&quot; is the most similar with score 0.8.</li>
<li>Apply a threshold <span class=""math-container"">$t$</span>: if the max similarity is higher than <span class=""math-container"">$t$</span>, then the corresponding known object is returned; otherwise the system says that the input object is &quot;completely new&quot;.</li>
</ol>
<p>Of course ML could still be used, for instance to tune the similarity measure or to find the optimal threshold.</p>
","0","2","64377","14816"
"85996","<p>I don't know, there might be disasters which happened due to ML but my guess is that it might be hard to really consider ML as the cause: ML is always deployed in a setting designed by humans for a very specific task, and the design must take into account the statistical nature of ML. So I would tend to interpret most problems as caused by a wrong design, not by ML itself.</p>
<p>If you are looking more generally into problems in the applications of ML, as far as I know the most common problems cited are:</p>
<ul>
<li>Increasing existing bias, for instance face recognition being is more accurate with white than black faces.</li>
<li>Lack of transparency, especially in critical applications like medicine, military, law enforcement</li>
<li>Lack of a legal responsibility framework, e.g. who is at fault if a self-driving car causes an accident?</li>
</ul>
","1","2","64377","14816"
"86003","<blockquote>
<p>If we are using label encoder we would only need to convert gender however if that maps male = 0, female = 1 wouldn't the machine treat female &gt; male?</p>
</blockquote>
<p>You are correct, using label encoder to encode categorical features is wrong in general, for the reason you mention. Note that <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""nofollow noreferrer"">scikit documentation</a> advises against using it with features, it's supposed to be used only with a response variable.</p>
<p>In the particular case of a binary variable like &quot;gender&quot; to be used in decision trees, it actually does not matter to use label encoder because the only thing the decision tree algorithm can do is to split the variable into two values: whether the condition is <code>gender &gt; 0.5</code> or <code>gender == female</code> would give the exact same results.</p>
<p>Also note that whether the variable is interpreted as ordinal or not is a matter of implementation. For example in Weka it's possible to specify that a feature is categorical (&quot;nominal&quot;).</p>
<blockquote>
<p>and if it ignores ordinality it will ignore level1 &lt; level2.</p>
</blockquote>
<p>Not necessarily, because in theory it's possible to have features with different types (e.g. some categorical and some numerical). However this may depend on the implementation as well.</p>
","1","2","64377","14816"
"86013","<p>First, the question is too broad because there are many different kinds of text classification tasks. For example one wouldn't use the same approach for say spam detection and author profiling (e.g. predict the gender of the author), two tasks which are technically text classification but have little in common (and there are many others).</p>
<p>Second, even with a more specific kind of problem, the question of the type of model is misleading because a lot of what makes a ML system perform better than another in text classification is due to other things: the type and amount of training data of course, but also crucially the features being used. There are many options in terms of representing text as features, and these different options usually have a massive impact on performance. I even think that most of the time the choice of a type of classification model does not matter as much as the design of the features.</p>
<p>Finally I'm actually going to answer the question but probably not in the way OP expects: the worst model in any classification task is exactly like the best model, but it swaps the answers in order to have as many wrong predictions as possible (e.g. class 1 -&gt; class 2, class 2 -&gt; class 3, .., class N -&gt; class 1).
Since it's a lot of work to implement the best classifier just to obtain the worst one, a close to worst one can be done with a <em>minority</em> baseline classifier: just predict every instance as the least frequent class in the training data.</p>
<p>I hope a few of the things I said will be helpful, even though it's probably not what OP wished for! :)</p>
","1","2","64377","14816"
"86017","<p>The question is actually about understanding what it means to &quot;take imbalance into account&quot;:</p>
<ul>
<li>Micro-average &quot;takes imbalance into account&quot; in the sense that the resulting performance is based on the proportion of every class, i.e. the performance of a large class has more impact on the result than of a small class.</li>
<li>Macro-average &quot;doesn't take imbalance into account&quot; in the sense that the resulting performance is a simple average over the classes, so every class is given equal weight independently from their proportion.</li>
</ul>
<p>Is it actually a good idea to &quot;take imbalance into account&quot;? It depends:</p>
<ul>
<li>With micro-average, a classifier is encouraged to focus on the largest classes, possibly at the expense of the smallest ones. This can be considered a positive because it means that more instances will be predicted correctly.</li>
<li>With macro-average, a classifier is encouraged to try to recognize every class correctly. Since it is usually harder for the classifier to identify the small classes, this often makes it sacrifice some performance on the large classes. This can be considered a positive in the sense that it forces the classifier to properly distinguish the classes instead of lazily relying on the distribution of classes.</li>
</ul>
<p>One could say that it's a kind of quantity vs. quality dilemma: micro-average gives more correct predictions, macro-average gives attention to actually distinguishing the classes.</p>
<p>Very often one uses macro with strongly imbalanced data, because otherwise (with micro) it's too easy for the classifier to obtain a good performance by relying only on the majority class. Your data is not strongly imbalanced so it's unlikely this would happen, but I think I would still opt for macro here.</p>
","4","2","64377","14816"
"86019","<p>Assuming the feature selection method is always the same, on an external data (or final training set), you would simply apply the exact same method. The actual set of selected features does not matter.</p>
<p>If there was any difference in the selection method, for instance if you are selecting different number of features, you would do like with any other hyper-parameter: select the best model according to the nested CV process, then apply the same hyper-parameters (including for instance number of features) when training the final model.</p>
","1","2","64377","14816"
"86050","<p>It's just a simple idea that I often recommend to understand what happens in cases like this:</p>
<p>You could try to do an ablation study, where you train the model with varying sizes of training data (e.g. 10%, 20%,..., 100% of your full training data). Given that your problem is instability, it would be even better to do every size several times with different random subsets. Then you plot the performance as a function of the data size, for example with boxplots in order to visualize the variance. The evolution of the performance (and its variance) would show you whether there is enough training data given the complexity of the model: if yes, the curve increases and then starts stabilizing around its maximum. If not, the curve is still in a phase of increase when it reaches 100%, meaning that more data would be needed to allow the model to stabilize.</p>
<p>Of course the disadvantage is that you need to run the training/testing many times, so this might not be practical.</p>
","0","2","64377","14816"
"86096","<p>Your thinking is correct, it is unefficient to recalculate TFIDF on the whole collection every time a document is added.</p>
<p>There are two parts in <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">TF-IDF</a>:</p>
<ul>
<li><span class=""math-container"">$TF(w,d)$</span> is the Term Frequency of a word <span class=""math-container"">$w$</span> in a particular document <span class=""math-container"">$d$</span>, it's independent from other documents. It's just defined as the proportion of <span class=""math-container"">$w$</span> in <span class=""math-container"">$d$</span>.</li>
<li><span class=""math-container"">$IDF(w)$</span> is the Inverse Document Frequency: it is defined as the log of the inverse of the document frequency, where the document frequency of a word is the proportion of documents in the collection which contain this word:</li>
</ul>
<p><span class=""math-container"">$$IDF(w)=log\left(\frac{|D|}{| \{d \in D \text{ such that } w\in d \} |}\right)$$</span></p>
<p><span class=""math-container"">$IDF(w)$</span> depends on all the documents so it's costly to re-calculate. However updating IDF whenever there is a new documents can be implemented efficiently:</p>
<ul>
<li>Maintain a global map of the <strong>document frequency</strong> (not the IDF) for every word. This map is easy to update: for every <strong>distinct</strong> word <span class=""math-container"">$w$</span> in the new document, increment <span class=""math-container"">$m[w]$</span>.</li>
<li>Whenever a TF-IDF weight is needed, collect the required document frequency and apply the IDF formula.</li>
</ul>
<p>I don't know whether this can be done with scikit API or not, but it's not too big a task to implement it yourself (and it's a good little exercise to understand TFIDF ;) )</p>
","0","2","64377","14816"
"86125","<p>I think you made a mistake here:</p>
<blockquote>
<p>i was able to achieve great results for my approach when i calculate the TFIDF and PCA first and then do the Test/Train split afterwards.</p>
</blockquote>
<p>This evaluation was flawed due to data leakage: both the TFIDF and the PCA should be calculated using only the training set.</p>
<p>This is more or less the source of your issue with how to proceed in production: normally in production one should just apply the same process as with your test set, but here you were able to prepare your test set differently (and wrongly).</p>
<ul>
<li>For the TFIDF step, the IDF should be computed only using the training set. Note that in the test set you might have to deal with out of vocabulary words.</li>
<li>For the PCA step, I'm not too sure how to proceed but I found <a href=""https://stats.stackexchange.com/q/55718/250483"">this explanation</a>.</li>
</ul>
","0","2","64377","14816"
"86134","<p>The difference is explained in the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.</p>
</blockquote>
<p>So it's not a matter of number of features, it's about how the values of the features are interpreted: Multinomial can deal with multiple discrete values, whereas Bernouilli deals only with binary variables.</p>
<p>The doc also mentions this option:</p>
<blockquote>
<p><strong>binarize: float or None, default=0.0</strong></p>
<p>Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.</p>
</blockquote>
<p>Since you didn't provide a value for this option in your code, the default 0.0 applies. This means that when using Bernouilli all the variables are converted to
binary variables: anything lower or equal to 0 is 0, anything higher is 1. This explains why Bernouilli works with your data, albeit not as well as Multinomial: probably for many features in your data the fact that the value is zero or not is a good indication for the label.</p>
","0","2","64377","14816"
"86156","<p>I'm afraid I have zero experience with ElasticSearch, so I can't help with that but here are a few thoughts about the process you propose.</p>
<p>Overall I think the approach makes sense, especially in terms of efficiency but also check the following points:</p>
<ul>
<li>This is not strictly speaking a clustering task, in the sense that the system doesn't find itself how to separate the clusters since this is based on a predefined threshold.</li>
<li>The threshold is crucial and you need a method to determine it. You might also want to plan for updating the threshold as the collection grows. The process is likely to lead to some very big clusters and many very small ones (unique document in a cluster).</li>
<li>If you assign a document to the best match cluster, it means that a document belongs to a single cluster. Depending on the application this could be problematic, with some documents expected in a cluster but found in another.</li>
</ul>
","0","2","64377","14816"
"86173","<p>It simply depends on what is the goal of the task:</p>
<ul>
<li>If the final goal is still to predict Y after detecting anomalies (i.e. probably using the output of anomaly detection as a feature), then Y cannot be used since it wouldn't be available in a realistic test set.</li>
<li>If it's just a completely different task in which Y is available as an input, then why not use it.</li>
</ul>
<p>With 500k instances, A single additional variable with 3 possible values has an extremely low risk of causing overfitting.</p>
<p>Note that since classification didn't work, it's likely that there is little relationship between the features and Y (otherwise there was some mistake in the classification experiment).</p>
","0","2","64377","14816"
"86205","<p>It's more a matter of complexity of the model than of the class of algorithms. Of course some classes of algorithm produce more complex models than others by construction, but this is not always the case. For example the complexity of a Decision Tree usually depends on the options/hyper-parameters: maximum depth, pruning, minimum number of instances in a branch. If these parameters are set to produce a small tree then the risk is bias (underfitting), but if they are set to produce a large tree then the risk is variance (overfitting).</p>
<p>The complexity of a model depends mostly on the number and nature of its parameters, so as a first approximation the number of parameters is a reasonable quantitative measure of complexity (see this <a href=""https://datascience.stackexchange.com/q/80531/64377"">closely related question</a>). Also keep in mind that most models try to use all the features they are provided with, so the number of features also has a high impact on the model complexity.</p>
","0","2","64377","14816"
"86236","<p>First, this is <em>not</em> <a href=""https://en.wikipedia.org/wiki/Sentiment_analysis"" rel=""nofollow noreferrer"">sentiment analysis</a>, which is about quantifying &quot;affective states&quot;, i.e. something along the lines of &quot;like / don't like&quot;. This would rather correspond to a specific application of general text classification: predicting whether a sentence contains a contact request.</p>
<p>Now about the specific problem: you're right, it could be a good idea to start by identifying some clearly positive sentences using pattern matching. There are many approaches which can be used to expand this initial set of positive sentences:</p>
<ul>
<li><a href=""https://en.wikipedia.org/wiki/Semi-supervised_learning"" rel=""nofollow noreferrer"">semi-supervised learning</a></li>
<li>Using semantic similarity between words (for instance with word embeddings) to find more candidate terms for pattern matching</li>
<li>Using similarity between sentences to find potential new positive cases, then label these manually (this could be done iteratively)</li>
</ul>
<p>Note that all the above approaches could work but they are biased in favor of the known positive cases, i.e. they risk missing positive cases which have little similarity with the original ones. The only option to avoid this issue is to take a random subset of sentences and label all of them as either positive or negative.</p>
","0","2","64377","14816"
"86325","<p>In your results you can observe the usual problem with imbalanced data: the classifier favors the majority class 0 (I assume this is class &quot;ham&quot;). In other words it tends to assign &quot;ham&quot; to instances which are actually &quot;spam&quot; (false negative errors). You can think of it like this: with the &quot;easy&quot; instances, the classifier gives the correct class, but for the instances which are difficult (the classifier &quot;doesn't know&quot;) it chooses the majority class because it's the most likely.</p>
<p>There are many things you could do:</p>
<ul>
<li>Undersampling the majority class or oversampling the minority class is the easy way to deal with class imbalance.</li>
<li>Better feature engineering is more work but it's often how to get the best improvement. For example I guess that you use all the words in the emails as features right? So you probably have too many features and that probably causes overfitting, try reducing dimensionality by removing rare words.</li>
<li>Try different models, for instance Naive Bayes or Decision Trees. Btw Decision Trees are a good way to investigate what happens inside the model.</li>
</ul>
","1","2","64377","14816"
"86338","<p>This is the formal definition of an interval (from <a href=""https://en.wikipedia.org/wiki/Interval_(mathematics)"" rel=""nofollow noreferrer"">Wikipedia</a>):</p>
<blockquote>
<p>In mathematics, a (real) interval is a set of real numbers that
contains all real numbers lying between any two numbers of the set.
For example, the set of numbers x satisfying 0 ≤ x ≤ 1 is an interval
which contains 0, 1, and all numbers in between.</p>
</blockquote>
<p>This definition doesn't tell us what to do to represent an interval for ML purposes... and that's normal :)</p>
<p>For ML purposes we need to decide a suitable <strong>representation</strong>, and there's rarely a mathematically pure or unique way to represent a mathematical object. Your confusion comes from the fact that the sites you mention propose their own representation, i.e. they consider a particular definition of interval, presumably because it suits their goal. The second one in particular is clearly defining an &quot;interval variable&quot; as an object in a particular language, with constraints which have nothing to do with the above definition.</p>
<p>Now to answer your question: an interval is not atomic, since it's not a single value. So strictly speaking it's neither a numerical nor an ordinal.</p>
<p>But you're probably not very satisfied with this answer, since you still want to be able to use this kind of feature. How? As usual, it depends:</p>
<ul>
<li>Most of the time an interval can be <strong>converted</strong> to an ordinal value (note that it's a conversion, i.e. we simplify the data to make it usable). The conversion to ordinal is typically done by numbering the possible intervals (following their order) with integers. If the variable is going to be a label in a classification task, it can even be converted to a categorical variable.</li>
<li>However there are cases where an ordinal would not be a good representation, in particular if the intervals are not equal and the ML method involves distances. In such cases it's probably safer to convert the interval to a numerical value, typically the mean of the interval. This way both order and distances are preserved.</li>
</ul>
","0","2","64377","14816"
"86341","<p>I'm not really knowledgeable about the modern techniques but I can tell you about the old ones: ;)</p>
<p>First there are two main approaches to dimensionality reduction: <a href=""https://en.wikipedia.org/wiki/Feature_selection"" rel=""nofollow noreferrer"">feature selection</a> and <a href=""https://en.wikipedia.org/wiki/Feature_extraction"" rel=""nofollow noreferrer"">feature extraction</a>. You're using the former, which consists in discarding some of the original features. The latter consists in some kind of &quot;merging&quot; of similar variables, it can be worth trying especially if you have redundant features.</p>
<p>As you rightly noticed, feature selection based on individual features is rarely optimal. There are methods which can take the full set of features as a basis for selection, in particular <a href=""https://medium.com/analytics-vidhya/the-surprisingly-effective-genetic-approach-to-feature-selection-7eb2b080b713"" rel=""nofollow noreferrer"">genetic feature selection</a>.</p>
","0","2","64377","14816"
"86382","<p>Evaluating any task consists in defining the task formally so that there is a way to define what is a correct output as objectively as possible. For example a good Machine Translation system produces a good translation if it has the same meaning as the input sentence and is grammatically correct in the target language.</p>
<p>Assuming that this task of preprocessing is formally defined, then the evaluation should measure how &quot;correctly preprocessed&quot; is the output:</p>
<ul>
<li>are the stem and lemma always correct?</li>
<li>are the stop words and only the stop words removed?</li>
<li>Etc.</li>
</ul>
<p>Usually one would build a test set, manually add the correct output and then compare the system output against this gold standard.</p>
<p>However &quot;preprocessing&quot; is generally not considered a task by itself, because by definition it's a step for another task. Importantly, the steps of preprocessing depend on the other task, they are not always the same. For example stop words removal makes sense only for tasks based on distributional semantics, i.e. related to the topic. Preprocessing may also include steps which depend on the volume of data.</p>
","0","2","64377","14816"
"86383","<blockquote>
<p>I want to know how to come up with ground truth(relevancy label) if it's not available?</p>
</blockquote>
<p>There's simply no way to <strong>properly</strong> evaluate a system if nobody knows what the output is supposed to be. However there are ways to work around a lack of annotated data:</p>
<ul>
<li>Ask a panel of annotators to grade the quality of the output on a sample. Disadvantage: if a relevant instance is never predicted, the annotators are unlikely to notice it.</li>
<li>Compare the output to a state of the art system. Disadvantage: the evaluated system can only be as good as the reference system, any error by the reference system is considered correct.</li>
<li>Generate artificial data with an automatic method. Disadvantage: the evaluation relies on the quality of the artificial data, so in theory one has to prove that the artificial data is as good as real data... which is usually harder than actually collecting real data.</li>
</ul>
<blockquote>
<p>In that case, how researchers calculate precision and recall? or how do they generate it?</p>
</blockquote>
<p>They can't. It would be like grading an exam paper without knowing the correct answers.</p>
<p>This is why benchmark datasets are so important for the research community and are published as proper scientific contributions.</p>
","1","2","64377","14816"
"86433","<p>Interesting task :)</p>
<p>I think even with a good amount of training data it will be difficult for a regular NER model to perform well with new books titles and authors:</p>
<ul>
<li>The book may contain persons names which are not authors.</li>
<li>The book titles are difficult to identify as such in general. For example &quot;the Republic&quot; might or might not be about the book, and if the only indication the model can use is the capitalization it's probably going to make some errors.</li>
</ul>
<p>To be clear, I think it could work to some extent but it would probably make quite a lot of errors.</p>
<p>On the other hand you could obtain a database of books, for instance from Wikipedia (there might be better resources), and you could use this in two ways:</p>
<ol>
<li>Directly identify the books/authors in the documents by simple string matching. I would imagine that even if the coverage of the resource is not perfect, this method would easily catch a majority of occurrences.</li>
<li>In case the above method is not sufficient, it provides you with some good training data from which you could train a NER model in order collect titles which don't exist in the database. Note that there might be issues due to the unknown books being labelled as negative in the training data, so ideally you would have to go manually through the training data and annotate the remaining cases.</li>
</ol>
","1","2","64377","14816"
"86468","<p>A simple option would be to use the average number of views per day, this way the value is normalized and independent of how recent the video is. Of course averaging is simplistic, since the views don't follow a linear trend.</p>
<p>A more advanced option would be to model the evolution of views across time (for instance assuming a normal distribution), then estimate the parameters for every particular video and compare the predicted total number of views. This would be more complex due to the difficulty of estimating the parameters from an incomplete distribution.</p>
","2","2","64377","14816"
"86470","<p>There's no way to have a complete summary of a large dataset like this, you have to analyze what can be relevant, decompose into more specific pieces of information and then find the best way to visualize each specific part on its own.</p>
<p>The first thing would be to plot the distribution of this parameter of interest across subjects and/or observations.</p>
<p>If you want to look at the individual level and there are too many values, you can simply pick a random subset (say 100 subjects) and plot these. Then you do it again with a different random subset in order to distinguish real patterns from variations due to chance.</p>
","1","2","64377","14816"
"86472","<blockquote>
<p>Lets say my image is called frame and x, y, w and h are xmin, ymin, xmax and ymax</p>
</blockquote>
<p>You're confusing <span class=""math-container"">$w$</span> with <span class=""math-container"">$xmax$</span> and <span class=""math-container"">$h$</span> with <span class=""math-container"">$ymax$</span>: Usually <span class=""math-container"">$w$</span> is the <strong>width</strong> of the crop whereas <span class=""math-container"">$xmax$</span> is the <strong>horizontal position</strong> of the end of the crop. Similarly <span class=""math-container"">$h$</span> is the height and <span class=""math-container"">$ymax$</span> is the vertical position of the end of the crop.</p>
<p>Logically since <span class=""math-container"">$x$</span> is the (horizontal) start of the crop and <span class=""math-container"">$w$</span> is the width, we can obtain <span class=""math-container"">$xmax$</span> like this: <span class=""math-container"">$xmax=x+w$</span>.</p>
<p>Example: in a 100x100 image, let's say we want to crop a 20x20 square in the centre: <span class=""math-container"">$x=40, y=40, w=20, h=20, xmax=60, ymax=60$</span>.</p>
<p>In the following code:</p>
<blockquote>
<pre><code>frame = frame[y:y + h, x:x + w]
</code></pre>
</blockquote>
<p>the operator <code>:</code> is used to represent a sequence (for instance <code>3:7</code> means <code>3,4,5,6</code>) so <code>y:y + h</code> represents the sequence from <code>y</code> to <code>y+h</code>, i.e. from <span class=""math-container"">$y$</span> to <span class=""math-container"">$ymax$</span>. Same for <code>x+w</code>, so this line would select the part of the array corresponding to the crop.</p>
<p>Your second example is wrong due to the same confusion, the actual code is:</p>
<pre><code>face = frame[startY:endY, startX:endX]
</code></pre>
<p>In this case the author is directly using the end coordinate <code>endY</code> (same as <span class=""math-container"">$ymax$</span>) instead of calculating it as <code>startY+h</code>.</p>
","1","2","64377","14816"
"86478","<p>This is correct, there's no guarantee at all, not even a high probability. As usual It depends on the type of model, the data, the number and distribution of the classes.</p>
<p>However there's of course a higher <em>chance</em> that the instance would be correctly classified. That's why one shouldn't use a test set containing training instances to estimate the performance of the model, since there's a high risk the performance would be overestimated (<a href=""https://en.wikipedia.org/wiki/Leakage_(machine_learning)"" rel=""nofollow noreferrer"">data leakage</a>).</p>
","0","2","64377","14816"
"86480","<p>To be honest I didn't understand the problem very well, but it looks like <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow noreferrer"">genetic learning</a> could be a good direction to study.</p>
<p>Essentially a genetic algorithm works by iteratively trying many combinations of values for some features, evaluating &quot;something&quot; based on the features, selecting the best combinations and starting again with a combination of the best features so far. The goal is to eventually converge to an optimal combination of values.</p>
","0","2","64377","14816"
"86509","<p>A couple ideas:</p>
<ul>
<li>Train a supervised binary classification model with undersampling crash=0 (negative) or oversampling crash=1 (positive). Due to resampling, this model is likely to be biased towards the positive class so it will predict the true negative instances which are similar to positive ones as positive. Normally this is a problem (False Positive) but in this particular case this is what you want. The proportion of resampling will determine the level of similarity to positive cases you want to consider. You could also use a probabilistic model and use the predicted probability.</li>
<li>Train a <a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">one-class classification</a> model using only the positive instances (crash=1), then apply it to the negative instances. The ones predicted as positive are similar to the positive ones.</li>
<li>Clustering: cluster all the instances regardless of their class, then identify the clusters which have the highest proportion of positive instances. The negative instances which are also in these clusters share some similarities to the positive ones.</li>
</ul>
","1","2","64377","14816"
"86534","<p>There's some confusion about different kinds of output and their corresponding evaluation:</p>
<ul>
<li>One can consider the top N results as predicted positive, any result lower than N as predicted negative. In this option one can use binary classification evaluation measure: precision, recall, f1-score would be the standard measures in this case.</li>
<li>One can consider the ratings/scores assigned to the full set of results. In this case there are two options:
<ul>
<li>if the numerical results are comparable, e.g. same kind of rating, then standard regression evaluation measures can be used, for instance RMSE.</li>
<li>if not, then it's still possible to compare the order of the results. <a href=""https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"" rel=""nofollow noreferrer"">Spearman rank correlation</a> is a common evaluation measure in this case.</li>
</ul>
</li>
</ul>
<p>It seems that in your case you could use either the classification or the ranking evaluation measures. Of course, any of these evaluation methods requires gold standard results in order to compare the predictions against them.</p>
","1","2","64377","14816"
"86551","<p>If you have a list of words for every topic you can indeed try to directly measure the similarity of this list against a sentence, but it's likely that a sentence doesn't always contain one of the topic words so it might not work very well.</p>
<p>A more advanced method would be to obtain a semantic representation for every topic (or topic word) from an external corpus. Any large corpus can be used for that, it doesn't have to be related to your input data. The traditional way to do that is to extract a context vector for every target word by counting the co-occurrences of the target in the corpus: for every occurrence of the target word, take for instance a window of 5 words to its left and 5 words to its right. Then count how many times every context word appears in the window of the target across the whole corpus. This way the final context vector contains the distribution of the context words, i.e. a representation of the meaning of the target word. Comparing this context vector against a sentence is likely to produce a more accurate semantic similarity score.</p>
<p>There are many variants about the exact definition of the context vector: usually stop words are removed but one could also use TF-IDF and/or other kinds of normalization. The more modern version of this method is probably to use word embeddings, but I'm not knowledgeable enough about this.</p>
","1","2","64377","14816"
"86564","<p>To make sure everything is clear let me quickly summarize what we are talking about. precision and recall are evaluation measures for binary classification, in which every instance has a ground truth class (also called gold standard class, I'll call it 'gold') and a predicted class, both being either positive or negative (note that it's important to clearly define which one is the positive one). Therefore there are four possibilities for every instance:</p>
<ul>
<li>gold positive and predicted positive -&gt; TP</li>
<li>gold positive and predicted negative -&gt; FN (also called type II errors)</li>
<li>gold negative and predicted positive -&gt; FP (also called type I errors)</li>
<li>gold negative and predicted negative -&gt; TN</li>
</ul>
<p><span class=""math-container"">$$Precision=\frac{TP}{TP+FP}\ \ \ Recall=\frac{TP}{TP+FN}$$</span></p>
<p>In case it helps, I think a figure such as the one on the <a href=""https://en.wikipedia.org/wiki/Precision_and_recall"" rel=""nofollow noreferrer"">Wikipedia Precision and Recall page</a> summarizes these concepts quite well.</p>
<p>About your questions:</p>
<blockquote>
<ol>
<li>if avoiding false-positives matter the most to me, i should be measuring precision; And if avoiding false-negatives matters the most to me, i should be measuring recall. Is my understanding correct?</li>
</ol>
</blockquote>
<p>Correct.</p>
<blockquote>
<ol start=""2"">
<li>Suppose, I am predicting if a patient should be given a vaccine, that when given to healthy person is catastrophic and hence should only be given to an affected person; and I can't afford giving vaccine to healthy people. assuming positive stands for should-give-vaccine and negative is should-not-give-vaccine, should I be measuring Precision? or Recall of my classifier?</li>
</ol>
</blockquote>
<p>Here one wants to avoid giving the vaccine to somebody who doesn't need it, i.e. we need to avoid predicting a positive for a gold negative instance. Since we want to avoid FP errors at all cost, we must have a very high precision -&gt; precision should be used.</p>
<blockquote>
<p>Suppose, I am predicting if an email is spam(+ve) or non-spam(-ve). and I can't afford a spam email being classified as non-spam, meaning can't afford false-negatives, should I be measuring Precision? or Recall of my classifier?</p>
</blockquote>
<p>We want to avoid false negative -&gt;  recall should be used.</p>
<p>Note: the choice of the positive class is important, here spam = positive. This is the standard way, but sometimes people confuse &quot;positive&quot; with a positive outcome, i.e. mentally associate positive with non-spam.</p>
<blockquote>
<ol start=""4"">
<li>What does it mean to have high precision(&gt; 0.95) and low recall(&lt; 0.05)? And what does it mean to have low precision(&gt; 0.95) and high recall(&lt; 0.05)?</li>
</ol>
</blockquote>
<p>Let's say you're a classifier in charge of labeling a set of pictures based on whether they contain a dog (positive) or not (negative). You see that some pictures clearly contain a dog so you label them as positive, and some clearly don't so you label them as negative. Now let's assume that for a large majority of pictures you are not sure: maybe the picture is too dark, blurry, there's an animal but it is masked by another object, etc. For these uncertain cases you have two possible strategies:</p>
<ul>
<li>Label them as negative, in other words <strong>favor precision</strong>. Best case scenario, most of them turn out to be negative so you will get both high precision and high recall. But if most of these uncertain cases turn out to be actually positive, then you have a lot of FN errors: your recall will be very low, but your precision will still be very high since you are sure that all/most of the ones you labeled as positive are actually positive.</li>
<li>Label them as positive, in other words <strong>favor recall</strong>. Now in the best case scenario most of them turn out to be positive, so high precision and high recall. But if most of the uncertain cases turn out to be actually negative, then you have a lot of FP errors: your precision will be very low, but your recall will still be very high since you're sure that all/most the true positive are labeled as positive.</li>
</ul>
<p>Side note: it's not really relevant to your question but the example of spam is not very realistic for a case where high recall is important. Typically high recall is important in tasks where the goal is to find all the <em>potential</em> positive cases: for instance a police investigation to find everybody susceptible of being at a certain place at a certain time. Here FP errors don't matter since detectives are going to check afterwards but FN errors could cause missing a potential suspect.</p>
","1","2","64377","14816"
"86579","<p>I don't think there's any standard, but there might be some exceptions in very specific cases where the distribution of the scores is known precisely.</p>
<p>There's no standard because in general the optimal value of the threshold strongly depends on the task and the data. That's why thresholds are usually determined empirically based on the desired outcome. In other words, a threshold can be seen as an hyper-parameter: its optimal value can be found by maximizing the performance of the target task on a training set (or validation set).</p>
","1","2","64377","14816"
"86582","<p>I might misunderstand something but it looks to me like you're trying to find a complex method for a simple problem: if there are many strings which occur multiple times in the list, you should deduplicate the list before comparing all the pairs. You could use a set, but since you will need to count how frequent each string is you should probably directly create a map (dictionary) which stores the frequency for every string (just iterate over the list of strings, then increment the frequency of this string (key) in the map).</p>
<p>Depending how many distinct strings you have, this simple step might be enough to allow you to compare all the pairs of strings efficiently.</p>
<p>Then you could for example decide on a threshold for frequency, for instance keep only the strings which appear at least 10 times. For any string which appear less than 10 times, replace with the frequent string (more than 10 times) which has the highest similarity with it.</p>
","1","2","64377","14816"
"86607","<p>Cosine similarity won't work very well because it's only based on whether the rank at position <span class=""math-container"">$i$</span> is the same in vector 1 and vector 2.</p>
<p>For instance the vectors <code>[3,2,4,1,5]</code> vs <code>[2,3,5,1,4]</code> will have very low similarity because 4 positions out of 5 are different, even though there are only two swaps between (2,3) and (4,5).</p>
<p>A much better way to measure the similarity between two rankings is <a href=""https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"" rel=""nofollow noreferrer"">Spearman Rank Correlation</a>.</p>
<p>Note that (if I'm not mistaken) in this case you could also directly use Pearson correlation, since the numeric values are already ranks. Spearman just ranks the values before applying Pearson correlation. So normally the two will give you the same result.</p>
<p>Note also that this method won't give more weight to the top of the ranking than the bottom. I'm not aware of a measure which does this, other than defining a custom weighted measure.</p>
","1","2","64377","14816"
"86619","<p>If I understand correctly, you noticed that a lot of correct predictions were due to a case which always has the same label and is very frequent in your data, right?</p>
<p>Let me say that I think you had a good reasoning: you analyzed the problem, designed a potential solution and then tried it. This kind of problem is very common, but in this case the solution is not really to delete the repeated cases in your training data.</p>
<p>What happens is that you have imbalanced data, that is you have a class which appears very frequently: when you deleted the very frequent instance you mentioned, your data size went from 50000 to 2000, so this instance represents 96% of your data. This instance has probably almost always the same label, so my guess is that this instance with this class as label represents 85% of your data. At first you thought your classifier was working well since you had 85% accuracy, but it's very easy for the classifier to reach 85% of correct predictions if the most frequent class represents 85% of the data: it just needs to always answer this class.</p>
<p>The first lesson here is that accuracy is not a good evaluation measure for imbalanced data: it's too simple, it can be very high even though the classifier doesn't do anything useful. That's why you should probably look at precision, recall and f1-score by class, and typically at macro f1 score as a global evaluation score.</p>
<p>Second point, I'm going to mention resampling because it's usually the standard answer to imbalanced data: the idea is simply to artificially &quot;re-balance&quot; the distribution of the classes in the data by either:</p>
<ul>
<li>undersampling the frequent class, that is using only a random subset of this class. Note that your idea to delete the frequent instance is actually not so different.</li>
<li>oversampling the other classes, that is repeating the rare instances as many times as necessary to make the data balanced.</li>
</ul>
<p>Importantly, the resampling must be done only on the training set, otherwise the evaluation on the test set is biased.
Resampling is quite easy, but the results are often disappointing. This is because quite often the problem is deeper: the classifier is just not able to distinguish the classes from the information provided by the features.</p>
<p>That brings me to my final point: the fact that you get 10% accuracy after removing the frequent case shows that the classifier doesn't have a clue what it's doing. Most of the time designing a good ML system needs some serious work with the features, <em>especially with text data</em>. The type of model (and parameters) can also play an important role. You didn't explain what kind of model and what kind of features you use, so there's not much I can say about this. Don't hesitate to post a new question with these details, and <em>maybe</em> we can help you improve this model.</p>
","1","2","64377","14816"
"86646","<p>My first thought was that it's difficult to formally define this concept:</p>
<blockquote>
<p>&quot;b&quot; is likely to contain &quot;a&quot; with some &quot;error&quot;</p>
</blockquote>
<ul>
<li>On the one hand, there is the idea that <code>a</code> is a substring of <code>b</code>. This question should have a boolean answer: either it does contain it, or it doesn't.</li>
<li>On the other hand there is the idea of approximate matching: <code>b</code> should contain a substring <code>c</code> which is &quot;similar enough&quot; to <code>a</code>. In general the question of similarity between two strings is answered with a numerical value, usually a real number between 0 and 1.</li>
</ul>
<p>As far as I know the only way to solve this issue is to consider that there is a threshold on the similarity score between <code>a</code> and <code>c</code>, where <code>c</code> is any substring of <code>b</code>. This way the answer becomes boolean.</p>
<p>However there might be a way around this by considering the substring operation as part of the similarity calculation. In particular the <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein edit distance</a> can account for insertions/deletions of characters, which is what a substring is w.r.t the string which contains it.</p>
<p>More interestingly, it is possible to assign a different cost to any particular edit operation in the Levenshtein distance. So it's probably possible to define a variant of Levenshtein where insertions at the beginning or at the end would have cost 0, thus making the final distance <span class=""math-container"">$x$</span> between <code>a</code> and <code>b</code> equivalent to &quot;<code>b</code> contains a substring <code>c</code> which has distance <span class=""math-container"">$x$</span> against <code>a</code>&quot;.</p>
<p>The way I would try to implement this is:</p>
<ol>
<li>Calculate the regular edit distance, keeping the matrix used for the calculation</li>
<li>From the matrix, count how many insertions were done but only at the beginning and at the end, then substract this value from the distance.</li>
</ol>
<p>Note that there might be flaws in my idea, I didn't try it :D</p>
","1","2","64377","14816"
"86673","<p>About the type of problem: apparently here the goal is not to do any kind of supervised learning, it seems to be more a kind of descriptive task.</p>
<p>The first thing I would try to do is to align the two datasets, i.e. merge them based on events in one occuring at the same time and place as in the other one. In order to do that you need to be able to compute for any two events whether they appear in the same area around the same time. I guess you'll need a window, for example defining two events as related if they occur within 1h of each other and are distant by less than 10km. Be careful about noise in the data: the time of a telecom incident might be the time it's reported, not the time it actually happens.</p>
<p>Once you have linked potential related events, you can merge the two datasets into one. You should probably preserve the events which have no match in the other dataset for statistical purposes, so use an <a href=""https://en.wikipedia.org/wiki/Join_(SQL)#Outer_join"" rel=""nofollow noreferrer"">outer join</a>.</p>
<p>At this stage you have an exploitable dataset. I would start by doing plots such as overall probability distribution of related events, probability of related event by location. Probability of related events across time.</p>
","0","2","64377","14816"
"86695","<p>I didn't read the paper in detail but I can see that the class label is used in Algorithm 2 &quot;branch and bound&quot; p. 3: it's not very clear because it's only used through Information Gain (IG). I assume that the IG is calculated between the feature and the class, in order to find the &quot;most discriminative feature&quot;, i.e. the one which gives the most information about the class.</p>
","0","2","64377","14816"
"86734","<blockquote>
<p>I am aware this is incredibly basic, but any input/ recommendations would be much appreciated (e.g., is anything unclear?).</p>
</blockquote>
<p>In general it is strongly recommended to communicate knowledge in the most simple way possible, as long as it's accurate. Yes a line plot is simple but there's nothing wrong with that, in this case I can't think of any better way to convey the observation accurately.</p>
<blockquote>
<p>My main concern is whether I have adequately displayed the data and
whether I can do anything useful to improve (e.g., moving average)?</p>
</blockquote>
<p>It perfectly shows the increasing trend which is your main point:</p>
<ul>
<li>A moving average would not really help in this case: it's useful only when there's too much noise/variation in the data and the general trend is hard to see. Here you apparently have enough data and/or the data is stable enough, so there's simply no need.</li>
<li>You took care of making the plotted value a proportion per 10,000 tokens, and this is also a good idea since an increase in absolute value could be biased: if the total number of abstracts increases, the proportion of abstracts containing the target terms would naturally increase as well even though this doesn't show anything.</li>
</ul>
<p>Overall this looks very good to me. Just a small note, in your description you could mention that the rate by year is an average: &quot;approximately 0.037 units per year <em>in average</em>&quot;.</p>
","0","2","64377","14816"
"86742","<p>[heavily edited: in the first version of this answer I mistakenly interpreted &quot;average precision&quot; as &quot;precision&quot;]</p>
<p>Based on the information shown on the graph the model is clearly overfit even with the maximum number of instances (visible from the  large difference between training and testing performance). This means that the model is too complex given the number of instances. Based on the evolution of the curves, you'd need a lot more instances for the two curves to come close to each other.</p>
<p>You could indeed try resampling, but it's not sure that it would solve the problem: it would likely improve recall but also decrease precision (more false positive errors). .</p>
<p>As often I suspect that the problem is deeper: the features are just not good indicators for the label. I'd suggest working on this issue to see if it can be improved.</p>
","1","2","64377","14816"
"86773","<p>No I don't think this is good: this means that you replace both variables with only an estimated value of C2 based on C1 (or of C1 based on C2, it's not totally clear to me).</p>
<p>Assuming you really want to avoid having two highly correlated features, you could instead:</p>
<ul>
<li>just arbitrarily pick one of them (which is similar to what you're doing but &quot;cleaner&quot;)</li>
<li>simply use the average of the two values (which might be similar to what you intended to do?)</li>
</ul>
<p>Over a whole set of features you could use more advanced methods such as <a href=""https://en.wikipedia.org/wiki/Feature_extraction"" rel=""nofollow noreferrer"">feature extraction</a>.</p>
<p>But it's important to make sure you actually need to do this: correlated features are a problem for some ML methods, but not all of them. For example having correlated features with Decision Trees wouldn't cause any problem. In any case it's worth experimenting with/without the feature in order to observe what actually happens with your data.</p>
","0","2","64377","14816"
"86791","<p>The principle in supervised ML is quite simple: the &quot;method&quot; which is going to be used to predict the response variable must be fully determined from the training set and only from the training set. In other words, anything which doesn't belong to the training set cannot be used.</p>
<p>As a consequence, feature engineering, i.e. choosing how to prepare/represent/normalize features must be done using only the training set. This includes any feature selection/extraction step.</p>
<p>Note that once the final data preparation process is fully determined, it can and should be applied <em>exactly the same way</em> on the test set or in production. This means that for instance normalization does not involve recomputing any parameter, it uses the ones calculated on the training set.</p>
<p>See also a few related questions:</p>
<ul>
<li><a href=""https://datascience.stackexchange.com/q/54908/64377"">Data normalization before or after train-test split?</a></li>
<li><a href=""https://datascience.stackexchange.com/q/82073/64377"">Why you shouldn&#39;t upsample before cross validation</a></li>
<li><a href=""https://datascience.stackexchange.com/questions/86115/using-pca-as-features-for-production/86125#86125"">Using PCA as features for production</a></li>
</ul>
","3","2","64377","14816"
"86796","<p>Supervised ML works on the assumption that the test data follows the same distribution as the training data. This is not always the case in real-world use cases, but it's at least necessary that the test data is &quot;mostly similar&quot; to the training data.</p>
<p>As a consequence, a BoW model can only be applied to data which uses mostly the same vocabulary, with a mostly similar distribution over the words. It is true that out of vocabulary (OOV) words frequently appear in the test data, because words in natural languages follow a Zipf distribution so there are many words which occur rarely. The general assumption in BoW ML models is that since these words occur rarely, they can reasonably be ignored.</p>
<blockquote>
<ol>
<li>During test time, while creating the term-frequency matrix, what if none of the words from my training BoW are found in some of my test emails/smses. Then, wouldn't the document vectors be zero vectors for those datapoints. How should I tackle this?</li>
</ol>
</blockquote>
<p>This event is supposed to be unlikely: a sentence usually contains at least a few common words (again due to the Zipf distribution). However this could happen with a very short text message. Anyway there is nothing special to do about it: all the words are ignored, the vector contains only zeros indeed, the model gives a prediction for this vector of features.</p>
<blockquote>
<ol start=""2"">
<li>What if a new word from my test email/sms doesn't exist in BoW?</li>
</ol>
</blockquote>
<p>This is the traditional case of OOV words mentioned above. The most simple (and probably most common) option is to completely ignore the unknown word. With some probabilistic models smoothing can be used to account for the existence of OOV words, but as far as I know this is used only with n-grams.</p>
<blockquote>
<ol start=""3"">
<li>How do I choose my BoW so as to improve my prediction accuracy?</li>
</ol>
</blockquote>
<p>Experimentally: use a validation set in order to evaluate several methods, then select the one which performs best and apply only this one on the test set.</p>
","0","2","64377","14816"
"86806","<p>I have some bad news: your model is doing nothing useful.</p>
<ul>
<li>From the confusion matrix you can see that the model predicts class 0 for 68% of the instances and class 1 for 32%: (1446+1355)/4093=0.68 and (676+616)/4093=0.32.</li>
<li>You can also see that when the true class is 0 the model predicts 68% of the instance as class 0 and 32% as class 1: 1446/2122=68% and 676/2122=32%. Almost same result when the true class is 1: 1355/1971=69% and 616/1971=31%.</li>
</ul>
<p>This means that for any instance (no matter the true class) the model randomly predicts the label: class 0 with 68% chance, class 1 with 32% chance. You can also see this problem from the fact the sum of the top-left to bottom right diagonal (correctly classified cases) is 2062, almost the same as the bottom-left to top-right diagonal (errors) which is 2031. A good model would have a much larger difference between correct cases and incorrect cases.</p>
<p>This is why the accuracy is only 50.4%, barely more than a random baseline with two classes, and this is why the recall for each class corresponds exactly to the arbitrary probability that the model assigns to them.</p>
<blockquote>
<p>should I use recall in my decision for choosing whether to try to predict if the outcome will be a 0 or 1?</p>
</blockquote>
<p>This doesn't make sense: the recall is an evaluation score, it can only be calculated after the predictions have been made.</p>
<blockquote>
<p>Also I had my model using predict_proba and using the second value from the array return which would give me the probability of the test data outcome being a 1, but from this classification report it looks like I should be trying to predict the probability of the outcome being 0 as it is more descriptive when it comes to recall.</p>
</blockquote>
<p>There are at least two misunderstandings here:</p>
<ul>
<li>By itself, the fact that the model predicts class 0 more often than class 1 is neither good or bad. It would be completely wrong to choose to evaluate the performance using only the recall on the most frequent class: it's true that it's a higher value, but it's not at all a good performance indicator. For example, if one uses a model which always predicts class 0 (100% of the time) then by definition this classifier will have recall 100% for class 0, even though this model is pointless.</li>
<li>In binary classification, the probability of predicting the two classes are mathematically bound together: <span class=""math-container"">$p(C=0)+p(C=1)=1$</span>. So choosing to predict the outcome 0 instead of 1 doesn't change anything about what the model does, it would be exactly the same model with the same performance.</li>
</ul>
<p>What I suggest you should try is to work on your features: normally the features should be indicators that the model uses to predict the class, but unfortunately your current features are terrible at their job. Assuming that the features &quot;make sense&quot; for the task, it should be possible to &quot;present them&quot; to the model so that it can actually use them efficiently. This is called feature engineering: it's often the hard part of the job but also the most likely to really make things work :)</p>
","1","2","64377","14816"
"86809","<p>You don't need any ML here: in the question you just gave the full algorithm to find the target output ;) At best a ML system would find just the same method after using a lot of annotated data.</p>
<p>The method is essentially what you said:</p>
<ul>
<li>Group the users by their card number or email (easily done with a map for each), this gives you the cases of &quot;sure&quot; identical users.</li>
<li>Group the users by their name, this gives you the cases of &quot;possible&quot; identical users. For this part you might want to use string similarity measures.</li>
</ul>
","2","2","64377","14816"
"86834","<p>A precision-recall curve (as well as a ROC curve) is made from the different thresholds which can be used to separate predictions between positive and negative instances. These thresholds naturally depend on the possible values predicted by the classifier.</p>
<p>For example, if the classifier predicts only 3 different values <span class=""math-container"">$a&lt;b&lt;c$</span>, then the threshold <span class=""math-container"">$t$</span> can only be in these positions:</p>
<ul>
<li><span class=""math-container"">$t&lt;a$</span></li>
<li><span class=""math-container"">$a&lt;t&lt;b$</span></li>
<li><span class=""math-container"">$b&lt;t&lt;c$</span></li>
<li><span class=""math-container"">$c&lt;t$</span></li>
</ul>
<p>Each of these positions will correspond to one pair of values for precision and recall, so exactly one point in the curve. Therefore the shape of the curve (how smooth it is) depends on how many <em>different</em> values the classifier predicts.</p>
<p>Thus a stairs-shaped curve shows that the classifier predicts only a few different values. There can be various reasons for that, but since you mention ensemble learning I guess that the final predicted value is based on the proportion of individual learners predicting the positive class, right? If yes that would explain it: if there are <span class=""math-container"">$N$</span> individual learners then the final predicted value can only be <span class=""math-container"">$0,1/N,2/N,...,N$</span>. This means a maximum of <span class=""math-container"">$N+1$</span> different values and therefore a maximum of <span class=""math-container"">$N+1$</span> points on the curve.</p>
","1","2","64377","14816"
"86839","<p>I propose to look at this a bit differently. The different classification status of the two classes are related as follows:</p>
<ul>
<li><span class=""math-container"">$TP_1=TN_0$</span></li>
<li><span class=""math-container"">$FP_1=FN_0$</span></li>
<li><span class=""math-container"">$FN_1=FP_0$</span></li>
<li><span class=""math-container"">$TN_1=TP_0$</span></li>
</ul>
<p>From there we have:</p>
<p><span class=""math-container"">$$FPR_0=\frac{FP_0}{FP_0+TN_0}=\frac{FN_1}{FN_1+TP_1}=1-\frac{TP_1}{FN_1+TP_1}=1-TPR_1$$</span></p>
<p>and naturally <span class=""math-container"">$FPR_1=1-TPR_0$</span> for the same reasons. Therefore when one switches the labels on a ROC curve, one obtains a mirror image of the curve using the top-left to bottom-right diagonal as the symmetry axis. It follows that the AUC is identical.</p>
<p>However this does not hold with a PR curve, because there is no such symmetry between precision and recall of the two classes. A counter-example should be enough to prove this:</p>
<p>Suppose a test set with 100 instances, 90/10 of which have true class 0/1. Let's assume the classifier predicts all the instances as class 0 except 2 instances as class 1, one true 0 and one true 1.</p>
<p><span class=""math-container"">$$
\begin{array}{|c|c|c|}
\hline
&amp;predicted~0 &amp; predicted~1 \\
\hline
\hline
true~0 &amp; 89 &amp; 1 \\
\hline
true~1 &amp; 9 &amp; 1 \\
\hline
\end{array}
$$</span></p>
<p>Thus we have:</p>
<p><span class=""math-container"">$P_0=0.92, R_0=0.99, P_1=0.5, R_1=0.1$</span></p>
<p><em>[edited]</em> Assuming there is only this point on the PR curve and the two following extreme cases:</p>
<ul>
<li>Predict every instance as class 0: <span class=""math-container"">$P_0=0.9, R_0=1, P_1=NaN, R_1=0$</span></li>
<li>Predict every instance as class 1: <span class=""math-container"">$P_0=NaN, R_0=0, P_1=0.1, R_1=1$</span></li>
</ul>
<p>Note: I'm not totally sure about the proper way to deal with <span class=""math-container"">$P=NaN$</span> but I think it's ok to assume P=1 in order to put these points on the curve (this is not essential to the point anyway).</p>
<p>It's easy to see that the AUC is very different whether the focus class is 0 or 1:</p>
<ul>
<li>In the former case the curve is entirely in the top part of the graph and the area is almost the maximum.</li>
<li>In the latter the curve is mostly in the bottom part of the graph and the area is less than 0.5.</li>
</ul>
","2","2","64377","14816"
"86850","<p>Neural Networks are notoriously good at performance and bad at interpretability, i.e. it's very difficult (almost impossible) to explain why a particular prediction was made. It's even more difficult to link the prediction with the features, since the NN does a lot of intermediate calculations where all the features play a role.</p>
<p>The notoriously good models for interpretability are the simple ones: a decision tree is very intuitive to interpret, assuming the tree is not too large. A human can actually follow the tree based on the conditions on the features.</p>
","1","2","64377","14816"
"86903","<blockquote>
<p>Is there a fundamental difference from creating a model for each value in a category?</p>
</blockquote>
<p>Yes there is.</p>
<p>If a model is trained for each specific value of a variable (a category), then only the subset of data for this category can be used to train and test the model. As a consequence each model has a smaller number of instances to be trained from. Consequences:</p>
<ul>
<li>In the case of a small category, there might not be enough instances to obtain a reliable model.</li>
<li>Every model is independent. This can be good or bad depending on whether this independence is also true in the data or not, or to what extent:
<ul>
<li>If the features behave in a completely different way depending on the category, then it's better to create individual models since each can really exploit the specific patterns for this category.</li>
<li>If the features have a very similar behavior across the categories, then independent models by category would potentially lose a lot of information.</li>
</ul>
</li>
</ul>
<p>In conclusion the choice often depends on:</p>
<ul>
<li>How much data is available for each category.</li>
<li>How independent are the other features with respect to the category.</li>
</ul>
","1","2","64377","14816"
"86936","<p>Yes absolutely, in fact that's exactly how one obtains actual clusters with <a href=""https://en.wikipedia.org/wiki/Hierarchical_clustering"" rel=""nofollow noreferrer"">hierarchical clustering</a>.</p>
<p>With hierarchical clustering it's possible to decide the number of clusters (i.e. the level of &quot;precision&quot;) after obtaining the tree (dendogram).</p>
","0","2","64377","14816"
"86956","<p><em>Disclaimer: This answer describes the thoughts I had about this problem, I don't offer any guarantee about their validity so use at your own risks ;)</em></p>
<p>There are two distinct parts in this problem:</p>
<ul>
<li>Finding an optimal threshold over three classes</li>
<li>Improving performance with respect to the minority class</li>
</ul>
<p>First part: I might be wrong but as far as I know there's no way to select a particular probability threshold when there are three classes. And if there were a way, it probably wouldn't be from the ROC plot since the curves for the different plots are independent, so picking a point on the curve would correspond to different thresholds for different classes and I don't see how this would be manageable. The only way that I know is what you did: label with the class which has the maximum probability.</p>
<p>Second part: if you want to force the model to take care of all the classes, you could also try optimizing with macro-AUC, but that would probably lead to the same problem of decreasing the micro-AUC since there would be more errors of true neutral or high predicted as low.</p>
<p>The way I see it, the three way model doesn't work well: the imbalance of the minority class is not that bad, it's only 2 or 3 times less than the other classes. So the fact that the model almost completely dismisses this class is a bit strange. I also see that instances of the true class low are almost as often predicted as class high than class neutral, even though I would expect that the vast majority of errors on class low to be predicted as neutral.</p>
<p>So my vague intuition is that maybe the system could be designed in a way which avoids the three-way problem for the model. I can think of two options in this perspective:</p>
<ul>
<li>A two-steps system where the first model classifies between low and neutral+high then the second one between neutral and high (or first with low+neutral vs. high and then low vs. neutral). This way each model is binary classification so you have more control over the thresholds at each step. Normally this is not recommended in classification, but here I assume that the classes are not truly categorical.</li>
<li>Pushing the same idea further: there seems to be an order between the classes low &lt; neutral &lt; high, so maybe it would be possible to treat the task as a regression problem. This might help the model avoiding these &quot;big&quot; errors between low and high. Importantly you would also have flexibility with the predicted values: there would be two thresholds to determine and these could be found to optimize any appropriate evaluation measure.</li>
</ul>
","1","2","64377","14816"
"86967","<p>From a statistical point of view this is impossible if one doesn't have any data at the fine-grained level. Any statistical inference must be based on a sample from which specific patterns can be observed.</p>
<p>If there is no data at the fine-grained level, any calculation is based on assumptions. For example one may assume that a variable is proportional to population (linear relation). But why not assume that the variable is a polynomial function of the temperature? Or that it is related to the prevalence of a particular gene? The main issue is that without any data there's no way to test any of these assumptions, so no there can be not reliable conclusion.</p>
","0","2","64377","14816"
"86968","<p>Rule of thumb: if a human with a lot of time wouldn't be able to do it, there's little chance a machine could. In this case if you ask somebody to classify whether a place is a park or not, they just can't do it correctly if the place name doesn't contain any indication that it is a park.</p>
<p>In a case like this you would need to use external resources in order to add indications about the place. For example you could search the place name in Wikipedia, and if Wikipedia says that it's a park then you add a boolean feature &quot;true&quot; for the place.</p>
<p>More generally I suspect that your approach is not very solid: you looked at a few results and noticed that your matching system makes mistakes about parks. If you look at more results there's a good chance that you will find more mistakes about various kinds of cases. Fuzzy matching works only when the strings are quite similar, so it's unavoidable that the system will make mistakes one way or the other. Even if you try to fix every kind of mistake one by one with some ad hoc exception, the system will never be completely correct. Importantly, this would make the system completely specific to the dataset, so it wouldn't generalize well to new instances.</p>
<p>This is an issue related to task specification and design: the automatic system can only achieve a certain level of quality, so maybe the task should be framed differently. For example, the system could be in charge of finding <em>candidate</em> pairs for matching, and a human expert would annotate the actual matches among the candidates. Or you could just decide to live with the errors, if they are not critical for the application.</p>
","1","2","64377","14816"
"87019","<p>In general, the max depth parameter should be kept at a low value in order to avoid overfitting: if the tree is deep it means that the model creates more rules at a more detailed level using fewer instances. Very often some of these rules are due to chance, i.e. they don't correspond to a real pattern in the data.</p>
<p>Overfitting is visible in your graphs from the quite high difference between training and test set performance. One can observe that the performance on the test set increases with the first few values of depth (I'm guessing until around 5 or so), and it starts decreasing after that. So the optimal point (performance and no overfitting) is the point that you mention, it's after this point that the model is overfit.</p>
<p>The stable part of the graph is probably due to another parameter (e.g. min. number of instance per leaf) which stops the model from overfitting even more. If the model was free to overfit as much as it wants, it would probably reach max performance on the training set and very low performance on the test set.</p>
<p>Note: I'm confused why you're using both accuracy and MAE, normally the former is for classification and the latter for regression.</p>
","1","2","64377","14816"
"87024","<p>No: a CMS, like many other software tools, processes data. But this process is predefined, it's not &quot;learnt&quot; from the data.</p>
<p>Machine Learning is supposed to involve at least one step where the system &quot;learns&quot; from the data, i.e. uses some algorithmic process to determine the characteristics (patterns) inside the data. In supervised learning there is a specific training stage where the model &quot;learns&quot;, which is followed by a testing stage where the model is applied to some new data. In unsupervised learning, learning and applying happen in a single stage: the model discovers the patterns and directly produces an output based on these pattern.</p>
","1","2","64377","14816"
"87025","<p>The difficulty here is the design: if one just represents every &quot;class&quot; as an instance with a vector of its co-occurrences with other &quot;classes&quot;, then the clustering will only group the full group of instances/classes by how similar they are with other groups.</p>
<p>Instead I think that an instance should represent a pair of &quot;classes&quot;, using the <a href=""https://en.wikipedia.org/wiki/Pointwise_mutual_information"" rel=""nofollow noreferrer"">PMI</a> as a measure of how strongly they are associated.</p>
<p>An advanced option would be to represent each &quot;class&quot; as a node in a graph and add an edge between any two classes which occur together, weighted by their PMI value. Then use a graph-clustering algorithm to group the &quot;classes&quot;.</p>
","0","2","64377","14816"
"87055","<p>I'm no expert with signal processing but my first attempt to solve the complexity issue would be to downsample the time series. For instance if you resample a 10k points series to say 100 points, methods such as DTW can be applied to get a first approximation of the &quot;global&quot; similarity between 2 signals. This would take much less time than comparing the full time series, and then based on the result (for instance using a threshold) only the pairs of signals with high similarity would be compared fully. In other words, the first step acts as a filter so that only a subset of the pairs need a detailed comparison.</p>
","1","2","64377","14816"
"87068","<p>This is usually done by carefully choosing two things:</p>
<ul>
<li>The sentence representation. Word count is the most simple option but there can be many others: TFIDF weights, with/without removing stop words, with/without lemmatization, etc. In a DL approach the sentence would be represented as a sentence embedding.</li>
<li>The similarity measure between two sentences. Again there are many options, in BoW approaches the standard ones would include counting words in common (e.g. Jaccard) and cosine TFIDF.</li>
</ul>
<p>So the answer is: it depends on the similarity score. A complex similarity score like cosine TFIDF rarely produces ties so the highest score can be selected. More simple methods give ties, and then the logical answer is to return all the tied sentences.</p>
","1","2","64377","14816"
"87084","<blockquote>
<p>Or is there any algorithm/function that bins continuous variables in most practical way.</p>
</blockquote>
<p>Sure there is, but that's the wrong question: the standard method for discretizing a continuous variable consists in splitting the values into equal intervals, that's it. Of course it doesn't guarantee any of the 5 conditions, since these conditions are about additional constraints almost exclusively related to expert knowledge and the specifics of the data.</p>
<p>Note that these conditions can certainly be automated, there's no need for manual verification. There might be some domain-specific packages which do this for you, but there's no reason a standard ML/statistics library would provide methods for every specific problem like this one.</p>
","-1","2","64377","14816"
"87118","<p><em>[edit] See also Carlos' answer, I think it's better than mine.</em></p>
<p>You should use one hot encoding for the categorical features. Replacing categorical values with numerical ones would be a bad idea, because it introduces order between the values and the model would try to find patterns based on this order (e.g. 'x &lt; 4').</p>
<p>If there were really too many different values then it's often a good idea to remove or replace the least common ones, but it doesn't seem to be an issue with this data.</p>
<p>For the record, more than 15 different values is nothing to be afraid of. For example when working with text data it's common to work with thousands of values for a word.</p>
","3","2","64377","14816"
"87176","<p>The idea is to find the documents which are not well represented in the current labeled data. The first point is indeed a bit vague and can probably be interpreted in different ways. My interpretation would be something like this:</p>
<ul>
<li>For every document <span class=""math-container"">$d_u$</span> in the unlabeled data, count the number of words in common with every document <span class=""math-container"">$d_l$</span> in the labeled data. This value is the &quot;match score&quot; between <span class=""math-container"">$d_u$</span> and <span class=""math-container"">$d_l$</span>.
<ul>
<li>Note: I think that this value should be normalized, for example using the <a href=""https://en.wikipedia.org/wiki/Overlap_coefficient"" rel=""nofollow noreferrer"">overlap coefficient</a>. Note that other similarity measures could be used as well, for instance cosine-TFIDF.</li>
</ul>
</li>
<li>As output from the above step, for a single document <span class=""math-container"">$d_u$</span> one obtains a &quot;match score&quot; for every labeled document. The average across the labeled documents gives the &quot;average match&quot; for <span class=""math-container"">$d_u$</span>.</li>
</ul>
","1","2","64377","14816"
"87177","<p>Since there are no predefined topics, the task is unsupervised: the goal is to group tweets which are semantically similar together (as opposed to classification, which requires training a model to predict among specific classes).</p>
<p>The standard unsupervised approach is <a href=""https://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow noreferrer"">topic modelling</a>. In the traditional LDA approach, a topic model groups the documents into clusters and also provides the probability of a word given a topic, so a list of &quot;top words&quot; by topic can be extracted from the model. LDA requires the number of topics as input parameter but <a href=""https://en.wikipedia.org/wiki/Hierarchical_Dirichlet_process"" rel=""nofollow noreferrer"">Hierarchical Dirichlet Processes</a> can be used to avoid this issue (it's less common however).</p>
","1","2","64377","14816"
"87201","<p>[Note: essentially my answer is the same as @ncasas, just an alternative phrasing]</p>
<p>Classification belongs to <a href=""https://en.wikipedia.org/wiki/Supervised_learning"" rel=""nofollow noreferrer"">supervised learning</a> whereas clustering belongs to <a href=""https://en.wikipedia.org/wiki/Unsupervised_learning"" rel=""nofollow noreferrer"">unsupervised learning</a>:</p>
<ul>
<li>In supervised learning there is a training stage during which some instances (examples) are provided together with their answer (the <strong>target</strong>). During training the model &quot;studies&quot; all the examples in the training data (represented with <strong>features</strong>) in order to be able to find the target from the features. After it has been trained, the model can be applied to new instances and <strong>use their features to predict their target</strong>. In short the main characteristics of supervised learning are:
<ul>
<li>The goal is to predict a specific piece of information defined from the start (the target).</li>
<li>It requires some training data: features and answers for a large set of instances.</li>
</ul>
</li>
<li>In unsupervised learning the goal is to <strong>discover the patterns</strong> within the data. There is no predefined target and no training stage (thus no need for annotated data). Unsupervised learning can only do general tasks based on comparing instances, such as <strong>clustering</strong> (grouping similar instances together) or <strong>ranking</strong> (ordering instances relatively to each other).</li>
</ul>
<p>This is the fundamental difference between classification and clustering. Based on this understanding:</p>
<blockquote>
<p>What's the difference between data classification and clustering (from a Data point of view)</p>
</blockquote>
<p>From a strict data point of view, the difference is the requirement for annotated data in classication. There is no such requirement for clustering.</p>
<blockquote>
<p>Is data classification a sub topic of data clustering ?</p>
</blockquote>
<p>No because they belong to different families of ML which have different goals.</p>
<p>Example:</p>
<ul>
<li>In spam classification (supervised task) a model is trained with some documents (usually emails) labelled as spam or not spam. The resulting model can predict whether a new document is spam or not.</li>
<li>In topic modelling (unsupervised task) a model groups semantically similar documents together, based on the words they contain.</li>
</ul>
<p>The first task separates documents into classes, but these classes are predefined: here spam vs. non-spam. The model uses features specifically as indicators for this goal. It would use features in a completely different way if the classes were news vs. entertainment, business vs. personal, or sci-fi vs. romance. Hence the term <strong>supervised learning</strong>: the model focuses on what it is told (trained) to focus on.</p>
<p>Topic modelling separates documents into several clusters, but even if we assume exactly two clusters these are extremely unlikely to correspond to spam vs. non-spam (or news vs. entertainment, etc.). A clustering algorithm follows a neutral similarity method which uses the features indiscriminately. The main outcome are the clusters themselves, which represent unknown patterns in the data. For example applying topic modelling in a large collection of documents may lead to discover what are the main categories of documents: the new knowledge is the existence of these groups. Clustering is <strong>unsupervised</strong> because it doesn't follow a predetermined goal.</p>
","1","2","64377","14816"
"87235","<p>I'm not particularly expert in this but I'm quite sure that the variations in the price of anything depend mostly on external factors: news of the day, economic indicators, stockmarket movements, etc.</p>
<p>As a rule of thumb, if a human with a lot of time can't do it, usually a ML model can't do it either. In this case if an expert in finance is given the history of the price but no access to any other information I doubt they can predict anything other than a conservative guess that the price will stay the same.</p>
<p>I don't see how the model can do anything better than this without some good indicators as features, and probably it would need a large number of complex indicators. For example I would assume that news like <a href=""https://themerkle.com/top-4-bitcoin-exchanges-which-disappeared-under-suspicious-circumstances/"" rel=""nofollow noreferrer"">these</a> affect the Bitcoin price, but gathering this kind of information and integrating it into a model certainly requires a lot of work.</p>
","1","2","64377","14816"
"87267","<blockquote>
<p>Is it possible to create a predictive model for a dataset that consists of only positive occurrences of the dependent variable?</p>
</blockquote>
<p><a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">One-class classification</a> is a type of classification algorithm which does exactly that.</p>
<p>In one-class classification the principle is to discover the patterns which characterize the instances of the class, assuming that everything which doesn't follow these patterns doesn't belong to the class. The model is trained using only examples from the class, and when applied the model predicts a probability that the input instance belongs to the class. By putting a threshold on the probability the model can be used as a binary classifier.</p>
<p>In terms of methods/implementations, I know <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html"" rel=""nofollow noreferrer"">one-class SVM</a> but there are probably other methods as well.</p>
","0","2","64377","14816"
"87269","<p>I'm not aware of any standard options to do that, but I could suggest a couple directions. Note that in both ideas it's possible to compare topics individually or globally.</p>
<h3>Applying evaluation measures on the instances</h3>
<p>In this idea you would apply the model obtained from dataset A on all the instances of dataset B (and of course you can do the converse as well). This would give you a prediction of topic for every instance in B (typically using the max probability topic for the instance) from both model A and model B.</p>
<p>Then you would need a method to match the topics between A and B, i.e. convert topics from A to B. This can be done using the number of instances they have in common, I know that there is some literature and several methods which have been used in supervised evaluation measures for word sense induction, I would guess that there are similar things for topic modeling (not sure). Once the matching is done, various evaluation measures can be used to compare the topics. The idea is to consider the topics obtained from A as the predictions to be compared against the gold-standard topics from B. Note that most measures are symmetrical, so it doesn't matter which one is considered the prediction or the gold.</p>
<h3>Comparing words probabilities</h3>
<p>I'm not familiar with top2vec but usually it's possible to obtain the top words for every topic together with their associated probability. Assuming this is possible here, the idea would be to compare either:</p>
<ul>
<li>The top N words for every pair of topics (every topic from A against every topic from B). This can be done very simply by counting the number of words in common between the two top N words: the more the topics are similar, the more they have words in common in their top N.</li>
<li>the whole probability distribution of the words for every pair of topics, using a distance measure such as <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""nofollow noreferrer"">KL divergence</a> (there are other options). This value will represent how closely related two topics are.</li>
</ul>
","0","2","64377","14816"
"87315","<p>What you're doing is manual feature selection based on the test set. You're right that it's not correct to proceed this way: in theory, feature selection should be done using only the training set and a validation set, not the final test set. The risk is data leakage: you're modifying the model using information from the test set. Maybe the performance is better without these features on the test set because they happen to be bad for this particular current test set by chance. As a result the model could be overfit, and you wouldn't be able to detect this problem on this test set since it's the source of the overfitting.</p>
<p>So in principle it's always better to separate the data first, keep the test set aside until the final evaluation and use a validation set for intermediate evaluation until the final model (including the set of features) is determined.</p>
<p>In practice, it sometimes happens that we realize that we should have done something differently after applying the model on the final test set. It's a mistake but it's not the end of the world, usually the risk of bias is low. As you said, obviously we don't have to ignore an information which is important for the performance of the model. However if you know that you're going to repeat this procedure with several features, you should defintely do it using a separate validation set (taken from the training data), not the test set: the more you use the test set like this, the higher the risk of data leakage and bias.</p>
","5","2","64377","14816"
"87336","<p>As far as I'm aware there is no correct/standard way to apply topic modelling, most decisions depend on the specifics of the case. So below I just give my opinion about these points:</p>
<blockquote>
<ol>
<li>I have removed, before cleaning the data (removing mentions, stopwords, weird characters, numbers etc), all duplicate instances (having all three columns in common), in order to avoid them influencing the results of topic modelling. Is this right?</li>
<li>Should I, for the same reason mentioned before, remove also all retweets?</li>
</ol>
</blockquote>
<p>In general there is no strict need to deduplicate the data, doing it or not would depend on the goal. Duplicate documents would affect the proportion of the words which appear in these documents, and in turn the probability of the topic these documents are assigned to. If you want the model to integrate the notion of popularity/prominence of tweets/words/topics, it would probably make sense not to deduplicate and keep retweets. However if there is large amount of duplicates/retweets the imbalance might cause less frequent tweets/words to be less visible, possibly causing less diverse topics (the smallest topics might get merged together for instance).</p>
<blockquote>
<ol start=""3"">
<li>Until now, I thought about classifing using the &quot;per-document-per-topic&quot; probability. If I get rid of so many instances, do I have to classify them based on the &quot;per-word-per-topic&quot; probability?</li>
</ol>
</blockquote>
<p>I'm not sure what is called the &quot;per-document-per-topic&quot; probability in this package. The typical way to use LDA in order to cluster the documents is to use the posterior probability of topic given document (this might be the same thing, I'm not sure): for any document <span class=""math-container"">$d$</span>, the model can provide the conditional probability of every topic <span class=""math-container"">$t$</span> given <span class=""math-container"">$d$</span>. The sum of this value across topics sums to 1 (it's a distribution over topics for <span class=""math-container"">$d$</span>), and for classification purposes one can just select the topic which has the highest probability given <span class=""math-container"">$d$</span>.</p>
<blockquote>
<ol start=""4"">
<li>Do I have to divide the dataset into testing and training? I thought that is a thing only in supervised training, since I cannot really use the testing dataset to measure quality of classification.</li>
</ol>
</blockquote>
<p>You're right, you don't need to split into training and test set since this is unsupervised learning.</p>
<blockquote>
<ol start=""5"">
<li>Antoher goal would be to classify twitterers based the topic they most are passionate about. Do you have any idea about how to implement this?</li>
</ol>
</blockquote>
<p>The model gives you the posterior probability distribution over topics for every tweet. From these values I think you can obtain a similar distribution over topics for every tweeter, simply by marginalizing over the tweets by this author <span class=""math-container"">$a$</span>: if I'm not mistaken, this probability <span class=""math-container"">$p(t|a)$</span> can be obtained simply by calculating the mean of <span class=""math-container"">$p(t|d)$</span> across all the documents/tweets <span class=""math-container"">$d$</span> by author <span class=""math-container"">$a$</span>.</p>
","1","2","64377","14816"
"87363","<p>The explanation follows from understanding what a ROC curve is made of.</p>
<p>First, a reminder: a ROC curve represents the performance of a soft binary classifier, i.e. a classifier which predicts a numerical value (usually a probability) which represents the likelihood of the instance to be positive.</p>
<p>The points of a ROC curves correspond to what happens when splitting the instances at different thresholds of their predicted value/probability. For example let's say we have 10 instances with the following predictions and status, ordered by decreasing predicted value:</p>
<pre><code>predicted value   gold status
  1.0                  P
  0.95                 P
  0.95                 P
  0.85                 N
  0.70                 P
  0.55                 P
  0.52                 N
  0.47                 P
  0.26                 N
  0.14                 N
</code></pre>
<ul>
<li>If the threshold is set at 0.60, we have 4 TP, 1 FP, 2 FN and 3 TN.</li>
<li>If the threshold is set at 0.75, we have 3 TP, 1 FP, 3 FN and 3 TN.</li>
<li>etc.</li>
</ul>
<p>If the classifier does a decent job, the top part should contain only gold positive instances and the bottom part only negative instances, since the classifier predicts a very high (resp. very low) value for these instances. By contrast the middle part is more likely to contain errors.</p>
<p>This means that with a decent classifier, we encounter only positive instances until a certain threshold from the top: in my example, there are 3 P instances above threshold = 0.85, so any threshold chosen between 0.85 and 1 will have no FP instances. This implies FPR=0 with TPR ranging from 0 to 3/10, hence a vertical line on the ROC curve from the origin to (0,0.3).</p>
<p>Applying the same observation for the lowest values in my example, we see than when the threshold is between 0 and 0.47 (excluded) we have no FN, so we have a perfect TPR=1 and therefore an horizontal line from (1,1) to (0.8, 1).</p>
<p>These two extreme parts of the ranking are what the author calls &quot;most confident predictions&quot; and &quot;least most confident predictions&quot;. Btw the latter is not a very good term because actually the bottom part is where the classifier is very confident about the instance being negative. If anything, the part where the classifier is the least confident is around the middle of the ranking.</p>
<p>[edit: forgot to answer this question]</p>
<blockquote>
<p>What important information can I gain from this?</p>
</blockquote>
<p>So these two straight lines show how far the classifier can go:</p>
<ul>
<li>without making any false positive error (bottom left vertical line)</li>
<li>without making any false negative error (top right horizontal line)</li>
</ul>
<p>It can be useful in applications where one wants to have perfect precision at the cost of recall, or the converse. However in general (optimizing F1-score for instance) the optimal threshold is somewhere in the middle of the curve.</p>
<p>Of course the longer the line, the better the classifier in general. Geometrically, it's clear that when these two lines are long then the area under the curve (AUC) is large. Note that in theory it's possible for a classifier to have a few errors close to the extremes (i.e. short lines) while still performing well in the middle range, but in practice it's quite rare.</p>
","2","2","64377","14816"
"87431","<p>I would start with the simple option: represent every person as a boolean vector in which every position represents the answer to a particular question (the length is the total number of questions). Then you can apply any standard clustering algorithm, such as <a href=""https://en.wikipedia.org/wiki/K-means_clustering"" rel=""nofollow noreferrer"">K-means</a> (<a href=""https://en.wikipedia.org/wiki/Hierarchical_clustering"" rel=""nofollow noreferrer"">hierarchical clustering</a> would probably also work with data like this).</p>
","1","2","64377","14816"
"87433","<p>Based on your explanation, <span class=""math-container"">$z$</span> is not a parameter so your function can be simplified like this:</p>
<p><span class=""math-container"">$$f(x,y,a) = x*y*z(x)*a\text{, where } x,y \in \mathbb{Q}\cap[0,1000], a\in\{2,3\} $$</span></p>
<p>Note that <span class=""math-container"">$z(x)$</span> is a <a href=""https://en.wikipedia.org/wiki/Piecewise"" rel=""nofollow noreferrer"">piecewise function</a>. Also <span class=""math-container"">$a$</span> can have only two values so <span class=""math-container"">$f$</span> can be divided into <span class=""math-container"">$f_2$</span> and <span class=""math-container"">$f_3$</span>, each of these having only two arguments so it simplifies things.</p>
<p>I don't know Tableau or Powerbi so I can't advise about that. I use R and I played a bit with your function. For a global visualization I came up with a kind of heat map like this:</p>
<p><a href=""https://i.stack.imgur.com/VbvoE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VbvoE.png"" alt=""enter image description here"" /></a></p>
<p>In case you're interested, I did it like this with R:</p>
<pre><code>library(ggplot2)

myrandomdata &lt;- function(n) {
  a2 &lt;- data.frame(x=runif(n,0,1000),y=runif(n,0,1000),z=20,a=rep(2,n))
  a3 &lt;- data.frame(x=runif(n,0,1000),y=runif(n,0,1000),z=20,a=rep(3,n))
  df &lt;- rbind(a2,a3)
  df<span class=""math-container"">$z[df$</span>z&lt;=500] &lt;- 10
  df<span class=""math-container"">$z[df$</span>z&lt;=200] &lt;- 5
  df<span class=""math-container"">$z[df$</span>z&lt;=100] &lt;- 2
  df<span class=""math-container"">$f &lt;- df$</span>x *df<span class=""math-container"">$y * df$</span>z * df$a
  df
}

df &lt;- myrandomdata(50000)
ggplot(df,aes(x,y,colour=f))+geom_point()+facet_grid(a~.)+scale_color_gradient(low=&quot;blue&quot;, high=&quot;red&quot;)
</code></pre>
<p>(using random sampling is probably not the standard way but it was the easy way)</p>
","1","2","64377","14816"
"87466","<p>As I was playing with this problem, <a href=""https://datascience.stackexchange.com/a/87465/64377"">Djib just wrote an answer</a> which is certainly better than whatever I could have come up with. To illustrate Djib's point, here is a small demonstration that as soon as there are more than 2 classes there's no value of <span class=""math-container"">$k$</span> which guarantees the absence of tie (except if <span class=""math-container"">$k=1$</span> of course).</p>
<p>By definition we have <span class=""math-container"">$k=|c_1| + |c_2| + ... + |c_n|$</span>, where <span class=""math-container"">$|c_i|$</span> is the number of instances for the class <span class=""math-container"">$c_i$</span>. Let <span class=""math-container"">$|c_1|\geq |c_2|$</span> be the number of instances for the top two classes. A tie is when (at least) the top two classes have the same number of instances: <span class=""math-container"">$|c_1| = |c_2|$</span> (it doesn't matter when there is a tie which is not at the top).</p>
<p>It's easy to prove that for any integer <span class=""math-container"">$k&gt;1$</span>, there are always two integers <span class=""math-container"">$a\geq b\geq 0$</span> such that <span class=""math-container"">$2a+b=k$</span>: if <span class=""math-container"">$k$</span> is even, pick <span class=""math-container"">$a=k/2$</span> and <span class=""math-container"">$b=0$</span>. If <span class=""math-container"">$k$</span> is odd, pick the integer part of <span class=""math-container"">$k/2$</span> as <span class=""math-container"">$a$</span> and <span class=""math-container"">$b=1$</span>.</p>
<p>So for any <span class=""math-container"">$k$</span> it's always possible to find <span class=""math-container"">$a\geq b\geq 0$</span> such that <span class=""math-container"">$|c_1|=|c_2|=a$</span> and <span class=""math-container"">$|c_3|=b$</span>. Therefore it's always possible to have a tie at the top of the ranking of at least 3 classes in <span class=""math-container"">$k$</span>-NN, whatever the value of <span class=""math-container"">$k$</span>.</p>
<p>Example with 3 classes and <span class=""math-container"">$k=4$</span> (note that <span class=""math-container"">$k&gt;M$</span>): a tie happens when <span class=""math-container"">$|c_1|=|c_2|=2$</span> and <span class=""math-container"">$|c_3|=0$</span>.</p>
","3","2","64377","14816"
"87493","<p>The standard method to detect overfitting is to evaluate the model on both the training set and the test set. If the performance obtained on the training set is very high but much lower on the test set, then it's likely that the model is overfit.</p>
<p>Naturally this method works only with a proper test set: if there has been some data leakage from the test set to the training set, then the evaluation on the test is biased (the performance value is unreliable).</p>
","4","2","64377","14816"
"87503","<p>If I understand correctly, the goal will be for the model to be provided with a value <span class=""math-container"">$n$</span> and predict the optimal <span class=""math-container"">$p$</span> which minimizes <span class=""math-container"">$f(n,p)$</span>.</p>
<p>From this point of view, it looks like a simple <a href=""https://en.wikipedia.org/wiki/Regression_analysis"" rel=""nofollow noreferrer"">regression problem</a>. You could probably train a regression model: for every  point in your training set, the predictor is <span class=""math-container"">$n$</span>, the response variable is the <span class=""math-container"">$p$</span> value which minimizes <span class=""math-container"">$f(n,p)$</span> for this <span class=""math-container"">$n$</span>. Note that all the points which don't correspond to the minimum are irrelevant in this context.</p>
<p>I'd suggest you start by plotting this function from your data (not the one with a fixed <span class=""math-container"">$n$</span>, the one which maps <span class=""math-container"">$n$</span> to the optimal value of <span class=""math-container"">$p$</span>). It would be useful  to know what it looks like, in particular to choose an appropriate regression method.</p>
<p><em>[edit] About dealing with noise.</em></p>
<p>The standard statistical way to deal with noise is to do nothing at all :)</p>
<p>How much data do you have? Because if you have enough data, statistically the noise should balance itself out. For instance if you have 100 such graphs for a fixed <span class=""math-container"">$n$</span>, the mean optimal <span class=""math-container"">$p$</span> should be close enough to the true optimal <span class=""math-container"">$p$</span>. Even if for a few values of <span class=""math-container"">$n$</span> the data doesn't contain the optimal value of <span class=""math-container"">$p$</span>, across sufficiently many values of <span class=""math-container"">$n$</span> this noise is unlikely to be significant. What matters is for the regression model to correctly represent the general trend, and not to capture the small variations due to noise.</p>
<p>I would start with a graph made of one boxplot for each value of <span class=""math-container"">$n$</span> and varying <span class=""math-container"">$n$</span>, where the set of values represented in a boxplot are all the mnimum <span class=""math-container"">$p$</span> in the data for this value of <span class=""math-container"">$n$</span>.</p>
<p>[edit 2] Another interesting visualization: you can represent your full data as a heatmap with <span class=""math-container"">$n$</span>/<span class=""math-container"">$p$</span> on the X/Y axis and the colour based on the value of <span class=""math-container"">$f(n,p$</span>)$.</p>
","0","2","64377","14816"
"87543","<p>It would be a bad idea to use one hot encoding because this would make the problem <a href=""https://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow noreferrer"">multi-label</a>. This would make the problem much more complex for the model and would very likely lead to lower performance, or it would require much more data in order to reach the same performance as regular classification. This is because there are much more combinations possible in the multi-label problem: every instance can have any number of classes, from zero to all of them. Your training data would not have these cases, but the algorithm would assume that they can happen. So in your case this higher level of complexity is pointless since there can be only one class.</p>
<p>With scikit the standard way to encode the target in a multiclass task is to use the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""nofollow noreferrer"">LabelEncoder</a>.</p>
","0","2","64377","14816"
"87563","<p>If a feature is truly constant across all the instances, then it's useless for classification and it can be removed.</p>
<p>But in any other case, as in your example, a feature can be useful even if constant with most instances. In your example it might turn out that the target class is strongly correlated with type_of_residence, so it would be a mistake to get rid of it since your model can take advantage of it.</p>
<p>Unless you have a good reason to get rid of a feature (expert knowledge), keep all of them. If you must reduce the number of features (for instance to avoid overfitting), then the selection should be based on information about which features are the least informative (e.g. using some correlation measure).</p>
<p>Ideally the best way to know which features are useful is to try to train a classifier and evaluate it with every possible subset of features (but it's rarely possible in practice)</p>
","1","2","64377","14816"
"87566","<p>In general the point of reference is the state of the art: very often there have been people who have built similar datasets in the past, possibly in a different domain or with a different application in mind. Their work (typically published academic papers and/or code) can be used as a baseline: how did they proceed, which problems did they deal with and how, were there any flaws found later with the data, etc.</p>
<p>When the decisions made during the process are supported by the state of the art, you have more arguments to defend the quality of your dataset. Of course it's not a guarantee, but it's a kind of insurance: if it turns out that there's a flaw in your dataset, it can't be held against you unless there was a way to foresee the issue based on similar works.</p>
","1","2","64377","14816"
"87676","<p>Clustering would indeed give you something like group1, group2, etc., i.e. it would assign every instance to a group meant to represent similar instances together. With the model you could also assign any new instance to one of the groups in the model, i.e. find the group that this instance is the most similar to. In general clustering doesn't give you which predictors are the most important for a group (there might be ways to calculate this from the model, but afaik it's not standard).</p>
<p>Note that with clustering alone you can't predict the target variable for a new subject. However you could obtain the proportion of positive/negative for every group, i.e. the chances of being positive/negative knowing that the instance belongs to a particular group. You could also do the opposite: chances of belonging to a particular group knowing whether the instance is positive/negative.</p>
<p>An important question is whether you want to include the target variable in the features used for clustering:</p>
<ul>
<li>If it's included, the model uses it like any other feature to group instances by their similarity.</li>
<li>If it's not included, the model itself doesn't know it so the proportion of positive/negative in a cluster doesn't depend on it. This might be more insightful, depending on what you want to obtain. Another advantage of not including it is that you can still use the outcome of the clustering for training a binary classification model afterwards, typically by adding the cluster an instance belongs to as a feature.</li>
</ul>
<p>As you can see, there are a few important design decisions to make depending on the goal.</p>
<p><em>[edit: answer to comment]</em></p>
<p>In order to use the clustering as a basis for classifying any new instance, here are a few general options:</p>
<ul>
<li>Basic option: predict the cluster the instance belongs to, then just output the probability of positive of the cluster.</li>
<li>Clustering model + classification model: train a classifier with all the training instances, with one additional categorical feature corresponding to the cluster the instance belongs to. The classification might ge improved compared to only the original features.
<ul>
<li>Variant: add one feature for every possible cluster, where the value represents how similar the instance is to each cluster (most clustering methods can calculate a distance or similarity against any cluster).</li>
</ul>
</li>
<li>Multiple independent classification models: based on the clustering, train a classification model for every cluster independently, using only the instances from this cluster. Advantage: this allows the classification model to take into account more fine-grained patterns within this cluster. Disadvantage: the model has less instances to be trained from, especially for small clusters.</li>
</ul>
","2","2","64377","14816"
"87684","<p>From a quick look, my guess would be that maybe they mine data from some predefined target websites, typically social media: twitter, facebook, etc. I could imagine that they capture trends in the sense of &quot;what people are talking about&quot; on social media platforms, but maybe they have other sources and methods.</p>
<p>There are Natural Language Processing methods which can be used: the basis would be some form of topic modelling, then there could be some more specific applications such as event detection. We can't know how much manual post-processing is involved though, there could be a manual filtering to remove irrelevant stuff.</p>
","0","2","64377","14816"
"87699","<p>This is a <a href=""https://en.wikipedia.org/wiki/Chord_diagram"" rel=""nofollow noreferrer"">chord diagram</a>. It is usuallly generated from a square matrix like the one you have. This can be done with specific libraries at least in R (<a href=""https://www.rdocumentation.org/packages/circlize/versions/0.4.11"" rel=""nofollow noreferrer"">circlize</a> package) and Python (<a href=""https://plotly.com/python/v3/filled-chord-diagram/"" rel=""nofollow noreferrer"">plotly</a>). It's also possible <a href=""https://www.d3-graph-gallery.com/chord.html"" rel=""nofollow noreferrer"">to do it with d3.js</a> but apparently not so easily.</p>
<p>General advice: I find the &quot;X Graph Gallery&quot; websites quite convenient for exploring different types of graphs: <a href=""https://www.r-graph-gallery.com/"" rel=""nofollow noreferrer"">R graph gallery</a>, <a href=""https://python-graph-gallery.com/"" rel=""nofollow noreferrer"">Python graph gallery</a> and <a href=""https://www.d3-graph-gallery.com/"" rel=""nofollow noreferrer"">D3 graph gallery</a>.</p>
","2","2","64377","14816"
"87710","<p>Overfitting is visible when the performance on the training set is much higher than the test set, so you clearly had overfitting with your first experiment already.</p>
<p>Cross-validation is not a way to see overfitting by itself, it's a way to obtain a more reliable performance value by using several splits (removes chance factor) and using an overall larger test set (the full data).</p>
<p>One thing I notice in your experiment is that the <code>max_depth</code> parameter is set to 17 which is very high, especially in case your training set is not too large. This means that you're allowing the model to create very big and complex trees, so there is more chance that it would represent small patterns which happen by chance in the data. Try reducing this parameter value, hopefully this will avoid or at least decrease the overfitting. If it's not enough there might be some work to do at the level of features: too many features and/or not enough instances can cause overfitting as well.</p>
","1","2","64377","14816"
"87738","<p>In this task you're missing something: you don't have any features to represent a specific visitor.</p>
<p>This means that the best movie that your model can predict is the one which is selected the most often by any visitor. As a consequence, the only thing that the model can learn from such a dataset is to associate every possible sequence with the most frequently selected movie given this sequence. Of course it would have to generalize a bit for sequences which don't appear in the training data, but that's the most ML there is in this task. In theory the task can be done just by counting the joint frequency sequence+selected-movie in order to calculate the conditional probability selected-movie given sequence.</p>
<hr />
<p>[edit]</p>
<p>The standard way to represent a set would be one hot encoding. It's doable even with 2000 features, assuming there are enough instances in the data. In this basic option you could certainly reduce the number of features by removing the movies which are never/rarely proposed or never/rarely selected. This would be unlikely to achieve performance, because if there are no or few cases where a movie is proposed/selected, then the model cannot use this information (or if it does it's going to cause overfitting).</p>
<p>I could think of a couple alternative approaches:</p>
<ul>
<li><p>Apply clustering on the sequences of movies as OHE (without the target variable), then train a model with for every cluster: this way there are less features to deal with for every model.</p>
</li>
<li><p>Just rank all the movies by order of preference based on how often the movie is selected. Given that there is no user preference, it can be assumed that there is a total order on the movies:</p>
</li>
<li><p>the top preferred movie is always selected, whatever the other movies proposed</p>
</li>
<li><p>the 2nd top preferred movie is always selected, unless the first one is proposed as well</p>
</li>
<li><p>... the <span class=""math-container"">$n^{th}$</span> top is always selected, unless another one higher in the top is proposed as well</p>
</li>
</ul>
<p>To predict the selected movie, just pick the highest ranked among the 3-5 proposed.</p>
","0","2","64377","14816"
"87747","<p>Actually I think that you are mostly correct, except that <em>in my understanding</em> &quot;with/without replacement&quot; applies only to selecting one subset, not across subsets. This means that if we have a training set of instances <span class=""math-container"">$T=\{t_1,..,t_N\}$</span>:</p>
<ul>
<li>With bagging, a particular sample can contain duplicate instances, in other words it is not a subset of <span class=""math-container"">$T$</span> but a <a href=""https://en.wikipedia.org/wiki/Multiset"" rel=""nofollow noreferrer"">multiset</a> where an instance <span class=""math-container"">$t_i$</span> may occur several times. Of course the probability to pick the same instance <span class=""math-container"">$t_i$</span> <span class=""math-container"">$n$</span> times decreases quickly when <span class=""math-container"">$n$</span> increases, depending on the size <span class=""math-container"">$N$</span>.</li>
<li>With pasting a sample is a subset of <span class=""math-container"">$T$</span> and cannot contain the same instance <span class=""math-container"">$t_i$</span> twice. However another sample is drawn again from the full set <span class=""math-container"">$T$</span>, which means that it is possible for an instance to be selected in several different samples.</li>
</ul>
<p>In theory, the size of a sample can be higher than <span class=""math-container"">$N$</span> with bagging but not with pasting.</p>
<p>Note that I'm referring to multiset for the sake of clarity but formally <span class=""math-container"">$T$</span> itself is not a set since in theory it may contain the same instance twice.</p>
","1","2","64377","14816"
"87771","<p>It is unusual to have such a bad question as an assignement. Maybe it's just misguided phrasing by the question author, but I'm pretty sure that the vast majority of experts in ML consider the idea of &quot;best ML algorithm in general&quot; as a mistake.</p>
<p>First let me explain why this is a bad question: in ML there is no way to guarantee that a particular algorithm/method performs better than any other on any dataset in general. There can be exceptions under very specific constraints, but there is no algorithm which is universally the best for non-linearly separable classes. There is actually a theoretical result called the <a href=""https://en.wikipedia.org/wiki/No_free_lunch_theorem"" rel=""nofollow noreferrer"">No Free Lunch Theorem</a> which is often interpreted as a proof that there cannot be a universally best classification/regression algorithm.</p>
<p>Now that the context of this bad question is clearly established, as a subjective personal opinion my answer would be based on interpreting the question as: which algorithm is the least sensitive to linear separability? In this context I would say kNN because this method is not concerned at all about linear separability: a new instance is classified based on its closest instances in the training data, so it does not rely on separating the data points into groups at all.</p>
","1","2","64377","14816"
"87824","<p>This looks quite similar to <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition (NER)</a>, which is traditionally done with a sequence labeling model such as <a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">Conditional Random Fields</a>. Normally NER is used when:</p>
<ul>
<li>The list of possible entities is not predefined: the training data might contain &quot;Mr James Smith&quot; but the test data could contain &quot;Mr John Doe&quot;. In other words, the classes are open.</li>
<li>It is assumed that the context of the text can help the model predict an entity. For example in a sentence like &quot;Today X said that ...&quot;, the word &quot;said&quot; after X should help the model predict that X is either a person or an organization, but it cannot be a location.</li>
</ul>
","2","2","64377","14816"
"87826","<p>It's true that the nltk book doesn't seem clear about this. Traditionally NER models are trained with <a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">Conditional Random Fields</a> so I searched for &quot;nltk crf&quot; and found <a href=""https://stackoverflow.com/questions/12501165/how-may-i-use-crf-in-nltk"">this SO question</a> which points to <a href=""https://nbviewer.ipython.org/github/tpeng/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb"" rel=""nofollow noreferrer"">this detailed example for NER</a>.</p>
<p>To answer your questions:</p>
<ol>
<li>nltk itself doesn't appear to propose a CRF model, the example above relies on an interface with CRFSuite (as mentioned in the SO question). It's probably possible to use other interfaces such as StanfordNER.</li>
<li>The tricky part is to define your own features, i.e. the conditions that the model uses as features at every step in the sequence. This is how you can specify any kind of specific &quot;grammar rule&quot; you want the model to use.</li>
</ol>
<p>The example above looks complete but I didn't test anything.</p>
","0","2","64377","14816"
"87843","<p>I agree that it looks like an optimization problem, but the parameters are not completely clear. Here are some vague ideas I have about the design of such a system:</p>
<ul>
<li>Let's assume a unit of time for which you have data and by which you attempt to predict things, for instance by the hour.</li>
<li>I think the first part needs to be about predicting energy needs by unit of time, based on past data and using any relevant factors as features (e.g. temperature, week day or not, etc.). I think this would be a reasonably standard time series regression problem.</li>
<li>The second part would be the optimization of energy consumption or carbon footprint: assuming we know the energy needs for a given period of time by unit of time, what are the optimal times for storing/discharging the battery? I don't know enough about the technical factors here, but I guess it's possible to simulate the target (say carbon footprint) based on different scenarios, and then select the optimal scenario. It seems to me that the problem at this stage could be simple enough to simulate all the scenarios and find the exact optimal times, that would probably correspond to a grid search. but in case this is too complex there are ML techniques which could estimate them, for instance genetic algorithms.</li>
</ul>
","1","2","64377","14816"
"87892","<p>I'm not expert in images classification, I'm just going to give some general advice here.</p>
<p>The strategy should be progressive, for instance:</p>
<ol>
<li>Proof of concept: devise a first draft of annotation process with some general guidelines, then take a random subset of a few hundreds images and try annotating them following the guidelines. Observe carefully the problems found at the annotation stage: any ambiguous cases, any problems with the granularity of the classes (for instance a class which is too general), potential error cases. Devise a training/testing setup to train a mockup system, test it with the small dataset, fix any bugs in the process, possibly try different methods. The goal of this stage is to iron out the different stages, possibly rule out some options which turn out to be unworkable.</li>
<li>Prototype: have a team of a few laypeople annotate a few thousands images following the updated annotation guidelines. Again pay attention to any problem, especially human errors in the process. At least a subset of the images should be annotated by several annotators, in order to detect differences. Ask annotators for feedback: were there any difficulties, was it fun, would they do it as a game or not, etc. At this stage you can start having a real ML system, albeit not optimal yet. Analyze the performance of the system on the different classes, possibly try different methods, estimate the minimum number of images per class to obtain decent performance. At this stage a lot of other tests could be done, and it should become clear how accurate the manual annotation is and whether annotators need to be compensated or not.</li>
</ol>
<p>It's only around this stage that the final annotation process can be fully designed. Depending on the strategy, you could consider devising a strategy for iterative manual annotation: some classes are going to be quickly learned by the model, so it could make sense to use the model in order to propose for annotation at the next round the images for which the model fails. Be careful to avoid bias, keep evaluating and refining the model at every round of annotations.</p>
","1","2","64377","14816"
"87893","<p>Naive approach: define a similarity or distance function, say for instance cosine similarity.</p>
<ol>
<li>Calculate the similarity score between any pair <span class=""math-container"">$(x_i\in A, y_j\in B)$</span></li>
<li>Define a precision level, say <span class=""math-container"">$\epsilon=0.000001$</span>. The assumption is that it's extremely unlikely that two vectors would be this close by chance in <span class=""math-container"">$A$</span>.</li>
<li>For every <span class=""math-container"">$y_j\in B$</span>, find the set <span class=""math-container"">$c(y_j) = \{ x_i\in A\ |\ sim(x_i,y_j)\geq 1-\epsilon \}$</span></li>
<li>Obtain the union: <span class=""math-container"">$C(B)=\{x_i\in A\ |\ \exists y_j\in B: x_i\in c(y_j) \}$</span></li>
</ol>
<p>The proportion of elements of <span class=""math-container"">$A$</span> which are &quot;equal&quot; to an element in <span class=""math-container"">$B$</span> is:</p>
<p><span class=""math-container"">$$\frac{|C(B)|}{|A|}$$</span></p>
","1","2","64377","14816"
"87903","<p>With complex data it's rare not to have any overfitting (or underfitting). Ideally one wants to avoid strong overfitting, and if given a choice between two models it's clearly safer to use one which isn't overfit. But if it's impossible to avoid, from a practical perspective there's no reason not to use a model for this reason, in my opinion. The problem with an overfit model is simply that it's going to perform poorly on fresh instances, so the worst that can happen is the performance that you observe on the test set.</p>
<p>Just an idea: I don't know if it makes sense with your data but it might be worth trying <a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">one-class classification</a> on the minority class. This could be a way to avoid the imbalance issue, and could be followed by a second stage of regular binary classification.</p>
","1","2","64377","14816"
"87910","<p>The other answers make sense but I would be more categorically negative about the idea:</p>
<blockquote>
<ol>
<li>Is this approach a correct approach, or logical with respect to machine learning principles ?</li>
</ol>
</blockquote>
<p>No, it's not. The parameters of a ML model (whether supervised or unsupervised) are estimated using a particular set of features designed as the input for the problem. Changing the input (features) changes the definition of the problem as well, so the solution (model) obtained for the first problem is unlikely to work as well on the new problem.</p>
<blockquote>
<ol start=""2"">
<li>Will it affect the model accuracy and if yes then how ?</li>
</ol>
</blockquote>
<p>It's very likely to decrease the performance of the model.</p>
<p>Normally the features used in the model are chosen because they are likely to &quot;help&quot; the model. If they are &quot;helpful&quot; then the model will rely on them, and therefore removing them will cause the model to fail.</p>
<blockquote>
<ol start=""3"">
<li>If I have to provide features B and C, then can I populate them with zeros and then provide it to trained model for making predictions.</li>
</ol>
</blockquote>
<p>You sure can, but it's a bad idea.</p>
<blockquote>
<ol start=""4"">
<li>Will the action taken in step 3) affect the model accuracy, if yes and then why and how ?</li>
</ol>
</blockquote>
<p>Same as point 2: the performance is very likely to drop. Replacing valuable indications for the model with arbitrary values is the equivalent of randomly switching blood samples in a biology lab, it causes wrong tests and wrong results.</p>
<p>Another way to look at it: if what you propose was possible, it would mean that it's possible in general to remove one feature and obtain the same performance. So let's say we have performance P with features (A,B,C,D,E), and when we remove A we still have performance P. Then by our assumption we can also remove B and still obtain performance P, and then do it again until we obtain a model with 0 features which has performance P. This is a contradiction, so the hypothesis that it's possible to remove a feature without losing performance is false.</p>
","0","2","64377","14816"
"87944","<p>I think that the issue depends on what you'd expect the model to learn:</p>
<ul>
<li>If the model is supposed to &quot;know&quot; the users it has seen during training, i.e. exploit the user id in order to infer particular choices for a specific user, then I don't see the point in adding this kind of frequency feature: the model already &quot;knows&quot; what choices this user tends to make, and it's supposed to know it only from the training set. Disadvantage: for any new/unique user not seen during training, the model probably has to fall back to a generic prediction.</li>
<li>If the user id is not used (removed from the features), then I think it makes sense to add the frequency features. I would see this mostly as a matter of feature engineering: let's assume that there are 3 features which represent how many times so far this user has chosen option A, B and C. The counting is made from any previous data available for this user. Of course, this is assuming that the same features can be calculated at testing time (equivalently, in a production environment).</li>
</ul>
<p>So in the second option the features exist before any question about what is the training set and what is the test set. However they introduce a potential risk of data leakage, so what I would do is to separate the training set and test set based on the users, i.e. make sure that users in the training set and test set don't overlap.</p>
","1","2","64377","14816"
"87945","<p>Generating artificial errors is generally risky in NLP, because it's difficult to make sure that the type and distribution of errors correspond exactly to real human errors. If the artificial errors diverge from real errors and a model is trained based on this data, the model will appear to have very good performance since it will rely on the patterns used to generate the errors. However it might not perform well with real data, and it would be difficult to detect it.</p>
<p>That being said, it's been a problem which has been studied for quite a while so the state of the art should help: <a href=""https://scholar.google.com/scholar?q=generate%20grammatical%20errors"" rel=""nofollow noreferrer"">Google Scholar gives a lot of references</a>, probably some of these papers provide existing implementations as well. One may notice that the concerns I mentioned above are a recurrent question, with some <a href=""https://arxiv.org/pdf/1907.08889.pdf"" rel=""nofollow noreferrer"">recent papers</a> analyzing how much artificial errors actually help.</p>
","2","2","64377","14816"
"87952","<p>First, you are right that word co-occurrence graphs have been used in various applications of NLP. More precisely in applications related to the <strong>meaning</strong> of the words, typically for <a href=""https://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow noreferrer"">topic modelling</a> or <a href=""https://en.wikipedia.org/wiki/Word-sense_induction"" rel=""nofollow noreferrer"">word sense induction</a>. These applications follow the linguistic principle that words which occur in similar contexts tend to have a similar meaning. This is the basis of <a href=""https://en.wikipedia.org/wiki/Distributional_semantics"" rel=""nofollow noreferrer"">distributional semantics</a>.</p>
<p>However this principle is unlikely to help finding words which have a similar syntactic function, simply because the co-occurrences in a sentence don't correspond to their syntactic role: a sentence usually contains many distinct POS tags, it doesn't have a specific POS category. Basically, <strong>word co-occurrences are useful for semantics, not syntax</strong>.</p>
<p>If the goal is to implement a POS tagger, why not use <a href=""https://nlpprogress.com/english/part-of-speech_tagging.html"" rel=""nofollow noreferrer"">state of the art methods</a> known to work very well for POS tagging? There is a lot of training data available, for instance the <a href=""https://universaldependencies.org/"" rel=""nofollow noreferrer"">Universal Dependencies</a> corpora.</p>
<p>Note that POS tagging is normally a supervised task (based on annotated data), whereas your idea appears to be about finding groups of similar POS in an unsupervised way. This would be another story entirely, probably involving <a href=""https://en.wikipedia.org/wiki/Grammar_induction"" rel=""nofollow noreferrer"">grammatical inference</a>.</p>
","0","2","64377","14816"
"87984","<p>This is essentially information retrieval: usually there is a collection of documents and the goal is to find the document which is the most similar to a given query (what you call the &quot;semantic concept&quot;).</p>
<p>The traditional way to do that is to convert the collection of documents as vectors, typically with <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">TFIDF</a> weight but there are many options (I assume that recent approaches favor using <a href=""https://en.wikipedia.org/wiki/Word_embedding"" rel=""nofollow noreferrer"">word (or document) embeddings</a>). Then a similarity measure (for example cosine) is used to measure the similarity between every document and the query, and the most similar document is selected. Since in this case there would be a single document, you could use a threshold on the similarity level in order to answer yes or no (concept found or not).</p>
","1","2","64377","14816"
"87985","<p>Generally I would suggest to look for differences with the general pattern, the general pattern being the answers from most users. A very basic way to do that:</p>
<ul>
<li>Calculate for every question the proportion of text 1, store it as a vector. This distribution is the &quot;mean vector&quot;.</li>
<li>For every player represent the vector of their answers: for every question the value is 1 if they chose text 1, 0 if they chose text 2.</li>
<li>For every player compare the vector of their answer with the &quot;mean vector&quot;, for instance with cosine similarity (or any other distance/similarity measure).</li>
</ul>
<p>Then look at the distribution of the similarity scores: normally most players have a high similarity scores, so if some of them have very low similarity it's likely that they didn't do the test seriously.</p>
<p>If you want a more advanced method, you could cluster the players' vectors or do anomaly detection.</p>
<hr />
<p>[edit based on comment]</p>
<ul>
<li>Take the proportion of &quot;text 1&quot; for every participant and compare to the mean proportion across all participants. This would catch the most obvious outliers, but probably not any subtle pattern.</li>
<li>Measure how much correlation there is between the question number and the answer: normally the two are independent so the correlation should be very low, if not there is pattern. Note that I think there would be a better statistical test than Pearson correlation, but I don't remember which one would suit this case (Spearman correlation might be relevant, I'm not sure).</li>
</ul>
","0","2","64377","14816"
"87986","<p>The intersection of the precision and recall curves is certainly a good choice, but it's not the only one possible.</p>
<p>The choice depends primarily on the application: in some applications having very high recall is crucial (e.g. a fire alarm system), whereas in some other applications precision is more important (e.g. deciding if somebody needs a risky medical treatment). Of course if your application needs high recall you'd choose a threshold before 0.6, if it needs high precision you'd choose a threshold around 0.85-0.9.</p>
<p>If none of these cases apply, people usually choose an evaluation metric to optimize: F1-score would be a common one, sometimes accuracy (but don't use accuracy if there is strong class imbalance). It's likely that the F1-score would be optimal around the point where the two curves intersect, but it's not sure: for example it might be a bit before 0.8, when the recall decreases slowly and the precision increases fast (this is just an example, I'm not sure of course).</p>
<p>My point is that even if it's a perfectly reasonable choice in this case, in general there's no particular reason to automatically choose the point where precision and recall are equal.</p>
","3","2","64377","14816"
"88035","<p>You could use clustering with a more basic similarity measure, for example cosine or even simply the proportion of words in common (e.g. Jaccard, overlap coefficient). This should gives you groups of sentences which are &quot;quite similar&quot; against each other, whereas sentences in different clusters are supposed to be very different. This way you would only have to compute WMD distance between smaller groups of sentences. By increasing the number of clusters the clusters would be smaller so there would be less WMD computation needed, however there is more risk to miss a pair of sentences since they could end up in different clusters.</p>
","1","2","64377","14816"
"88051","<p>The task you describe corresponds exactly to <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition (NER)</a>. This is a standard task for which there are many available libraries. NER is usually done with sequence labelling models such as <a href=""https://en.wikipedia.org/wiki/Conditional_random_field"" rel=""nofollow noreferrer"">Conditional Random Fields</a>. Sequence labelling implies that the data is provided as a sequence which looks like this:</p>
<pre><code>During    &lt;features ...&gt;  O
the                       O
process                   O
of                        O
protein                   O
synthesis                 O
,                         O
X_token1                  B_classX
X_token2                  I_classX
X_token3                  I_classX
was                       O
used                      O
.                         O
</code></pre>
<p>Here I'm using the common BIO format (Begin, Inside, Outside an entity) but there are variants. The model is trained with data annotated in this way, where there can be additional features (very often POS tag and others). Then when fresh text is provided (with the features) the model predicts the BIO tag for every token.</p>
<p>There has been a lot of research and resources produced for extracting specific entities in the specific context of biomedical data, so you might be interested in exploring these specific resources as well.</p>
<ul>
<li><a href=""https://www.nlm.nih.gov/bsd/medline.html"" rel=""nofollow noreferrer"">Medline</a>, <a href=""https://www.ncbi.nlm.nih.gov/pmc/"" rel=""nofollow noreferrer"">PMC</a> are huge collections of biomedical abstracts/papers</li>
<li>There are many tools for extracting biomedical annotations based on Medline/PMC data: <a href=""https://www.ncbi.nlm.nih.gov/research/pubtator/"" rel=""nofollow noreferrer"">PubTator</a>, <a href=""https://ctakes.apache.org/"" rel=""nofollow noreferrer"">cTakes</a>, <a href=""https://allenai.github.io/scispacy/"" rel=""nofollow noreferrer"">SciSpacy</a>,etc.</li>
</ul>
","1","2","64377","14816"
"88075","<p>Identifying correlation in data is possible with standard statistical tools, and it can be done programmatically. Correlation requires a representative sample, so in a case like the single graph that you show it wouldn't work: you'd probably need at least 20-30 such graphs in order to have a chance to confidently show correlation.</p>
<p>Identifying causation (cause and effect) is much harder. To the best of my knowledge it cannot be done from raw data without any additional knowledge. This is simply because with any dataset, there's no way to know if some external variable plays a causative role in the observations.</p>
<p>The way causation is proved is through a specific experimental setting, i.e. it's <strong>the design of the experiment</strong> which allows collecting data susceptible to prove causation. <a href=""https://en.wikipedia.org/wiki/A/B_testing"" rel=""nofollow noreferrer"">A/B testing</a> is a method used for this kind of experiment. Example: suppose we want to test whether new drug A has an effect on outcome B. Design the experiment to observe the response variable B on a representative sample of patients. Randomly split the sample into two subsets: the first one is the control group, they are given a placebo. The second is given drug A. At the end of the experiment one can prove causation if and only if the second group shows a significantly different outcome compared to the control group.</p>
","1","2","64377","14816"
"88110","<p>You simply cannot use these data points for this test, you have to use only the data points for which both variables have a value. This will make the sample smaller, so it's less likely to result in a significant difference.</p>
","1","2","64377","14816"
"88112","<p>There are probably a good few options, as usual I would start with something simple (possibly similar to what you have in mind).</p>
<ul>
<li>Simple year1 vs year2 or year2 vs year3 global comparison: proportion of customer who go from number X in year N to number Y in year N+1. A heatmap would probably work well with this kind of data, and if you want some fancy visualization to impress your boss you could try a <a href=""https://python-graph-gallery.com/chord-diagram/"" rel=""nofollow noreferrer"">chord diagram</a> or a <a href=""https://python-graph-gallery.com/sankey-diagram/"" rel=""nofollow noreferrer"">Sankey diagram</a>. Of course you can vary the input with average of the past 2 years, etc.</li>
<li>Since you have data by territory and it appears to show different patterns, it might be interesting to look at something by territory. I'm not sure which value should be used as a &quot;summary&quot; of the trend for a territory, I can think of a few options:
<ul>
<li>simple difference of the mean number of products between year N and year N+1 (i.e. shows positive or negative trend)</li>
<li>some measure of how different the distributions are, i.e. how much variation from year N to year N+1</li>
<li>Simple Pearson correlation over all the customers between number of products in year N vs year N+1.</li>
</ul>
</li>
</ul>
","0","2","64377","14816"
"88197","<p>I wouldn't interpret the first graph the same way. As far as know (and bear in mind that nobody knows everything), there's no clear way to distinguish between noisy features and bias (underfitting). But in either case there's no reason why the validation/CV loss would be much higher than the training loss, this is actually a clear sign of overfitting: if the model was not able to generalize, the training loss would be as high as the validation loss. Instead the model succeeds in representing some patterns, but it turns out that the patterns it captures don't apply (at least not as well) on the validation set. In other words, the model captures patterns which happen by chance in the training set: it overfits.</p>
<p>It's possible that adding more instances would help reduce the overfitting, as the CV loss appears to still decrease significantly in the 800-900 zone. But the difference is very large, so it's not sure that it can be solved only with more data.</p>
<p>I agree that the second graph shows some overfitting (as well), but not as much as the first one. In my opinion, the fact that the lowest difference between the two curves is not at the end of the curve points to either a model which is not stable yet (needs more data) or maybe there are simply not enough points on the curve to observe the real trend (i.e. the low difference at 240 and/or the higher one at 300 might happen by chance). This is indeed variance, but it's not very high
in my opinion.</p>
","1","2","64377","14816"
"88199","<p>By principle in supervised ML any decision which affects the model should be made using only the training set, in order to avoid data leakage. Following this principle requires the training/test split to always be done first.</p>
<p>Quite often the decision seems minor and has little chance to cause data leakage, like in your case. It's tempting to make our life easier and use the whole data for some pre-processing like this. However it's a good idea to follow this principle as strictly as possible not because the effect of data leakage would be huge, but because it would be impossible to detect it if it happens: when it happens, data leakage means that the performance on the test set is biased (overestimated), and there's simply no way to know by how much (except by collecting a new test set, of course).</p>
<p>Your concern that the split of the data could impact the analysis of correlation is a good example of the problem: suppose there would be a difference indeed. This means that there is data leakage, since the features are different depending directly on the test data. In this case the evaluation on the test set is likely to be biased.</p>
<p>Another way to explain the same idea is this: if something happens or doesn't happen in the training set by chance, then it's important to see if the model is able to focus on the real patterns and ignore the noise. The only way to obtain a reliable answer to this question is if nothing in the model depends on the test set.</p>
","2","2","64377","14816"
"88205","<p>I'm going to just answer about the difference between the two functions <code>qcut</code> and <code>cut</code> because it's a very important difference:</p>
<ul>
<li>the first <code>qcut</code> is indeed about quantiles, which means that it's about dividing the data <strong>into bins, each containing an equal number of points</strong>. For instance if you use deciles it means that there is the same number of people in the first decile as in the last (or any other), so the useful information is <strong>the range of values</strong> for each bin. For example if the first decile ranges from 0 to 500, then 10% of the people are paid less than 500.</li>
<li>the second <code>cut</code> is not about quantiles, it simply creates bins which all cover an <strong>interval of values of equal length</strong>. For instance the first interval would be 0-500, the second 500-1000, etc. In this case the number of points in every bin is usually different, and that's the useful information. Typically this is how a histogram is built: equal intervals but possibly different frequency (number of data points) in every bin.</li>
</ul>
<hr />
<p>[edit following OP's comments]</p>
<blockquote>
<p>Data points are reported salaries for each company role (actually, it is an average value reported )</p>
</blockquote>
<p>Ok so the data that you have contains a distribution of the salaries for every role.</p>
<blockquote>
<p>The X axis, simply the id corresponding to each role</p>
</blockquote>
<p>It's not a very good idea to transform an ordinal/categorical variable into a continuous one, in a case like this you could instead plot a bar for every role (with the name of the role on the X axis). Even more precise, you could plot a boxplot for every role, keeping the order of the roles by mean salary. This way it would be possible to visualize not only the mean (usually replaced by median in a boxplot) but how much variations there are for each role.</p>
<p>Another visualization idea would be to plot the full distribution of salaries with a difference color for every role. In this case X would usually be the salary, Y would be how many people are in this interval (bin) of salary, and stacked bars with different colors show the proportion by role. This would show not only the link between salary and role but also how many people are in each category.</p>
<blockquote>
<p>how to bin the distribution (qcut or cut)</p>
</blockquote>
<p>I suspect that this is the wrong question, because apparently you're not very familiar with these concepts and how to represent them. So I would suggest that you first play with the visualization ideas I mentioned: normally these don't require you to do any binning yourself, the library should take care of it (see <a href=""https://python-graph-gallery.com/"" rel=""nofollow noreferrer"">examples in Python</a>).</p>
<p>Knowing whether you need <code>cut</code> or <code>qcut</code> depends on what you want to do with the result. As I said above, quantiles are useful for solving questions such as &quot;80% of the employees have a salary lower than X&quot;, whereas <code>cut</code> directly represents &quot;X% of the employees have a salary between 100 and 150&quot;. Ultimately the two represent the same information, just under different perspectives. If the difference is still confusing, I'd suggest you start with regular bins with <code>cut</code> because quantiles are slightly more complex to interpret.</p>
","1","2","64377","14816"
"88208","<h3>Preliminary note</h3>
<p>as far as I know, people may use the term <a href=""https://en.wikipedia.org/wiki/Feature_extraction"" rel=""nofollow noreferrer"">feature extraction</a> in slightly different ways:</p>
<ul>
<li>referring to automatic methods used for dimensionality reduction which involve transforming the features (and not only <a href=""https://en.wikipedia.org/wiki/Feature_selection"" rel=""nofollow noreferrer"">selecting a subset of features</a>).</li>
<li>referring to the general process of designing and engineering the features before training/testing a model on these features.</li>
</ul>
<p>Please be aware that I'm not 100% sure that the terminology I use myself is standard. (but I don't think it matters for the question you're asking).</p>
<h3>Answer</h3>
<p>Used in the context you mention, the term certainly refers to the general process of feature engineering (2nd point above).</p>
<p>In traditional ML models the model directly uses the features exactly as they are provided to the training algorithm. For example, a decision tree classification model tries to find the best conditions in the features to discriminate the data. This means that the algorithm selects a single feature and directly uses its range of values to build a condition, something like this:</p>
<pre><code> if color == 'green' then go to subtree 1 else go to subtree 2
</code></pre>
<p>Thus the type of information that can be used by a model is strictly limited by the algorithm and the features which are provided to it: there's no way to consider any information which is not directly available as a feature. As a consequence the stage of feature engineering is crucial. For example in text classification there are a lot of choices to make: provide the words frequency or TFIDF as features? Provide all the word or remove stop words? Lemmatize or not? Add POS tags? And these are just some very standard questions, specific tasks may require a very detailed analysis. Importantly, it's not possible to just provide all the features one can think of because this would almost certainly cause the model to overfit due to the high number of features. Generally in this stage the goal is to give the model the best and most precise clues that will help it make correct predictions, and also avoid giving it too many irrelevant features to prevent overfitting.</p>
<p>That's the stage that DL doesn't need: thanks to a much more complex architecture and algorithmic process, a DL model can itself select and combine the features it needs to perform well.</p>
<blockquote>
<p>I thought discover it is part of the task of the machine learning algorithm.</p>
</blockquote>
<p>Sure, but in traditional ML in order for the model to discover the most important patterns for a specific task, the features have to be prepared in a way which maximizes the chances of the model to find these. Simile: if a teacher tells their students to revise page 63 of the textbook for the test, it's likely that the students will perform better than if they have no clue about the topic and need to revise the whole textbook.</p>
","1","2","64377","14816"
"88229","<p>The first idea that comes to mind is a similarity measure such as cosine. It's often used with sparse vectors (text represented as vectors over the vocabulary).
There are many options for distance/similarity measures:</p>
<ul>
<li>Basic set measures like overlap coefficient or Jaccard</li>
<li>Entropy-based measures such as <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""nofollow noreferrer"">KL divergence</a>.</li>
<li>...</li>
</ul>
","1","2","64377","14816"
"88250","<p>The common case of missing values for which data is imputed or removed assumes that missing values appear randomly in the data, so the absence of value has no relevance to the task.</p>
<p>From your description, in your data the fact that a value is missing is significant by itself. So I'd say that yes, it makes sense in this case to represent this information as a categorical variable. Note that it can be represented as a special value for the score feature indeed, but it doesn't have to be the same variable.</p>
","2","2","64377","14816"
"88368","<p>As you can see this MCC formula is for binary classification, so you can only calculate its results by considering the problem as binary.</p>
<p><em>[edited to clarify OP's confusion]</em> What is a confusion matrix? It shows for every true class <span class=""math-container"">$X$</span> as a row and every predicted class <span class=""math-container"">$Y$</span> as a column how many instances have true class <span class=""math-container"">$X$</span> and are predicted as <span class=""math-container"">$Y$</span>. If there are only two classes (binary classification), the only possibilities are</p>
<ul>
<li><span class=""math-container"">$X$</span> positive and <span class=""math-container"">$Y$</span> positive -&gt; TP</li>
<li><span class=""math-container"">$X$</span> positive and <span class=""math-container"">$Y$</span> negative -&gt; FP</li>
<li><span class=""math-container"">$X$</span> negative and <span class=""math-container"">$Y$</span> positive -&gt; FN</li>
<li><span class=""math-container"">$X$</span> negative and <span class=""math-container"">$Y$</span> negative -&gt; TN</li>
</ul>
<p>However when there are more than two classes (multiclass classification) it's impossible to use this distinction positive/negative directly, so there are no general TP,FP,FN,TN cases.</p>
<p>With multiple classes one can calculate binary classification metrics <strong>for every class</strong>. This is done by considering the target class as positive and all the other classes as negative (as if they are merged into one big negative class).</p>
<p>Example: suppose we have classes A, B, C. <strong>If we focus on class A</strong>, the confusion matrix is like this:</p>
<pre><code>    A   B   C
A   TP  FN  FN
B   FP  TN  TN
C   FP  TN  TN
</code></pre>
<p>to present it another way:</p>
<pre><code>         A    B or C
A        TP    FN 
B or C   FP    TN  
</code></pre>
<p>Now if we focus on class B the confusion matrix becomes:</p>
<pre><code>    A   B   C
A   TN  FP  TN
B   FN  TP  FN
C   TN  FP  TN
</code></pre>
<p>In your code the TP and TN categories are swapped:</p>
<pre><code>TP(i)=C(i,i);
...
TN(i)=sum(C(:))-TP(i)-FP(i)-FN(i);
</code></pre>
","1","2","64377","14816"
"88369","<p>You have a combination of two problems in your data:</p>
<ul>
<li>imbalance</li>
<li>missing values</li>
</ul>
<p>In your experiments there's a confusion about what the true distribution of the data is (or should be): either the &quot;real data&quot; is 97% no, or the &quot;real data&quot; is after removing missing values in which case it's almost balanced. It's very important to decide this based on the problem that you're trying to solve: &quot;in production&quot;, does the model have to produce a prediction for every instance even if it has missing values? If yes the true distribution is 97% no (original problem). If no, then the model only predicts for &quot;complete&quot; instances, i.e. many instances are discarded due to missing values (and this happens much more often with &quot;no&quot;).</p>
<p>This is a crucial point because whichever way you train the model, it should be evaluated on a test set which reflects the true distribution of the data.</p>
<p>I would assume that your real goal is to predict for every instance, i.e. you don't want to ignore instances even if they have missing values. I will try to address both options though:</p>
<ul>
<li>option A: real data is 97% no.</li>
<li>option B: real data is 58% yes.</li>
</ul>
<blockquote>
<ol>
<li>Why did this method outperform other oversampling and undersampling techniques?</li>
</ol>
</blockquote>
<p>The two methods were evaluated on two very different test sets, so the performance between them is simply not comparable.</p>
<ul>
<li>If the resampling experiments were <a href=""https://datascience.stackexchange.com/q/15630/64377"">properly evaluated on the original data</a> (<em>not</em> resampled), then they provide you with a reliable estimate of the performance in option A.</li>
<li>If the resampling experiments were (wrongly) evaluated on resampled data, then the difference is certainly due to the missing values, because imputing a very large proportion of the data causes a lot of noise. In this case the resampling experiments are neither valid for option A (wrong distribution in the test set) nor B (missing values in the test set).</li>
</ul>
<blockquote>
<ol start=""2"">
<li>Is it acceptable that what was previously a minority class is now the majority class or can this cause overfitting?</li>
</ol>
</blockquote>
<p>It depends which problem you're trying to solve:</p>
<ul>
<li>For the original problem &quot;option A&quot;, no it is not acceptable to modify the distribution in the test set.</li>
<li>For the new problem &quot;option B&quot;, the majority class is &quot;yes&quot;. The original data with 97% &quot;no&quot; is irrelevant.</li>
</ul>
<blockquote>
<ol start=""3"">
<li>Is it realistic to build a model that relies on input with no missing data for the majority class samples?</li>
</ol>
</blockquote>
<p>This is about specifying the exact problem you want to solve, that's for you to decide :)</p>
","2","2","64377","14816"
"88376","<p>One way to achieve this is to use a clustering method based on a custom similarity/distance measure. For example you could defined the similarity measure between two instances as:</p>
<p><span class=""math-container"">$$sim(\langle v_1, n_1\rangle,\langle v_2, n_2\rangle)=\frac{1}{2} cosine(v_1,v_2)\ +\ \frac{1}{2} \left(1-\frac{|n_1-n_2|}{max(n_1,n_2)}\right)$$</span></p>
<p>This measure gives the same weight to the similarity between the vectors (<span class=""math-container"">$v_1$</span> and <span class=""math-container"">$v_2$</span>) and the similarity between the numerical values (<span class=""math-container"">$n_1$</span> and <span class=""math-container"">$n_2$</span>). Note that since this similarity measure is normalized, you can convert it to a normalized distance measure: <span class=""math-container"">$d=1-s$</span>.
Of course you should define the exact measure based on what the values represent, this is just an example.</p>
<p>You could use this measure with a hierarchical clustering method or a graph clustering method (with edges based on similarity value).</p>
","0","2","64377","14816"
"88522","<p>If the model is not derived from the data then it must be built manually, so non data driven means rule based.</p>
<p>This was the big trend in AI in the 80s before Machine Learning, these pre-ML automatic prediction systems were called <a href=""https://en.wikipedia.org/wiki/Expert_system"" rel=""nofollow noreferrer"">expert systems</a> and they were quite successful at the time in industry (here are some <a href=""https://en.wikipedia.org/wiki/Expert_system#Applications"" rel=""nofollow noreferrer"">examples of applications</a>).</p>
<p>The way one would build a system for a regression task is essentially this: do the whole regression analysis manually, find the parameters and hard-code them in the prediction system.</p>
<p>As far as I know Machine Learning has pretty much made this kind of rule-based systems obsolete, due to their total lack of flexibility and the very high cost in manual labor to build one.</p>
","0","2","64377","14816"
"88541","<p>I think the closest standard NLP task would be <a href=""https://en.wikipedia.org/wiki/Relationship_extraction"" rel=""nofollow noreferrer"">relationship extraction</a>. In general it's a quite complex task which involves NER, syntactic analysis and semantic role labeling.</p>
<p>Note that there are various works using the term &quot;event extraction&quot; (for example <a href=""https://towardsdatascience.com/natural-language-processing-event-extraction-f20d634661d3"" rel=""nofollow noreferrer"">this</a>), but as far as I know there is no clear definition of the task. It's often related to putting events on a timeline, this would be quite different from your goal but possibly related.</p>
<p>A basic approach would be to treat the problem as a sequence labeling task like NER: given some annotated &quot;events&quot; in a training corpus, the model might be able to learn the patterns and detect any new &quot;event&quot; in a text.</p>
","1","2","64377","14816"
"88557","<p>Quite often with massive datasets the model doesn't actually need the whole data. So I think the first step is to check whether using the whole data is useful: run an ablation study where you use say 1%, then 2%, 3%, .., up to say 10% of the data (adapt the levels to your case of course). Each run consists in training on the x % subset and evaluating on a validation set (be sure to separate your real final test set before anything, this study should only use a validation set).</p>
<p>The goal is to estimate how much gain in performance is obtained by adding training data. Plotting the performance as a function of the amount of data should give a decent idea of the trend, even if it doesn't reach the point of maximal performance, i.e. when more data doesn't improve performance anymore. With this information you can make a better decision about how to proceed with the real training.</p>
","1","2","64377","14816"
"88560","<p>I can think of two options, there are probably more:</p>
<ul>
<li>Train a binary classification &quot;top model&quot; which predicts whether the instance belongs to  model 1 or model 2. The two final models only need to be trained with their respective data and only predict their respective N or M classes. The performance of the top model is crucial in this option.</li>
<li>Train an open set classification model. It's a bit similar to what you're doing currently by adding a class &quot;other model&quot; to the first model, except that &quot;other&quot; is not a regular class and you don't need to train the model with instances from this class. This is related to <a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">one-class classification</a> and requires a specific training method which can handle open-set classification.</li>
</ul>
","0","2","64377","14816"
"88581","<p>To add a few points to the good answers already proposed:</p>
<p>In general this kind of site can only propose standard methods applied to standard data. This can be pretty useful, but it covers only a small proportion of the vast diversity of problems addressed by data science.</p>
<p>In a broad sense, data science has applications in virtually every imaginable domain (e.g. medicine, astronomy, self-driving cars, machine translation...) and with every possible kind of digital data (e.g. text, speech, images, video...).
The scope of this kind of website is limited to one domain, typically solving standard business problems using standard business data. It's easy to see that the world is full of non-standard problems just by browsing a few questions here on DataScienceSE.</p>
","6","2","64377","14816"
"88631","<p>Assuming the goal is to give the true performance of the model, then it should be the performance obtained by applying the final model on the test set. The fact that the performance on the validation set can be observed during training is irrelevant.</p>
<p>So the question becomes: which model should you select based on observing the performance on the validation set during training? You can select the model which gives the maximum accuracy, but the real performance is obtained when you evaluate it on a fresh test set.</p>
<p>The second question depends on which performance measure you want to optimize: if you want to optimize accuracy, then you should pick the model which obtains the highest accuracy.</p>
","1","2","64377","14816"
"88642","<p>From a Machine Learning perspective, the answer would be to let the non tech users annotate a sample of products with scores and then train a model which predicts scores for all the products. Then the model could be refined and updated by regularly repeating this process, possibly using <a href=""https://en.wikipedia.org/wiki/Active_learning_(machine_learning)"" rel=""nofollow noreferrer"">active learning</a> in order to select the most useful instances and minimize the number of required manual annotations.</p>
<p>Many variants are possible: users could be asked to just say whether they think a score is too high/low, or compare two products and say which one should have a higher score, etc.</p>
","0","2","64377","14816"
"88643","<p>I think it's always a good idea to start with a basic option. Once you have a decent basic model, you can try to improve performance with something more advanced and you would have a baseline so you know if the performance of the advanced model is really good or just ok.</p>
<p>In this case the basic option would be to train 100 binary classifiers, one for every label. The text could be represented as a TFIDF vector. I would suggest removing the rare words, it can significantly improve performance. Then a traditional model like Naive Bayes, Decision Trees or SVM can be used.</p>
<p>The number of labels by itself is not an issue, however their distribution matters, especially if some of them have very few instances (expect bad performance on these).</p>
","0","2","64377","14816"
"88676","<p>I think your question depends almost entirely on the actual problem that you're trying to represent, i.e. it depends on (your) expert knowledge.</p>
<p>Yes, data generated this way should be considered synthetic. However there's no general requirement to add noise: adding noise (as well as how much and in which way) is normally meant to make synthetic data more realistic, i.e. closer to real data.</p>
<p>In general with synthetic data it's important to show that the data is representative enough of the real phenomenon it represents. Again this depends on expert knowledge but most of the time it's essential to make the distribution of the data realistic as well. So yes, if you know the true probability distribution it's certainly a good idea to make the synthetic data follow it.</p>
","0","2","64377","14816"
"88812","<p>Let me first clarify the general principle of classification with text data. Note that I'm assuming that you're using a &quot;traditional&quot; method (like decision trees), as opposed to Deep Learning (DL) method.</p>
<p>As you correctly understand, each individual text document (instance) has to be represented as a vector of features, each feature representing a word. But there is a crucial constraint: every feature/word must be at the same position in the vector <em>for all the documents</em>. This is because that's how the learning algorithm can find patterns across instances. For example the decision tree algorithm might create a condition corresponding to &quot;does the document contains the word 'cat'?&quot;, and the only way for the model to correctly detect if this condition is satisfied is if the word 'cat' is consistently represented at index <span class=""math-container"">$i$</span> in the vector for every instance.</p>
<p>For the record this is very similar to one-hot-encoding: the variable &quot;word&quot; has many possible values, each of them must be represented as a different feature.</p>
<p>This means that you cannot use a different index representation for every instance, as you currently do.</p>
<blockquote>
<p>Vectors generated from those texts needs to have the same dimension Does padding them with zeroes make any sense?</p>
</blockquote>
<p>As you probably understood now, no it doesn't.</p>
<blockquote>
<p>Vectors for prediction needs also to have the same dimension as those from the training</p>
</blockquote>
<p>Yes, they must not only have the same dimension but also have the same exact features/words in the same order.</p>
<blockquote>
<p>At prediction phase, those words that hasn't been added to the corpus are ignored</p>
</blockquote>
<p>Absolutely, any <em>out of vocabulary word</em> (word which doesn't appear in the training data) has to be ignored. It would be unusable anyway since the model has no idea which class it is related to.</p>
<blockquote>
<p>Also, the vectorization doesn't make much sense since they are like [0, 1, 2, 3, 4, 1, 2, 3, 5, 1, 2, 3] and this is different to [1, 0, 2, 3, 4, 1, 2, 3, 5, 1, 2, 3] even though they both contain the same information</p>
</blockquote>
<p>Indeed, you had the right intuition that there was a problem there, it's the same issue as above.</p>
<p>Now of course you go back to solving the problem of fitting these very long vectors in memory. So in theory the vector length is the full vocabulary size, but in practice there are several good reasons not to keep all the words, more precisely to remove the <em>least frequent</em> words:</p>
<ul>
<li>The least frequent words are difficult to use by the model. A word which appears only once (btw it's called a <a href=""https://en.wikipedia.org/wiki/Hapax_legomenon"" rel=""noreferrer"">hapax legomenon</a>, in case you want to impress people with fancy terms ;) ) doesn't help at all, because it might appear by chance with a particular class. Worse, it can cause overfitting: if the model creates a rule that classifies any document containing this word as class C (because in the training 100% of the documents with this word are class C, even though there's only one) and it turns out that the word has nothing specific to class C, the model will make errors. Statistically it's very risky to draw conclusions from a small sample, so the least frequent words are often &quot;bad features&quot;.</li>
</ul>
<ul>
<li>You're going to like this one: texts in natural language follow a <a href=""https://en.wikipedia.org/wiki/Zipf%27s_law"" rel=""noreferrer"">Zipf distribution</a>. This means that in any text there's a small number of distinct words which appear frequently and a high number of distinct words which appear rarely. As a result removing the least frequent words reduces the size of the vocabulary very quickly (because there are many rare words) but it doesn't remove a large proportion of the text (because the most frequent occurrences are frequent words). For example removing the words which appear only once might reduce the vocabulary size by half, while reducing the text size by only 3%.</li>
</ul>
<p>So practically what you need to do is this:</p>
<ol>
<li>Calculate the word frequency for every distinct word across all the documents in the training data (<em>only</em> in the training data). Note that you need to store only one <code>dict</code> in memory so it's doable. Sort it by frequency and store it somewhere in a file.</li>
<li>Decide a minimum frequency <span class=""math-container"">$N$</span> in order to obtain your reduced vocabulary by removing all the words which have frequency lower than <span class=""math-container"">$N$</span>.</li>
<li>Represent every document as a vector using only this predefined vocabulary (and fixed indexes, of course). Now you can train a model and evaluate it on a test set.</li>
</ol>
<p>Note that you could try different values of <span class=""math-container"">$N$</span> (2,3,4,...) and observe which one gives the best performance (it's not necessarily the lowest one, for the reasons mentioned above). If you do that you should normally use a <em>validation set</em> distinct from the final test set, because evaluating several times on the test set is like &quot;cheating&quot; (this is called <a href=""https://en.wikipedia.org/wiki/Leakage_(machine_learning)"" rel=""noreferrer"">data leakage</a>).</p>
","5","2","64377","14816"
"88832","<p>As far as I understand, the task you try to do would require a system to correctly analyze the sentence at a semantic level. In general this is <a href=""https://en.wikipedia.org/wiki/Natural-language_understanding"" rel=""nofollow noreferrer"">Natural Language Understanding</a> and there's no solution to this problem.</p>
<p>Given your examples it looks like one problem is separating the different parts of the sentence, so that you can analyze the correct part of the sentence for each entity independently. This is doable: you can use shallow or deep parsing methods to extract the chunks or subtrees, and then you could apply sentiment analysis to just to the part which contains the target entity.</p>
<p>In general don't expect perfect results with sentiment analysis or any semantic task. Even though DL has brought great progress in the recent years, these problems are active research questions (and might still be researched for a long time).</p>
","1","2","64377","14816"
"89010","<p>Thank you for the layman explanation, the technical part is a bit hard to follow.</p>
<p>As far as I understand, ultimately the goal would be to be able to process these long lists of descriptions and automatically extract a short standardized string for each element in the list.</p>
<p>The main choice to make is between supervised classification or unsupervised clustering:</p>
<ul>
<li>In classification the model learns to associate features (for instance &quot;does the description contains the word 'Inlet'?&quot;) with output labels (classes). The classes are predefined and the model needs to be trained on some annotated data (hence 'supervised').</li>
<li>In clustering the model groups instances (descriptions represented by their features) by their similarity. There is no label as output.</li>
</ul>
<p>In your scenario it seems to me that you need the output to represent specific characteristics known in advance, so this belongs to supervised classification, more precisely text classification. You would need a representative sample as training data: many descriptions with their expected label/class. A simple text classification system would represent the words in the description as features over the whole vocabulary (across all descriptions), then during training the model would find which words are good indicators of which label/class. After training the model can be applied to any new description to predict its label/class.</p>
<p>Text classification is very common, you can find a lot of material/libraries online, maybe even some web or GUI software to do the whole thing but I don't know any.</p>
<p>You could try clustering but my guess is that it won't work the way you want, because it's meant for cases where one doesn't expect any particular categories.</p>
","1","2","64377","14816"
"89011","<ol>
<li>It completely depends on the type of model. Some models need to represent the features with parameters: for example Naive Bayes with numerical features needs to have a way to calculate the probability based on the value, and the most common case is to assume that the features follow a normal distribution. On the other hand whether a feature is normally distributed or not doesn't matter at all for Decision Trees.</li>
<li>Yes, it can be very informative in order to know whether this feature is a good indicator or not: the more different the distributions, the more easily the algorithm can distinguish the classes using this feature.</li>
<li>No, don't change the distribution of a feature (unless you have a specific reason to do so, e.g. based on expert-knowledge for this particular data). Any way you would do that would certainly alter the overall distribution of the data and/or the way the features are related within an instance, so the model would not learn from the true distribution and therefore its predictions on real data would likely go wrong.</li>
</ol>
","1","2","64377","14816"
"89040","<blockquote>
<p>Since these examples can't be learned, why not remove them and retrain the final model with only the 92% of the data that can be learned?</p>
</blockquote>
<p>In general, I think this is a bad idea for the reasons below. This being said, the only sure way to know with a particular dataset is to experiment.</p>
<ul>
<li>This would modify the distribution of the data. If the errors tend to happen more often with a particular class or a specific combination of features (this is quite likely), these cases would not be seen anymore by the model. Sometimes modifying the distribution can lead to better performance (e.g. when using resampling), but it can also do the opposite. So this is a bias with unknown effects on the performance of the model.</li>
<li>A more serious problem is that these error cases are likely to be important for the model. Typically errors happen with borderline cases, and these cases are usually very important for the model to learn the optimal way to separate the classes. By removing them it might be easier for the model to separate the classes during training, but that's not a good thing since it won't have all the information it needs in order to minimize errors. As a consequence it's likely that it won't find the right optimum and therefore will make <em>more</em> errors.</li>
</ul>
","2","2","64377","14816"
"89079","<p>In this kind of optimization problem a <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow noreferrer"">genetic algorithm</a> is often a good approach, assuming computing the value of  <code>f()</code> is not too costly.</p>
<p>The idea is to represent the 6 parameters as &quot;genes&quot;. In the first generation their values are assigned randomly, then each &quot;individual&quot; in the generation (combination of parameters) is evaluated (i.e. calculate <code>f</code>), and the top performing &quot;individuals&quot; are selected. The next generation is obtained by cross-over and random mutation, and the process is repeated until <code>f</code> converges to a maximum.</p>
","1","2","64377","14816"
"89082","<p>What this shows is that the protocol is not a very discriminative feature:</p>
<ul>
<li>the probability of class 1 given http is 10/(109+10)=0.084</li>
<li>the probability of class 1 given https is 180/(180+1560)=0.103</li>
</ul>
<p>If these conditional probabilities were very different this feature would be more helpful to predict the class, but they differ only slightly. Note that the feature might still be useful, but it doesn't have a huge impact on its own. In case you're interested to know if the difference is significant (i.e. not due to chance), you could do a <a href=""https://en.wikipedia.org/wiki/Chi-squared_test"" rel=""nofollow noreferrer"">chi-square test</a>.</p>
<blockquote>
<p>Does it make sense to say that the most websites having class 0 have an https protocol, even if I have a dataset with class imbalance?</p>
</blockquote>
<p>It is factually correct, but most websites having class 1 also have https so it's not a very useful information (and on its own this information might be confusing for some readers).</p>
<blockquote>
<p>For a model, I would consider resampling techniques. Should I work on this after the resampling, or it would make sense to check features importance with other tests (e.g., Pearson correlation, if it his appropriate in this case)?</p>
</blockquote>
<p>Feature selection can done either before or after resampling, it doesn't matter. The two things are independent of each other because the level of correlation between a feature and the class is independent from the proportion of the class.</p>
<p>I don't think Pearson correlation is good for categorical variables. I think conditional entropy would be more appropriate here (not 100% sure, there might be other options).</p>
","4","2","64377","14816"
"89125","<p>I looked very quickly and only at the first paper so I might miss something but it looks to me like the task is a binary classification problem. If this is correct then there's no need for averaging the F1-score.</p>
<p>Also in this paper the authors even give the formula of the F1-score (!), so I'd say that they are quite thorough in their description of the evaluation measures they use. I would take this as an additional indication that there's no averaging,  since it's unlikely that they wouldn't mention it.</p>
","0","2","64377","14816"
"89129","<p>The fact that a feature has low correlation with the target variable shows that it's not a good indicator on its own, but that doesn't mean that it can't be useful for the model when combined with the other features.</p>
<p>The only way to know if these features are useful is to use them to train a model, then evaluate on a validation set and see if it improves performance.</p>
<blockquote>
<p>the p-value is less than 0.05</p>
</blockquote>
<p>Is this the result of a correlation significance test? It depends on the test but in general a p-value lower than 0.05 means that <em>there is a significant difference</em>, i.e. in this case it probably means that the correlation is truly not zero. Anyway imho this wouldn't prove anything with respect to using these features or not.</p>
","1","2","64377","14816"
"89166","<p>Cross-validation (CV) is a method meant essentially to accurately evaluate a model using some training data. As a consequence CV doesn't have to be used when training the final model, usually one simply uses the whole training data for that. Note that using one of the CV models would have two disadvantages: there's no reason to select one or the other (the performance on one split is not relevant), and it doesn't exploit the whole training set.</p>
<p>I also notice that you didn't mention any test set evaluation in your steps. A common mistake is to select a model based on parameter tuning (step 3) using CV, and just assume that the CV performance is reliable. It is not because the fact that a choice was made (whether about parameters, models, features...) means that this step is still part of the training process, therefore there needs to be evaluation on a different test set afterwards. In other words: the fact that some parameters perform better than others could be due to chance, so it's important to evaluate the selected model (and only this model) on a fresh test set.</p>
","1","2","64377","14816"
"89202","<p>In a binary problem there's no reason to average the ROC (or any other metric). Normally micro and macro performances are used to obtain a single performance value based on individual binary classification measures in the case of multiclass classification.</p>
<p>So here what happens is this:</p>
<ul>
<li>The ROC curves for the two classes are <a href=""https://datascience.stackexchange.com/a/86839/64377"">the mirror of each other</a> (using the top-left bottom-right diagonal as symmetry axis), because they represent exactly the same points but with the positive and negative classes swapped.</li>
<li>The macro-average curve is the average of both curves, which doesn't make a lot of sense since both already have the same shape. This is why all the AUC values are identical for macro, class 0 and class 1.</li>
<li>The micro-average ROC is the weighted average, so it's made mostly of the majority class (around 75%): since most points in the majority class are correctly predicted as the majority class, the performance looks much better. This is related to the fact that <a href=""https://datascience.stackexchange.com/q/64694/64377"">the micro-average F1-score is equal to accuracy</a>, although in the ROC I'm not sure how the points are weighted exactly.</li>
</ul>
<p>In my opinion, this is a good illustration of why ROC curves should be used very carefully (or not at all) in a multiclass setting. ROC curves are meant for binary (soft) classification, they are useful and interpretable in this context but not necessarily in another context.
In general it also illustrates that it's not because something can be calculated that the resulting value makes sense ;)</p>
","4","2","64377","14816"
"89309","<blockquote>
<p>I want to know where Regression analysis is most used at</p>
</blockquote>
<p><a href=""https://en.wikipedia.org/wiki/Regression_analysis"" rel=""nofollow noreferrer"">Regression analysis</a> is used for analyzing the relationship between some independent variables and a dependent variable. In particular it can be used for predicting/forecasting the dependent variable. In Machine Learning a regression task is a supervised task where the target (dependent variable) is numerical (as opposed to classification). It's used in a huge range of applications.</p>
<blockquote>
<p>what's its competitor methods</p>
</blockquote>
<p>There's no competitor since it's a whole domain: any method which does the same thing is a regression method so it's part of regression analysis.</p>
<blockquote>
<p>how least square method relates to regression analysis.</p>
</blockquote>
<p><a href=""https://en.wikipedia.org/wiki/Least_squares"" rel=""nofollow noreferrer"">Least Squares Regression</a> is a specific method which can be used in regression analysis. Its most common use is with linear regression, which is a specific case of regression for a linear function (it's one of the most simple forms of regression).</p>
","0","2","64377","14816"
"89339","<p>I think there might be a bit of confusion here: what is usually called a stable model is a model for which the performance doesn't vary (or not significantly) when sampling a different subset of training data or test data. In other words a model is stable <strong>if chance doesn't affect its performance</strong>. Typically one can use cross-validation in order to assess the stability of a model: if the variance is high between different splits then the model is unstable. Unstability is often a sign of overfitting, so one should be very cautious with an unstable model even if it seems to perform well: the risk is that the model might obtain good performance on the test set by chance, which means that it will actually perform poorly on new data but there will be no way to detect the problem. This is why people might favor a stable model to a high performance in some applications, because the consequences of deploying an unstable model in production would be costly.</p>
<p>But what you are talking about is not general model stability, it's <strong>stability of the performance across time</strong> for time-based data. That's a completely different question, it's about whether the model can represent time variations accurately. There are many types of models which can do that, but of course the more complex the model the more data it needs and the higher the risk of overfitting.</p>
","1","2","64377","14816"
"89412","<p>I don't think it's possible to know for sure if PU learning would work in your setting or not. It's certainly relevant to cases like the one you describe, so it would be worth trying. But there are other valid options, and even within PU learning there are different approaches to choose from (you might be interested in <a href=""https://datascience.stackexchange.com/q/26751/64377"">this question</a>).</p>
<p>In my opinion the alternative you propose with regression makes some sense and it might work, but it's not very &quot;clean&quot; in terms of design: first the choice of 0.1 is arbitrary (why not 0.2 or 0.05 or ...?). Second, it means that you're telling the regression algorithm that &quot;this instance should have probability 0.1&quot; for many negative instances and also for a few negative instances: this is different than saying &quot;I don't know the target value for this instance&quot;.</p>
<p>Note that you could also consider <a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">one class classification</a> in this kind of setting, (as part of PU learning or not).</p>
","1","2","64377","14816"
"89459","<p>Note that I don't know nmslib and I'm not familiar with search optimization in general. However I know Okapi BM25 weighting.</p>
<blockquote>
<p>How do they both (bm25, nmslib) differ?</p>
</blockquote>
<p>These are two completely different things:</p>
<ul>
<li>Okapi BM25 is a weighting scheme which has a better theoretical basis than the well known TFIDF weighting scheme. Both methods are intended to score words according to how &quot;important&quot; they are in the context of a document collection, mostly by giving more weight to words which appear rarely. As a weighting scheme, Okapi BM25 only provides a representation of the documents/queries, what you do with it is up to you.</li>
<li>nmslib is an optimized similarity search library. I assume that it takes as input any set of vectors for the documents and query. So one could provide them with vectors made of raw frequencies, TFIDF or anything else. What it does is just computing (as fast as possible) the most similar documents to a query, using whatever representation of documents is provided.</li>
</ul>
<blockquote>
<p>How can I pass bm25 weights to nmslib to create a better and faster search engine?</p>
</blockquote>
<p>Since you mention that the results based on BM25 are satisfying, it means that the loss of quality is due to the nmslib search optimizations. There's no magic, the only way to make things fast is to do less comparisons, and sometimes that means discarding a potentially good candidate by mistake. So the problem is not about passing the BM25 weights, it's about understanding and tuning the parameters of nmslib: there are certainly parameters which allow the user to select an appropriate trade off between speed and quality.</p>
","2","2","64377","14816"
"89470","<blockquote>
<p>Random Model Classifier</p>
</blockquote>
<p>It feels like there's a word missing here, you probably mean &quot;Random Forest [model] classifier&quot; or &quot;Random Fields classifier&quot;?</p>
<blockquote>
<p>the confusion matrix</p>
</blockquote>
<p>This is an important piece of information, because if you have a new (annotated) sample this allows you to compare a few things:</p>
<ul>
<li>compare the distribution of the classes in the original test data and the new dataset</li>
<li>calculate the expected performance by class, analyze the differences if you evaluate the model against the new sample</li>
<li>...</li>
</ul>
<blockquote>
<p>I would like to know is it right to say that &quot;model might have learn the pattern even tough all the confidence score is between 0.50 and 0.55&quot;</p>
</blockquote>
<p>It would be technically correct to say that indeed, but maybe not for the reasons that you mention (see below). While this predicted value is often called a confidence score or probability, it doesn't necessarily represents the confidence of the model in a normalized range [0,1] like one would expect, even though ideally it's supposed to. The details depend on the exact type of model, but in general in ML the model is only as good as the data it's trained with so most of the time the predicted score is just the &quot;best guess&quot; of the model. So it shouldn't be interpreted as &quot;probability of class C&quot;, and anyway in the case of classification the only use of the value is to compare two instances x and y relatively to each other: if x has a higher predicted score than y, then the model considers x more likely than y to be in class C. The actual value doesn't matter and it's better not to rely on it.</p>
<blockquote>
<p>Because I would like to explain that since the all the predictions score are concentrated around a single value it is not right to say that the model learnt and probably if some one would have tried to plot AUC it would have been straight line.</p>
</blockquote>
<p>This conclusion is incorrect. To be clear, it might be true in some particular case, but one cannot conclude this from the information given in the question. Why? Because the ROC curve (and the AUC) don't depend at all on the actual values of the score, they depend only on the order of the instances according to the score. The ROC curve shows the performance for different theresholds separating the predicted positive/negative instances. Whether the scores range in [0.51,0.52], [0,1] or even [-1234,+5678] doesn't have any influence, it's the proportion of negative instances under the threshold and positive ones above it which determine the performance at any given point.</p>
<p>In case you're interested, I gave some more detailed explanations about ROC curves in <a href=""https://datascience.stackexchange.com/a/87363/64377"">this answer</a>.</p>
","1","2","64377","14816"
"89502","<p>There's a bit of confusion about your task: first, apparently by &quot;emails&quot; you mean email <em>addresses</em>, not the full content of the emails, right?</p>
<p>What you're doing looks like <a href=""https://en.wikipedia.org/wiki/Record_linkage"" rel=""nofollow noreferrer"">record linkage</a>. The complexity of comparing every pair of records (here email addresses) can be decreased using the technique called blocking, where roughly similar pairs are grouped together in order to minimize the number of real comparisons (see Wikipedia page).</p>
<p>Now you're talking about using embeddings, and in this case what you'd need is <a href=""https://datascience.stackexchange.com/q/61491/64377"">character embeddings</a>. However it's very unlikely that embeddings will help with the efficiency issue, quite the opposite actually.</p>
","2","2","64377","14816"
"89595","<p>Technically this is <a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">sequence labeling</a>, the most common application being <a href=""https://en.wikipedia.org/wiki/Named-entity_recognition"" rel=""nofollow noreferrer"">Named Entity Recognition</a>.</p>
<p>However it looks like in this case you're trying to solve a problem of <a href=""https://en.wikipedia.org/wiki/Coreference#Coreference_resolution"" rel=""nofollow noreferrer"">coreference resolution</a>, which is a quite difficult task in general. I think this usually involves a more complex model than simple sequence labeling, but I'm not an expert in this. You might want to search around this topic, there are certainly some relevant <a href=""https://nlpprogress.com/english/coreference_resolution.html"" rel=""nofollow noreferrer"">papers</a> and <a href=""https://nlp.stanford.edu/projects/coref.shtml"" rel=""nofollow noreferrer"">tools</a> about it.</p>
","2","2","64377","14816"
"89628","<p>It looks like a resource allocation problem. I can't think of any statistical method which would help here, but maybe there is.</p>
<p>I think the process can be simplified by first calculating the amount in excess and in &quot;shortage&quot;. I don't know if there's a formal method for this but I would try something like the following:</p>
<ul>
<li>Distribute <span class=""math-container"">$a$</span> based on the weights. Then calculate for every entity:
<ul>
<li>its excess amount: X has 0, Y has 15 (75-60)</li>
<li>its missing amount: X has 0, Y has 0.</li>
<li>its capacity for additional amount. X has 15 (40-25) and Y has 0.</li>
<li>its capacity for giving away some amount. X has 15 (25-10) and Y has 20 (60-40)</li>
</ul>
</li>
<li>Calculate the sum of excess amount <span class=""math-container"">$S_{excess}$</span>, sum of missing amount <span class=""math-container"">$S_{missing}$</span>, and capacity <span class=""math-container"">$S_{plus}$</span>, <span class=""math-container"">$S_{minus}$</span>. Let <span class=""math-container"">$S = S_{excess} - S_{missing}$</span>:
<ul>
<li>If <span class=""math-container"">$S&gt;0$</span>
<ul>
<li>If <span class=""math-container"">$S&gt;S_{plus}$</span>, then no solution</li>
<li>Otherwise (1) fill every entity which has amount missing to their minimum, (2) distribute amount S by filling any entity which has capacity for more.</li>
</ul>
</li>
<li>If <span class=""math-container"">$S&lt;0$</span>
<ul>
<li>If <span class=""math-container"">$|S|&gt;S_{minus}$</span>, then no solution.</li>
<li>Otherwise (1) unload every entity which has amount in excess to their maximum, (2) take total amount |S| from any entity which capacity for giving away.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>If I'm not mistaken this requires only two steps, each step going over all the entities.</p>
","1","2","64377","14816"
"89633","<p>The task is a specific case of  NER (technically NER is a sequence labeling task, a special case of classification).</p>
<p>I think you would have two main options:</p>
<ul>
<li>Apply a pre-trained NER model: most deal with time entities but not always very accurately, and it wouldn't be specifically adapted to your data so you wouldn't obtain the distinction between the three types of values. Advantage: no need for training data.</li>
<li>Train your own NER model: that's the ideal scenario in terms of performance, assuming you have (or can have) a good amount of annotated data for training.</li>
</ul>
","1","2","64377","14816"
"89711","<blockquote>
<p>I tried with different features: in one dataset with less features engineering, i.e., using only features from Text, I got a maximum value of F1-score equal to 68%. With more features, that I thought to be significant for improving the model, I am getting max 64%, that is weird considering the problem (email classification for spam detection).</p>
</blockquote>
<p>Typically this would happen if the model is overfit: not enough data and/or too many features make the model pick patterns which happen by chance in the training data.</p>
<p>Usually with text one has to remove the least frequent words in order to avoid overfitting. You might also want to check the additional features, remove anything which happens too rarely.</p>
<blockquote>
<p>Also, the confusion matrix, has given me weird outputs</p>
</blockquote>
<pre><code>       0      1
0    [[2036  161]
1    [   1 2196]]
</code></pre>
<p>Observations:</p>
<ul>
<li>True class 0 has 2036+161 = 2197 instances, true class 1 has 1+2196=2197 instances. These results are obtained with the resampled data.</li>
<li>Assuming class 1 is positive: 2196 True Positives (TP), 2036 TN, 161 FP (true positive predicted as negative) and 1 FN (true negative predicted as positive).
<ul>
<li>recall = 0.999, precision = 0.932. That's an f1-score somewhere higher than 0.95 (probably due to the resampled data).</li>
</ul>
</li>
<li>The second confusion matrix is also clearly obtained with the resampled data, and it shows perfect performance (F1-score is 1).</li>
</ul>
<p>These matrices show the performance obtained on the resampled data, so it's similar to the performance on the training data. Since the performance on a real test set is much lower, this confirms strong overfitting.</p>
","1","2","64377","14816"
"89851","<p>You can use these words with their weight as a vector representation of the document. The important point is to make all the documents vectors over the full vocabulary, so that any position <span class=""math-container"">$i$</span> in any vector always represents the same word <span class=""math-container"">$w_i$</span>. This means that a vector should contain zeros in all the positions corresponding to a word which is not in the document.</p>
<p>Using these vectors you can indeed use k-means to cluster the documents. Of course the quality of the results depends on the data: if there are very few words in common, it cannot work very well.</p>
","0","2","64377","14816"
"89853","<p>It depends on many factors: number of instances, number of classes, token/type ratio, etc. A common basic technique in order to avoid having too many words is to discard the ones which appear rarely, for example by discarding all the words which appear in less than 3 documents. Selecting only the top <span class=""math-container"">$N$</span> most frequent words is unusual, even though it's a similar idea. There is a risk to end up mostly with words which appear frequently and don't have a good discriminative power.</p>
<p>The proper way to determine an optimal threshold, for example the minimum frequency <span class=""math-container"">$N$</span>, is to run a training/testing experiment for every value of <span class=""math-container"">$N$</span> and select the value corresponding to maximum performance. Note that this experiment should use a validation set different from the final test set in order to avoid data leakage.</p>
","0","2","64377","14816"
"89864","<p>This approach makes some sense but it's not the best approach for several reasons.</p>
<p>First, this control variable might not always be last in importance because it's possible that some other variable also don't have any impact at all on the target variable (outcome).</p>
<p>More importantly, the concept of a control group/control variable is useful in cases where one cannot evaluate objectively the effect of a method. Typically a drug trial needs a control group because of the placebo effect and various other biases. The control group plays the role of a baseline, and the performance of the method is measured relatively to this baseline.</p>
<p>In supervised classification, there is a much more direct way to objectively evaluate the effect of the method: this is exactly what we do when we evaluate the performance on a proper test set (fresh instances). Additionally there are simple ways to compare the performance of a system to a baseline: if there is no previous similar system for the task, one can simply use a random baseline or a majority baseline classifier.</p>
","1","2","64377","14816"
"89900","<p>A very simple measure of imbalance would be the standard deviation of the classes proportions.</p>
<ul>
<li>Since it's based on proportions one can compare the imbalance between different datasets</li>
<li>This takes into account all the classes, so if there are many classes it would give a different value depending on whether there are many small and many large classes (higher imbalance overall) or if there is only one outlier class (lower imbalance overall).</li>
</ul>
","2","2","64377","14816"
"90031","<blockquote>
<p>Why are the training and test accuracy almost identical?</p>
</blockquote>
<p>Nearly identical performance on the training set and test set is a good outcome, it means the model is doing what it's supposed to do. To give an intuitive comparison:</p>
<ul>
<li>The performance on the training set is equivalent to how well a student can redo the exercises which have been solved by the teacher during class. The student might just have memorized the answers by heart, so it's not a proof that they understand.</li>
<li>The performance on the test is equivalent to how well the student can solve some similar exercises that they haven't seen before in a test. This is a much better indication that the student truly understands the topic.</li>
</ul>
<blockquote>
<p>Does this mean that there is no overfitting?</p>
</blockquote>
<p>Yes, it proves that there's no overfitting. To keep with my comparison, overfitting is equivalent to memorizing the answers.</p>
<p>However there can be other problems which bias the result:</p>
<ul>
<li>The performance on the test set is 2 points higher than the performance on the test set. This probably means that the test is very small, because if it was a large enough sample the performance wouldn't be higher. If the test set is too small, the performance is less reliable (any statistics obtained on a small sample is less reliable).</li>
<li>Accuracy can be a misleading evaluation measure. It only counts the proportion of correct predictions, so if a large proportion of instances belong to the same class then the classifier can just predict any instance as this class and obtain high accuracy. For example here if the majority class is around 63-65%, then it's possible that the classifier didn't learn anything at all. Looking at precision/recall/F1-score gives a more accurate picture of what happens.</li>
</ul>
<p>[edit] Important note: as Nikos explained in a comment below, my answer assumes that you have a proper test set, i.e. that the train and test sets are sufficiently distinct from each other (otherwise there could be data leakage and the test set performance would be meaningless).</p>
","2","2","64377","14816"
"90032","<p>I don't know ludwig or even &quot;parallel CNN&quot; but there's a clear problem: this is not a confusion matrix, this is a heat map.</p>
<ul>
<li>A confusion matrix shows the number of instances for every combination of predicted/true label. It's for categorical labels, you could indeed obtain TP/FP/FN/TN classification status from it.</li>
<li>A heatmap is for numerical values, as can be seen from the scale on the right side.</li>
</ul>
<p>It's very likely that this heatmap is made of the <em>probabilities</em> of the labels for every instance, rather than the labels themselves (also the choice of colours is unusual but this doesn't matter).</p>
<p>The good news is that it appears that most instances which have 0 or 1 as true label are correctly predicted (high value for 1,1 and 0,0).</p>
<p>The bad news is that there's an issue in the labels themselves: apparently a large number of instances (at least a third I think) don't have any label and are considered 'UNKNOWN'. And apparently the system just considers these 'UNKNOWN' instances as a third class, so your labels are not binary actually. It manages to recognize this class quite well, but it's probably an issue you should investigate.</p>
","0","2","64377","14816"
"90034","<p>There are many options and many different problems here, I think you should start simple and then try to improve on it.</p>
<p>Imho the most basic option is to restructure the dataset so that there's one instance for every patient at any given time <span class=""math-container"">$t$</span>, for instance every month. The binary label represents whether the patient was admitted to hospital in the period <span class=""math-container"">$t$</span> to <span class=""math-container"">$t$</span>+60 days. The features should represent whatever relevant indicators can help the model based on past data (before <span class=""math-container"">$t$</span>), for instance mean/standard deviation of the blood pressure in the past N months, possibly with various values of <span class=""math-container"">$N$</span> and/or a sliding window. This way both short-term and long-term information can be integrated. Occasionally you might have missing values, it's important to take this into account.</p>
<p>In this design there will be several instances for every patient, and this could cause a bias in the evaluation. There are two options: either there is one independent model for every patient, but this is probably too complex. Or the datasets is made of all the patients, and in this case it's important to avoid having the same patient in the training set and test set.</p>
<p>The problem of patients who would take an ambulance to the ER on a regular basis is a preprocessing problem: if possible, these instances should either be discarded or re-labelled as if it was a regular consultation, for example.</p>
","1","2","64377","14816"
"90123","<p>Sorry I'm not going to read the paper, but in general it's perfectly possible to use binary values 0/1 as numerical regression targets. A regression model doesn't care about having a continuous sample in the training set, it just calculates the relationship between the features and the target variable. For example a linear regression needs only two points to learn a model in theory. Of course this doesn't work if there are specific patterns either between the two target values or outside them, the model can only generalize on the full continuous range. As a result it will mostly predict values in the range [0,1] but it can also predict values outside of it.</p>
","0","2","64377","14816"
"90124","<p>Welcome to the biomedical domain, one of the few domains in NLP where there are too many resources to choose from :)</p>
<ul>
<li>Data resources:
<ul>
<li><a href=""https://www.nlm.nih.gov/medline/medline_overview.html"" rel=""nofollow noreferrer"">Medline</a> is a database corpus of <em>30 millions</em> abstracts.</li>
<li>Each Medline abstract is annotated with <a href=""https://www.nlm.nih.gov/mesh/mbinfo.html"" rel=""nofollow noreferrer"">Mesh descriptors</a>, Mesh being a structured hierarchy of medical concepts.</li>
<li><a href=""https://www.ncbi.nlm.nih.gov/pmc/"" rel=""nofollow noreferrer"">PubMed Central (PMC)</a> is a database of around 6 millions full biomedical articles (not only abstracts).</li>
<li><a href=""https://www.nlm.nih.gov/research/umls/index.html"" rel=""nofollow noreferrer"">UMLS</a> is a database of millions of medical terms grouped by concept, themselves grouped by semantic type (e.g. disease, gene, etc.)</li>
<li><a href=""https://www.ncbi.nlm.nih.gov/research/pubtator/"" rel=""nofollow noreferrer"">PubTator</a> is a resource which provides all the Medline and PMC documents fully annotated with a combination of Mesh and other ontologies.</li>
</ul>
</li>
<li>Software tools:
<ul>
<li><a href=""https://metamap.nlm.nih.gov/"" rel=""nofollow noreferrer"">MetaMap</a> is the venerable annotator system which annotates any medical text with UMLS labels.</li>
<li><a href=""https://ctakes.apache.org/"" rel=""nofollow noreferrer"">cTakes</a> is another annotator system which is more specialized with clinical texts.</li>
<li><a href=""https://allenai.github.io/scispacy/"" rel=""nofollow noreferrer"">SciSpacy</a> is a Spacy variant specialized for biomedical text. It can also annotate medical terms with UMLS labels.</li>
</ul>
</li>
</ul>
<p>I think that's all the main ones that I know of... so far.</p>
<p>From your description it looks to me like you probably just need cTakes or SciSpacy. In case you're going to start working with Medline or PMC, be aware that these datasets are massive (a few hundreds GBs).</p>
","2","2","64377","14816"
"90150","<p>It's normal: LDA tries to maximize the likelihood of the data according to the parameters by finding the right probabilities for the parameters. Usually at the beginning increasing the number of topics allows the model to separate topics more precisely and therefore obtain a higher likelihood. But at some point (depending on the data), increasing the number of topics cannot help the model anymore because the topics are already separated to the maximum and using all the topics would actually decrease the likelihood.</p>
<p>So it's a sign that you don't need that many topics. Note that it doesn't mean that the number of &quot;used&quot; topics is optimal for the application, it's often a balance to find.</p>
","1","2","64377","14816"
"90174","<p>The NER model performance on a particular text depends on which data it was trained with originally, and naturally the standard models (like <code>en_core_web_sm</code>) are trained with English data which doesn't contain a lot of names from non-US/UK origin (same for other kinds of entities like organizations or locations).</p>
<p>Better performance can be achieved by training your own model with your own labelled data, but that requires you (or somebody) to annotate a reasonably large sample of data manually.</p>
","1","2","64377","14816"
"90192","<blockquote>
<p>Does this mean this baseline model is not good for comparing to my experimental models and I should use baseline_0 instead, or that baseline_1 is good but that the f1 score is not good for making these comparisons?</p>
</blockquote>
<p>By definition a majority classifier like baseline_1 is stronger than a random classifier like baseline_0 (unless the classes are uniformly distributed) since it always selects the majority class and therefore predicts more correct instances.</p>
<p>Here the recall is perfect for baseline_1 because the positive class is the majority class. In most tasks the minority class is chosen as the positive class, so this wouldn't happen. However that doesn't mean that the baseline is bad for the task, and it doesn't mean that F1-score is wrong either. With some tasks it can even happen that it's difficult for a &quot;real&quot; classifier to beat the baseline, but this doesn't necessarily mean that the &quot;real&quot; classifier is bad. A baseline classifier is just an indication of how a naive method performs.</p>
","0","2","64377","14816"
"90296","<blockquote>
<p>When the dimension is high, all data are approximately at the same distance away from each other.</p>
</blockquote>
<p>Yes, but this is the worst case scenario of the curse of dimensionality:</p>
<ul>
<li>This happens only if the data is extremely sparse. In real cases it's not really &quot;all data&quot;, because some data points are less sparse than others, and it's not really &quot;the same distance&quot;, because there can be small but meaningful differences in the distance between points.</li>
<li>There are known mitigation measures which can prevent this worst case scenario to happen, in particular various feature selection/extraction methods. So in real cases it's simply a mistake to work with some very high dimensionality data left &quot;untreated&quot;.</li>
</ul>
<blockquote>
<p>Why not to encode nominal categorical variables of high cardinality with randomly distributed points in some high-dimensional space but such that the number of dimensions is much less than we would get with the one-hot encoding?</p>
</blockquote>
<p>This would correspond to a random feature extraction method: instead of trying to group similar features together when reducing the dimensions, features are just randomly projected into a low-dimension space. The disadvantage is clear: instead of simplifying the data while preserving the patterns, the patterns in the data are just ignored. It's likely that the data would become pretty much useless after that.</p>
<p>The goal is not just to reduce dimensionality for the sake of it, it's to reduce dimensionality in a way which leaves the most important characteristics of the data as intact as possible, or even emphasize them by removing the noise. This method would be equivalent to randomly selecting features instead of selecting the most informative ones.</p>
<p>For example, in the case of text data there is often high dimensionality due to the high number of distinct words. A basic but very effective method consists in discarding words which appear rarely: this reduces the dimensions a lot (due to the Zipf distribution of words in a text), prevents noise due to words which happen by chance, and preserves most of the patterns which happen with moderately frequent words.</p>
","0","2","64377","14816"
"90323","<p>It's a matter of defining exactly which problem you want to solve, and there might be many variants:</p>
<ul>
<li>If the goal is really to estimate &quot;time completion&quot;, then imho you should use only completed tasks, since the other tasks haven't been &quot;completed&quot;. Note that in this case you're counting time actually spent on the task.</li>
<li>If the goal is to estimate &quot;time of solving the task&quot;, whether by completing it or cancelling it, then you're counting the duration between the time the task was initialized and the time it was either completed or cancelled. Note that in this case the duration may include time spent on other tasks.</li>
</ul>
<p>In both cases above, I don't see any proper way to include tasks which are still pending. My idea for these cases would be to calculate a different statistic, something like &quot;rate of completed tasks after X days&quot; for instance.</p>
","1","2","64377","14816"
"90431","<blockquote>
<p>But the alternative is to navigate a treebank, through &quot;type of&quot; relationships… much, much faster and cheaper.</p>
</blockquote>
<p><a href=""https://wordnet.princeton.edu/"" rel=""nofollow noreferrer"">WordNet</a> provides exactly this: it is a lexical database in which words are grouped by synonyms, with several types of relations between groups in particular hypernyms/hyponyms (more general/more specific).</p>
<p>The database can be <a href=""https://wordnet.princeton.edu/download"" rel=""nofollow noreferrer"">downloaded</a> and there is a <a href=""https://www.nltk.org/howto/wordnet.html"" rel=""nofollow noreferrer"">library to use it through nltk</a>.</p>
","1","2","64377","14816"
"90488","<p>You have two problems:</p>
<ol>
<li>Technical problem: As 10xAI said in a comment, if the target also belongs to the features then the model should very easily predict every instance correctly. So you should obtain perfect performance on the test set. The ML model doesn't care about &quot;cyclic dependency&quot;, it uses any good indicator it receives as feature. Since you mention a reduction in accuracy, it means that there is an error somewhere in the process.</li>
<li>Semantic problem: which problem are you trying to solve with this ML setup? If the goal is to distinguish software vs. non-software and you have as input a category &quot;software&quot; then it's pointless to train a ML model. You can obtain the same result much more efficiently with a simple condition:</li>
</ol>
<pre><code>for each instance:
  if instance.category == &quot;software&quot;: 
     instance.answer=&quot;software&quot; 
  else:
     instance.answer=&quot;non-software&quot;
</code></pre>
","0","2","64377","14816"
"90534","<p>Here are a few comments from my perspective:</p>
<ul>
<li>First, methodologically this kind of approach must be done carefully, but people who work this way are rarely careful. training/evaluating many different types of models and/or parameters is equivalent to doing parameter tuning, i.e. it's essentially <em>training a model</em>. As a consequence the performance should be evaluated on a validation set, and after that the best model is selected and only this model should be evaluated on the (unseen) test set. A common mistake is to evaluate all the models on the same test set and assume that this gives the real performance. This is wrong, the best performance could have been obtained by chance and therefore the true performance is likely lower.</li>
<li>While it is normal to try to maximize performance and to some extent this can involve a fair amount of experimental evaluation, trying many different models/parameters blindly is in my opinion very poor practice and it rarely leads to the best results (but it can lead to an illusion of good result, see point above). Most of the time there is much more to gain by doing a fine-grained analysis of the data, including studying expert knowledge about the context, than blindly relying on black box algorithms. A simple example is feature engineering: sometimes some simple redesigning of the features can drastically improve performance, but there's no ML technique which can replace this (DL methods may come close to it, but usually at a much higher computational cost).</li>
<li>Final point: the quest for maximum performance is often misguided. Sometimes performance is used as a lazy excuse for not doing the job properly: what really matters in production? Is it really 0.3% more F1-score or an interpretable result? Was the evaluation measure chosen to really reflect the quality of the prediction or just &quot;because it's standard&quot;? And the training/test sets, do we know if the production data will look exactly the same? If not, is the ML method robust enough to handle these variations? When optimizing performance one often relies on the model catching the most subtle patterns, which means a higher risk of overfitting and mediocre performance in production. More generally, there are more questions (at least in research) about explainability, the role of human actors in a ML process, and of course the computational complexity and the environmental cost of the ML method. All of these points are not reflected by a performance measure, but they matter.</li>
</ul>
<p>Overall I would consider this kind of approach as mediocre: it's inelegant, it's prone to mistakes (which often won't be detected) and it's short-sighted because it ignores other dimensions of the quality of a ML system.</p>
<p>By the way this approach can be automated. Personally I would not be very proud (and probably even worried) if my job could be done entirely by a program.</p>
<blockquote>
<p>Have people found that this is fairly common?</p>
</blockquote>
<p>In research my guess is that this would be common enough in low-level publications, but it's unlikely to pass at any reputable journal/conference (because expert reviewers can usually detect it).</p>
<blockquote>
<p>How can you work on a machine learning problem in a non-random-try-everything kind of way?</p>
</blockquote>
<p>That's the thing, there's no recipe... But that's the beauty of it, isn't it? :)</p>
<p>In my opinion it's a bit like medicine: a good doctor has a good intuition because they have a solid theoretical background and a lot of practice. They don't just follow some rigid manual, they see the patient as a unique individual and they do their best to understand every aspect of the problem.</p>
","2","2","64377","14816"
"90535","<p>Simple approach:</p>
<ol>
<li>Define an appropriate distance or similarity measure between two such data frames. It's unlikely that there is a standard measure for whatever game this represents. For example you could have a distance measure which sums every value in each data frame and then return the absolute difference between the two values, but this is unlikely to correctly represent the semantic of the game.</li>
<li>Implement this distance or similarity measure between two such data frames as a function</li>
<li>Call this function for every possible pair of data frames (that's 49,995,000 comparisons, it's doable).</li>
</ol>
<p>Fair warning: a heatmap of an unordered matrix 10k x 10k might not be very exploitable.</p>
<p>Note that once you have the distance/similarity function, you could also:</p>
<ul>
<li>use a clustering algorithm to group the data frames by similarity</li>
<li>detect outliers</li>
</ul>
","1","2","64377","14816"
"90544","<blockquote>
<p>Can machine learning algorithms predict random number</p>
</blockquote>
<p><strong>No, absolutely not</strong>.</p>
<p>ML is not a magical process, it works by identifying patterns in the data. In the case of supervised learning (i.e. with training) it means finding in the features the most relevant indications to help predict the target variable.</p>
<p>Randomness is the complete opposite of &quot;patterns&quot;, by definition something truly random is not predictible in any way.</p>
<p>Intuitively if a human with a lot of time cannot do a task, then ML cannot do this task either.</p>
","1","2","64377","14816"
"90548","<p>First things first: be sure to use a validation set in order to determine the optimal threshold, do not use the final test set.</p>
<p>Another general point: it depends what you mean by &quot;false alarm&quot;, but in general you should not assume that it is possible for the classifier to keep the error rate under a predefined value (unless you have already done an experiment which shows this, of course). If by &quot;false alarm&quot; you mean only one type of error then it is usually possible to keep one type of error under a threshold at the expense of the other type of error. For instance if the goal is to minimize false positive errors, then one can let the classifier make as many false negative errors as necessary in order to keep the false positive rate under the threshold.</p>
<p>Now about quantifying uncertainty:</p>
<ul>
<li>The first approach that you propose makes sense, but as you said it's not symmetric and also it wouldn't allow to convey uncertainty in any practical way. Ideally you'd want to be able to say something like &quot;there's an X% chance that the prediction is correct&quot; but you cannot obtain that from a difference with the threshold.</li>
<li>As far as I understand your second approach goes in the right direction but I'm not sure if the details are clear. So what you could do is to determine experimentally the rate of error (either FPR or TPR depending on which side of the threshold) for different intervals of predicted <span class=""math-container"">$\hat{y}$</span>. For example if <span class=""math-container"">$T=0.9$</span> you can calculate the FNR for every interval <span class=""math-container"">$[0.0,0.1]\ [0.1,0.2]\ ...\ [0.8,0.9]$</span>. This way for any predicted <span class=""math-container"">$\hat{y}$</span> you can provide the corresponding error rate.</li>
</ul>
","1","2","64377","14816"
"90655","<p>The general principle about finding text based on meaning similarity is called <a href=""https://en.wikipedia.org/wiki/Distributional_semantics"" rel=""nofollow noreferrer"">distributional semantics</a>. The main idea is that words with a similar meaning statistically tend to co-occur with the same context words. This is the basis for many standard tasks like <a href=""https://en.wikipedia.org/wiki/Topic_model"" rel=""nofollow noreferrer"">topic modeling</a>, <a href=""https://en.wikipedia.org/wiki/Word-sense_disambiguation"" rel=""nofollow noreferrer"">word sense disambiguation/induction</a>, and generally for everything related to semantics.</p>
<p>The problem you're describing is essentially <a href=""https://en.wikipedia.org/wiki/Information_retrieval"" rel=""nofollow noreferrer"">information retrieval</a>: given a query expressed as one or several words, find the &quot;documents&quot; in a corpus which are semantically similar to this query.</p>
<p>[edit] Depending what the data looks like, you might be interested in the more specific task of <a href=""https://nlpprogress.com/english/semantic_textual_similarity.html"" rel=""nofollow noreferrer"">semantic textual similarity</a>.</p>
","0","2","64377","14816"
"90666","<p>Intuitively the problem of imbalanced data can be understood like this: if a classifier is not really sure how to classify an instance but it knows that most instances belong to class X, then whenever there's a doubt predicting class X is always the best decision. As a consequence, the classifier unavoidably assigns class X too often since all the &quot;unsure&quot; cases end up being labeled as the majority class X.</p>
<p>So first it's important to understand that resampling is not automatically the &quot;cure&quot; for imbalanced data:</p>
<ul>
<li>Resampling doesn't provide the classifier with more information, it just presents it in a different way in order to force the classifier to pay more attention to the minority class.</li>
<li>In case the data is easy to separate, the classifier can do a perfectly good job without resampling. This means that whenever possible it's better to improve the features rather than using resampling, because that's what can actually help the classifier doing its job.</li>
</ul>
<p>This being said, resampling is a useful technique in some cases. Assuming the standard choice of the minority class as the positive class, resampling will only increase recall at the expense of precision: as said above, the difference happens when the classifier cannot easily predict the class for an instance. In such a case it has two choices:</p>
<ul>
<li>Assigning the negative majority class: more likely to be correct (True Negative), a small risk of False Negative error.</li>
<li>Assigning the positive minority class: more likely to be incorrect (False Positive), a small chance to be correct (True Positive)</li>
</ul>
<p>Without resampling the classifier favors the first option, so it has few FP errors but quite a lot of FN errors. Therefore it can have quite high precision but low recall.</p>
<p>With resampling the classes are equal, so the classifier stops favoring the first option. Therefore it makes less FN errors but more FP errrors, which means that it increases recall at the expense of precision.</p>
","1","2","64377","14816"
"90694","<p>One uses a frequency or TFIDF representation in the features when the target directly depends on specific words. For example for spam classification words like &quot;cheap, free, viagra, exclusive...&quot; are direct indicators of the target label.</p>
<p>In your case the target doesn't directly depend on specific words, it depends whether the same words appear in both the customer and product descriptions. This is an indirect relationship and most regular ML algorithms cannot really deal with that. So your design is unlikely to work in my opinion.</p>
<blockquote>
<p>Until now, I only got to build a similarity pairwise coefficient (using this), but there is no classification, and I know using labelled data can help. In particular, similarity measure gives the same weight to any text coincidence, but some coincidences should be more important than others.</p>
</blockquote>
<p>This makes more sense for your purpose: use only the similarity score as a feature and train a model to predict the label. Technically the model will only learn the optimal threshold to separate the labels, so you can just use linear regression for instance. You could improve this method by calculating different types of similarity measures and provide all of them as features.</p>
<p>Note: if you use TFIDF vectors for measuring similarity, it doesn't give the same weight to every word. However don't expect perfect result: a lot depends on the data itself, are you sure that the customer description gives useful indications about the products they're interested in? For example if a customer description contains the word &quot;computer&quot; it doesn't mean they're interested in every possible type of computer.</p>
","0","2","64377","14816"
"90753","<p>It might make sense, but it depends what you're trying to do:</p>
<ul>
<li>If the goal is to predict the binary target for any instance, a classifier will perform much better.</li>
<li>If the goal is to group instances by their similarity, loosely taking the binary target into account indirectly, then clustering in this way makes sense. This would correspond to a more exploratory task where the goal is to discover patterns in the data, focusing on the features which are good indicators of the target (it depends how good they actually are).</li>
</ul>
","0","2","64377","14816"
"90906","<p>It's not a great dataset for NER because normally NER relies on trigger words close or inside the NE. For example &quot;he went to X&quot; would indicate that X is a location. Here there's no usable context and even the order of the lines can change.</p>
<p>What I would try here is to classify the lines, because apparently there's a kind of structure to each sample:</p>
<ul>
<li>sender/recepient</li>
<li>email address</li>
<li>phone no</li>
<li>website</li>
<li>Instructions (e.g. &quot;handle with care&quot;)</li>
</ul>
<p>Once all the types of lines which are not a product are eliminated, the ones remaining are likely to describe the product.</p>
","0","2","64377","14816"
"90916","<p>Strictly speaking Machine Learning is not the answer to this problem imho, because a ML method always works by generalizing what is in the data. In other words ML methods are meant to make some guesses while statistically minimizing the risk of error: if you don't want any generalization or risk of error, you don't want ML.</p>
<p>There might be some symbolic methods which could do what you describe, but essentially it's just a very simple deterministic method:</p>
<ol>
<li>Obtain the frequency for each distinct instance (including label) in the training data in a map</li>
<li>For each distinct instance (only features) keep the most frequent one and discard all the others.</li>
<li>When applying, just look up the instance in the map and assign the corresponding label.</li>
</ol>
<blockquote>
<p>a cumbersome db infrastructure with columns, keys, indexes and other boring things.</p>
</blockquote>
<p>There's a confusion here: the question of using a database or not is irrelevant here since it's a matter of how you store the data, it's just not related to using ML or not.</p>
<p>Usual advice: use the right tool for the right job... even if it's boring ;)</p>
","1","2","64377","14816"
"90922","<p>As far as I understand, this looks like a distance-based problem. I think <a href=""https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"" rel=""nofollow noreferrer"">k Nearest Neighbors</a> could work well with this kind of data.</p>
","0","2","64377","14816"
"90963","<p>I can give you some basics but the proper way to do this is to learn the prolog language, and it's very different from any other programming language.</p>
<p>In Prolog you define predicates (similar to functions) which contain conditions for the predicate to be satisfied, for example:</p>
<pre><code>dt(Feat1, Feat2, Feat3, labelA) :-
    Feat1 =&lt; 3,
    Feat2 == 'green',
    Feat3 &gt;= 0.54.

dt(Feat1, Feat2, Feat3, labelB) :-
    Feat1 =&lt; 3,
    Feat2 == 'green',
    Feat3 &lt; 0.54.

dt(Feat1, Feat2, _Feat3, labelC) :-
    Feat1 &gt; 3,
    Feat2 == 'green'.

dt(_Feat1, Feat2, _Feat3, labelB) :-
    Feat2 == 'blue'.
</code></pre>
<ul>
<li>Here the predictate is <code>dt</code> and it has 4 clauses. In order for <code>dt</code> to be satisfied at least one of the clauses needs to to be true (i.e. all the conditions it contains must be true).</li>
<li>Variables are represented with a capital, for instance <code>Feat1</code>, <code>Feat2</code>, <code>Feat3</code>. Prolog will try to find an instanciation of the variables which satisfies one of the clauses (it will actually try all the possibilities until one of them works, it's a solver).</li>
<li>In each clause above the label is the last parameter of the predicate. Basically each clause means &quot;if all the conditions after <code>:-</code> are satisfied, then the label is labelX&quot;.</li>
</ul>
<p>Assuming the 4 clauses above have been saved in a file <code>dt.pl</code>, one can use for instance the SWI Prolog interpreter like this:</p>
<pre><code>?- consult('dt.pl').
true.
</code></pre>
<p>Now the way to use this toy decision tree would be:</p>
<pre><code>?- dt(4,'green',0.3, Label).
Label = labelC .

?- dt(4,'blue',0.3, Label).
Label = labelB.
</code></pre>
<ul>
<li>Note the capital L for <code>Label</code>: it means that now we let prolog find a value of <code>Label</code> which satisfies the predicate <code>dt</code> given the values provided for the features.</li>
</ul>
<p>This is the basic idea. A more advanced and more generic representation of the DT would be possible using recursion and unification, but this would require an advanced understanding of Prolog.</p>
","0","2","64377","14816"
"90983","<p>Your description corresponds to the general problem of grammatical inference (or <a href=""https://en.wikipedia.org/wiki/Grammar_induction"" rel=""nofollow noreferrer"">grammar induction</a>): as opposed to statistical models, the goal is to learn the exact language represented as a formal grammar. Importantly this is symbolic learning, i.e. there is no concept of minimizing error because errors are not acceptable at all.</p>
<p>As far as I know the field is rather theoretical, as opposed to statistical ML: one formally proves that some classes of languages can or cannot be learnt under specific constraints. A type of learning framework which used to be studied in this context (not sure if it's still active) is <a href=""https://en.wikipedia.org/wiki/Language_identification_in_the_limit"" rel=""nofollow noreferrer"">language identification in the limit</a>. A quite different framework is <a href=""https://en.wikipedia.org/wiki/Probably_approximately_correct_learning"" rel=""nofollow noreferrer"">Probably approximately correct learning (PAC)</a>, but in this is more general and not strictly about grammatical inference (to the best of my knowledge).</p>
<p>So I'm not aware of any real dataset: in general arbitrary datasets don't have the kind of formal constraints which are relevant for this kind of problem. But it's possible that some parts of the literature contain at least artificial datasets.</p>
","0","2","64377","14816"
"91006","<p>The assumption in supervised ML is that the test set follows the same distribution as the training set. It's clear from your description that this assumption is not satisfied in your data, so it's not very surprising that things don't work well. Beyond the difference in the range of the dependent variable, if the training and test set have been collected independently it's likely that the distributions of the features also differ.</p>
<p>So how to fix this depends on the goal of the task and in particular why the test set and training set don't follow the same distribution:</p>
<ul>
<li>If the difference is not meaningful, i.e. there is no reason to assume that real production data would be different than the training set, then the experiment should be redone after properly randomizing the split between training and test set.</li>
<li>If the difference is meaningful, i.e. the test set is purposefully different from the training set because the production data will also be different, then the setup should be redesigned to account for that. For example there could be a step of semi-supervised learning on the test set, which would adapt the model to the new data distribution while leveraging the information from the training data.</li>
</ul>
","1","2","64377","14816"
"91018","<p>My intuition would be to try to integrate the information about the products directly in the original model. Typically the possible products in a shipment can be represented as boolean features (one hot encoding), but this part might need some feature engineering if there are too many different products:</p>
<ul>
<li>simple option: only a small set of features representing types of products (I'm assuming that it's not the specific product which causes custom delays, it's the type of product)</li>
<li>advanced option: feature selection/extraction to reduce the number of features</li>
</ul>
<p>Generally a joint model (a single model which deals with all the information at once) tends to perform better, in particular because in the other option errors in the first model propagate to the second one. Also the two models option doesn't allow the second model to leverage any specific feature from the first one.</p>
<p>Note that this is just my intuition, I could be wrong.</p>
<p>Side note: probably this is already taken into account but I guess that the value of the shipment is also an important factor for customs delays.</p>
","1","2","64377","14816"
"91035","<p>I don't know if there are other clustering methods which would work with this amount of data, but with K-means I would suggest this:</p>
<ol>
<li>Run K-means with a varying number of instances picked randomly and study how much variation there is between the centroids depending on the data size (you can also study the variation across different random samples). I would expect the centroids to stabilize quite quickly with respect to data size: assuming the centroids become stable with data size N, there's no point running the full K-means process with more data.</li>
<li>Having obtained a model with N data points (the centroids), the model can be applied to all the remaining data points in order to find which cluster they belong to.</li>
</ol>
<p>This is much more efficient than running the K-means process over the whole data since in the second stage the centroids are fixed, the algorithm doesn't have to iteratively update them.</p>
","1","2","64377","14816"
"91064","<p>Overall the paper is not very clear so there are a few uncertainties, but the general approach is this:</p>
<ul>
<li>Their main idea is to create new features which represent the &quot;outlyingness&quot; of the instance. They use several different methods in order to detect outliers, however they do not explain how exactly are the new features created.</li>
<li>One of the methods they use to detect outliers is based on hierarchical clustering: the result of such clustering is a binary tree in which the most distant clusters/instances are connected last, i.e. close to the root of the tree. Their assumption is that an outlier tends not to be close to any other instance or cluster, therefore they are connected last. While the method makes sense, it's not clear in the paper whether they only retain the very last instance as outlier or the last few instances (or any other variant).</li>
</ul>
<p>So the clustering and the feature creation are only indirectly related:</p>
<ol>
<li>The clustering is used to detect one or several outliers in the instances, and several other methods are used for the same purpose.</li>
<li>Based on the detection of these outliers, one or several new features are created which contain a value describing the &quot;outlier status&quot; of the instance. The simplest option would be to create a single boolean feature which is true if the instance was detected as outlier by any of the methods, but one can imagine more advanced options. For example, based on the hierarchical clustering one can obtain the order in which the instances are connected, and the rank of every instance can be used as a feature.</li>
</ol>
","0","2","64377","14816"
"91128","<p>The first thing to do is to reorganize the data in a way which suits the problem that you're trying to solve: since the goal is to predict the number of each type of vehicle by month, your data should contain a column for this number.</p>
<p>Here you need to count the number of rows for each month and each type of vehicle so that you obtain something like this:</p>
<pre><code>v_type   district    month number
Advanced District 1  jan   1
General  District 1  jan   2
Advanced District 2  jan   0
General  District 2  jan   3
Advanced District 1  feb   2
General  District 1  feb   3
...
</code></pre>
<p>After that you will probably need to represent the month (and year) in a way that a regression algorithm can use, typically an integer starting at month 1.</p>
","1","2","64377","14816"
"91163","<p>I don't know about any specific recommendation related to BERT, but my general advice is this:</p>
<ul>
<li>Do not to systematically use oversampling when the data is imbalanced, at least not before specifically identifying performance issues caused by the imbalance. I see many questions here on DataScienceSE about solving problems which are <strong>caused</strong> by using oversampling blindly (and often wrongly).</li>
<li>In general resampling doesn't work well with text data, because language diversity cannot be simulated this way. There is a high risk to obtain an overfit model.</li>
</ul>
","1","2","64377","14816"
"91181","<p>Since the process can be run independently on every file/batch, I would tend to recommend processing each file one by one for the sake of scalability:</p>
<ul>
<li>Depending on the details of the task there could be some minor optimizations which can be done only by processing the whole data at once, like things related to I/O and memory usage. I can't think of any significant improvement obtainable this way.</li>
<li>By itself processing the files independently won't bring any performance improvement either, but it has a massive advantage in terms of scalability: the process can be distributed easily by using multiple cores and processing any number of files on each core. This option offers flexibility for processing the current data (if you can afford multiple cores/machines, the process can be faster) and importantly any future version of the data, even if it grows very large.</li>
</ul>
","0","2","64377","14816"
"92233","<p>Disclaimer: this answer might be disappointing ;)</p>
<p>In general my advice would be to carefully analyze the errors that the model makes and try to make the model deal with these cases better. This can involve many different strategies depending on the task and the data. Here are a few general directions to consider:</p>
<ul>
<li>Most of the time the imbalance is not the real problem, the real problem is why the model can't differentiate between the classes. Even in case of extreme imbalance if the classes are easy to discriminate a model can perform very well. The imbalance only causes the model to assign the majority class when it doesn't have enough indication to decide, so it resorts to the conservative choice.</li>
<li>If the minority class is really small in absolute terms it's likely that there's not enough language diversity in the positive instances (data sparsity). This will usually cause a kind of overfitting which can be hidden by the fact that the model almost always assigns the majority class. In this case the goal should be to treat the overfitting, so the first direction is to simplify the model and/or the data representation.</li>
<li>Sometimes it can make sense to consider alternative ML designs: in a regular classification problem a model relies on the distribution of the classes, by principle. Some alternative approaches might not as influenced by the distribution, for example one-class classification. Of course it's not suited for every problem.</li>
</ul>
<p>Overall my old-school advice is not to rely too much on technical answers such as resampling methods. It can make sense sometimes, but it shouldn't be used as some kind if magical answer instead of careful analysis.</p>
","0","2","64377","14816"
"92248","<p>A simple representation of the problem in terms of a genetic algorithm could be something like this:</p>
<ul>
<li>A gene represents a chemical and the values it takes represent the proportion of the chemical in the drug</li>
<li>An individual is a combination of N genes/chemicals, each with their particular proportion</li>
<li>A cross-over is the combination of the genes of two individuals A and B, either by picking randomly the value of either A or B for each gene or by some other kind of aggregation. A mutation is the random modification of a gene to a different proportion.</li>
</ul>
<p>The standard genetic algorithm works like this:</p>
<ol>
<li>Randomly pick a set of say 100 individuals (first generation)</li>
<li>Calculate the &quot;performance&quot; of every individual, i.e. evaluate how good this particular combination of chemicals proportions is.</li>
<li>Select say the top 10 individuals according to their performance, then produce the next generation of 100 individuals by cross-over among these top 10. Optionally add some random mutations to the new individuals' genes.</li>
<li>Iterate again from step 2. Keep iterating unless some stop condition is satisfied, for example the average performance over the last 5 generations doesn't increase anymore.</li>
</ol>
<p>As far as I understand the task, the main problem is the evaluation in step 2: if there is no automatic way to evaluate the performance of a combination of chemicals, it's impossible to use the genetic algorithm. In theory the evaluation doesn't need to be automatic, one could imagine doing a manual experiment for every individual, but that means performing thousands of experiments, a lot of them probably useless.</p>
","0","2","64377","14816"
"92283","<p>Your reasoning is correct that the gold standard class is the same for all the instances in a single leave-one-out test set (under the assumption that the test patient cannot become sick at some point in time, thus having both healthy and sick status).</p>
<p>What you're missing is the aggregation across multiple test sets: a full leave-one-out experiment repeats this process for every single patient, i.e. if there are N patients then there are N unique pairs of (training set, test set). Here is a pseudo code to show this clearly:</p>
<pre><code>correct,incorrect=0    
for every patient p:
    training_set = all the patients except p
    test set = patient p
    model = train(training_set)
    // to simplify I count the patient as one instance, it's easy to count instances instead
    if model.predict(test_set) == gold_standard(p):
       correct += 1
    else:
       incorrect += 1 
accuracy = correct / (correct + incorrect)
</code></pre>
<p>The calculation <em>across all the patients</em> can lead to some predictions being correct and some others being incorrect, this is why the accuracy, precision or recall can be any value between 0 and 1.</p>
","4","2","64377","14816"
"92326","<p>There might be a smarter method but I would simply try to fit a model without <span class=""math-container"">$X_i$</span> for every feature <span class=""math-container"">$X_i$</span> (and also a reference model with all the features). By contrast the model where <span class=""math-container"">$X_{49}$</span> is removed should obtain the lowest variance if <span class=""math-container"">$X_{49}$</span> is responsible for a lot of variance.</p>
<p>Be careful that in general a feature which causes a lot of variance is an important one, since if it wasn't important then it wouldn't have much impact on the target.</p>
","0","2","64377","14816"
"92367","<p>It's a mistake to use <code>LabelEncoding</code> for a categorical feature, it should be used only for a categorical target variable. This is because it converts values to integers, hence introducing an arbitrary order over the values.</p>
<p>It's not the values which don't appear in the training set which cause the poor performance (you can check), it's very likely because your model overfits: since there are many different values in the features, you would need a massive number of instances so that the model gets a representative sample for every value. Of course data is never like that, and it's clear from your description that some values occur too rarely (that's why some occur only in the test set).</p>
<p>The solution is to simplify the data, so that the model doesn't rely on patterns which appear by chance in the training set:</p>
<ul>
<li>Replace values which appear rarely with a special value, e.g. <code>RARE_VALUE</code>. Try different thresholds for the minimum frequency.</li>
<li>Encode categorical features with one hot encoding (OHT).</li>
<li>Since the rare values were removed, the number of OHT features will be lower. In order to avoid overfitting, the ratio instances / features should be high enough.</li>
<li>In case there are still values in the test set which don't occur in the training set, replace them with your special value <code>RARE_VALUE</code>.</li>
</ul>
","0","2","64377","14816"
"92465","<p>There is a lot of research being done around exploiting biomedical text data in general and clinical notes in particular. I'm not up to date with the whole domain (it's a big one) but let me sketch a few possible directions.</p>
<ul>
<li>The standard text classification approach: consider every possible treatment as a class, the goal being to predict for each instance of subjective+objective notes the correct class(es). Note that the design can either be standard multiclass classification or multi-label classification. All the regular text classification techniques can be used, from traditional methods like decision trees to using DL with word/text embeddings.</li>
<li>The sequence to sequence approach: the principle here is for the model to represent how an input sequence (here subjective+objective notes) is transformed into an output sequence (treatment). The standard example for this kind of task is machine translation, but this design is used in many other problems.</li>
<li>The semantic representation with a third-party ontology. This approach relies on existing biomedical resources, especially normalized vocabularies like <a href=""https://www.ncbi.nlm.nih.gov/mesh"" rel=""nofollow noreferrer"">Mesh</a> and/or <a href=""https://www.nlm.nih.gov/research/umls/index.html"" rel=""nofollow noreferrer"">UMLS</a>. One would use an automatic annotation tool (e.g. <a href=""http://ctakes.apache.org/"" rel=""nofollow noreferrer"">cTakes</a>) to extract the medical terms from the subjective+objective notes, and the treatments could also be encoded with the controlled vocabulary. This can greatly facilitate the job of the model by providing it with normalized features and classes. Once encoded, the data can be used in a classification setting similar to the first approach.</li>
</ul>
<p>About software/data resources: there are a lot, but as far as I know nothing which does exactly what you need out of the box. <a href=""https://allenai.github.io/scispacy/"" rel=""nofollow noreferrer"">SciSpacy</a> is a Python module for processing biomedical data. There are datasets available (e.g. <a href=""https://n2c2.dbmi.hms.harvard.edu/track1"" rel=""nofollow noreferrer"">shared tasks data</a>), and various research prototypes and resources. See also <a href=""https://datascience.stackexchange.com/a/90124/64377"">this answer</a> for pointers to various biomedical resources.</p>
","1","2","64377","14816"
"92469","<p>This is a kind of confirmation bias, it's often found in recommender systems and it's hard to beat: since the system is designed to find similar books, it's normal that it returns the most similar books but it can be disappointing for the user. This bias is especially strong if there are few past books to refer to, i.e. the reference sample used by the model is too small to represent the user's preferences.</p>
<p>As Nikos suggested in a comment, one way to diversify the recommendations is to broaden the set of similarity features. A more advanced way is to use some second-degree approach: the system doesn't only recommend books similar to a specific user's past, but finds other users who liked the same books and recommends books that they liked (similar to &quot;other users who like X also like Y&quot;).</p>
","1","2","64377","14816"
"92524","<p>Yes, a macro-average measure is the standard choice in this context: a macro-average score is simply the mean of the individual score for every class, thus it treats every class equally.</p>
<p>With an strongly imbalanced dataset, this means that a small class which has only a few instances instances in the data is given as much weight as the majority class. Since the former is generally harder for a classifier to correctly identify, the macro-average performance value is usually lower than a micro-average one (this is normal of course).</p>
","2","2","64377","14816"
"92555","<p>In general <a href=""https://www.cs.waikato.ac.nz/ml/weka/"" rel=""nofollow noreferrer"">Weka</a> can handle missing values, which are marked as <code>?</code> in the <a href=""https://www.cs.waikato.ac.nz/ml/weka/arff.html"" rel=""nofollow noreferrer"">.arff format</a>. However this doesn't mean that every learning algorithm in Weka has a specific mechanism to deal with them, and unfortunately the documentation is a bit poor about this point. So as far as I'm aware, one has to study the source code and/or do some experiments in order to find out exactly what a particular class does with missing values.</p>
<p>Weka is in Java but there is a <a href=""https://pypi.org/project/python-weka-wrapper3/"" rel=""nofollow noreferrer"">wrapper</a> for Python.</p>
","0","2","64377","14816"
"92601","<p>The traditional approach for this kind of problem would be <a href=""https://en.wikipedia.org/wiki/Language_model"" rel=""nofollow noreferrer"">an n-gram language model</a>. The language model is trained on a large corpus, then it's reasonably simple to calculate the most likely missing tokens for any incomplete sentence. <a href=""http://www.speech.sri.com/projects/srilm/"" rel=""nofollow noreferrer"">SRILM</a> was one of the most common toolkits, but there are probably many other libraries.</p>
","1","2","64377","14816"
"92772","<p>The baseline is not the word list by itself, the idea is to implement a simple classifier which works as follows:</p>
<ul>
<li>It receives as input a list of standard positive words P (e.g. &quot;good&quot;, &quot;great&quot;, &quot;nice&quot;, ...) and a list of standard negative words N (e.g. &quot;bad&quot;, &quot;depressing&quot;, &quot;annoying&quot;,...).</li>
<li>For every sentence to classify, it counts the number <span class=""math-container"">$p$</span> of words in the sentence which belong to P and the number <span class=""math-container"">$n$</span> of words which belong to N.</li>
<li>If <span class=""math-container"">$p&gt;n$</span> (resp. <span class=""math-container"">$n&gt;p$</span>) then the sentence is predicted as positive (resp. negative). If <span class=""math-container"">$n=p$</span> then the sentence is predicted as neutral.</li>
</ul>
<p>By evaluating the performance of this baseline classifier and comparing it to your NB classifier, you get a more accurate idea about the performance of your classifier. For example let's assume that NB obtains a F1-score of 83%: by itself this value is poor because there's no way to know if this is good or bad since it depends on the data, the algorithm, etc. Now if you evaluate the baseline and it obtains only 57% for example, then you know that the NB classifier is good (if the baseline happens to perform better than the NB classifier, then there's a problem).</p>
","0","2","64377","14816"
"92774","<p>Based on the description my guess is that any or all of the following problems occur:</p>
<ol>
<li>the data is not randomized, which could explain why the first fold get a very different performance than the others</li>
<li>perhaps the dataset is very small, causing massive variations in performance and overfitting.</li>
<li>in any case the model is very unstable, possibly due to overfitting caused by the model being too complex and/or dataset too small.</li>
</ol>
","1","2","64377","14816"
"92775","<p>Based on your comment I understand that you want to identify a particular syntactic category of words, for instance nouns. The task which identifies these categories is called <a href=""https://en.wikipedia.org/wiki/Part-of-speech_tagging"" rel=""nofollow noreferrer"">Part Of Speech (POS) tagging</a>.</p>
<p>I'm not familiar with any specific resource for Hindi or Marathi but there are probably some software that you can use. From a quick search I found a few links below:</p>
<ul>
<li><a href=""https://analyticsindiamag.com/top-nlp-libraries-datasets-for-indian-languages/"" rel=""nofollow noreferrer"">General resources for Indian languages</a></li>
<li><a href=""https://github.com/rkp768/hindi-pos-tagger"" rel=""nofollow noreferrer"">A POS tagger for Hindi</a></li>
<li><a href=""https://github.com/rootally/POS-Tagging-for-Hindi"" rel=""nofollow noreferrer"">Another more recent POS tagger for Hindi</a></li>
</ul>
","0","2","64377","14816"
"92843","<blockquote>
<p>We just encode the categorical variable to some sort of numerical representation (like one-hot encoding)</p>
</blockquote>
<p>The choice of representation matters, because it has to preserve the properties of a categorical variable: one-hot-encoding is a standard option, but directly encoding categorical values as integers is a mistake because it introduces order where there is none.</p>
<blockquote>
<p>the use-cases of such a method: under what kind of circumstances would using categorical data to predict a numeric value be useful?</p>
</blockquote>
<p>There are many applications for which regression from categorical data is useful. For example text data is often represented as &quot;bag of words&quot; (BoW), where each word is a categorical variable. Some tasks involve predicting a numerical target from some text input, for example:</p>
<ul>
<li>Predicting the grade of an essay</li>
<li>Predicting a score representing the likelihood of plagiarism (e.g.Turnitin)</li>
<li>Predicting sentiment of a text on a scale from 1 to 5</li>
</ul>
<p>There are many other examples, and not only from text data.</p>
<blockquote>
<p>And what type of data format should I have my data in before doing the encoding? (does having two columns with one numeric results and the other the corresponding category work?)</p>
</blockquote>
<p>The data can have as many independent variables (features) as needed. Since the categorical variables are typically one-hot-encoded, what matters is the number of features after encoding. It's often necessary to preprocess the data in order to avoid overfitting (typically rare values should be discarded).</p>
<blockquote>
<p>I would also like to know the different ways we can visualize and analyse the results of our model (and its predictions), especially if we have a sizeable amount of categorical variables.</p>
</blockquote>
<p>As far as I know there's no specific method for analyzing the result of the model, it's the same principle as with numerical features. If one wants to analyze the impact of an individual feature on the performance of the model, a simple technique is to train the model with/without this feature and compare the results.</p>
","1","2","64377","14816"
"92988","<p>Given that you have only 3 classes and that they closely depend on each other, I think it's worth trying a multiclass setting as WBM said. The idea is to label each video using the full combination of actions, since the maximum number of combinations is 2^3 = 8:</p>
<ul>
<li>R-I-F</li>
<li>R-I</li>
<li>R-F</li>
<li>R</li>
<li>I-F</li>
<li>I</li>
<li>F</li>
<li>none</li>
</ul>
<p>Probably some combinations of actions are impossible, so the number of classes is likely less than 8. Why this is a reasonable approach:</p>
<ul>
<li>The setup is exactly the same, i.e. you can use the same labels and the predictions can be used the same way as in your multi-label approach</li>
<li>This is a &quot;joint model&quot;, i.e. a model which learns everything together and therefore can exploit fine-grained distinction between classes (e.g. between R-I-F and R-I).</li>
</ul>
<p>However note that this kind of method may require more data, in particular it needs to have enough instances for each class.</p>
","0","2","64377","14816"
"93049","<p>From the description this is not a multilabel problem because:</p>
<ul>
<li>Each of the three &quot;classes&quot; (columns) must have a label. In a multilabel problem every class is optional.</li>
<li>Every &quot;class&quot; (column) appears to have a specific purpose subdivided into 4 labels. In a regular multilabel problem the labels are exchangeable, e.g. a document can have topics &quot;sports&quot; and &quot;society&quot; but not &quot;politics&quot;, all of these labels have no order and no specific role distinct from the others.</li>
</ul>
<p>It seems that you simply have three regular independent multiclass problems:</p>
<ul>
<li>problem 1 = predict &quot;class 1&quot;</li>
<li>problem 2 = predict &quot;class 2&quot;</li>
<li>problem 3 = predict &quot;class 3&quot;</li>
</ul>
<p>Note: the word &quot;class&quot; for the columns is confusing because these don't correspond to the regular concept of class.</p>
","1","2","64377","14816"
"93050","<p>The evaluation design depends on the goal, i.e. how the classifier is intended to be used in production. The goal of a one-class classifier is to find instances of this class among other instances, so it has to be evaluated with data which includes negative instances. The choice of the proportion and type of negative instances should be driven by the final goal.</p>
<p>Note that in terms of evaluation and cross-validation the setting is the same as regular supervised classification, the difference is only that negative instances are not used during training.</p>
","0","2","64377","14816"
"93129","<p>There can be other reasons related to the model but the most simple explanation is that the data contains contradicting patterns: if the same features correspond to different target values, there is no way for any model to achieve perfect performance.</p>
<p>Let me illustrate with a toy example:</p>
<pre><code>x   y
0   4
0   4
1   3
1   4
1   3
1   3
2   8
2   7
2   7 
2   6
</code></pre>
<p>The best a model can do with this data is to associate x=0 with y=4, x=1 with y=3 and x=2 with y=7. Applying this best model to the training set would lead to 3 errors and the MAE would be 0.3.</p>
","5","2","64377","14816"
"93157","<p>First there is a simple theoretical reason why relying on the probability provided by a NB model is not a good idea: what NB predicts is the posterior probability, i.e. the conditional probability <span class=""math-container"">$p(C_i|d)$</span> for every class <span class=""math-container"">$C_i$</span> for a given document <span class=""math-container"">$d$</span>. The class <span class=""math-container"">$C$</span> which is predicted is just the one which obtains the highest probability <span class=""math-container"">$p(C_i|d)$</span>. The sum of all the posterior probabilities for a particular <span class=""math-container"">$d$</span> is 1 since it's a conditional, and this means that a class is always predicted <em>relatively to the other classes</em>: a high probability for <span class=""math-container"">$C$</span> doesn't mean that the document is very likely to be <span class=""math-container"">$C$</span> in general, it only means that it is much <em>more likely to be <span class=""math-container"">$C$</span> than any of the other classes.</em> If it actually belongs to another class unknown to the model, there's no way the model can represent this.</p>
<p>There is also a practical reason why this is not a great idea: experimentally it is known that NB tends to systematically assign extreme probabilities (and to overfit easily but that's not the point). It does this whether or not the class is correct, and this can easily be checked with your data: if you plot the predicted probability for every instance with a different colour for correct/incorrect prediction, you're likely to see that even the incorrect instances were predicted with a very high probability. It's tempting to interpret the predicted probability as a confidence measure, but it's usually a mistake.</p>
<p>But there is hope: the proper way to deal with potential unknown &quot;classes&quot; (these are not classes technically) is not with regular classification methods, which always rely on a &quot;closed world&quot; assumption (only the classes seen during training exist). There are also open-set classification methods: these are much less common but they are not restricted to the known classes. I think the most common method is probably <a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">one-class classification</a>, in particular with <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html"" rel=""nofollow noreferrer"">one-class SVM</a>.</p>
","0","2","64377","14816"
"93159","<blockquote>
<p>can you apply a trained algorithm / model on its own training data if my interest is not in predicting forward but evaluating success vs likelihood of success (and still yield useful information)?</p>
</blockquote>
<p>Yes, absolutely. There are quite a few cases where applying a model on the training data is useful. The most common is probably to detect overfitting: a high difference in performance between the training and test set is a sign of overfitting. In general it can also be useful to know how the model performs on the training set in order to obtain an upper baseline for the performance.</p>
<p>Everybody says &quot;don't predict on the training set&quot; simply because it's a simple rule to remember and such an easy mistake to make for beginners. But as long as one understands what they are doing and knows that obviously the predictions obtained on the training set are biased, there's no problem.</p>
<p>The task described in the question makes sense to me, you have my permission to predict on the training set ;)</p>
<p>Just one more remark: there might be a bias in the training data itself, in the sense that if success for an event is very unlikely then the event will fail more often than suceed even when the condition of its success are satisfied. If the data contains this kind of cases, it means that the model is trained to predict &quot;fail&quot; for potentially successful features. Whether or not this impacts the model depends on whether there are enough similar but successful events in the data, I think.</p>
","1","2","64377","14816"
"93208","<p>It should work: the variable is ordinal so using numerical values makes sense.</p>
<p>So there's a bug somewhere, here are a few suggestions of things to look at:</p>
<ul>
<li>Possibly a type conversion error somewhere: make sure the variable is interpreted as numerical.</li>
<li>Check whether the model actually uses the variable: if not then it's likely some type error; if yes then I would investigate what goes wrong: for example it might help to plot this variable vs. target in the two cases where the variable is categorical or numerical.</li>
<li>Maybe some difference between the preprocessing of the training and test set: apply the model on the training set, if the performance is good then it's likely that there's something wrong in the preprocessing of the test set.</li>
</ul>
","1","2","64377","14816"
"93270","<p>Clustering doesn't work like this: for example k-means assigns an instance to the closest centroid, and since there is always a closest centroid there is a always a cluster that an instance &quot;belongs to&quot;.</p>
<p>So you need a different approach if you plan to have the possibility of in instance &quot;not in any group&quot;:</p>
<ul>
<li>redo the clustering on the full set of instances</li>
<li>apply a first step which detects outliers</li>
<li>train a one-class classification model for every cluster</li>
</ul>
","0","2","64377","14816"
"93299","<p>This happens if:</p>
<ul>
<li>There is no clear way to separate the clusters, i.e. there is only one large group of instances and the instances which are far from this group are too far from each other to form their own cluster.</li>
<li>The <a href=""https://en.wikipedia.org/wiki/Hierarchical_clustering#Linkage_criteria"" rel=""nofollow noreferrer"">linkage criterion</a> plays a big role: this is more likely to happen with single linkage clustering , because as the main cluster grows it becomes more likely to be the closest point to any other instance, therefore all the instances are &quot;attached&quot; to this cluster one by one. This is less likely to happen with complete-linkage clustering, but this option has other biases.</li>
</ul>
","1","2","64377","14816"
"93313","<p>There seems to be a confusion between multiclass and multilabel classification:</p>
<ul>
<li>Multiclass is the regular case where the task consists in predicting among N possible classes. For example an image can be either a dog or a horse or a cat, but always exactly one among these three animals.</li>
<li>Multilabel is the when the task consists in predicting a set. For example an image can be any subset of {dog, horse, cat}: it could be {dog, cat}, it could be {horse}, it could be {dog, horse, cat}, it could even be the empty set (no animal at all).</li>
</ul>
<p>Practically in the multilabel case you predict each possible animal <strong>independently</strong> as a binary problem, so for every image the system answers 3 questions:</p>
<ul>
<li>does this image contain a dog? (y/n)</li>
<li>does this image contain a horse? (y/n)</li>
<li>does this image contain a cat? (y/n)</li>
</ul>
<p>Because each question is predicted independently, it doesn't make sense to pick the class which has the max probability. In effect there are 3 independent binary classification problems and 3 corresponding confusion matrices.</p>
<p>Apparently you didn't mean to take into account the multilabel case and you don't have any image labelled with multiple animals, right? If so you should change the system to solve a regular multiclass problem.</p>
<p>It looks like the confusion might be due to the one-hot-encoding of the class: maybe you thought that the class being categorical, it would be a mistake to encode the class as a numerical value. This is true for categorical features but actually not for the target, you can perfectly use for instance <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""nofollow noreferrer"">LabelEncoder</a> to represent the class as a single target. This is much simpler and probably more appropriate for your problem.
One difference you will notice is that the probabilities obtained as predictions sum to one, because the classifier doesn't consider the classes as independent (as opposed to what happens in your current experiment: the probs don't sum to 1).</p>
","0","2","64377","14816"
"93366","<p>Technically BOW includes all the methods where words are considered as a set, i.e. without taking order into account. Thus TFIDF belongs to BOW methods: TFIDF is a weighting scheme applied to words considered as a set. There can be many other options for weighting the words in a set.</p>
<p>Compared to regular TF-weighted BOW, the TFIDF weighting scheme gives more weight to words which appear in fewer documents and less weight to words which appear in many documents. The rationale is that a word which appears in many documents is unlikely to be relevant since it doesn't help selecting the most similar document. Typically the most frequent words are grammatical words (also called stop words, e.g. determiners, pronouns, etc.), but in a corpus made of sci-fi books for example some words such as &quot;robot&quot; or &quot;planet&quot; will also be very common. On the contrary a word like &quot;elephant&quot; would be very rare in a sci-fi context, so it is given more weight because it's more discriminative.</p>
<p>This is meaningful in Information Retrieval tasks where the goal is to find a document similar to a query, and by extension it's useful in most tasks where the goal is to compare text documents by their semantic similarity. It is not meaningful and often counter-productive in classification tasks related to the style of text, as opposed to its semantic content.</p>
<p>Note that <a href=""https://en.wikipedia.org/wiki/Okapi_BM25"" rel=""nofollow noreferrer"">Okapi BM25</a> is a similar weighting scheme which is not as famous as TFIDF but has been proved to work better in most applications.</p>
","0","2","64377","14816"
"93383","<p>Disclaimer: I'm not at all expert in predicting stock market.</p>
<p>The error between the actual and the true evolution might simply be caused by the fact that the model is reproducing a pattern it observed in the past data: if in the past a small plateau was more often followed by a progressive decrease, then it makes sense to predict a decrease. This might include some level of overfitting, for example if the model relies on some very specific indication (e.g. &quot;the value has oscillated between 3143.6 and 2159.7 during 9 days&quot;) to make its prediction.</p>
<p>More generally at a semantic level I'm not surprised that this doesn't work very well: I would be very skeptical of any attempt to predict a stock market value based solely on the past performance of this value. A stock market value doesn't only depend on its past evolution, it depends on many external factors such as the general economic context, the market of the company, its strategy, and various other general news that can affect the value. Doing this is like trying to predict somebody's life expectancy knowing only their age: sure there's a good chance that they will still be alive the next day, but no long term prediction can be made without taking into account their health, lifestyle, wealth, environment, etc.
There's no magic, a ML model needs reliable indicators in order to make reliable predictions.</p>
","0","2","64377","14816"
"93429","<p>Below are two functions using my favorite packages:</p>
<ul>
<li>The first one shows a scatterplot of every column against the target column</li>
<li>The second one shows the correlation of every column with the target column, with confidence intervals (I found how to do that with ggplot <a href=""https://data-hacks.com/r-plot-data-confidence-intervals-using-ggplot2-package"" rel=""nofollow noreferrer"">here</a>).</li>
</ul>
<p>Code:</p>
<pre><code>library(ggplot2)
library(reshape2)
library(plyr)

scatterplot &lt;- function(data, targetColumn='Fertility') {
  d&lt;-melt(data,id.vars = targetColumn)
#  ggplot(d, aes_string('value',targetColumn))+geom_point()+facet_grid(variable~.)
  ggplot(d, aes_string('value',targetColumn))+geom_point()+facet_wrap(variable~.)
}


corplotCI &lt;- function(data, targetColumn='Fertility', method='pearson') {
  d&lt;-ldply(colnames(data), function(col) {
    if (col != targetColumn) {
      r &lt;- cor.test(data[,col], data[,targetColumn],method=method)
      data.frame(variable=col,cor=r<span class=""math-container"">$estimate, lowerCI=r$</span>conf.int[1],upperCI=r$conf.int[2])
    }
  })
  ggplot(d,aes(cor,variable))+geom_point(size=3)+geom_errorbarh(aes(xmin = lowerCI,xmax = upperCI),height=.5)+coord_cartesian(xlim=c(-1,1))
}
</code></pre>
<p>Usage:</p>
<pre><code>scatterplot(swiss)

corplotCI(swiss)
</code></pre>
","1","2","64377","14816"
"93699","<p>Thank you for providing a good reproducible code, I can't resist answering the question :)</p>
<p>Hope the following code helps:</p>
<pre><code>data &lt;- read.table(text = &quot;0    1   2   3   4   5
MR  155 211 64  14  1   1
Mob 0   393 51  2   0   0
SC  0   427 12  7   0   0
Act 0   386 45  15  0   0
Pain    0   379 62  5   0   0
Anx 0   355 73  18  0   0&quot;, header = TRUE)

#data<span class=""math-container"">$row &lt;- seq_len(nrow(data))
data$</span>row &lt;- row.names(data)
data2 &lt;- melt(data, id.vars =&quot;row&quot;)
data2<span class=""math-container"">$variable &lt;- sub(""X"",as.character(data2$</span>variable),replacement=&quot;&quot;,fixed=TRUE)

ggplot(data2, aes(x = variable, y = value, fill = row)) + 
  geom_bar(stat = &quot;identity&quot;) +
  xlab(&quot;HealthState&quot;) +
  ylab(&quot;Count&quot;) +
  theme_bw()
</code></pre>
","1","2","64377","14816"
"93754","<p>Everybody understands how to perform <span class=""math-container"">$k$</span>-fold cross-validation but there is often quite a lot of confusion about where/how to use it. So thanks for this good question :)</p>
<p>First, cross-validation is a statistical method for <strong>evaluation</strong>, not for training:</p>
<ul>
<li>Of course training is performed during cross-validation, but it is performed <span class=""math-container"">$k$</span> times and therefore there are <span class=""math-container"">$k$</span> models produced during the process. Each of these models is meant to be applied only to its corresponding test set, and in general it would be a mistake to use one of these models after the CV process (it would be an even bigger mistake to select the best of these <span class=""math-container"">$k$</span> models). If a final model is needed, it should be trained on the whole training set independently from the CV process.</li>
<li>The purpose of CV is to obtain a reliable evaluation of the performance of a method by minimizing the chance factor caused by splitting the data between training and test set. The variations across folds can also be used to assess the stability of the method: if there are large variations in the performance, the method is not stable and the resulting model is likely to overfit since it depends too much on the training set.</li>
</ul>
<p>When to use a separate test set?</p>
<ul>
<li>If the purpose is only to evaluate a single method reliably, then there is no need to train a final model or to evaluate it on a separate test set.</li>
<li>However quite often one uses CV to evaluate not one but multiple methods: with different algorithms, different hyper-parameters values, different features, etc. These cases fall in the category of parameter tuning, which is a kind of training: the goal is indeed to find the optimal values for some parameters, even if the parameters appear to be part of the design (e.g. some preprocessing option). Like any training stage, there is a risk of overfitting: using CV makes the risk quite low, but even with CV the risk exists (especially with a high number of methods). This means that the selection of the best method based on CV performance might be due to chance, this is why the proper methodology in this setting is to (1) select the best method according to CV performance; (2) train the final model using only this method on the full training set; (3) evaluate on a separate test set. This ensures that the performance obtained on the test set is reliable.</li>
</ul>
","2","2","64377","14816"
"93792","<blockquote>
<p>Saying that accuracy is measured to get how accurate the model performs, and F1 is how well the model performs</p>
</blockquote>
<p>This doesn't mean anything, it's obviously too vague.</p>
<p>The first things to check in order to understand this relationship are the definitions of accuracy and F1-score.
Wikipedia has a <a href=""https://en.wikipedia.org/wiki/Precision_and_recall"" rel=""nofollow noreferrer"">good page which explains how different classification evaluation measures are related</a>.</p>
<p>Observations on your results:</p>
<ul>
<li>The accuracy and F1-score are almost identical everywhere. This suggests that your data is probably quite well balanced, i.e. the difference in the number of positive vs. negative instances isn't very big. Why? Because if the data was imbalanced then the model would over-predict the majority class, and this would cause the F1-score to be much lower than the accuracy: assuming the majority class is the negative class, the recall would be somewhat low but the accuracy could still be high because most instances (majority class) would be correctly predicted.</li>
<li>As a consequence, there's no insight to gain from analyzing the relationship between accuracy and F1-score since they're virtually identical. The small differences might be due to the geometrical mean between precision and recall. F1-score is more informative in case of imbalanced data, but this is not the case here.</li>
<li>The F1-score is calculated only on the training data. It would be more useful to calculate it on the validation data.</li>
<li>There's some serious overfitting happening especially with the high learning rates, but with the low learning rates the fact that difference between training and validation accuracy increases is also worrying. Maybe the model is too complex or there are not enough instances in the data. Ideally the two accuracy values should converge.</li>
</ul>
","1","2","64377","14816"
"93831","<p>Based on a quick read of the paper linked in the comment:</p>
<blockquote>
<p>I just don't understand if your model (BERT or not) is only trained on these weak labels then you are treating them as &quot;ground-truth&quot;,</p>
</blockquote>
<p>Correct, but only for the training: the model is trained to recognize the labels obtained with a &quot;quick and dirty&quot; method.</p>
<blockquote>
<p>and more importantly, don't you already know how to create &quot;ground-truth&quot; (by the rules-based system) ??? What's the point of the 2nd step?</p>
</blockquote>
<p>No, because the real ground truth they are interested in is not those from the &quot;quick and dirty&quot; method. If they were, it would indeed be sufficient to run their rule-based system. The goal is to predict the labels obtained in what the authors call the &quot;Gold Standard Corpus&quot;, which was manually annotated and never seen by the model.</p>
<p>Typically the quick and dirty method will result in some classification errors. The point of tuning the model with these labels is to see if the model can extrapolate from these low-quality labels to high-quality labels. This ability to generalize beyond the training data is based on the underlying semantic information contained in the original BERT-like model. For example this model might be able to associate a specific sport like swimming with &quot;physical activity&quot;, even though the weak supervision doesn't contain this association.</p>
","2","2","64377","14816"
"93889","<p>I'm not aware of any literature specific to the case of classification based on binary features since it's just a subset of the general case, but it's definitely possible.</p>
<p>A very common example is traditional text classification, where the document is represented as a <a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow noreferrer"">bag of words</a>: there are different options but each word in the vocabulary can be represented as a boolean variable, representing whether it belongs to the document or not. For example (among many others), a <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB"" rel=""nofollow noreferrer"">Bernoulli Naive Bayes classifier</a> can be trained on such data.</p>
","1","2","64377","14816"
"93908","<p>It depends on the design of your task, there are two options:</p>
<ul>
<li>The task is regular <a href=""https://en.wikipedia.org/wiki/Multiclass_classification"" rel=""nofollow noreferrer"">multiclass classification</a>, i.e. every instance must belong to exactly one class. In this case it would be a mistake to one-hot-encode the class, it can simply be encoded as an int (for example with <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""nofollow noreferrer"">LabelEncoder</a>). The model will always predict exactly one class for one instance so the case of zero class is impossible.</li>
<li>The task is <a href=""https://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow noreferrer"">multi-label clasification</a>, i.e. every instance can belong to zero, one or multiple classes. In this case an instance can be predicted as belonging to no class at all, this is normal. In this setting the confusion matrix should <em>not</em> be done with a <span class=""math-container"">$n \times n$</span> matrix across classes, because the classes are independent (btw it's not only about the case of zero class, the case of multiple classes would also be impossible to represent this way). Instead there should be one binary confusion matrix for every independent class.</li>
</ul>
","1","2","64377","14816"
"93951","<p>The usual way is to present the top N (e.g. top 10) words for the cluster:</p>
<ul>
<li>With distance-based clustering like K-means, the top words can be picked as the closest ones to the centroid.</li>
<li>With probabilistic methods such as LDA, the top words are the ones with the highest probability for the topic.</li>
</ul>
","0","2","64377","14816"
"93976","<p>There are probably many different strategies but it's a difficult problem when the imbalance is as severe as it is here.</p>
<p>Without any correction the model is likely to ignore the smallest classes, as you noticed. However forcing the class weight as if the data is balanced is certainly too strong a correction. A middle ground would be to resample the training set instances yourself before fitting the model: by trying different ways to undersample the large classes and/or oversample the small classes you should be able to find an optimal tradeoff between the two extremes (use a separate validation set to determine the optimal combination).</p>
<blockquote>
<p>Is there a way to ensure the model predicts approximately the same proportion of samples for each category?</p>
</blockquote>
<p>Maybe I misunderstood but this looks like a bad idea: if the true proportions are not equal then the model shouldn't predict equal proportions either. The ideal scenario is for the model to predict the correct label every time, which implies predicting the true proportion for every class.</p>
<p>It might also be useful to analyze the performance in simpler configurations, e.g. by picking a few &quot;average size&quot; classes and observing how well the classifier discriminates between them only. The harder it is for a classifier to predict correctly, the more it relies on basic class proportion since it doesn't know any better.</p>
","1","2","64377","14816"
"94002","<p>In research, new evaluation measures are proposed all the time but very often for specific use cases: either because it's a new task for which there is no established evaluation standard, or because the new measure provably solves some issues/limitations with the previous evaluation standard.</p>
<p>The purpose of an evaluation measure (or performance indicator),  is to represent the quality of the predictions by any system for a given task. This implies that:</p>
<ul>
<li>An evaluation measure is selected specifically for a task. Some measures are very common because many tasks can be framed in a standard way, for example as a classification problem.</li>
<li>The central problem of evaluation is how to define &quot;quality&quot; in a formal way. A performance indicator is always a simplified representation of &quot;quality&quot;, but it must correlate at least approximately with what a human expert of the task would call &quot;quality&quot;.</li>
</ul>
<p>Naturally the first step for developing a new evaluation measure is to select the target task (or group of tasks). Then one has to demonstrate that the new evaluation measure brings some significant advantage in terms of measuring the quality of this task. Of course this is the hard part, and it always depends specifically on the task.</p>
<p>For example, there have been numerous evaluation measures proposed for Machine Translation (there's even a <a href=""http://www.statmt.org/wmt20/metrics-task.html"" rel=""nofollow noreferrer"">regular Shared Task</a> devoted to the topic). The way a MT evaluation measure is itself evaluated is against human judgements: if the automatic measure correlates strongly with human judgements about the quality of translation, then it's a good measure.</p>
","1","2","64377","14816"
"94047","<p>This question is definitely opinion-based and so is my answer, but let me try to clarify a few things:</p>
<ol>
<li>Don't count blindly on AI/ML to become more accurate in general, and even less for a particular project. Imho for your project to be future-proof, you need to test the current technology with a sample of your data (or some similar data) and make sure that the results are at least &quot;encouraging&quot;, i.e. decent enough. If the result are terrible or cannot be obtained at all, then there's probably a problem with the whole design and it's unlikely that future technology will solve this.</li>
<li>It's not always by providing more detailed information to ML that it gives better results, it strongly depends on the task. This is why I recommend testing first (see point 1).</li>
<li>Technically it's always possible to downsize resolution but not the opposite, so from this point of view it's clear that the higher the resolution the better. A <a href=""https://toolstud.io/video/filesize.php?width=1920&amp;height=1080&amp;framerate=60&amp;timeduration=15&amp;timeduration_unit=minutes&amp;compression=19290&amp;specificbitrate=100&amp;specificbitrate_unit=1000000"" rel=""nofollow noreferrer"">simple estimation</a> says that with your characteristics a single daily video would require around 2GB, so 2 years would require around 1.5TB.</li>
</ol>
","0","2","64377","14816"
"94066","<p>Here are a few options:</p>
<ul>
<li>Commercial cloud provider: AWS is the go-to cloud provider for virtually any need, they have options for everything. Check out <a href=""https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html"" rel=""nofollow noreferrer"">their options for Deep Learning</a>.</li>
<li>You could also <a href=""https://www.ebay.com/sch/i.html?_from=R40&amp;_trksid=p2499334.m570.l1313&amp;_nkw=gpu+server&amp;_sacat=11211"" rel=""nofollow noreferrer"">buy your own hardware</a></li>
<li>Finally from a data science perspective there is also the option of modifying the parameters of the model and/or data so that the training fits with the hardware limitations.</li>
</ul>
","2","2","64377","14816"
"94098","<p>The title of the graph says &quot;REC curve for various models&quot; but this curve is for a single model. It shows as Y the percentage of &quot;correct&quot; predictions by the model depending on what &quot;correct&quot; means, which is given as X: for example if correct means an error less than 10 ha (on X axis), then the model is correct around 55% of the time (on Y axis). In other words the graph splits the instances between correct/incorrect ones based on whether the error value (difference between true and predicted value) is lower than a threshold X.</p>
<p>In particular this graph shows that:</p>
<ul>
<li>this model predicts very few values with an error less than 8</li>
<li>this model very often (around 50%) predicts values which are 8-9 ha off the true value.</li>
<li>there are couple more values that the model predicts with an error around 15.</li>
<li>more than 40% of the predicted values are not visible on this graph, which means that their error is higher than 20.</li>
</ul>
<p>I'd suggest plotting the true vs. predicted value, it's easier to interpret (although it can be harder to see where are the large groups of instances).</p>
","0","2","64377","14816"
"94194","<p>The main problem with very little data is that it's almost impossible to know how representative the sample is. Some people would even say that less 20-30 data points cannot be representative of anything. Every single data point can have a huge impact on any model, so any prediction has a huge margin of error.</p>
<p>If one is going to train a model from a tiny sample, they should do everything possible to avoid overfitting: use very few features, use a very simple type of model with as few parameters as possible. Anything else is guaranteed to overfit.</p>
<p>In your example the behaviour of a customer can be very complex in reality, so it's particularly challenging (and potentially risky) to rely on such a small sample. Imho the first task is borderline but it can be tried, but I don't think the second one is realistic: revenue is not always regular, predictions in the long term are going to be barely better than random.</p>
","1","2","64377","14816"
"94195","<p>It would mean that the model trained with some irrelevant features is overfit: a good model uses only features which have some predictive power on the target variable, so if the sample is representative enough any irrelevant feature is ignored or assigned very little weight. However if the training dataset is too small or the model too complex then it uses details which happen by chance, so it could use some irrelevant features: this is overfitting.</p>
<p>In case the performance on the test set was high despite the model being overfit, it's likely that there is some data leakage additionally.</p>
","0","2","64377","14816"
"94256","<p>If some leaves are pruned then it means that the model cannot predict the instances which would normally have fallen into these leaves. The logic of a decision tree is to represent every possible instance: like any supervised learning method, it must be able to predict for every possible instance as input. In some cases where all the leaves have low purity, this condition would even make the model unable to predict anything at all. This is why there is no such algorithm, at least not as a regular supervised learning algorithm.</p>
<p>However you can rely on the probability predicted for the instance instead, this gives you exactly the same information in a way consistent with standard ML.</p>
","0","2","64377","14816"
"94361","<p>Here it's a mistake to one-hot-encode the class, because it turns the task into <a href=""https://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow noreferrer"">multi-label classification</a> instead of regular <a href=""https://en.wikipedia.org/wiki/Multiclass_classification"" rel=""nofollow noreferrer"">multi-class classification</a>. In your task an instance can only have a single class, so the class should be encoded as an int (for example with <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""nofollow noreferrer"">LabelEncoder</a>).</p>
<p>This is why the predicted probabilities don't sum to 1, because in multi-label classification the classes are independent of each other. For example the output [0.9,0.4,0.7] means that class 1 label is predicted true at 90%  and false at 10%, class 2 label is predicted true at 40% and false at 60%, class 3 label is predicted true at 70% and false at 30%. It wouldn't make sense to pick the maximum probability of the 3 classes in this scenario.</p>
<p>See also <a href=""https://datascience.stackexchange.com/a/93908/64377"">this answer</a>.</p>
","0","2","64377","14816"
"94386","<p>A few comments:</p>
<ul>
<li>The term &quot;POS&quot; (Part Of Speech) has the specific meaning of &quot;grammatical category&quot; so it's not really what you mean here, you should probably say &quot;custom entities tags/categories/labels&quot;.</li>
<li>Your plan makes sense, but be aware that that you're addressing a really difficult problem so even with a really good dataset and method it's not going to work perfectly (probably far from it).</li>
<li>You're probably going to need a very large amount of annotated data for this to work, because there are many different ways to express a medical problem so the model will need many examples to extract the relevant patterns.</li>
<li>The standard way to address this kind of problem is a bit different when there are terminology  resources for the language: the medical terms can be annotated directly simply by pattern matching. If you have such resources and they are complete enough, this saves you the need to manually annotate and train a custom model.</li>
<li>Doing a recommender system on top of that is a second problem, and I think you need to think carefully about the goal:
<ul>
<li>if it's meant to be used by non-experts, it wouldn't make sense to recommend research articles.</li>
<li>The vast majority of the research literature is published in English, so there would be an additional problem to address here.</li>
<li>In case you're considering the whole medical literature, you might have a complexity issue because it's huge (<a href=""https://pubmed.ncbi.nlm.nih.gov/about/"" rel=""nofollow noreferrer"">PubMed</a> contains around 30 millions abstracts, and that's without considering the <a href=""https://www.ncbi.nlm.nih.gov/pmc/"" rel=""nofollow noreferrer"">full articles in PMC</a>)</li>
<li>What would be the basis for recommendation? Is it based only on matching the terms in the query with the documents?</li>
</ul>
</li>
</ul>
","1","2","64377","14816"
"94395","<p>In general this would mean that P predicts only a small number of instances higher than 0.9 whereas R predicts most instances higher than 0.9. Therefore a weighted average of the two scores will fall somewhere in the middle, likely resulting in a moderate precision and moderate recall.</p>
<p>This can give significantly better results but only if the two classifiers are complementary, i.e. they predict instances in a way different enough from each other. Otherwise it's equivalent to tuning the threshold on a single classifier.</p>
","0","2","64377","14816"
"94411","<p>The main assumption in supervised learning is that the training set is a representative sample of the space of all possible instances for the problem.</p>
<p>This is why in the regular classification setting there is simply no way to consider a &quot;not in any class&quot; option: what the model learns is <strong>to distinguish between the classes seen in the training set</strong>, nothing else. In the standard <a href=""https://en.wikipedia.org/wiki/Multiclass_classification"" rel=""nofollow noreferrer"">one-vs-rest multi-class setting</a> It's important to understand that the model doesn't predict whether an instance is likely to belong to &quot;class A&quot; in general, it only predicts whether the instance is more likely to belong to class A <strong>compared to any other known class</strong>. This is a common misunderstanding, but one-vs-rest is not an answer to the problem of &quot;not in any class&quot;.</p>
<p>There are options to deal with this problem, but one must design the system so that the model can deal with such cases. In general the problem is actually open-set classification: in regular classification the set of possible classes is closed, as mentioned above. The open-set classification setting is much less standard and usually harder, since the model has to make a prediction about something it does not know from its training data. Here are a few options:</p>
<ul>
<li>Treat the problem as an <a href=""https://en.wikipedia.org/wiki/Anomaly_detection"" rel=""nofollow noreferrer"">outlier detection problem</a>: before running the regular classifier, eliminate abnormal instances which don't look like the regular training set cases.</li>
<li><a href=""https://en.wikipedia.org/wiki/Multi-label_classification"" rel=""nofollow noreferrer"">Multi-label classification</a>: in multi-label classification an instance may belong to zero, one or several classes. A binary classifier is trained for every class, and an instance may belong to none of the classes. This is more flexible than multi-class classification, but it's still closed-set classification: for example the model is supposed to have seen examples in the training set which don't belong to any class.</li>
<li><a href=""https://en.wikipedia.org/wiki/One-class_classification"" rel=""nofollow noreferrer"">One-class classification</a> is a type of classification where the model learns to identify a single class &quot;in general&quot;, not by contrast to any other class. This is proper open-set classification: if a model is trained for every known class and every model predicts the instance as negative, then the instance doesn't belong to any known class.</li>
</ul>
","0","2","64377","14816"
"94413","<p><em>Disclaimer: my knowledge about neural networks is very limited, so my answer is only based on general ML principles. Hopefully somebody will provide a more informed answer.</em></p>
<p>In supervised learning, the assumption is that the training set is a representative sample of the data, i.e. a random subset of the whole sample space. From this point of view it's easy to explain why your model doesn't generalize the function outside the training range: the model expects only points in the same range as the training set. You and I know that the function is periodic over <span class=""math-container"">$\mathbb{R}$</span>, but the model has no way to know that. For example the function could perfectly be &quot;if x&lt;20 then sin(x) else 0&quot; with the same training data. So your conclusion is exaggerated: it's not that you can only predict linear data, but you need at least to provide a representative training sample.</p>
<p>I'm aware of at least two other interesting questions on the topic of the generalization ability of neural networks:</p>
<ul>
<li><a href=""https://datascience.stackexchange.com/q/47787/64377"">Can a neural network compute y=x²?</a></li>
<li><a href=""https://datascience.stackexchange.com/q/56676/64377"">Can machine learning learn a function like finding maximum from a list?</a></li>
</ul>
<p>Imho it's quite revealing that, based on the answers, the two questions appear to be quite controversial.
It might also be of interest to note that in both questions there's an answer (<a href=""https://datascience.stackexchange.com/a/47803/64377"">here</a> and <a href=""https://datascience.stackexchange.com/a/56864/64377"">there</a>) which points out the theoretical limitation that the target function needs to be defined on a compact subset of <span class=""math-container"">$\mathbb{R}^n$</span>.</p>
<p>Another vaguely related remark: a technique used with some [many? most? I don't know] forecasting problems is to explicitly train the model to predict <em>the next point</em> (or some point in the future) based on past data. By analogy, if the goal of the task is to predict points outside a particular range then the training data should be made of instances which represent a sequence of values so that the model can learn to predict the next point in a sequence. This design makes more sense with respect to providing a representative sample as training set. My guess is that a periodic function would be more likely to be correctly approximated in this way, but I didn't test the idea.</p>
","1","2","64377","14816"
"94457","<p>As far as I know, automated reasoning with propositional logic can be done with a solver like Prolog (and it's not new). I don't know if it has been done but I don't think it would make sense to train a ML model for propositional logic, since it's entirely symbolic (as opposed to statistical): the right answer can be found deterministically.</p>
<p>The question of logic reasoning from text like in the proposed example is a bit different, because it involves a step of representing the text. I think it could make sense to train a model which converts a text into a formal logic proposition (and back). The logic reasoning should still be done with a tool meant specifically for that imho. Note that question answering doesn't involve any logical reasoning, even if it might look this way to a user. As far as I know a QA system learns patterns about matching a type of question to its corresponding answer, the system is completely oblivious to the meaning.</p>
","0","2","64377","14816"
"94458","<p>A few comments:</p>
<ul>
<li>There is no scientific domain of &quot;data science&quot;, instead there are multiple fields which are related to data science: statistics, ML, NLP, computer vision, signal processing... and a lot of other fields which overlap and/or focus on specific applications, for instance bioinformatics. All of these domains are highly active and specialized, so it would just be impossible to monitor every possible advance.</li>
<li>There is no unique recommended way: first, people disagree all the time about the best way to do X. Second, it's very rare that a method would become completely obsolete. For example TFIDF still makes sense in many use cases, with low-resource languages or when there are efficiency constraints for instance.</li>
<li>In order to comprehensively follow the state of the art, one would have to follow the research publications. At best it's doable for a specific domain, for example one can more or less get an idea of what happens in NLP by browsing through <a href=""https://www.aclweb.org/anthology/"" rel=""nofollow noreferrer"">the main conferences</a>. A more realistic option is to wait for the advances to reach the mainstream professionals, for example by browsing regularly through DataScienceSE and/or <a href=""https://stats.stackexchange.com/"">CrossValidated</a>.</li>
<li>Final suggestion: old books are very useful to fully understand why/how things are done a certain way. We often see errors here on DataScienceSE which are due to people trying to apply methods without understanding them.</li>
</ul>
","1","2","64377","14816"
"94474","<p>This is a problem known as <em>quality estimation</em> (QE) in the application of Machine Translation (there's a regular <a href=""http://www.statmt.org/wmt20/quality-estimation-task.html"" rel=""nofollow noreferrer"">Shared Task</a> about it). The goal is to train a model to predict the quality of the output of a ML system.</p>
<p>There is an important theoretical obstacle to this kind of task: if such a QE model was able to predict perfectly, then in theory it would be possible to create a perfect model for the original task by trying many different models/parameters until the QE system predicts a high level of quality. What this means practically is that a QE model can only be as good as the system it's trying to estimate since it relies mostly on the same information, otherwise the system is clearly sub-optimal. Essentially the best the QE system can do is to determine how hard it is to correctly predict a particular input image, but it cannot predict with any certainty whether the actual prediction is correct or not.</p>
","1","2","64377","14816"
"94570","<p>CRF models are not DL models, they use text features in the traditional way as categorical data (equivalent to one-hot-encoding). However they are different from other supervised ML methods because they exploit the <a href=""https://en.wikipedia.org/wiki/Sequence_labeling"" rel=""nofollow noreferrer"">sequential information</a> of the text.</p>
<p>Typically the text sequence is represented word by word, and each word can have several additional &quot;features&quot; usually represented as columns. For example:</p>
<pre><code>The     DET   the
dog     NOUN  dog
chases  VERB  chase
a       DET   a
cat     NOUN  cat
</code></pre>
<p>The real features used by the CRF are computed based on the rules defined before training the model. They can involve the position of the current word or any other position relative to the current one. The features are usually simple binary values, for examples &quot;doc[n-1][2] == 'DET'&quot; would represent the condition &quot;the previous word is a determiner&quot;: &quot;previous word&quot; is indicated as n-1 in the sequence, column 2 indicates the POS column. The exact syntax to specify the rules may differ with different CRF implementations but the principle is the same.</p>
","0","2","64377","14816"
"94654","<p>F1-score and AUC are two evaluation measures for binary classification but they are not comparable:</p>
<ul>
<li>F1-score measures the performance of a hard classifier, i.e. a system which predicts a class for every instance. Through precision and recall it compares for every instance the predicted class vs. the gold-standard class.</li>
<li>AUC measures the performance of a soft classifier, i.e. a system which predicts a probability (or a score) for every instance. The difference is that the system doesn't decide which class the instance belongs to, so informally it can be seen as an &quot;unfinished classifier&quot;. If one decides a threshold on the probability to separate the classes then it becomes a hard classifier.</li>
</ul>
<p>Both are fine to be used with imbalanced data, that's not a reason to pick one or the other.</p>
<p>AUC is useful to study the general behaviour of a method without deciding a particular threshold. Sometimes the choice of a particular threshold can have a strong impact on performance, so using AUC avoids the issue completely. However strictly speaking AUC doesn't give the performance of a classifier.</p>
","2","2","64377","14816"
"94670","<p>The total probability is simply the sum of all the probabilities from the different paths. In probability terms it's the union of disjoint events, that's why the probabilities can be summed.</p>
","1","2","64377","14816"
"94704","<p>Author gender prediction belongs to the category of <a href=""https://en.wikipedia.org/wiki/Stylometry"" rel=""nofollow noreferrer"">stylometry</a> tasks, i.e. related the the <em>style</em> of the text rather than the <em>content</em>. In general this distinction requires a quite different approach than standard content-based tasks, here are a few of the most common differences:</p>
<ul>
<li>Content words (nouns, adjectives, adverbs) are potentially less important, so TFIDF and similar methods make little sense. It depends on the data though.</li>
<li>Usually it's counter-productive to remove stop words, because they can be actually very useful and distinctive of an author's style (at least their proportion in a text).</li>
<li>For reasons which are not really clear, it's been found experimentally that bigrams and trigrams of characters are often better features than words. This has been observed at least in English, I don't know if this is applicable to other languages.</li>
</ul>
<p>In general it's not recommended to target a specific level of performance unless it's a baseline known to be reachable, e.g. when another model can do it on similar data. That being said, in the case of this task I would say that 0.7 F1-score is reasonably realistic, but it really depends a lot on the data.</p>
<p>In case you are interested in researching the literature in order to apply state of the art methods, there have been some shared tasks about this question, sometimes mixed with other predictions such as age. search for &quot;author profiling&quot; and &quot;gender profiling&quot; in <a href=""https://pan.webis.de/shared-tasks.html#author-profiling"" rel=""nofollow noreferrer"">the PAN Shared Tasks</a>. You might also find other relevant methods in the general category of &quot;author classification/verification&quot;.</p>
","0","2","64377","14816"
"94783","<p>This is the problem of out of vocabulary (OOV) words.</p>
<p>As a rule, the training should not use anything from the test set for several reasons:</p>
<ul>
<li>The risk of <a href=""https://en.wikipedia.org/wiki/Leakage_(machine_learning)"" rel=""nofollow noreferrer"">data leakage</a>, which would cause an overestimated performance on the test set.</li>
<li>During training the model cannot use these words to distinguish between classes anyway, since they are not present. So it would be pointless to include them.</li>
<li>In principle, the model is meant to be used with any new input text, not only the documents in the current test set. Using the test set words as features in the model actually restricts the model, since it cannot be applied to any other text than the test set.</li>
</ul>
<p>The correct way to deal with OOV words is either:</p>
<ul>
<li>To simply ignore them completely, i.e. filter them out before applying the model.</li>
<li>To account for this possibility in the model from the start. Typically this can be done by having a special token <code>UNKNOWN</code>. This option is often combined with filtering out rare words from the training set: every occurrence of rare words can be replaced by the <code>UNKNOWN</code> token.</li>
</ul>
<p>The cause of the higher performance when including these features is unclear. I suspect that the inclusion of these features causes the model to be different in a way that happens by chance to have a positive effect on the test set. It's quite unlikely in general, I'm not sure.</p>
","0","2","64377","14816"
"94807","<p>There are probably many variants but here are two simple approches:</p>
<ul>
<li>Using pretrained word embeddings, you can calculate the semantic similarity between two words. For example you could use cosine to measure the similarity between the vector of a target word (e.g. &quot;calm&quot;) and every word in the set (e.g. &quot;cloud&quot;). Then the mean across words in the set gives how much the set is associated to the target, and you can pick the target which has the max similarity.</li>
<li>Using <a href=""https://wordnet.princeton.edu/"" rel=""nofollow noreferrer"">WordNet</a> to directly obtain a semantic distance/similarity between words. The method is similar to the one above.</li>
</ul>
<p>Note that there are many improvements that can be done to these basic ideas, for example you could use a predefined set of words related to &quot;calm&quot; instead of just the word &quot;calm&quot; (you can get the most similar words from WordNet for instance). There also many options for the aggregation across the set of words.</p>
","0","2","64377","14816"
"94818","<p>I think there is a good bit of confusion in how you define Machine Learning:</p>
<ul>
<li>There are different types of learning: if the model is trained with data annotated with the &quot;correct&quot; answers, then it's supervised learning.</li>
<li>There are many different &quot;families&quot; (methods) for supervised ML. Your description focuses on Neural Networks, but there are many others.</li>
<li>Your last two steps probably refer to validation (during training) and/or evaluation (after training and on a separate test set), but it's not very clear.</li>
</ul>
<p>Supervised ML can be summarized as follows:</p>
<ul>
<li>The goal is for the system to find a function <span class=""math-container"">$f$</span> which transforms an input <span class=""math-container"">$x$</span> (features) into an output <span class=""math-container"">$y$</span> (target variable)</li>
<li>The system is provided with a sample of pairs <span class=""math-container"">$(x,y)$</span> (training data). Note that this sample is only a subset of the population data: after the training the system must be able to find <span class=""math-container"">$y$</span> for any input <span class=""math-container"">$x$</span>, not only the values of <span class=""math-container"">$x$</span> seen in the training data.</li>
<li>This is why the system must <em>generalize</em> from the data, i.e. find the patterns in <span class=""math-container"">$x$</span> which are useful to determine <span class=""math-container"">$y$</span>. If the system doesn't generalize and only stores exactly which <span class=""math-container"">$x$</span> gives which <span class=""math-container"">$y$</span> then it cannot predict the answer <span class=""math-container"">$y$</span> for any value <span class=""math-container"">$x$</span> which was not seen in the training data, and that's useless.</li>
</ul>
<p>In a sense the process of generalization can be seen as compressing the <em>knowledge</em> contained in the training data. However there are at least a couple major differences:</p>
<ul>
<li>The goal of compression is to represent a specific input <span class=""math-container"">$x$</span> using as little space as possible in a way which makes it possible to re-obtain the same <span class=""math-container"">$x$</span>, possibly with some loss. The goal of supervised ML is to predict an output <span class=""math-container"">$y$</span> for <em>any</em> <span class=""math-container"">$x$</span>. First <span class=""math-container"">$y \neq x$</span>, but more importantly the ML cannot produce back the data it was trained with (in general, see exception just below).</li>
<li>A ML model doesn't have to reduce the size of the training data. For example <a href=""https://en.wikipedia.org/wiki/Instance-based_learning"" rel=""nofollow noreferrer"">instance-based learning</a> just stores all the instances in order to later use it for predicting.</li>
</ul>
","1","2","64377","14816"
"94820","<p>It depends how much noise:</p>
<ul>
<li>If it's only a little noise, say for instance 2% of the target values are off by a small value, then you can safely ignore it since the regression method will rely on the most frequent patterns anyway.</li>
<li>If it's a lot of noise, like 50% of the target values are totally random, then unless you can detect and remove the noisy instances you can forget it: the dataset is useless.</li>
</ul>
<p>In general ML algorithms are based on statistical principles, to some extent their job is to avoid the noise and focus on the regular patterns. But there are two things to pay attention to:</p>
<ul>
<li>Is the noise truly random, or does it introduce some biases in the data? The latter is a much more serious issue.</li>
<li>Noisy data is even more likely to cause overfitting, so extra precaution should be taken against it: depending on the data, it might be necessary to reduce the number of features and/or the complexity of the model.</li>
</ul>
","0","2","64377","14816"
"95043","<p>Sure, ZeroR is a perfectly fine baseline. In this case I think it's better to call it a random baseline rather than a majority baseline, since it's just what it is.</p>
<p>To my knowledge this is the only basic baseline usable with any classification task. Other baselines would involve something more sophisticated based on the specific task. The standard way to have a more competitive baseline is to used a state of the art method for the task.</p>
<p>Btw the baseline or evaluation method doesn't depend on the learning algorithm, it doesn't matter if it's NB or any other classification method.</p>
","1","2","64377","14816"
"95045","<p>As Nikos said in a comment, this is a graph showing the different classes (digits) in the space of the latent variable.</p>
<p>First a remark: from the point of view of visualization design I think the authors made a mistake by using a gradient of colours for the classes, it should have been a completely different colour for every digit: here the digits are a categorical variable, there is no notion of order between them. This is misleading because the reader of the graph would naturally expect some logic in the gradient, but there is none.</p>
<p>The space itself is not interpretable, in the sense that the values on the X and Y don't have a meaning (for a human). However what this graph can tell you is how close or how distinguishable are two classes: ideally the different classes are fairly distinct from each other, which means that the model is sucessful at separating them.</p>
<p>Unfortunately due to the mistake I mentioned above it's quite difficult to see things clearly in this graph, but if I see the colours correctly there are a few things observable:</p>
<ul>
<li>the 1 is the big purple thing on the top right, it's mostly distinct from the rest.</li>
<li>The 0 is the smaller dark purple group on the top left, also quite distinct.</li>
</ul>
<p>The other digits are much more mixed with each other, it's hard to see anything:</p>
<ul>
<li>Apparently the 9 (yellow) spreads a bit everywhere in the centre, and there are many points for 7 (I think?) in the middle: this means that potentially the model could sometimes confuse 7 and 9 (it makes sense: the handwritten 7 and 9 can have the same shape).</li>
<li>The 6 seems to be the rather thin greenish thing which is closest to the 0, and I can imagine indeed that these two have a similar shape.</li>
</ul>
<p>My eyes are not that good so maybe I made some mistakes, but I hope you see the idea.</p>
<p>Basically the more the different classes/colours are well separated on the graph, the better the model, so you should be able to see some difference between some very good hyper-parameters and some very bad ones. But it would be pointless to rely on the graph for that: a good old evaluation measure (or confusion matrix) is much more accurate.</p>
","2","2","64377","14816"
"95078","<p>There's a good chance that your question will be closed I'm afraid, but here are a few thoughts:</p>
<blockquote>
<p>would differentiate a professional data scientist from me</p>
</blockquote>
<p>A professional data scientist is somebody who does data science for a living, so you definitely belong to the club, congrats!</p>
<p>Seriously, apparently, you have at least some symptoms of the <a href=""https://en.wikipedia.org/wiki/Impostor_syndrome"" rel=""nofollow noreferrer"">impostor syndrome</a>: your level is appropriate, you're able to do your job, yet you feel inadequate. The usual advice <a href=""https://academia.stackexchange.com/questions/11765/how-to-effectively-deal-with-imposter-syndrome-and-feelings-of-inadequacy-ive"">on AcademiaSE</a> (it's very common in academia) is to deal with the psychological aspect, optionally with some professional help.</p>
<p>Now about the myth of &quot;the real professional data scientist&quot;: data science has become vast and specialized. There's not even a clear definition of the scope of data science, let alone a shared understanding of which knowledge/skills a data scientist should have. Additionally, the field changes very fast, so it's humanly impossible to know everything.</p>
<p>What people usually recommend is to gain as much experience as possible, and especially in your case since it looks like you already covered the theory fairly well. You can just pick a topic you'd like to dig deeper into and go for it.</p>
<p>For the record, I find browsing and answering questions on DataScienceSE a very good way to keep up, discover things that I didn't know, and progress. Why answering is useful you ask? Because it forces me to (1) understand the problem and think about how I would address it. An intellectual ML design exercise, it's always good to practice. (2) explain things in a clear way, which is always a good exercise to check how clear things are in my mind.</p>
","2","2","64377","14816"
"95113","<p>I don' know Spacy custom NER but it's unlikely that the model is optimized on recall, otherwise it would label absolutely everything as an entity in order to reach perfect recall.</p>
<p>Your model happens to have a good recall, but it doesn't meant that the algorithm optimizes for this. There might be some technical parameters but it's very likely that the performance depends mostly on the training data. If you obtain very good recall but bad precision, it might be because the training data contains a higher proportion of entities than the test data. If this is the case, the way to improve precision is to provide more negative examples, e.g. sentences without any entities. This should improve precision since the model will be more careful about negative words, but it will probably also decrease recall.</p>
","1","2","64377","14816"
"95170","<p>I think there is a simple way to calculate this:</p>
<ol>
<li>For each class calculate the ratio new proportion / old proportion:</li>
</ol>
<ul>
<li>a: 0.3 / 0.1 = 3</li>
<li>b: 0.2 / 0.3 = 0.67</li>
<li>c: 0.5 / 0.6 = 0.83</li>
</ul>
<ol start=""2"">
<li>The max of these ratios is the only one which matters because it defines the hard limit in number of instances. For example in this case class a needs 3 times more data, so the full sample has to be reduced by 3. Let's say you have 1000 instances:</li>
</ol>
<ul>
<li>a keeps its 100 instances so the full size of the sample must be 100/0.3 = 333</li>
<li>b has 0.2 * 333 = 67</li>
<li>c has 0.5 * 333 = 166</li>
</ul>
<p>(I didn't check that this works in every case)</p>
","2","2","64377","14816"
"95190","<p>First, when working with big data most of the time it's more convenient to work with a random subset rather than the whole thing: usually during the design and testing stages there is no need to work with the full data since optimal performance is not needed.</p>
<p>Second, it's often useful to do an ablation study in order to check that using the full data is actually useful for the model. Sometimes training the model with a subset gives the same results as with the full available data,  so in this case there's no advantage using all the data.</p>
<p>Finally there are indeed cases where one needs to process a large dataset or run a long training process which cannot be done on a regular computer. There are various options depending on the environment:</p>
<ul>
<li>Buy the required hardware (it's rarely the best option but it needs to be mentioned)</li>
<li>Use a commercial cloud service such as <a href=""https://aws.amazon.com/"" rel=""noreferrer"">AWS</a></li>
<li>Some organizations have their own in-house computing servers/clusters. In particular if you're a student it's likely that you should have access to this kind of service through your university, ask around (afaik most decent universities provide it nowadays).</li>
</ul>
","5","2","64377","14816"
"95201","<p>Warning: I don't have any experience with Twitter API so I don't know if there are any specific limitations.</p>
<p><em>In general</em> with big data, assuming the goal is to apply a repetitive process, it's preferable to split the data into small chunks so that the process can be distributed on multiple cores, ideally using a computing cluster.</p>
<p>However in my experience there's a trade-off between convenience and speed: splitting the data and re-assembling it (<a href=""https://en.wikipedia.org/wiki/MapReduce"" rel=""nofollow noreferrer"">MapReduce</a>) requires more manipulations, which implies either more manual work (and possible errors) or more code to implement. This is why I usually consider a distributed design only if either it's easy to implement or it would be really too long to compute sequentially.</p>
<p>In the case you describe, processing a single CSV containing all the data would take around 300x2=600 hours, which is around 25 days... I would probably try to directly process the files in parallel. For example if you have access to only 8 cores it would take around 300x2/8=75 hours, but if you have access to 40 cores it's only around 15 hours, etc.</p>
<p>An additional advantage of batch processing is that if there is an error during the process, you don't need to re-process the whole data but only the batches which failed.</p>
","1","2","64377","14816"
"96284","<p>General remark: your two new models give very different results in terms of precision and recall, I find this a bit surprising. I would probably try different learning methods (e.g. decision trees, SVM) in order to investigate if this is really due to the features or not.</p>
<p>a) Absolutely, the threshold should be specific to the model and features, it would be suboptimal to keep the same value.</p>
<p>b) Typically one would check whether there is a statistically significant difference in the performance between the models. I'm not sure which significance test is appropriate here.</p>
<p>c) Then you should evaluate your models with <a href=""https://en.wikipedia.org/wiki/F-score#Definition"" rel=""nofollow noreferrer""><span class=""math-container"">$F_{\beta}$</span>-score</a> instead of just F1, with <span class=""math-container"">$\beta$</span> higher than 1 in order to favour recall. I'd say at least 2, it depends how much more costly is a FN than a FP error. Note that the threshold should also be selected based on this measure.</p>
<p>d) I would certainly try different methods, because it's always possible that another method would perform better. I always recommend decision trees because they are robust and interpretable.</p>
<p>e) I'm not sure but there might be a confusion here: <code>class_weight=balanced</code> means that you give a higher importance to the minority class than what it really represents in the data, in other words the learning algorithm will work as if the two classes have the same number of instances. So as far as I know there would be no point in resampling additionally to using the weights. However you could provide manual weights, for instance 0.1,0.9 if you want to favour detecting the minority class even more (that would increase recall). By the way I would suggest to also try without using weights at all: it's unlikely to be good in terms of recall but that it could be useful just to know the precision/recall in this case.</p>
<p>f) For tuning hyper-parameters the important point is to use a validation set different from the final test set. For every &quot;method&quot; (set of features) independently:</p>
<ol>
<li>Use grid search to determine the best parameters for the method, evaluating on the validation set only.</li>
<li>Pick the best parameters. At this stage if you want you can train the final model using both training set+validation set (but only with the selected parameters).</li>
</ol>
<p>Then you can apply the 3 final models on the unseen test set and compare their performance. Essentially the parameter tuning is part of the training process (it's a kind of &quot;meta-training&quot;), so there's no data leakage as long as you don't use the test set for determining the best parameters.</p>
","1","2","64377","14816"
"96315","<p>In my experience these cases are almost unavoidable in practice, but it's not a real problem:</p>
<ul>
<li>The sequential model learns to classify each token as B,I or O based on the features. Occasionally it might find a case in the test data (sometimes even in the training data if it's noisy) where the most likely class based on the features is I even though the previous token is O. Normally it's rare since the model didn't see any O-I sequence in the training data.</li>
<li>It doesn't really matter: one can easily run a post-processing script which converts any O-I sequence into O-B if necessary.</li>
<li>Note that if you really don't want to obtain this kind of inconsistency you could opt for a different tagging scheme, for example only O,I. However in regular NE tasks the indicators which capture the start of an entity are important and might differ from the ones which characterize a continuing entity, so this would probably decrease performance. Note that other schemes exist, for example BILOU but since it's more complex it tends to cause more inconsistencies in general.</li>
</ul>
","0","2","64377","14816"
"96352","<p>Let's first assume a textbook version of the &quot;data&quot;. OP's suggestion is easy to implement: one just need a map containing all the training instances as keys with the true answer as value.</p>
<ul>
<li>First, there's the problem of noise and inconsistencies in the training data: if the exact same instance appears 8 times labelled as class A and 2 times as class B, the model will be wrong 20% of the time (more about this case below).</li>
<li>If there are any numerical features an/or many features, the space of all possible instances is potentially infinite and it's quite unlikely to find the exact same instance twice. This makes this solution rarely useful and questionable efficiency-wise.</li>
<li>On the contrary, if the training data almost completely covers the space of instances then it makes the model almost useless, the map is sufficient (possibly completing any missing instances in the training data).</li>
</ul>
<p>Let's assume a balanced scenario where the distribution of the instances makes the map solution relevant. There is still the general problem of: what is exactly &quot;the data&quot;?</p>
<p>If we look at it from a standard ML perspective, the data is made of instances, with each instance made of specific values for every feature. But how is the set of features defined? It can be all the information available, or it can be a subset of the information available.</p>
<ul>
<li>First, all the information <em>available</em> is not always all the information needed to make a correct prediction. For example the true answer might depend on the local weather at the customer's location, or whether the customer is in a good mood, etc. This information is not available. This can be the reason why the model makes wrong predictions even on instances seen in the training set. For a non-expert customer their environment feels obvious, as if the machine should know this.</li>
<li>Even if we assume that the information available to the model is sufficient to predict the true answer, there can be the case where the model ignores some features. Obviously this would happen with feature selection, but it can also be because some features are not informative enough or because they have too many different values which would cause overfitting. This is an important point: the model needs to <em>generalize</em> in order to make predictions for every possible case, and generalizing involves ignoring the details to some extent. This can also be due to the type of model, for instance linear regression cannot properly represent non-linear factors.</li>
<li>The model can even have more information than the customer. For example the model might exploit the time of an action to make its prediction: if the customer does the same action at different times, they might assume that they should obtain the same result as the first time but they don't. This is because the model found during training that statistically most customers at time A want result X while most customers at time B want result Y. Of course a few customers will not obtain the result they expect, because they don't realize that the model uses an information that they don't take into account themselves.</li>
</ul>
<p>With all these examples I'm trying to show that &quot;the data&quot; is not so clearly defined in general, the customer and the model might have a different perspective about it: two instances which look the same to the customers may differ for the model and conversely. If possible, I think it's more beneficial to understand why such errors happen: it can be something which could be fixed in the model, or it can be something which has to be explained to the customer.</p>
","0","2","64377","14816"
"96527","<p>This looks completely normal to me: your dataset has 26x180=4680 instances, so the test set should have 4680x0.33=1544.4 instances. According to the classification report it contains 1545 instances, which is consistent with this calculation.</p>
<p>It's important to understand that by default the dataset is split between training and test set randomly <em>across all the instances</em>, without taking their class into account. This means that by chance some of the classes can have a bit more or a bit less than 33% instances in the test set. This is what can be observed in the classification report and it's not a problem.</p>
<p>Sometimes this can be an issue when there are classes which have very few instances in total. In this case one should use <a href=""https://en.wikipedia.org/wiki/Stratified_sampling"" rel=""nofollow noreferrer"">stratified sampling</a> in order to apply the proportion to each class independently.</p>
","0","2","64377","14816"
"96561","<p>I can think of a couple options to collect a sample of medical institutions:</p>
<ul>
<li><p>Wikipedia has <a href=""https://en.wikipedia.org/wiki/Lists_of_hospitals"" rel=""nofollow noreferrer"">a list of hospitals by country</a> (isn't Wikipedia amazing?)</p>
</li>
<li><p>Many countries have some kind of national directory of medical institutions, but that would probably be difficult to scrap and specific to each country.</p>
</li>
<li><p><a href=""https://www.nlm.nih.gov/research/umls/index.html"" rel=""nofollow noreferrer"">UMLS</a> has a category (&quot;semantic group&quot;) for &quot;Health Care Related Organization&quot; (T093, see <a href=""https://lhncbc.nlm.nih.gov/semanticnetwork/download/SemGroups.txt"" rel=""nofollow noreferrer"">here</a>), which means that a list of such organizations can be directly collected from the UMLS data. I thought this would be a good option, but I did a quick test and it appears to contain only names of departments, no proper nouns, for example:</p>
<pre><code>Community occupational therapy clinic
Abortion Center
Area Health Education Center
</code></pre>
</li>
</ul>
<p>Given that UMLS is closely related to PubMed, my guess is that it's not a very good direction, but maybe I didn't dig deep enough. Fair warning: Processing the whole PubMed/PMC is quite a lot of work in my experience.</p>
","2","2","64377","14816"
"96581","<p>This Universal Sentence Encoder that you link is trained specifically on English data, so it's going to work very poorly on any other language (to be clear, it's likely to produce garbage).</p>
<p>Unfortunately it's quite unlikely that you'll find a similar pre-trained model for Macedonian. You would have to train your own model from Macedonian data, and you need a really large amount. Btw that's the main reason why these pre-trained models are often trained on English only, since there's a lot of English text available. In case you want to try this, there is a <a href=""https://github.com/UniversalDependencies/UD_Macedonian-MTB/blob/dev/README.md"" rel=""nofollow noreferrer"">Macedonian corpus</a> as part of the <a href=""https://universaldependencies.org/"" rel=""nofollow noreferrer"">Universal Dependencies project</a>.</p>
","1","2","64377","14816"
"96582","<p>As Nikos said, the dataset is not linearly separable: one cannot draw a single straight line which has all the red points on one side and all the green points on the other.</p>
<p>There is an obvious repetition pattern on <span class=""math-container"">$x_2$</span>. Let's assume that <span class=""math-container"">$x_2$</span> ranges from 0 to 100 on this graph, the value <span class=""math-container"">$x_2$</span> modulo 50 would make it linearly separable. That would be feature engineering.</p>
<p>Notice that <span class=""math-container"">$x_1$</span> doesn't have any impact on the class. So imho the best algorithm in this case is to just map <span class=""math-container"">$x_2$</span> intervals to the class.</p>
","0","2","64377","14816"
"96606","<p>Text summarization is an active research topic and the question is very broad. For a detailed answer one has to study the state of the art, there's a <a href=""https://scholar.google.com/scholar?q=nlp+abstractive+summarization"" rel=""nofollow noreferrer"">rich literature</a> on the topic.</p>
<p>From a recent <a href=""https://www.sciencedirect.com/science/article/abs/pii/S0957417418307735"" rel=""nofollow noreferrer"">survey paper</a>:</p>
<blockquote>
<ul>
<li>Deep Learning Models capture both the syntactic and semantic structure.</li>
<li>Requirement of large data set limits the use of Deep Learning Models.</li>
</ul>
</blockquote>
","0","2","64377","14816"
"96631","<p>It's similar to a <a href=""https://en.wikipedia.org/wiki/Violin_plot"" rel=""nofollow noreferrer"">violin plot</a>, which shows the shape of the distribution of a variable. However here the X axis shows only categorical values, it's not clear if the shape is based on some underlying numerical variable (this is a requirement for a violin plot).</p>
<p>Violin plots can be made <a href=""https://www.python-graph-gallery.com/violin-plot/"" rel=""nofollow noreferrer"">in Python</a> or <a href=""https://www.r-graph-gallery.com/violin.html"" rel=""nofollow noreferrer"">in R</a>.</p>
","3","2","64377","14816"
"96649","<p>First, <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer""><code>CountVectorizer</code></a> produces a matrix of <em>token counts</em>, not TF-IDF weights. In order to obtain TF-IDF weights you would have to use <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"" rel=""nofollow noreferrer"">TfidfVectorizer</a>.</p>
<p>If the goal is to study term frequency, there is no point using TF-IDF since TF-IDF weights are different from frequency. TF-IDF is used to reduce the weight of tokens which appear frequently compared to tokens which appear rarely. Moreover, TF-IDF weights are at the level of a document, so they cannot be used as a measure of global comparison across all documents. Across documents you could use the IDF (Inverse Document Frequency) part only, but then why not simply use Document Frequency.</p>
<p>Note that the same applies to a token count matrix: the values are at the level of a document. In order to find the global frequency one has to sum across the documents for every token.</p>
<p>Finally if you are trying to find the most frequent terms/n-grams of any length, it's difficult to compare frequencies between n-grams of different length. Additionally you're going to find real &quot;terms&quot; mixed with frequent grammatical constructs, for example &quot;it is&quot; is not a term but it's a frequent n-gram.</p>
","1","2","64377","14816"
"96715","<p>As far as I know it's very unlikely that a categorical variable with billions of possible values could be a good predictor for a ML model, but there is certainly some underlying information related to the IP address which are good predictors. So it's a problem of feature engineering not in an technical sense but in a design sense, i.e. using expert knowledge in order to provide the most relevant information to the model.</p>
<p>I don't know the task or the data but you could study what makes the IP useful:</p>
<ul>
<li>location: maybe using a feature representing the country or a more specific location based on the IP would work.</li>
<li>historically known IP: for example some boolean features could represent whether this IP connected in the past hour/day/week/...</li>
<li>third-part info: features representing whether this IP belongs to some whitelist/blacklist</li>
<li>...</li>
</ul>
","0","2","64377","14816"
"96720","<p>There is one obvious problem with this task: the result is not a real review, it's <strong>a generated text which looks like a review</strong>. Given that the point of a product review is usually to provide the reader with some information about the product, it's not clear to me how this task would be useful: if the review can be made without even testing the product, its informative value is zero (it's even misleading).</p>
<p>Similarly one could train a model to generate &quot;news articles&quot;: if the model is well trained the generated text would look exactly like real news, but there's very little chance that the content would correspond to what actually happens in the world.</p>
","1","2","64377","14816"
"96729","<p>Evaluation is always based on the task, not on the method. Since the dictionary-based method gives an output similar to the ML-based approach, you can evaluate it in the same way, using a test set with gold-standard labels (preferably the same test set as the other method, or at least similar in size).</p>
<p>Maybe what confuses you is that the dictionary-based method doesn't require training so you there's no need for splitting the data between training and test set.</p>
<p>Note: the dictionary-based approach is a <a href=""https://en.wikipedia.org/wiki/Heuristic"" rel=""nofollow noreferrer"">heuristic</a> method.</p>
","0","2","64377","14816"
"96770","<p>This kind of problem is called <a href=""https://en.wikipedia.org/wiki/Record_linkage"" rel=""nofollow noreferrer"">record linkage</a> (or sometimes entity matching or other variants). The task consists in finding among a list of strings representing entities (persons or organizations) those which represent the same actual entity.</p>
<p>There are two main approaches (which can be combined):</p>
<ul>
<li>String similarity matching methods. See for example <a href=""https://datascience.stackexchange.com/a/85647/64377"">this question</a> or <a href=""https://datascience.stackexchange.com/q/63325/64377"">this one</a>. Note that in case the list of companies is large, there can also be an efficiency issue: see this <a href=""https://datascience.stackexchange.com/q/54570/64377"">question</a>.</li>
<li>Databases or third-party resources. See for example <a href=""https://datascience.stackexchange.com/q/56406/64377"">this related question</a>.</li>
</ul>
","3","2","64377","14816"
"96829","<p>The predictions are always 0 due to the massive imbalance in the data.</p>
<p>The positive class represents only 0.01% of the data. In this context, for the model to &quot;take the risk&quot; of predicting some instances as positive, it would need some very strong indicators. Essentially the model needs to be 99.99% sure when it predicts an instance as positive, because if the confidence is any lower then the model would make more False Positive errors than it finds True Positive instances. Since it never reaches this level of confidence for a positive instance, it predicts every instance as negative.
You could look at the probability distribution in your current experiment: chances are that the probabilities are all ridiculously low.</p>
<p>If you really want to see some instances predicted as positive, you can:</p>
<ul>
<li>undersample the negative class and/or oversample the positive class. This will force the model to predict more positive instances since the risk of error is relatively lower.</li>
<li>Change the threshold for predicting the class: the default is to pick the class which has the highest probability, in other words, to predict a positive only if the probability of positive is higher than 0.5. If you set the threshold to a very small value <span class=""math-container"">$p$</span> instead, the instances which obtain a higher probability than <span class=""math-container"">$p$</span> are going to be predicted as positive.</li>
</ul>
<p>Naturally, both options will cause a lot more False Positive errors on the test set, which must follow the true distribution of course.</p>
","1","2","64377","14816"
"96849","<p>[completely edited after clarification from OP]</p>
<p>A histogram is built by making bins of equal size across the range of values taken by the variable. For example if the variable ranges from 0 to 500 one might decide to create 50 bins of size 10. Then the actual values of the distribution are counted by bin: every value between 0 and 9 goes into the first bin, every value between 10 and 19 goes into the second bin etc.</p>
<p>The number of discrete values does not matter (in fact the values can be continuous) because the values are binned, i.e. they are grouped by how close they are to each other (with arbitrary interval bounds).</p>
<p>I can see that the data you have is already formatted as</p>
<pre><code>&lt;value&gt; &lt;frequency&gt;
</code></pre>
<p>The problem you have certainly comes from the fact that this format is incorrect for the function: typically histogram functions create the bins themselves, so there's no need to have counted the values beforehand. This means that you should provide a single vector containing all the values as many times as they occur.</p>
<p>Alternatively you could create the bins yourself beforehand: decide the intervals then count how many values in each bin. Then use a simple bar plot to show the count for every bin. This option is usually less convenient.</p>
","1","2","64377","14816"
"96863","<p>This is a very strange design:</p>
<ul>
<li>The goal is to train an ensemble classification model.  In general there is <strong>no strong reason to use only subsets</strong> of the data to train the individual learners, let alone to use a strict partition of the data. It might make sense to train different learners with different subsets in order to make the final model more stable, but at least the <strong>sampling should be made with replacement</strong>, i.e. allowing an instance to be used for several learners. Forcing every instance to be used in only one learner is very likely to make most or all the individual learners very weak, in turn making the final model certainly not as a good as a simple classifier using all the data.</li>
<li>Using clustering to prepare the subsets is even stranger, I cannot think of any rationale for this: not only it can be expected that some clusters would correspond to specific classes, it's actually a good thing: it means that the data contains some patterns which correspond more or less to the classes. If there were no such pattern the classification problem would be unsolvable. It's counter-productive to use the clusters as training subsets for the individual learners, since one wants these models to be able to distinguish the classes. So they need examples of several classes, and preferably not a completely biased sample otherwise they can't do their job properly. If you really want to use subsets of data, the <strong>subsets should simply be picked randomly</strong>, not obtained by clustering.</li>
<li>In case you really want to use clustering, the resulting cluster for every instance <strong>can be used as a feature</strong> for the classification stage. Not as a way to obtain subsets of data.</li>
</ul>
<p>There is no clustering method which lets you specify constraints based on class, because that's not at all the logic of a clustering algorithm. What you could use is <a href=""https://en.wikipedia.org/wiki/Stratified_sampling"" rel=""noreferrer"">stratified sampling</a>, i.e. picking instances randomly class by class in order to make sure that every subset contains the same distribution across classes.</p>
","5","2","64377","14816"
"96898","<p>Assuming that there are almost always clear markers at the beginning of the enumeration, i.e. either &quot;required skills&quot; or &quot;nice to have&quot; (or any variant of these two), I would suggest trying to add custom features for example:</p>
<ul>
<li>last marker seen from the current position, a categorical value for either &quot;Nicetohave&quot; or &quot;mandatoryskill&quot; (actually two boolean features with OHE)</li>
<li>distance in number of words from the current position to the last marker seen</li>
</ul>
<p>Obtaining values for these features would require a preprocessing step where the markers are extracted and/or labelled, probably with some simple string matching (assuming that there are not too many variants of the markers).</p>
","0","2","64377","14816"
"96928","<p>Based on the explanations in the comments, what you need is to format the data so that every sample has the right 'group' label. Since there are only 6 columns I would just do it manually like below.</p>
<pre><code>marte &lt;- read_xlsx(&quot;TrinucleotideFrequency06182021.xlsx&quot;)

a40 &lt;- data.frame(gene=marte<span class=""math-container"">$Trinucleotide,value=marte[,2])
b40 &lt;- data.frame(gene=marte$</span>Trinucleotide,value=marte[,3])
c40 &lt;- data.frame(gene=marte<span class=""math-container"">$Trinucleotide,value=marte[,4])
a80 &lt;- data.frame(gene=marte$</span>Trinucleotide,value=marte[,5])
b80 &lt;- data.frame(gene=marte<span class=""math-container"">$Trinucleotide,value=marte[,6])
c80 &lt;- data.frame(gene=marte$</span>Trinucleotide,value=marte[,7])

# these should not be necessary:
colnames(a40) &lt;- c('gene','value')
colnames(b40) &lt;- c('gene','value')
colnames(c40) &lt;- c('gene','value')
colnames(a80) &lt;- c('gene','value')
colnames(b80) &lt;- c('gene','value')
colnames(c80) &lt;- c('gene','value')

gp1 &lt;- rbind(a40,b40,c40)
gp1<span class=""math-container"">$group &lt;- rep('40s',nrow(gp1))
gp2 &lt;- rbind(a80,b80,c80)
gp2$</span>group &lt;- rep('80s',nrow(gp2))

ggplot(marte, aes(x = gene, y = value, color = group)) +geom_boxplot()+ theme(axis.text.x = element_text(angle = 90))+coord_flip()
</code></pre>
<p><a href=""https://i.stack.imgur.com/G7ovi.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G7ovi.jpg"" alt=""Plot"" /></a></p>
<p>Explanation: for each sample a dataframe which contains only the values coming from the three columns of this sample. The three parts are concatenated by rows (<code>rbind</code>) with the columns <code>gene</code> and <code>value</code>, then the column <code>group</code> is added containing the id of the group (sample) in every row.</p>
<p>For the record, doing boxplots with only two values is not great, a boxplot is supposed to show the distribution of a set of values. I would suggest simple barplots instead, with the two values shown as different bars.</p>
","0","2","64377","14816"
"96984","<blockquote>
<p>Is it possible to directly label contents like this? Is there any specific process to do this? Do I need to validate my labeling ?</p>
</blockquote>
<p>Yes, it's definitely possible to define sentiment classes in this way. One can reasonably assume that the review score is a good approximation of the review sentiment.</p>
<p>it's just a method to define the gold standard, there's no particular process for that. It's important to realize that defining the gold standard is an important part of designing the task itself, as opposed to designing a system which tries to solve the task.</p>
<p>In some cases it makes sense to prove that whatever is used as gold standard corresponds to the goal of the task, but in this case it's straightforward: it's safe to assume that a user who writes a review gives as score a value which corresponds to their overall sentiment.</p>
<p>Even if this is a reasonable design, it's also important to notice the limitations:</p>
<ul>
<li>By discretizing the score into 3 classes, the score information
is simplified. For example the difference between a 7 and a 10 is lost.</li>
<li>The arbitrary cut-off points cause a threshold effect. Normally there is less difference between a 2 and a 3 than between a 3 and a 6, but the classes reverse this relation.</li>
</ul>
<p>Note that sentiment analysis does not have to be a classification task (predicting a categorical variable), it can also be defined as a regression task (predicting a numerical variable). In this case the target variable could be the score itself, and that would avoid some of the problems mentioned above. This is also a design choice, it depends mostly on what the application is for.</p>
","1","2","64377","14816"
"97010","<p>In <a href=""https://en.wikipedia.org/wiki/Natural_language_processing"" rel=""nofollow noreferrer"">Natural Language Processing</a> it's crucial to choose the representation of the data and the design of the system based on the intended task, there is no generic method to represent text data which fits every application. This is not a simple technical problem, it's an important part of designing the system.</p>
<p>The simplest method to structure text data is to represent the sentence or document as a <a href=""https://en.wikipedia.org/wiki/Bag-of-words_model"" rel=""nofollow noreferrer"">bag of words</a> (BoW), i.e. a set containing all the tokens in the sentence or document.  Such a set can be represented with One-Hot-Encoding (OHT) over the full vocabulary (all the words in all the documents) in order to obtain structured data (features). Many preprocessing variants can be applied: remove stop words, replace words with their lemma, filter out rare words, etc. (don't neglect them, these preprocessing options can have a huge impact on performance).</p>
<p>Despite their simplicity, BoW models usually preserve the semantic information of the document reasonably well. However they cannot handle any complex linguistic structure: negations, multiword expressions, etc.</p>
","2","2","64377","14816"
"97016","<p>There's no difficult time complexity issue, you just need to understand what <code>GridSearchCV</code> does, it will clarify everything. It's actually very simple:</p>
<p><a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"" rel=""nofollow noreferrer"">sklearn documentation</a> says that <code>GridSearchCV</code> performs an &quot;exhaustive search over specified parameter values for an estimator&quot;.
An <a href=""https://en.wikipedia.org/wiki/Brute-force_search"" rel=""nofollow noreferrer"">exhaustive search</a> means that the algorithm simply iterates over all the possibilities in the search space.
The search space consists of all the combination of values for the parameters, that is:</p>
<ul>
<li><code>base_estimator__criterion</code>: 2,</li>
<li><code>base_estimator__splitter</code>: 2,</li>
<li><code>base_estimator__max_depth</code>: 3,</li>
<li><code>base_estimator__min_samples_leaf</code>: 3,</li>
<li><code>n_estimators</code>: 9,</li>
<li><code>learning_rate</code>: I don't know because <code>range(0.5, 10)</code> gives an error, let's say 5 for instance.</li>
</ul>
<p>Now that gives us <span class=""math-container"">$2*2*3*3*9*5 = 1620$</span> combinations of parameters. By default <code>GridSearchCV</code> uses 5-fold CV, so the function will train the model and evaluate it <span class=""math-container"">$1620*5=8100$</span> times. Of course the time taken depends on the size and complexity of the data, but even if it takes only 10 seconds for a single training/test process that's still around 81000 sec = 1350 mn = 22.5 hours.</p>
<p>By comparison, your second set of parameters contains only <span class=""math-container"">$3*3*3=27$</span> combinations. Since it took 20 minutes, we can estimate that a single combination (full CV) requires 0.7 mn to run. This means that in the first case above you would need <span class=""math-container"">$1620*.7=1200$</span> mn = 20 hours for the full process (approximately).</p>
<p>The usual solutions are:</p>
<ul>
<li>decrease the number of combinations, as you did in the second case</li>
<li>parallelize the process with the <code>n_jobs</code> argument, that will roughly divide the time by <span class=""math-container"">$n$</span>. Of course it works only if you have multiple cores and you don't plan to use them during this time ;)</li>
</ul>
","1","2","64377","14816"
"97070","<p><em>I'm not totally sure about my answer so please take it with a grain of salt.</em></p>
<p>I think you shouldn't worry too much about the initial model being overfitted:</p>
<ul>
<li>This is likely to happen since the initial dataset is small, so the model might have no choice other than to capture patterns that happen by chance.</li>
<li>The process of active learning is intended to &quot;correct&quot; the initial model progressively. This is not only about the model capturing new details, it can also be about the model re-evaluating previous patterns based on the data.</li>
</ul>
<p>So my intuition would be to just let the model overfit a bit if it has to. However, if the model overfits a lot and/or is too complex, it means that it will require a lot (possibly too many) instances to be labeled. Depending on the context, this could be a more serious problem: the initial model should be decent enough for the active learning process not to need the labeling of many/all instances.</p>
","3","2","64377","14816"
"97100","<blockquote>
<p>I then compare the results of this model with a model that I train on my own separately with the best hyperparameters, and I get better results with my own model. Why is that?</p>
</blockquote>
<p>I would say that the most likely explanation is overfitting with the model selected by grid search: when trying the different combinations of parameters, it's possible that one happens to be very good by chance on the CV split. The CV is supposed to counter this effect of chance but it cannot perfectly avoid it, especially if there are many different combinations of parameters and/or the dataset is small. As a result this model would be selected even though it's not truly better in general, and it obtains sub-optimal performance on the test set.</p>
<blockquote>
<p>Does GridSearchCV still use cross validation when it's training on the full data with the best hyperparameters?</p>
</blockquote>
<p>I don't know the implementation but this wouldn't make any sense, so no.</p>
<blockquote>
<p>Is it the case that GridSearchCV is doing something extra that's hurting the model?</p>
</blockquote>
<p>I don't think so but the process of tuning the parameters is a kind of training so it can also lead to overfitting.</p>
","0","2","64377","14816"
"97116","<p>The important concept here is <strong>evaluation</strong>:</p>
<ul>
<li>Either one wants to apply the model for the purpose of evaluating it. In this case, the test data must be labelled because in supervised learning one evaluates a model by comparing its predictions against the true answers.</li>
<li>Or one just wants to apply the model in order to obtain its predictions for some data.</li>
</ul>
<p>There might be confusion about the term &quot;test set&quot;:</p>
<ul>
<li>One can always apply a supervised model to some new instances, and it's quite common to call any data on which the data is applied &quot;test data&quot; by contrast to the training data (technically it should probably be called &quot;production data&quot; actually). Ultimately a supervised model is intended to predict the target variable on some unlabelled data (that's the whole point) so of course one can apply the model to some unlabelled &quot;test&quot; data.</li>
<li>But from the point of view of building an ML model, only evaluation matters: applying the model to some unlabelled data is not really relevant since it doesn't provide any information about how well the model works. This is why in virtually every ML book/course about supervised learning the test set is understood as a labelled test set, i.e. which can be used for evaluation.</li>
</ul>
<p>If you have a labelled training set but no labelled test set, it means that you will have to split your training set in order to evaluate the model. Otherwise, when you apply the model to the test set you would have no idea whether the predictions are correct or not, so the model would be useless.</p>
","2","2","64377","14816"
"97136","<p>This is related to <a href=""https://en.wikipedia.org/wiki/Language_model"" rel=""nofollow noreferrer"">language modeling</a>.</p>
<p>A language model can predict the likelihood of a sentence after being trained on a large corpus of sentences. A language model does not deal with the semantics of the sentence, it can only assess how plausible the sentence is from a statistical point of view.</p>
","1","2","64377","14816"
"27360","<p>There is a way to do this. <a href=""https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model"" rel=""nofollow noreferrer"">See the documentation</a>! It is very good.</p>

<p>I assume you mean that you build a network using Keras (which contains recurrent GRU layers) and would like to save the model after some training, then restart from the same point e.g. with new data or just to push the model further.</p>

<h3>Example</h3>

<p>Assuming you want to analyse some images of shape (150, 150, 3):</p>

<pre><code>from keras.models import Model, load_model
from keras.layers import Input, GRU, Dense

my_model = Model()

my_input = Input(shape=(150, 150, 3))    # using Tensorflow's dims: 'channels last'
gru_layer - GRU()(input)
output = Dense(32)(gru_layer)

# tell the Model class which layers are the start and end of the model
model = Model(inputs=my_input, outputs=my_output)

# save it!
my_model.save('my_model.h5')

# some time later.... a new python session
my_reloaded_model = load_model('my_model.h5')

# continue using the model
</code></pre>

<h3>Explanation</h3>

<p>At the point the model is built, we can compile and train it the usual way e.g. <code>my_model.fit(...)</code></p>

<p>Once training is finished, we can save the model two different ways:</p>

<ol>
<li>just the model weights, using <code>my_model.save_weights('my_weights.h5')</code></li>
<li>the whole model and its meta data, using <code>my_model.save('my_model.h5')</code></li>
</ol>

<p>In either case, you will need the library <code>h5py</code> (see the documentation)</p>

<p>Option 2 preserves more information from the model. Quoting the documentation, it saves:</p>

<blockquote>
  <ul>
  <li>the architecture of the model, allowing to re-create the model</li>
  <li>the weights of the model</li>
  <li>the training configuration (loss, optimizer)</li>
  <li>the state of the optimizer, allowing to resume training exactly where you left off.</li>
  </ul>
</blockquote>

<p>To continue on the with model where you ended and saved, it is as simple as:</p>

<pre><code>my_model = keras.models.load_model('my_models.h5')
</code></pre>

<p>Now you can train it further or introspect the model and so on.</p>

<h3>Links</h3>

<ul>
<li>Details on the <a href=""https://keras.io/models/model/"" rel=""nofollow noreferrer""><code>Model</code> class</a></li>
<li>Details of <a href=""https://keras.io/layers/recurrent/#gru"" rel=""nofollow noreferrer"">GRU</a></li>
<li>Details of <a href=""https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model"" rel=""nofollow noreferrer"">saving models</a></li>
</ul>
","2","2","45264","13013"
"27361","<p>As you are working on image classification and would also like to implement some data augmentation, you can combine the two AND load the batches directly from a folder using the mighty 'ImageDataGenerator` class.</p>
<p>Have a look at <a href=""https://keras.io/preprocessing/image/"" rel=""noreferrer"">the execellent documentation!</a></p>
<p>I won't copy and paste the example from that link, but I can outline the steps that you go through:</p>
<ol>
<li><p>create the generator class:   <code>data_gen = ImageDataGenerator()</code></p>
</li>
<li><p>If you want it to perform on-the-fly augmentation for you, that can be    specified when creating the class: <code>data_gen = ImageDataGenerator(samplewise_center=True, ...)</code></p>
</li>
<li><p>If you use augmentation processes that require some statistics about the dataset, e.g. <strong>feature_wise</strong> normalisation (not sample-wise), you must prepare the generator by showing it some of your data: <code>data_gen.fit(training_data)</code>. This <code>fit</code> method simply precomputes things like the mean and standard deviation, which is later used for normalisation.</p>
</li>
<li><p>the generator goes into the model's <code>fit_generator</code> method, and we call the <code>flow_from_directory</code> method of the generator:</p>
<p><code>model.fit_generator(training_data=data_gen.flow_from_directory('/path/to/folder/'), ...)</code></p>
</li>
</ol>
<p>You can also create a separate generator using ImageDataGenerator for your validation data, where you should then not apply the augmentation, so that validation tests are done on real data, giving you and accurate picture of the model's performance.</p>
<p>In any case, these generators will theoretically run forever, generating batches from your folder. Therefore I recommend using a callback function from Keras to stop when a certain criteria is met. See the <a href=""https://keras.io/callbacks/#earlystopping"" rel=""noreferrer"">documentation for the EarlyStopping class</a>. You could also do this manually, but Keras makes it very simple!</p>
<p>If you want fine-grained control, you could do all of the above manually, loading enough samples from disk for a single batch, performing some augmentation and then running the <code>model.train_on_batch()</code> method. If you want to get into the details, you might be best first learning the Keras way, then progress to your own detailed models combing Tensorflow as required. The two can be used toegether very nicely!</p>
","8","2","45264","13013"
"27535","<p>LSTM layers require data of a different shape.</p>

<p>From your description, I understand the starting dataset to have 3125 rows and 1000 columns, where each row is one time-step. The target variable should then have 3125 rows and 1 column, where each value can be one of three possible values. So it sounds like you're doing a classification problem. To check this in code, I would do:</p>

<pre><code>&gt;&gt;&gt; X.shape
(3125, 1000)

&gt;&gt;&gt; y.shape
(1000,)
</code></pre>

<p>The LSTM class requires each single sample to consist of a 'block' of time. Let's say you want to have a block of 100 time-steps. This means <code>X[0:100]</code> is a single input sample, which corresponds to the target variable at <code>y[100]</code>. this means your window size (a.k.a number of time-steps or number of lags) is equal to 100. As stated above, you have 3125 samples, so <code>N = 3125</code>. To form the first block, we unfortunately have to discard the first 100 samples of <code>y</code>, as we cannot form an entire block of 100 from the available data (we would end up needing the data points before <code>X[0]</code>).</p>

<p>Given all this, an LSTM requires you to deliver batches of shape <code>(N - window_size, window_size, num_features)</code>, which translates into <code>(3125 - 100, 100, 1000)</code> == <code>(3025, 100, 1000)</code>.</p>

<p>Creating these time-blocks is a bit of a hassle, but create a good function once, then save it  :)</p>

<p>There is more work to be done, perhaps look at more in depth examples of my explanation above <a href=""https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/"" rel=""noreferrer"">here</a>... or have a read of the <a href=""https://keras.io/layers/recurrent/#lstm"" rel=""noreferrer"">LSTM documentation</a>, (or better still, <a href=""https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py#L1906"" rel=""noreferrer"">the source code!</a>).</p>

<p>The final model would then be simple enough (based on your code):</p>

<pre><code>#Create model
model = Sequential()
model.add(LSTM(units=32, activation='relu',
               input_shape=(100, 1000))    # the batch size is neglected!
model.add(Dense(3, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam',
              metrics=['accuracy'])
</code></pre>

<p>Have a look at <a href=""https://keras.io/getting-started/sequential-model-guide/#specifying-the-input-shape"" rel=""noreferrer"">the documentation regarding input shape for the <code>Sequential</code> model</a>. It basically says that we don't need to specify the number of batches within <code>input_shape</code>. It can be done with e.g. <code>batch_size=50</code>, if you require it to be a fixed number.</p>

<p>I know the <code>input_shape</code> argument is not in the documentation for <code>LSTM</code>, but the class itself inherits from <a href=""https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py#L212"" rel=""noreferrer""><code>RNN</code></a>, which in turn inherits from <a href=""https://github.com/keras-team/keras/blob/master/keras/engine/topology.py#L191"" rel=""noreferrer""><code>Layer</code></a> - so it will be able to use the info you provide.</p>

<p><strong>One last tip:</strong> if you plan on adding several LSTM layers ('stacking' them), then you shall need to add one more argument to all but the <em>last</em> <code>LSTM</code>, namely, the <code>return_sequences=True</code>.</p>
","10","2","45264","13013"
"27566","<p>There doesn't appear to be an easy way - there is no mention of an API to get the data programmatically.</p>

<p>You could try contacting them via Twitter or something. Perhaps they would be happy to give you a dump of the data.</p>

<p>Otherwise, you might have to go down the route of writing a scraper e.g. using <a href=""http://www.seleniumhq.org/"" rel=""nofollow noreferrer"">Selenium</a> and <a href=""https://www.crummy.com/software/BeautifulSoup/"" rel=""nofollow noreferrer"">BeautifulSoup</a>. That would allow you to:</p>

<ol>
<li>execute the search</li>
<li>download the file</li>
<li>navigate to the next search results page</li>
<li>repeat from step 2. until finished</li>
</ol>

<p>They might be fairly easy, but do respect the websites wishes - perhaps they have a file called <code>robots.txt</code> which will tell you if they mind you doing that or not. In any case, build some <code>time.sleep()</code>s into your code, as not to bombard their server!</p>
","1","2","45264","13013"
"27572","<p>The input shape for an LSTM must be <code>(num_samples, num_time_steps, num_features)</code>. In your example case, combining both cities as input, <code>num_features</code> will be 2x3=6.</p>

<p>If you lump all your 365 time steps into one sample, then the first dimension will be 1 - one single sample! You can also do sanity check by using the total number of data points. You have 2 cities, each with 365 time-steps and 3 features: 2x365x3= 2190 . This is obviously the same as 1x365x6 (as I said above) - so it would be a possibility (Keras will run) - but it obviously won't learn to generalise at all, only giving it one sample.</p>

<p>Have a look <a href=""https://datascience.stackexchange.com/questions/27533/keras-lstm-with-1d-time-series/27535#27535"">at this relevant question</a>, which I recently answered. There I speak a little about using a <em>rolling window</em> (check the comments of the answer for more info). That will buy you more samples if you need them.</p>

<p>If you want to train a single model with data for both cities as input, then making predictions for both cities at each time-step is as simple as defining a final <code>Dense</code> layer, which outputs 2 units. Your validation/test data must then of course contain a tuple of (city1, city2).</p>

<p>A perhaps more sophisticated way to approach this would be to create datasets on a single-city basis, then train several sub-models on each city individually (say for 5 layers), then <code>Merge</code>/<code>Concatenate</code> them and put several further layers on top. This will mean you are combining the learnt features of each city, which are in turn being combined to a higher level of abstraction. Here is <a href=""https://1.bp.blogspot.com/-7P_ZGBc8s3c/V4TWwNokHpI/AAAAAAAABsk/P0KChOrEYkAEOFmQNE_EMkrjNAnnruluACLcB/s1600/concat.tif"" rel=""noreferrer"">the first image I got from a search engine</a>, which sketches the idea.</p>
","16","2","45264","13013"
"28124","<p>I suggest having a look at <a href=""https://keras.io/preprocessing/image/"" rel=""nofollow noreferrer"">the relevant documentation</a>.</p>
<p>There, it states:</p>
<blockquote>
<p>rotation_range: Int. Degree range for random rotations.</p>
<p>...</p>
<p>horizontal_flip: Boolean. Randomly flip inputs horizontally.</p>
</blockquote>
<p>Saying each of the operations is applied randomly, I would say, means your images will be generated sometime with and sometimes without the augmentation steps, independently from one another.</p>
<p>If that doesn't convince you, here is the <a href=""https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py#L679"" rel=""nofollow noreferrer"">relevant snippet from the source code</a>:</p>
<pre><code>if self.horizontal_flip:
    if np.random.random() &lt; 0.5:
        x = flip_axis(x, img_col_axis)
</code></pre>
<p>The code for the rotation step is a little more involved, but contained within the same class that I liked above.</p>
","1","2","45264","13013"
"30908","This answer goes a little bit in a different direction, but I hope it still answers your question. It uses the idea of a rolling forecast/prediction.

<p>Because you use the word <em>horizon</em>, I will assume you mean that you would like to predict 10 days into the future at a given time step. There are a few ways of doing this. With this kind of time-series problem, it is common to make the assumption that only a certain history will influence the next few time steps (neglecting seasonal effects). </p>

<h3>Example in words:</h3>

<p>So in your case, you might use e.g. the previous 60 days, and predict the next 10.
Taking your 100 rows of data as an example, this means you can actually make <code>(100 - 60 - 9) = 31</code> predictions, each prediction of 10 time steps ahead (we will need these 31 <strong>predictive_blocks</strong> later). From 100 rows we lose the first 60 to fit the first model. Of the remaining 40 rows of data, we can predict 10 steps ahead (rows 61-70), then we shift the whole thing one row further and repeat. The last prediction of 10 future points would be for rows 91-100. After that we cannot predict 10 steps anymore, so we stop - and this is why we have to subtract that extra 9. [There are of course ways to continue making prediction, as to use all the data]</p>

<h3>Example with a thousand words:</h3>

<p>Let me paint the picture; to help explain the idea of a shifting window prediction.</p>

<p>For each train set (e.g. from <code>t=0</code> to <code>t=5</code> in red - train set 1), you want to predict the following H time steps (corresponding to t=6 in orange - test set 1). In this, your horizon is simply one i.e. <code>H=1</code>.</p>

<p><a href=""https://i.stack.imgur.com/Iz3ij.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Iz3ij.png"" alt=""Basic sketch of a rolling out-of-sample forecast""></a></p>

<p>From what I understand, you would like to predict the next 10 days, meaning you need <code>H=10</code>.</p>

<p>In order to try this with your example, I think you will need to make two changes.</p>

<h3>Change #1</h3>

<p>The shape of your train and test sets will need to match the new horizon. Each sample of your model input (the <code>x_train</code> and <code>x_test</code> can stay the same as before. However, each sample in your test set will have to contain the next <code>H=10</code> values of the label, not just a single value.</p>

<p>Here is a rough example of how you might do this:</p>

<pre><code># Define our horizon
H = 10

# Create data split, using values from my example above
window_size = 60
num_pred_blocks = 31    # as computed above

# Loop over the train and test samples to create the sliding window sets
x_train = []
y_train = []
for i in range(num_pred_blocks):
    x_train_block = x_train[i:(i + window_size)]    # 31 blocks of 60 * num-columns
    x_train.append(x_train_block)
    y_train_block = y_train[(i + window_size):(i + window_size + H)]    # 31 blocks of 10 * 1
    y_train.append(y_train_block)
</code></pre>

<p>Because you are doing out-of-sample testing, your predictions are already interesting to look analyse. Once this runs, you can then create the equivalent test datasets with the new data you mentioned.</p>

<p>Without knowing your data too well, I don't know if your should be predicting the y-values of the same row as the input, or of the following row. Additionally, depending on your data, you could be including the past values of <code>y</code> in each of the <code>x_train</code> blocks. In this case you'd simply swap <code>x</code> for the whole table i.e. <code>data[cols]</code>, where <code>new_cols = ['Demand'] + cols</code>.</p>

<h3>Change #2</h3>

<p>You will need to make the model reflect this horizon, by forcing it to output <code>H</code> values.</p>

<p>Here is an example of how to specify the model:</p>

<pre><code># Define our horizon
H = 10

# Create the model using the parameterised horizon
fit1 = Sequential ()
fit1.add(LSTM(output_dim = 4, activation='tanh', input_shape =(4, 1)))
fit1.add(Dense(output_dim=30, activation='sigmoid')
fit1.add(Dense(output_dim=H))    # our horizon is produced!
</code></pre>

<p><strong>Note:</strong>
In your model specification, you don't need to add the final linear <code>Activation</code>, as the preceding Dense layer by default includes a linear activation. See <a href=""https://keras.io/layers/core/#dense"" rel=""nofollow noreferrer"">the excellent documentation here</a>.</p>

<p>This is a big topic and there are many things that you could try out. I agree with the comments on your question, that you will need a lot more data to allow an RNN to make a meaning representation of the model.</p>

<p>If you are not just doing this to learn about LSTMs etc., another practical approach might be to look into simpler time-series models such as an <a href=""https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average"" rel=""nofollow noreferrer"">ARIMA model</a> (do not be intimidated by the complicated name - it is much simpler than an LSTM). Such models can be constructed quite easily with Python, using <a href=""http://www.statsmodels.org/devel/index.html"" rel=""nofollow noreferrer"">the statsmodels package</a>, which has <a href=""http://www.statsmodels.org/devel/generated/statsmodels.tsa.arima_model.ARIMA.html#statsmodels.tsa.arima_model.ARIMA"" rel=""nofollow noreferrer"">a nice implementation</a>.</p>
","6","2","45264","13013"
"30911","<p>Reading the relevant paragrpah, we can gain a little more insight. It seems to come down to the idea of <strong>delayed reward</strong>. By only updating the model (i.e. the <em>agent</em>) every once in a while, we are inherently inducing the agent to make moves which are benficial over longer periods of time.</p>

<p>Here is the paragraphs from <a href=""https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724"" rel=""nofollow noreferrer"">the post</a>:</p>

<blockquote>
  <ul>
  <li>Delayed reward — Keeping the pole in the air as long as possible means moving in ways that will be advantageous for both the present
  and the future. To accomplish this we will adjust the reward value for
  each observation-action pair using a function that weighs actions over
  time.</li>
  </ul>
  
  <p>To take reward over time into account, the form of Policy Gradient we
  used in the previous tutorials will need a few adjustments. The first
  of which is that we now need to update our agent with more than one
  experience at a time. To accomplish this, we will collect experiences
  in a buffer, and then occasionally use them to update the agent all at
  once. These sequences of experience are sometimes referred to as
  rollouts, or experience traces. We can’t just apply these rollouts by
  themselves however, we will need to ensure that the rewards are
  properly adjusted by a discount factor</p>
  
  <p>Intuitively this allows each action to be a little bit responsible for
  not only the immediate reward, but all the rewards that followed. We
  now use this modified reward as an estimation of the advantage in our
  loss equation. With those changes, we are ready to solve CartPole!</p>
</blockquote>

<p>The agent might otherwise want to just do the optimal move to keep the pole upright for the next milllisecond, but with this <strong>buffer</strong>, we are essentially only allowing the agent to consider larger chunks of time.
This will take longer to train, but should in theory (well, the assumption behind these rollouts...) lend itself to creating a more thoughtful agent.</p>

<p>In other contexts, such as a standard CNN used to classify images, this approach would make less sense as there isn't a temporal dimension to the task.</p>

<p>Saving up the errors and making one big update can have several other explanations in general:</p>

<ol>
<li>I can save compute, as we could formulate the graph to then only perform backprop after our buffer is full. For buffer size not close to 1, this would potentially halve the training time (making the gross simplication of an assumption that learning occurs at the same rate)</li>
<li>It may be equivalent to using batches, in that it could help smooth the learning process. If you have a batch size of 1 sample, the error that is measures and used to update weights may be quite erratic. Using a batch has a nice averaging/smoothin effect. Saving up errors in a buffer, one could argue, may have a similar effect.</li>
</ol>

<p>I should add, I haven't personally seen this in practice outside of reinforcement learning.</p>
","1","2","45264","13013"
"30939","<p>I see in the code for the MLPRegressor, that the final activation comes from a general initialisation function in the parent class: <code>BaseMultiLayerPerceptron</code>, and the logic for what you want is shown around <a href=""https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/neural_network/multilayer_perceptron.py#L271"" rel=""nofollow noreferrer"">Line 271</a>.</p>

<pre><code># Output for regression
if not is_classifier(self):
    self.out_activation_ = 'identity'
# Output for multi class
...
</code></pre>

<p>Then during a foward pass this <code>self.out_activation_</code> is called (<a href=""https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/neural_network/multilayer_perceptron.py#L112"" rel=""nofollow noreferrer"">defined here</a>):</p>

<pre><code># For the last layer
output_activation = ACTIVATIONS[self.out_activation_]
activations[i + 1] = output_activation(activations[i + 1])
</code></pre>

<p>That ominous looking variable <code>ACTIVATIONS</code> is simply a dictionary ,with the keywords being the descriptions you can choose as a parameter in your MLP, each mapping an actual function. Here is <a href=""https://github.com/scikit-learn/scikit-learn/blob/a24c8b464d094d2c468a16ea9f8bf8d42d949f84/sklearn/neural_network/_base.py#L98"" rel=""nofollow noreferrer"">the dictionary</a>:</p>

<pre><code>ACTIVATIONS = {'identity': identity, 'tanh': tanh, 'logistic': logistic,
               'relu': relu, 'softmax': softmax}
</code></pre>

<p>With all of this information, you might be able to come up with a few ways of putting in your custom function. Off the top of my head, I can't see a quick way to simply provide a function. You could for example:</p>

<ol>
<li>define your function where all the other activation functions are defined</li>
<li>add it to that <code>ACTIVATIONS</code> dictionary</li>
<li>make <code>self.out_activation_</code> equal to your custom function (or even a new parameter in <code>MLPRegressor</code></li>
<li>cross your fingers it doesn't break something somewhere else</li>
<li>run it and solve the inevitable small adaptations that will be necessary in a few places</li>
</ol>

<p>I'm, afraid I have never looked at the source code of that library before, so cannot give more nuanced advice. Perhaps there is a beautifully elegant way to do it that we have both overlooked.</p>
","2","2","45264","13013"
"30977","<p>I will outline some points about the libraries and point you to some good comparisons that I have read. The GitHub star counts are just another reference point to help compare popularity. While I don't condone simply following the masses, those stars do help you see what a lot of other people think about the frameworks.</p>

<h3>Tensorflow</h3>

<ul>
<li>is very well documented</li>
<li>scales up into production, being able to use many GPUs or <a href=""https://en.wikipedia.org/wiki/Tensor_processing_unit"" rel=""nofollow noreferrer"">Google TPU</a>s</li>
<li>allows flexible creation of DL architectures, using basic building blocks</li>
<li>is backed by Google</li>
<li>has a very large following - just look at how many stars it has received over on GitHub!</li>
</ul>

<p><a href=""https://i.stack.imgur.com/9wM46.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9wM46.png"" alt=""Tensorflow GitHub stars as of April 2018""></a></p>

<p>For reference, the awesome <a href=""http://www.numpy.org/"" rel=""nofollow noreferrer"">NumPy</a> only has <a href=""https://github.com/numpy/numpy"" rel=""nofollow noreferrer"">7059 stars</a>!</p>

<h3>CNTK</h3>

<ul>
<li>is gaining popularity (they started a bit later than Google)</li>
<li>has very efficient implementations for specific use cases, such as the usage of CNNs and LSTMs in the text domain</li>
<li>is nicely linked to other Micosoft products, like their <a href=""https://azure.microsoft.com/en-gb/overview/what-is-azure/"" rel=""nofollow noreferrer"">Azure</a> based toolkits</li>
<li>scales well in production</li>
<li>is backed by Microsoft</li>
<li>has a good following on Github:</li>
</ul>

<p><a href=""https://i.stack.imgur.com/9b4ZM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9b4ZM.png"" alt=""CNTK GitHub stars as of April 2018""></a></p>

<h3>Theano</h3>

<p>While still open source (and so able to be further developed by the community), the team behind Theano <a href=""https://groups.google.com/forum/#!msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ"" rel=""nofollow noreferrer"">announced</a> that they will no longer actively develop it. This means it will now likely fall even further behind other leading frameworks, and new functionality coming from ongoing research are not likely to make it into the library.</p>

<p>We can see that many people admire Theano, but given that it is basically the oldest DL framework, the star count tells a tale:</p>

<p><a href=""https://i.stack.imgur.com/KMALj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KMALj.png"" alt=""enter image description here""></a></p>

<h3>Additional points</h3>

<p>Have a look for some overviews and comparisons of the deep learning frameworks in general to get a better understanding of when you might use one over another. Here are some I found useful: </p>

<ol>
<li><a href=""https://deeplearning4j.org/compare-dl4j-tensorflow-pytorch"" rel=""nofollow noreferrer"">Comparing top deep learning frameworks</a></li>
<li><a href=""https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750"" rel=""nofollow noreferrer"">Battle of the deep learning frameworks</a></li>
<li><a href=""https://agi.io/2018/02/09/survey-machine-learning-frameworks/"" rel=""nofollow noreferrer"">Choosing a machine learning framework</a>   - has a good conclusion</li>
</ol>

<p>I would suggest reading about the difference between static and dynamic computational graphs. Tensorflow e.g. builds and compiles a static graph of your model and then pushes data through. PyTorch on the other hand allows you to create your model dynamically, giving more freedom during the development of new architectures.</p>

<p>A common workflow would be to develop and do research with PyTorch, then try to write the final production code in Tensorflow for deployment.</p>

<p>One last point - you should be aware of the library Keras. This is a wrapper around the base libraries, such as Tensorflow, Theano and CNTK - maybe more in the future). It is highger-level and easier to use that the others, but behind the curtains it is really just using one of the libraries you are asking about. It is just change on flag in a config file to swap between them!</p>

<p>Just for completeness, the Keras GitHub star banner:</p>

<p><a href=""https://i.stack.imgur.com/KAgTZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KAgTZ.png"" alt=""Keras GitHub star count as of April 2018""></a></p>

<p><em>[All gitHub star counts as of April 2018]</em></p>

<hr>

<p><strong><em>I realise some of these points may become outdated and untrue over time. In which case, feel free to leave a comment and I can revise my post.</em></strong></p>
","7","2","45264","13013"
"31045","<p><strong>Logits interpreted to be the unnormalised</strong> (or not-yet normalised) <strong>predictions</strong> (or outputs) <strong>of a model. These can give results, but we don't normally stop with logits, because interpreting their raw values is not easy.</strong></p>

<p>Have a look at <a href=""https://en.wikipedia.org/wiki/Logit"" rel=""noreferrer"">their definition</a> to help understand how <em>logits</em> are produced.</p>

<h3>Let me explain with an example:</h3>

<p>We want to train a model that learns how to classify cats and dogs, using photos that each contain either one cat or one dog. You build a model give it some of the data you have to approximate a mapping between images and predictions. You then give the model some of the unseen photos in order to test its predictive accuracy on new data. As we have a classification problem (we are trying to put each photo into one of two classes), the model will give us two scores for each input image. A score for how likely it believes the image contains a cat, and then a score for its belief that the image contains a dog.</p>

<p>Perhaps for the first new image, you get logit values out of <code>16.917</code> for a cat and then <code>0.772</code> for a dog. Higher means better, or ('more likely'), so you'd say that a cat is the answer. The correct answer is a cat, so the model worked!</p>

<p>For the second image, the model may say the logit values are 1.004 for a cat and 0.709 for a dog. So once again, our model says we the image contains a cat. The correct answer is once again a cat, so the model worked again!</p>

<p>Now we want to compare the two result. One way to do this is to normalise the scores. That is, <strong>we normalise the logits</strong>! Doing this we gain some insight into the confidence of our model.</p>

<p>Let's using <a href=""https://en.wikipedia.org/wiki/Softmax_function#Softmax_Normalization"" rel=""noreferrer"">the softmax</a>, where all results sum to <code>1</code> and so allow us to think of them as probabilities:</p>

<p><span class=""math-container"">$$\sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}} \hspace{20mm}   for \hspace{5mm} j = 1, …, K.$$</span></p>

<p>For the first test image, we get</p>

<p><span class=""math-container"">$$prob(cat) = \frac{exp(16.917)}{exp(16.917) + exp(0.772)} = 0.9999$$</span>
<span class=""math-container"">$$prob(dog) = \frac{exp(0.772)}{exp(16.917) + exp(0.772)} = 0.0001$$</span></p>

<p>If we do the same for the second image, we get the results:</p>

<p><span class=""math-container"">$$prob(cat) = \frac{exp(1.004)}{exp(1.004) + exp(0.709)} = 0.5732$$</span>
<span class=""math-container"">$$prob(dog) = \frac{exp(0.709)}{exp(1.004) + exp(0.709)} = 0.4268$$</span></p>

<p>The model was not really sure about the second image, as it was very close to 50-50 - a guess!</p>

<p>The last part of the quote from your question likely refers to a neural network as the model. The layers of a neural network commonly take input data, multiply that by some parameters (weights) that we want to learn, then apply a <strong>non-linearity</strong> function, which provides the model with the power to learn non-linear relationships. Without this non-linearity, a neural network would simply be a list of linear operations, performed on some input data, which means it would only be able to learn linear relationships. This would be a massive constraint, meaning the model could always be reduced to a basic linear model.
That being said, it is not considered helpful to apply a non-linearity to the logit outputs of a model, as you are generally going to be cutting out some information, right before a final prediction is made. Have a look for <a href=""https://stats.stackexchange.com/questions/163695/non-linearity-before-final-softmax-layer-in-a-convolutional-neural-network"">related comments in this thread</a>.</p>
","39","2","45264","13013"
"31106","<p><em>[NOTE: I have not myself worked through Aurélien Geron's tutorials, but I have read the book]</em></p>

<hr>

<p>On an intuitive level, I can persuade myself that the training would actually be slower for a pretrained model. In other words, it could make sense that the <strong>rate</strong> at which a error decreases (or accuracy increases) might be lower. The fact that training accuracy is lower is (for me at least) a little more complex and, perhaps, case specific.</p>

<h3>Rate of learning</h3>

<blockquote>
  <p>However the pretraining seems to make training slower.</p>
</blockquote>

<p>Using a pretrained model, we have essentially taken a set of weights, which are already (at least partially) optimised for one problem. They are geared towards solving that problem based on the dataset they received, which means they expect the input to correspond to a certain distribution. You have frozen the first two layers with this line:</p>

<p><code>if pretraining:
    training_op = optimizer.minimize(loss, var_list=[weights3, biases3])</code></p>

<p>Freezing two layers (in your case, out of three), intuitively kind of restricts the model.</p>

<p>Here is a somewhat contrived analogy that I might use to explain such cases to myself. Imagine we had a clown who could juggle with three balls, but now we want them to learn to use a fourth ball. At the same time, we ask an amateur to learn how to juggle, also with four balls. Before measuring their rate of learning, we decide to tie one of the clown's hands behind their back. So the clown already knows some tricks, but is also constrained in some way during the learning process. In my mind, the amateur would most likely learn a lot faster (relatively), as there is more to learn -  but also because they have more freedom to explore the parameter space i.e. they can move more freely using both arms.</p>

<p>In the setting of optimisation, one might imagine that position of the pretrained model on a loss curve is already in a place where gradients are very small in certain dimensions (don't forget, we have a high-dimensional search space). This ends up meaning that it cannot as quickly make changes to the output of the weights whilst backpropagating errors, as the weight updates are multiples of these potentially small optimised weights.</p>

<p>...<em>Ok - might sounds plausible, but this only addresses the problem of slow learning - what about the fact that the actual training accuracy is lower that that of the model with random initialisation??</em></p>

<h3>Intial training accuracy</h3>

<blockquote>
  <p>I expected the pretrained network would start with a lower error (compared to the network not using pretraining)...</p>
</blockquote>

<p>Here I tend to agree with you. In the optimal case, we could take a pretrained model, use the initial layers as they are and just fine-tune the final layers. There are, however, some cases in which this might not work.</p>

<p>Looking into related literature, there is a possible explanation from the abstract of the paper: <a href=""https://arxiv.org/abs/1411.1792"" rel=""nofollow noreferrer"">How transferable are features in deep neural networks? (Yosinski et al.)</a>:</p>

<blockquote>
  <p>Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected.</p>
</blockquote>

<p>I find the second reason to be particularly interesting and relevant to your setup. This is because you actually only have three layers. You are therefore not allowing must freedom to fine-tune, and the final layer was likely very dependent on its relationship to the preceding layer.</p>

<p>What you might expect to see as a result of using a pretrained model, is rather that the final model exhibits better generalisation. This may indeed come at the cost of a lower test accuracy on the hold-out set of the specific dataset you train on.</p>

<p>Here are <a href=""http://cs231n.github.io/transfer-learning/#tf"" rel=""nofollow noreferrer"">another thoughts</a>, summarised well by the amazing (and free) <a href=""http://cs231n.stanford.edu/"" rel=""nofollow noreferrer"">Stanford CS231n</a> course:</p>

<blockquote>
  <p><em>Learning rates</em>. It’s common to use a smaller learning rate for ConvNet weights that are being fine-tuned, in comparison to the (randomly-initialized) weights for the new linear classifier that computes the class scores of your new dataset.</p>
</blockquote>

<p>In your code, the learning rate seems to be fixed for all learning phases at <code>0.01</code>. This is something you could experiment with; making it smaller for the pretrained layers, or just starting with a lower learning rate globally.</p>

<p>Here is a <a href=""https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/"" rel=""nofollow noreferrer"">comprehensive introduction to tranfer learning</a> that might give you some more ideas about why/where you might make some different modelling decisions.</p>
","1","2","45264","13013"
"31115","<p>From simple inspection of your plot, I could make a few conclusions and list things to try. (This is without knowing any more about your setup: training parameters and model hyperparameters).</p>

<p>It looks like the loss is decreasing (put a line of best fit through the validation loss). It also looks like you might be able to train for longer to improve results, as the curve is still headed downwards.</p>

<h3>First I will try answer your title question:</h3>

<blockquote>
  <p>what is the cause of the fluctation in the validation loss?</p>
</blockquote>

<p>I can think of three possibilities:</p>

<ol>
<li><strong>Regularisation</strong> - to help smoothing the learning process and make the model weights more robust. Adding/increasing your regularisation will prevent large updates to weights being introduced.</li>
<li><strong>Batch size</strong> - is it relatively small (e.g. &lt; 20?). This would mean that the measured mean error at the end of the network is computed using only a few samples. With a batch size of, say <code>8</code>, then getting <code>4/8</code> correct and compared to getting <code>6/8</code> correct has a large <em>relative</em> difference when looking at the loss. Taking the mean of the errors with such small batches will lead to a not-so-smooth loss curve. If you have enough GPU memory/RAM, try increasing batch size.</li>
<li><strong>Learning Rate</strong> - might be too large. This is similar to the first point regarding regularisation. To make smoother improvements, you might need to slow down the pace of learning as you approach a loss-minimum. You can make this perhaps run on a schedule, whereby is is reduce by some factor (e.g. multiply it by 0.5) every time the validation loss has not improved after, say <code>6</code> epochs. This will prevent you from taking big steps and then maybe overshooting a minumum and just bouncing around it.</li>
</ol>

<p>Specific to your task, I would also suggest trying to perhaps <strong><em>unfreeze</em> another layer</strong>, to increase the scope of your fine-tuning. This will give the Resnet-18 a little more freedom to learn, based on your data.</p>

<h3>Regarding your last question:</h3>

<blockquote>
  <p>Is this something that I should be worried about, or should I just pick the model which scores the best on my performance measure (accuracy)?</p>
</blockquote>

<p>Should you be worried? In short, no. A validation loss curve like yours can be perfectly fine and deliver reasonable results; however, I would try some of the steps I mentioned above before settling for it.</p>

<p>Should you just pick the best performing model? If you mean taking the model at its point with best validation loss (validation accuracy), then I would say to be more careful. On your plot above, this might equate to around epoch 30, but I would personally take a point that has trained a little more, where the curve gets a little less volatile. Again, after having tried some of the steps outlined above.</p>
","4","2","45264","13013"
"31124","<p>I would first suggest trying to plot the results during training. How do your metrics (or at least the loss) vary over the training process for training and cross-validation datasets?
The loss at each iteration is appended to your GBC object <code>gb_model</code> in the <code>train_score_</code> attribute.</p>

<p>Normally, when there is such a big gap between training and test data, it indicates that you are overfitting to your training data, and that the model does not generalise well to unseen data. You could think about doing  shuffling your data, in order to balance the training/validation/test datasets - [if you are looking at a time-series problem, you should be careful as to how you do this].</p>
","0","2","45264","13013"
"31183","<h3>Thought #1</h3>

<p>From the short description you give, and assuming you have some data (or can synthesise it), I would have a go at training a Hidden Markov Model. Have a look here for <a href=""http://setosa.io/ev/markov-chains/"" rel=""nofollow noreferrer"">an awesome visual primer</a>. Intuitively, HMMs do what you describe.</p>

<p>There are some observables:</p>

<ol>
<li>how much candy you put in the machine,</li>
<li>how much comes out, and</li>
<li>which timestep we are in</li>
</ol>

<p>There are certain states (here just discrete states):</p>

<ol>
<li>amount of dispensed candy goes up (strictly increasing)</li>
<li>amount of dispensed candy goes down (strictly decreasing)</li>
</ol>

<p>Lastly, there is a <em>transition</em>: between these state, given the inputs. These are the observables, as far as you are concerned.</p>

<p>With HMMs, we are however assuming that there is more than meets the eye; there are some hiddens states (latent states), which are unobservable to use. The model keeps track of these and uses them in conjunction with the observables list above to decide how to act - it has the full mapping function. Below is a schematic diagram of such a model:</p>

<p><a href=""https://i.stack.imgur.com/ydQE8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ydQE8.png"" alt=""enter image description here""></a></p>

<p>This just shows several states, along with the probabilities of transitioning to the other states, or stating in the same state. Check out <a href=""http://www.blackarbs.com/blog/introduction-hidden-markov-models-python-networkx-sklearn/2/9/2017"" rel=""nofollow noreferrer"">the source</a> for a detailed description of the diagram. One can already imagine sketching this drawing for your problem.</p>

<p>For more on the theory of HMMs, I recommend this <a href=""http://melodi.ee.washington.edu/people/bilmes/mypapers/em.pdf"" rel=""nofollow noreferrer"">text based walkthrough by Jeff Bilmes</a>, and <a href=""https://www.youtube.com/watch?v=7KGdE2AK_MQ&amp;list=PLD0F06AA0D2E8FFBA&amp;index=94"" rel=""nofollow noreferrer"">this YouTube series by <em>mathematicalmonk</em> (chapter 14 of the playlist)</a>. If you go this route, you might consider trying out <a href=""http://pomegranate.readthedocs.io/en/latest/index.html"" rel=""nofollow noreferrer"">the Pomegranate library (in Python)</a>.</p>

<p>The final logic of deciding what to do, what action to take, is something you could either hard-code or use a model to take it for you. For example, a hard-coded approach would be something like:</p>

<pre><code>if current_state == 'normal':
    take_lots_of_candy()
elif current_state == 'unsure':
    take_some_candy()
elif current_State == 'dangerous':
    pause_taking_candy()
else:
    catch_other_states()
</code></pre>

<p>For a model-based approach, you could expand further on the ideas outlines below.</p>

<hr>

<h3>Thought #2</h3>

<p>Of course, you are in essence just mapping some input to some outputs. Almost any machine learning algorithm will have a good go at it and - depending on the complexity of the internal workings of your magical machine - will be better or worse. Algorithms such as boosting (e.g. via gradient descent) generally require you to put some kind of information into the model e.g. by specifying a regression equation, mapping inputs to outputs. A good implementation is the <a href=""https://cran.r-project.org/web/packages/mboost/index.html"" rel=""nofollow noreferrer""><code>mboost</code> package</a> in <strong>R</strong>. There is <a href=""https://cran.r-project.org/web/packages/mboost/vignettes/mboost_tutorial.pdf"" rel=""nofollow noreferrer"">a great tutorial</a> to help decide what might be important.</p>

<hr>

<h3>Thought #3</h3>

<p>Coming back to the point of just mapping inputs to outputs: a simple <a href=""http://scikit-learn.org/stable/modules/neural_networks_supervised.html"" rel=""nofollow noreferrer"">Multi-Layer Perceptron (MLP)</a> from Scikit-Learn - a.k.a. feed-forward neural network - should theoretically offer the flexibility to approximate any function you might throw at it (with a few assumptions).
You could also implement this using one of the various deep learning frameworks, should your MLP just not <a href=""https://www.phrases.org.uk/meanings/cut-the-mustard.html"" rel=""nofollow noreferrer"">cut the mustard</a>.</p>

<hr>

<h3>Final thoughts</h3>

<p>I would lastly like to highlight that your final model will only likely be as good as the data you feed it, and that while it may work well on your synthesised training data, there are no guarantees of generalisation, without putting more thought into the specific problem and specific distributions from which your data are sampled.</p>

<p>If you go the way of Markov models, you could perhaps then use their state predictions as inputs to further models; thus creating an ensemble.</p>

<p>If you want to get really fancy and more towards current state-of-the-art models in sequence analysis, you could venture into the world of Recurrent Neural Networks, e.g. utilising LSTM cells, which maintain an internal state representation, including new information and selectively discarding old information. This might be a good way to take all your points into consideration in one large model. The catch here would be that you generally require vast amounts of data to train such a model.</p>
","2","2","45264","13013"
"31189","<p>Having problems with dimensions is very common. As others have mentioned, the method <code>predict</code> expects to get a batch of images. You should run <code>model.summary()</code> to see what the expected dimensions of the input are. The batch size itself might have been designed to be flexible during training, by specifying <code>None</code> in the first dimension on the model <code>input_shape</code> parameter.</p>

<p>Without knowing more of the dimensions, I cannot say exactly where your problems lies. You could include that information by editing your question.</p>

<hr>

<p>It will likely help to have a look at the <a href=""https://keras.io/models/sequential/#predict"" rel=""nofollow noreferrer"">documentation</a> for predict function of the model objects.</p>

<p>There is an argument: <code>batch_size</code>, which defaults to <strong>32</strong> if not fixed by the model itself, which you can see from the <code>model.summary()</code>. If you set this equal to 1, perhaps you will get a prediction.</p>

<p>Below is a modified version of your code that I would expect to return a prediction.</p>

<pre><code>from keras.models import load_model
from keras.preprocessing import image
import numpy as np

# dimensions of our images    -----   are these then grayscale (black and white)?
img_width, img_height = 313, 220

# load the model we saved
model = load_model('model.h5')

# Get test image ready
test_image = image.load_img('/Images/1.jpg', target_size=(img_width, img_height))
test_image = image.img_to_array(test_image)
test_image = np.expand_dims(test_image, axis=0)

test_image = test_image.reshape(img_width, img_height*3)    # Ambiguity!
# Should this instead be: test_image.reshape(img_width, img_height, 3) ??

result = model.predict(test_image, batch_size=1)
print result
</code></pre>

<p><strong>Note</strong>: there is ambiguity in the dimensions where I highlight it. A colour image will have three dimensions: (height, width, 3). Black and white will have only two dimensions: (height, width).</p>
","2","2","45264","13013"
"31192","<p>Just a couple of ideas:</p>

<ol>
<li><p><strong>Batch size</strong>: 8 is quite a small batch, meaning the average loss that is computed might have high volatility. If you have enough memory, you could increase this.</p></li>
<li><p><strong>Diversity of input</strong>: try adding batch normalisation layers in the encoder part, to try smoother the input for the conv layers. You said there are quite a few features, so perhaps this makes for <em>noisy</em> input, which would benefit from being normalised.</p></li>
</ol>

<p>You could trying running the same experiment for 15 epochs, then plotting the training and validation losses (as they evolve perhaps, using the <a href=""https://keras.io/callbacks/#tensorboard"" rel=""nofollow noreferrer"">TensorBoard callback</a> alongside your others). Do they follow any patterns or converge after some time?</p>

<p>You could try using different initilisation methods, or even gradient clipping, in order to make training a little smoother - constraining the size of the updates to weights during backpropagation.</p>

<p>Finally, another <a href=""https://arxiv.org/abs/1710.10196"" rel=""nofollow noreferrer"">(brand-new!) result from research</a> into GAN models, shows that progressively increasing the size of the inputs to your models might help to smooth learning and also extract a more robust set of features, which generalise better. Have a read of <a href=""http://www.fast.ai/2018/04/30/dawnbench-fastai/#imagenet"" rel=""nofollow noreferrer"">this section of an article</a> from FastAI on their experience.</p>

<h3>EDIT: information regarding the ELU activation</h3>

<p>Using this activation my help learning, as it has a little more <em>flexibility</em> than e.g. the ELU, because it may also assume negative values. Here is an image from <a href=""https://arxiv.org/pdf/1511.07289v1.pdf"" rel=""nofollow noreferrer"">the original paper</a>:</p>

<p><a href=""https://i.stack.imgur.com/mpaGf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mpaGf.png"" alt=""enter image description here""></a></p>

<p>Here is the official definition (might slightly differ in the implementation of your framework):</p>

<p><a href=""https://i.stack.imgur.com/UKZZr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UKZZr.png"" alt=""enter image description here""></a></p>

<p>The authors mentions that this activation assists in pushing the mean activation closer to zero, just as batch normalisation does. In your case this might mean simply getting past the bumpy initial epochs and converging a little more quickly.</p>

<p>The final major point that the authors highlight is that the models they trained also generalised better - so you might enjoy an improved performance with your model using ELUs versus a model using ReLUs (assuming both are trained for similar time).</p>
","0","2","45264","13013"
"31213","<p>I think the confusion with the Inception module is the somewhat complicated structure. The point on the relevant <a href=""http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf"" rel=""nofollow noreferrer"">CS231n slide (#37)</a>, saying there are no FC layers is partially correct. (Remember this is only a summary of the model to get the main points across!).  In the actual part of the model being explained on that slide, they are referring only to the Inception modules:</p>

<p><a href=""https://i.stack.imgur.com/BVDcs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BVDcs.png"" alt=""Inception module""></a></p>

<p><strong>No FC layers!</strong></p>

<p>Definitions will, however, play a big role in deciding whether or not there are FC layers in the model.</p>

<hr>

<p>In the bigger scheme of things (beyond a single Inception module), we have first to distinguish between the train and test time architectures. </p>

<p>At train time there are auxilliary branches, which do indeed have a few fully connected layers. These are used to force intermediate layers (or inception modules) to be more aggressive in their quest for a final answer, or in the words of the authors, to be more <em>discriminate</em>.</p>

<p>From the paper (page 6 <a href=""https://arxiv.org/pdf/1409.4842.pdf"" rel=""nofollow noreferrer"">[Szegedy et al., 2014]</a>):</p>

<blockquote>
  <p>One interesting insight is that the strong performance
  of relatively shallower networks on this task suggests that the features produced by the layers in the
  middle of the network should be very discriminative. By adding auxiliary classifiers connected to
  these intermediate layers, we would expect to encourage discrimination in the lower stages in the
  classifier, increase the gradient signal that gets propagated back, and provide additional regularization.</p>
</blockquote>

<p>The slice of the model shown below displays one of the auxilliary classifiers (branches) on the right of the inception module:</p>

<p><a href=""https://i.stack.imgur.com/G5ItL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G5ItL.png"" alt=""Inception block with auxilliary classifier""></a></p>

<p>This branch clearly has a few FC layers, the first of which is likely followed by a non-linearity such as a <code>ReLU</code> or <code>tanh</code>. The second one simply squishes the 1000 input weights into whatever number of classes are to be predicted (coincidentally or not, this is a 1000 here for ImageNet).</p>

<p>However, at test time, these branches are not active. They were used simply to train the weights of the modules, but do not contribute to the final classification probabilities produces at the end of the entire model architecture.</p>

<hr>

<p>This all leaves us with just the suspsicious looking block right at the end of the model:</p>

<p><a href=""https://i.stack.imgur.com/IzOkd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IzOkd.png"" alt=""Final layers after last Inception module, before class probabilities are produced.""></a></p>

<p>There is clearly a big blue FC layer there!</p>

<p>This is where definitions come into play. It is somewhat subjective.
Is a fully connected layer one in which each $m$ weight is connected to each of $n$ nodes? Is it a layer in which representation are learned, and if so, does the layer require a non-linearity?  We know that neural networks requires the non-linearities, such as ReLU and tanh functions to be applied to the outputs of a layer (thinking in forward flow). Without these, neural networks would simply be a combinations of linear functions, and so going deeper wouldn't theoretically add any power as we essentially would just be performing a huge linear regression.</p>

<p>In this spirit, we can look at the final piece of the puzzle, and point out that tis final FC layer is noted to simply be linear! That is, it takes all the weights resulting from the preceding <code>Average Pooling</code> layer, and combines them into a <em>linear combination</em> of only 1000 values - ready for the softmax. This can all be understood from the tabular overview of the network architecture:</p>

<p><a href=""https://i.stack.imgur.com/FukvF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FukvF.png"" alt=""Start and end sections of the tabular model overview""></a></p>

<p>So, do you agree with the stanford guys or not? I do!</p>
","1","2","45264","13013"
"31215","<p>If you plug this kind of data into a standard network, e.g. an MLP, you will usually hope that the model actually extracts this information itself. You could introduce a dummy variable that encodes this information, but you run the risk that the model learns to just follow the dummy variable and doesn't learn its own powerful abstractions and feastures from the data.</p>

<p><strong>[EDITED: ]</strong> An example could be to create a weight based variable that is normalised to other physical characteristics, such as <code>DV = weight / (height + waist circumfrence)</code>. This should then, according to your assumptions, scale nicely with the output obesity.</p>

<hr>

<p>Another way that people might include <em>prior</em> information into a model, is to use a Probabilistic modelling, which incorporates ideas from Bayesian statistics. You can do things such as define a prior distribution for your outputs, conditioned on your inputs - essentially allowing you to provide information to the model (such as weight being correlated with obesity, <em>ceteris paribus</em>) - this then nudges the model to go along these lines.</p>

<p>If you would like to get into it, there are already some great libraries to make it really easy:</p>

<ol>
<li><a href=""http://mc-stan.org/users/interfaces/index.html"" rel=""nofollow noreferrer"">Stan</a> - which has interfaces to many languages: Python, R, State etc.</li>
<li><a href=""https://github.com/blei-lab/edward"" rel=""nofollow noreferrer"">Edward</a> - probabilistic programming with modern GPU accelration and deep learning integration (Tensorflow and Keras).

<ul>
<li><a href=""http://edwardlib.org/tutorials/supervised-regression"" rel=""nofollow noreferrer"">A Linear regression example</a></li>
<li><a href=""http://edwardlib.org/tutorials/bayesian-neural-network"" rel=""nofollow noreferrer"">A Bayesian Neural Network examples</a></li>
</ul></li>
</ol>

<p><a href=""https://arxiv.org/pdf/1801.07710.pdf"" rel=""nofollow noreferrer"">This</a> seems to be a nice overview of some of the methods and tools, but I haven't read through it all.</p>

<hr>

<p>Another way that I could suggest is to play with the architecture of your model and the idea of <strong>auxilliary models</strong>. Have a look at <a href=""https://datascience.stackexchange.com/a/31213/45264"">my recent answer</a> on a question talking about the Inception model from <a href=""https://arxiv.org/pdf/1409.4842.pdf"" rel=""nofollow noreferrer"">Szegedy et al.</a>.</p>

<p>The idea is that you have branches coming off the model at train time only, which also make predictions and produce error to be backpropagated through the preceding weights.</p>

<p>You could make a side model that predicts the obesity, based on e.g. the input weights and perhaps some related features extracted from the first layer of your neural network. This would make the idea or importance of this relationship more prominent during training and the weights would subsequently be tuned to take that into account.</p>

<p>At test time, you simply ignore these auxilliary branches.</p>
","1","2","45264","13013"
"31334","<p>Whether or not padding is ppropriate really depends on the entire structure of your dataset, how relevant the different variables/columns are and also the type of model you want to run at the end.</p>

<p>Padding would be used, whereby you would have to fix the length of each sample (either to the length of the longest sample, or to a fixed length - longer samples would be trimmed or filtered somehow to fit into that length). Variables that are strings can be padded with  empty strings, variables with number can be padded with zeros. There are however many other ways to pad, e.g. using the average of a numerical variable, or even model-based padding, padding with values that ""make most sense"" for filling gaps in that specific sample. Getting deep into it like that might more generally be called imputation, instead of padding - it is common in time series data, where gaps aren't always at one end of a sample.</p>

<p>Below I outline one approach to padding or standardizing the length of each sample. It it not specifically padding.</p>

<p>As you did not mention a programming language, I will give and code snippet in <strong>Python</strong>, but the same is easily achievable in other languages such as <strong>R</strong> and <strong>Julia</strong>.</p>

<h3>The Approach</h3>

<p>Based on the two examples you provide, it seems each example would be a calendar day, on which there are a variable number of observations.</p>

<p>There are also columns that are strings, and others are strings of numbers (e.g. column 5 in sample 2).</p>

<p>In time-series analysis in general, it is desirable to have a continuous frequency of data. That means have one day give one input. So my approach would be to make your data into a form that resembles a single input for each of the variables (i.e. columns) of each sample.</p>

<p>This is general approach, and you will have to try things out or do more research on how this would look in reality for your specific data at large.</p>

<h3>Timestamps</h3>

<p>I would use these as a kind of index, like the index in a <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html"" rel=""nofollow noreferrer"">Pandas DataFrame</a>. One row = one timestamp. Multiple variables are then different columns.</p>

<h3>Dealing with strings</h3>

<p>I will assume that your dataset has a finite number of possible strings in each column. For example, that column 4 (holding <em>names</em>), will always hold a name from a given set. One could perform <code>set(table2['column 4'])</code> to see which values there are (removing duplicates). Or even:</p>

<pre><code># Gather a single list containing all strings in column 4 of all samples
all_names = []
[]    # list comprehension to loop

# Check how many strings there are
from collections import Counter
counter = Counter(table2['column4'])
print(len(counter))                 # see how many unique values exist
print(counter)                      # see how many times each string appears
print(counter.most_common(5))       # see the most common 5 strings
</code></pre>

<p>Now assuming this shows there is a finite number (more than likely the case), You could look into using <a href=""https://machinelearningmastery.com/sparse-matrices-for-machine-learning/"" rel=""nofollow noreferrer"">a sparse representation</a> of each sample (that means for each day). For example, if all the words in the entire dataset were: <code>['hi', 'hello', 'wasup', 'yo', 'bonjour']</code> (duplicates removed), then for one single sample with column 4 holding e.g. <code>['hi', 'hello', 'yo', 'hi']</code>, your sparse representation for this sample would be: <code>[2, 1, 0, 1, 0]</code>, because the sample has two 'hi', one 'hello', zero 'wasup' and so on. This sparse representation would then be your single input for column 4 for the timestamp (that single sample).</p>

<p>It might be worth looking into something like the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html"" rel=""nofollow noreferrer"">DictVectorizer</a> and <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"" rel=""nofollow noreferrer"">CountVectorizer</a> from Scikit-Learn.</p>

<h3>Dealing with number columns</h3>

<p>As I mentioned right at the beginning, you could pad these to a chosen length, perhaps matching the length of the string based representation above (or not!), depending on your final model.
You can then pad the inputs with a value that makes sense for your model (see the kind of options I mentioned at the beginning of my answer).
This should land you with, once again, a single vector for the given day, containing the information in the numerical column.</p>
","1","2","45264","13013"
"31417","<p>This sounds a lot like a linear optimisation problem: <em>given certain resources and constraints, maximise profit</em>. If you are new to this, check out some of these resources:</p>

<ul>
<li><a href=""https://www.youtube.com/watch?v=o9YRFziAXUA"" rel=""nofollow noreferrer"">walkthrough of an example</a></li>
<li><a href=""https://www.rand.org/pubs/reports/R366.html"" rel=""nofollow noreferrer"">in depth, this book</a></li>
<li><a href=""https://www.analyticsvidhya.com/blog/2016/09/a-beginners-guide-to-shelf-space-optimization-using-linear-programming/"" rel=""nofollow noreferrer"">practical example using Python #1</a> - and <a href=""https://www.analyticsvidhya.com/blog/2017/10/linear-optimization-in-python/"" rel=""nofollow noreferrer"">here #2</a></li>
</ul>

<p>Machine learning is generally an approach to model and understand data, hopefully in such a way as to allow us to make predictions in the future, given some data that is representational of past observations. It more often than not considers a dynamic world and non-linear functions: given the state of the world, what should the ticket price be? Practically speaking, selling tickets doesn't work that way - it would be very confusing for customers if ticket prices always change! One more point that would concern me with your specific problem, is that your variables (ticket price, frequency of trains etc.) are not mutually exclusive from the number of tickets sold. If you just make each ticket cost $1000 dollars each, profit would be zero, because nobody would by a ticket. What I want to say is: we're not classifying cats and dogs here  :-)</p>

<p>With your problem and using machine learning, I would be more inclined to try answering questions such as:</p>

<ol>
<li>How many passengers can we expect next Saturday?</li>
<li>Which are peak travel times in a certain region?</li>
<li>which factors should we improve to increase customer feedback scores?</li>
</ol>

<p>Maybe you can try looking at one of these approached to help in the optimisation, i.e to help find your linear constraints.</p>

<hr>

<p>One can of course attempt to frame it as a pure <strong>machine learning model</strong>, and there are many ways of doing this; it is optimisation, after all. I think the first step, whether using some machine learning algorithm or a linear optimisation construction, is to understand your data, the effects of each factor and their relationship to your target variable to be maximised/minimised (profit/loss, respectively).</p>

<p>So, as I was told back when learning about this myself: ""The first step is to translate words into linear inequalities"". So you could think about the factors you have and understand which ones effect price. If there are constraints/limitations, these should also be considered. For example, if you know that you are not allowed to sell more than 10,000 tickets on one day due to capacity constraints of your system.
There are some interesting points made in <a href=""https://stats.stackexchange.com/questions/326970/function-maximisation-using-non-linear-regression"">this thread over on Cross-Validated</a>.</p>

<p>Now you understand the data a little better, you could think about which models to try out. Without knowing more about your data, I can't really offer more guidance as to which models might be worth trying, but hopefully this answer helps you along that path.</p>
","5","2","45264","13013"
"31445","<p>As was pointed out by <strong>@Stephan Rauch</strong> in his comment, the names of the columns are stored in <code>dataframe.columns</code> - the OP had a typo.</p>

<p>Below is a working example with dummy data, getting the same output as the user - using instead a little loop to compute the shifted values.</p>

<pre><code>from pandas import DataFrame

prices = dict(
    col1=[0, 1, 2, 3, 4, 5, 6],
    col2=[2, 3, 4, 5, 6, 7, 8],
    col3=[5, 6, 7, 8, 9, 10, 11],
    col4=[12, 13, 14, 15, 16, 17, 18])

dataframe = DataFrame.from_dict(prices)
print(dataframe)
new_col_names = ['t-2', 't-1', 't', 't+1']
dataframe.columns = new_col_names
print(dataframe)

# Number of columns we have
N = len(dataframe.columns)

for n, col in enumerate(dataframe.columns):
    shift_by = N - n - 1  # don't shift the final column
    dataframe[col] = dataframe[col].shift(periods=shift_by, axis=0)

print(dataframe)

# If desired, remove the new NaNs that appear in the first
final_dataframe = dataframe.drop(labels=dataframe.index[:N - 1], axis='index')
print(final_dataframe)
</code></pre>
","1","2","45264","13013"
"31462","<p>You can do this a few ways, which I can list in ascending order of effort:</p>

<ol>
<li><p>Pick a value that seems ok for you and your dataset by eye-balling it then simply cut variables below the theshold from the dataset</p></li>
<li><p>Create a function, which given a threshold, tells you how many variables would be removed, if you used that threshold. Then create a simple plot and see if there is a certain level that seems appealing (this depends on your target model once data is ready).</p></li>
<li><p>Use some smarter functions that d a little more for you, e.g. <a href=""http://topepo.github.io/caret/pre-processing.html#zero--and-near-zero-variance-predictors"" rel=""nofollow noreferrer"">the NearZeroVar function in the Caret package in R</a></p></li>
</ol>

<p>There are, however, some arguments as to whether these approaches in general are optimal. Have a look at some of the discussion on <a href=""https://stats.stackexchange.com/questions/145602/justification-for-feature-selection-by-removing-predictors-with-near-zero-varian"">this thread over at Cross-Validated</a>. The quote from the OP in that thread if from the book of the guy who wrote the above mentioned Caret package - Max Kuhn.</p>

<p>The arguments against this approach say that you may be moving variables that, although they have low variance, might actually be extremely powerful in explaining your target (dependent) variable.</p>

<p>A final approach I can suggest goes into the realm of <strong>covariance</strong>, that is to look at <a href=""https://en.wikipedia.org/wiki/Collinearity"" rel=""nofollow noreferrer"">the <em>collinearity</em></a> between pairs of variables. I have done this in the past and it worked out well for me. The basic algorithm would look something like this:</p>

<ol>
<li>Compute the covariance matrix across all your variables</li>
<li>Find the pair with the highest covariance

<ul>
<li>correlation if you prefer that measure</li>
<li>the plan is to remove one of these two variables</li>
</ul></li>
<li>using your covariance matrix from step 1, compute which of these two variables in step 2 has the highest sum of covariances with the rest of the variables</li>
<li>remove the variable with the highest sum of covariances</li>
<li>Repeat steps 1-4 until you arrive at your desired number of variables, or a threshold is reached in terms of covariances or individual variable variances.</li>
</ol>

<h3>EDIT:</h3>

<p>Here <a href=""http://scikit-learn.org/stable/modules/feature_selection.html"" rel=""nofollow noreferrer"">is the Scikit-Learn class</a> which can do basic variance thresholding for you - there is also <a href=""http://scikit-learn.org/stable/modules/feature_selection.html#variance-threshold"" rel=""nofollow noreferrer"">a short tutorial</a>. They also present some ways to do recursive feature selection, similar in nature to my final approach outlined above.</p>
","2","2","45264","13013"
"31896","<p>Do you have a link to the actual source you are referring to? I haven't myself seen it before in the context of association rules and interestingness.</p>

<hr>

<p>As was pointed out by <em>@Sean Owen</em> in his comment, it is one of a few standard notations for saying something is not in a <strong>set</strong> - i.e. the complement. This would make most sense, as we are generally referring to <strong>itemsets</strong> when computing the associations.</p>

<p>Regarding the complement and set notation, <a href=""https://en.wikipedia.org/wiki/Complement_(set_theory)"" rel=""nofollow noreferrer"">the Wikipedia article contains this</a>:</p>

<blockquote>
  <p>The absolute complement of A is usually denoted by $${\displaystyle  A^{\complement }}$$ </p>
  
  <p>Other notations include $$ {\displaystyle A^{\text{c}}}, \hspace{3mm} \overline A, \hspace{3mm} A',\hspace{3mm} \complement _{U}A \hspace{3mm} and \hspace{3mm} \complement A$$</p>
</blockquote>

<p>Of course a <em>bar</em> can also mean things such as the mean or expectation value of some values (I have even seen people using it to mean the unobserved or predicted value!), but with the information you provide, the complement seems most likely.</p>

<p>Here also from the same Wiki page linked above for those who are unsure, a visual definition of the complement:</p>

<p><a href=""https://i.stack.imgur.com/UzUPV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UzUPV.png"" alt=""enter image description here""></a></p>
","1","2","45264","13013"
"31897","<p><em>Immediate</em> often occurs simply to say that there are perhaps many supersets, e.g. when we have a tree like structure. There is also for example, the converse, an <em>immediate subset</em>.</p>

<hr>

<h3>Example</h3>

<p>Set <strong>A</strong> has a superset <strong>B</strong>, and that superset has its own superset <strong>C</strong>. In this case, <strong>A</strong> must also be contained within <strong>C</strong>, but the set <strong>B</strong> lies within that realm and so is the immedaite superset of <strong>A</strong>.</p>

<p>Perhaps it is best visualised with a Venn diagram, where <strong>B</strong> is the immediate superset of <strong>A</strong> and equally the immediate subset of <strong>C</strong>:</p>

<p><a href=""https://i.stack.imgur.com/Ke0F8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ke0F8.png"" alt=""enter image description here""></a></p>

<hr>

<p>There is some discussion <a href=""https://stats.stackexchange.com/questions/77465/maximal-closed-frequent-answer-included"">here on Cross-Validated</a> too, where the selected answer shows some further usage of the term (albeit in a slightly different context). Have a look at <a href=""http://www.cs.kent.edu/~jin/DM08/FIM.pdf"" rel=""nofollow noreferrer"">this source for some usage of the term</a>, which leads you to my understanding.</p>
","2","2","45264","13013"
"31928","<p>One could approach this in two general ways:</p>

<p>1) <strong>bottom up:</strong> thinking about unifying the data somehow to begin with</p>

<p>2) <strong>top down:</strong> deciding how the data needs to look based on the final model you wish to use</p>

<p>Do you already know which model you will use? If that is fixed (for whatever reason), you already know you need to get your data into the correct form, be it numerical or categorical.</p>

<p>As you pinned your question with the tag <code>regression</code>, I can tell you that you need to make your data all numerical, so regression can work.</p>

<hr>

<p>An example of making numerical data categorical would be to put it into bins. Imagine we have values ranging from zero to ten: [0.173, 7.88, 3.91, ...]. You could simply say that values between 0.00 and 0.99 are category A, values between 1.00 and 1.99 are category B, and so on.</p>

<h3>[Edit:]</h3>

<p>A slightly more sophisticated way of defining the bins to use would be to define the bins based on some characteristic statistics of your dataset. For example, have a look at the possible ways possible implemented within python's <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html"" rel=""nofollow noreferrer""><code>Numpy</code></a>. Of the available methods there, I have found the <strong>Doane</strong> method to work best - it will depend on your data though, so read the descriptions.</p>

<hr>

<p>Making categorical values numerical in a meaningful way depends a little more on you data. It is easy to make them numberic, but you should focus on doing it in such a way as to retain as much of the information each variable contains <strong>as well as</strong> the relative relationships between each of the categories that you started with. E.g. converting colours into integers would allow you to perform regression, but if yellow becomes 1 and purple 10, the model needs to be able to learn that purple isn't necessarily 10 times bigger than yellow, and that is difficult in the context of regression!</p>
","1","2","45264","13013"
"31974","<p>The default output activation of the Scikit-Learn <code>MLPRegressor</code> is 'identity', which actually does nothing to the weights it receives.</p>

<p>As was mentioned by @David Masip in his answer, changing the final activation layer would allow this. Doing so in frameworks such as Pytorch, Keras and Tensorflow is fairly straight-forward.</p>

<p>Doing it in your code with the <code>MLPRegressor</code> means using an object attribute that isn't a standard parameter, namely <code>output_activation_</code>.</p>

<p>Here are the built-in options that I can see in <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html"" rel=""nofollow noreferrer"">the documentation</a>:</p>

<blockquote>
<pre><code>activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’

Activation function for the hidden layer.

‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x
‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).
‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).
‘relu’, the rectified linear unit function, returns f(x) = max(0, x)
</code></pre>
</blockquote>

<p>Setting it's value to <code>logistic</code> gives you the property you would like, values between 0 and 1.</p>

<hr>

<p><strong>EDIT</strong>
After comments and update from OP: in their case, using <code>logistic</code> (sigmoid) as the final activation negatively affected results. So perhaps it is worth trying out all possible activation functions to investigate which activation best suits the model and data.</p>

<p>One further remark, at least within the context of deep learning, it is common practice not to use an activation at the final output of a neural network - for some thoughts around that discussion, <a href=""https://stats.stackexchange.com/questions/163695/non-linearity-before-final-softmax-layer-in-a-convolutional-neural-network"">see this thread</a>.</p>

<hr>

<p>That being said, below is a simple working example of a model that doesn't set it, and one that does. I use random numbers to make it work, but the take-away is that the predicted values for the altered model are always within the range from 0 to 1. Try changing the random seed and re-running the script.</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.neural_network import MLPRegressor

# To see an example where output falls outside of the range of y
np.random.seed(1)

# Create the default NN as you did
nn = MLPRegressor(
    solver='lbfgs',
    hidden_layer_sizes=50,
    max_iter=10000,
    shuffle=False,
    random_state=9876,
    activation='relu')

# Generate some fake data
num_train_samples = 50
num_test_samples = 50
num_vars = 2

X = np.random.random((num_train_samples, num_vars)) * \
    100  # random numbers between 0 and 100
y = np.random.uniform(0, 1, (num_train_samples, 1))  # uniform numbers between 0 and 1

X_test = np.random.random((num_test_samples, num_vars)) * 100
y_test = np.random.uniform(0, 1, (num_test_samples, 1))

# Fit the network
nn.fit(X, y)

print('*** Before scaling the output via final activation:\n')

# Now see that the output activation is (by default) simply linear i.e. 'identity'
print('Output activation by default: {}'.format(nn.out_activation_))
predictions = nn.predict(X_test)

print('Prediction mean: {:.2f}'.format(predictions.mean()))
print('Prediction max: {:.2f}'.format(predictions.max()))
print('Prediction min: {:.2f}'.format(predictions.min()))


print('\n*** After scaling the output via final activation:\n')

# Need to recreate the NN
nn_logistic = MLPRegressor(
    solver='lbfgs',
    hidden_layer_sizes=50,
    max_iter=10000,
    shuffle=False,
    random_state=9876,
    activation='relu')

# Fit the new network
nn_logistic.fit(X, y)


# --------------- #
#  Crucial step!  #
# --------------- #

# before making predictions = alter the attribute: ""output_activation_""
nn_logistic.out_activation_ = 'logistic'
print('New output activation: {}'.format(nn_logistic.out_activation_))

new_predictions = nn_logistic.predict(X_test)

print('Prediction mean: {:.2f}'.format(new_predictions.mean()))
print('Prediction max: {:.2f}'.format(new_predictions.max()))
print('Prediction min: {:.2f}'.format(new_predictions.min()))
</code></pre>

<p>Tested using Python 3.5.2.</p>
","1","2","45264","13013"
"32605","<h3>In practice</h3>

<p>Andrew Ng seems to be using the convention on the Theano framework. If you had 10 colour images, each 100 pixels high and 200 pixels wide, Theano models would expect input of the form:</p>

<blockquote>
  <p>(batch size, input channels, input rows, input columns)</p>
</blockquote>

<p>and so dimensions: <code>(10, 3, 100, 200)</code>. The three is because of the three RGb dimensions in a colour image.</p>

<p><a href=""https://www.tensorflow.org/programmers_guide/tensors#higher_ranks"" rel=""nofollow noreferrer"">Tensorflow</a> on the other hand reverse this order to instead use:
(num_obs, height, width, channels) - for the same example as above this becomes:</p>

<blockquote>
  <p>(batch size, input rows, input columns, input channels)</p>
</blockquote>

<p>meaning the dimensions of the input should be <code>(10, 100, 200, 3)</code>.</p>

<p>Keras works with Tensorflow and Theano and supports both conventions by simply allowing the user to set the position in which the number of channels are given. This can be set in a config file or using a specific environment variable upon setup. Have a look at <a href=""https://keras.io/backend/#what-is-a-backend"" rel=""nofollow noreferrer"">the relevant documentation</a>. </p>

<hr>

<h3>(Possible) justifications</h3>

<p>There are different points of view on what is more natural. If you come from a computer vision arena (or image processing in general), then libraries like <a href=""https://docs.opencv.org/2.4/doc/tutorials/core/mat_the_basic_image_container/mat_the_basic_image_container.html"" rel=""nofollow noreferrer"">OpenCV</a> use a coordinate system that has <code>(0,0)</code> in the top left of an image, and you specify a single pixel by given the vertical movement, then the horizontal movement from the origin. This means you give a <em>height</em>, then a <em>width</em>. One justification for this might be that many computer vision algorithms (e.g. colour filters) focus on difference across the channels of an image, but it is just convention.</p>

<p>In general graphing and mathematics, it is common to give the <strong>X</strong> coordinates first, then the <strong>Y</strong>, which means talking about horizontal movements, then vertical. Linear algebra in general is a good example of this.</p>

<p>All this being said, the best advice is to make sure you use the correct dimensions according to the documentation of the software you use. If you make your own software - you get to choose! (But make sure to document it!)</p>
","2","2","45264","13013"
"32667","<p>As neural networks are non-parametric, it would usually be the case that you train the combined model (as you describe it, but perhaps with additional FC-layers at the end) and let the model as a single entity learn/decide how best to combine the output of your two sub-models.</p>

<p>If you want to the hard-coded percentages you mention, then the most straight-forward way would be to do what you mention in the last paragraph. </p>

<p>People often train the same (or slightly differing models) independently on the same dataset, then average the predictions of these models on the test set, which can improve overall performance by integer percentages.</p>
","1","2","45264","13013"
"32672","<p>Practically speaking, you can of course throw this data into a model and see what happens. If the case you describe is rare within the dataset, it might not pose a huge problem, maybe just some undefined random behaviour for that particular set of input. However if it happens a lot, the results will likely be less pleasing. You can try it with and without removing your duplicate cases (assuming you yourself know which ones to remove?) - then just compare results.</p>

<hr>

<p>You could think about this issue as meaning that a function returns a value (your class attribute) from a distribution of possible answers. For example, that the inputs argument mean a value is selected from a discrete set of answers.</p>

<p>Methematically, your example doesn't lend itself to model fitting via a single function, e.g. using regression analysis, SVM models, neural networks and all other <em>standard</em> machine learning algorithms. The issue is that your data doesn't support the idea of a direct mapping, such as: $f : x \rightarrow y$. A function maps one argument <strong>X</strong> (or set of arguments) to another <strong>Y</strong>, but if you have a different output given identical input arguments, you are technically not talking about a function in the strictest sense. Since modelling is usually defining none other than a mapping from input to output, we are in essence defining a function.</p>

<p><a href=""https://en.wikipedia.org/wiki/Multivalued_function"" rel=""nofollow noreferrer"">Multi-value functions</a> do exist - perhaps there are some clues about which methods to try. A quick scan of the <em>Applications</em> sections in that linked article show there are times when your problem exists. Perhaps there are some methods that can cover your specific use-case, but without more information about what kind of model and data you are working with, it is hard to give more concrete suggestions.</p>
","0","2","45264","13013"
"32673","<p>A Generative Adversarial Network (GAN) takes the idea of using a <em>generator</em> model to generate fake examples and <em>discrimator</em> model that tries to decide if the image it receives is a fake (i.e. from the <em>generator</em>) or a real sample. This was <a href=""https://arxiv.org/abs/1406.2661"" rel=""nofollow noreferrer"">originally shown</a> with relatively simple fully connected networks.</p>

<p>A <a href=""https://arxiv.org/abs/1511.06434"" rel=""nofollow noreferrer"">Deep Convolution GAN (DCGAN)</a> does something very similar, but specifically focusses on using <strong>D</strong>eep <strong>C</strong>onvolutional networks in place of those fully-connected networks. Conv nets in general find areas of correlation within an image, that is, they look for spatial correlations. This means a DCGAN would likely be more fitting for image/video data, whereas the general idea of a GAN can be applied to wider domains, as the model specifics are left open to be addressed by individual model architectures.</p>

<p>The linked paper that proposed DCGANs specifically raises the topic of <strong>unsupervised-learning</strong>, and essentially wanted to marry the (at the time) recent success of conv nets with the new idea of GANs.</p>

<p>I also couldn't find any direct comparisons of when to use them, but there are plenty of articles that explain both models. <a href=""http://bamos.github.io/2016/08/09/deep-completion/"" rel=""nofollow noreferrer"">This is a good place to start</a> - after reading that, you could probably decide for yourself.</p>

<p>Regarding dimensions - I don't think the dimensions of your data would dictate which of the two variants to go for, other than of course influencing things that we always have to consider, such as training time, model complexity, capacity to learn and so on.</p>
","6","2","45264","13013"
"32706","<p>Normalising input data to a neural network is known to improve convergence properties, i.e. the model should converge quicker because the normalised data will not be so likely to produce huge or tiny gradients. Huge gradients make training erratic and less likely to converge (without applying other tricks), while tiny loss gradients lead to <em>neuron death</em> whereby certain neurons may get stuck, unable to get to an optimal weight value.</p>

<p>As you pointed out, normalising can be trivially simply to perform, so it might be worthwhile for both the final results and as a learning experience to try it out and compare your results.</p>

<p>You might also consider using other techniques such as batch normalisation, which inherently give you similar properties to normalising the entire dataset at the beginning.</p>
","1","2","45264","13013"
"32708","<p>It is difficult to give an exact solution, because it depends on what you eventually want to do with the data after you have merged it. If you only want to sort, like you mentioned, you could just use a pure database</p>

<p>Otherwise, you could look into <a href=""https://pandas.pydata.org/pandas-docs/stable/advanced.html"" rel=""nofollow noreferrer"">Pandas multi-indexing</a>. This will make things clearer when you look at the tables, however I find that I end up wanting to get back into your so-called <em>melted</em> form, for things such as plotting and input to neural networks.</p>

<p>Using <code>groupby</code> could indeed work, and I find those solution quite elegant. In your case you could just group by year and/or country and then apply the function for your use case.</p>

<p>Given your vernacular, you might be interested to look at the standard database-like functionality of a dataframe, such as <a href=""https://pandas.pydata.org/pandas-docs/stable/merging.html"" rel=""nofollow noreferrer""><code>join</code> and <code>merge</code></a></p>
","0","2","45264","13013"
"32711","<p>D3 (data Driven Documents) is a great and powerful tool, but does require a certain afinity for JavaScript. There are a few tools that piggy-back off it though and are more friendly to the aspiring data scientist and plotly is behind a few others. All are easier to use compared to D3 in my opinion.</p>

<p>Check out a few of the following:</p>

<ul>
<li><a href=""https://bokeh.pydata.org/en/latest/"" rel=""nofollow noreferrer"">Bokeh</a> (python)</li>
<li><a href=""https://plot.ly/products/dash/"" rel=""nofollow noreferrer"">Dash</a> (python - based on plotly, looks very promising!)</li>
<li><a href=""https://plot.ly/"" rel=""nofollow noreferrer"">Plotly</a> (many languages)</li>
<li><a href=""https://rstudio.github.io/shinydashboard/"" rel=""nofollow noreferrer"">Shiny</a> (R)</li>
</ul>

<p>Here is <a href=""https://blog.sicara.com/bokeh-dash-best-dashboard-framework-python-shiny-alternative-c5b576375f7f"" rel=""nofollow noreferrer"">a comparison of Bokeh and Dash</a>.</p>

<p>Here is <a href=""https://stackoverflow.com/questions/12977517/python-equivalent-of-d3-js"">a related question</a> on StackOverflow.</p>
","4","2","45264","13013"
"32930","<p>This seems about right.</p>

<p>You can use SciKit learn quite easily, as the predictions and test results you have should all be in NumPy arrays anyway. Take a look at <a href=""http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics"" rel=""nofollow noreferrer"">the regression metrics</a>. The metrics you have named are shown in the documentation to take lists, but here is an example showing they work just as well with NumPy arrays:</p>

<pre><code>In [1]: from sklearn.metrics import r2_score

In [2]: y_true = [3, -0.5, 2, 7]

In [3]: y_pred = [2.5, 0.0, 2, 8]

In [4]: r2_score(y_true, y_pred)
Out[4]: 0.94860813704496794

In [6]: import numpy as np

In [13]: y_true = np.array([3, -0.5, 2, 7])

In [14]: y_pred = np.array([2.5, 0.0, 2, 8])

In [15]: r2_score(y_true, y_pred)
Out[15]: 0.94860813704496794            # identical result
</code></pre>

<hr>

<p><strong>One last comment:</strong></p>

<p>17 examples doesn't sound like a lot. Look at model.summary() after compilation to see how many parameters your model has (<em>spoiler - it's 1360</em>). I would expect that you model (with your number of epochs and batch size etc.) will overfit, just memorising the dataset, and probably score 100%.</p>

<p>While this is a good sanity check to make sure your model can indeed learn, it might be a good idea to split a larger dataset (if available) into <code>train</code>, <code>validation</code> and <code>test</code> datasets. The simply use train and val as you have above, but in the prediction line, us the test set, which the model has never seen before. Unless your data is extremely homogenous, I wouldn't expect a accuracy metric near 100%.</p>
","0","2","45264","13013"
"32935","<p>The key part is, as you mentioned, batch size must be a value that divides without remainder into (I believe) the <code>train</code> and <code>validation</code> test set sizes.</p>

<p>One could find the highest common multiple (a.k.a. greatest common factor) of the dimensions of those two datasets, and that is your maximum batch size in a stateful LSTM. Check out this <a href=""http://www.bbc.co.uk/schools/gcsebitesize/maths/number/factorsmultiplesrev2.shtml"" rel=""nofollow noreferrer"">simple explanation</a> if that is unclear.</p>

<p>You could try just a simple loop over something like:</p>

<pre><code>for batch_size in range(128):
    try:
        model.train(...)
        print('Trained with batch size: {}'.format(batch_size))
    except:
        print('Couldn't use batch size: {}'.format(batch_size))
</code></pre>

<p>Just set #epochs to 1 and number of iterations to something small, or do whatever you can to reduce the time taken in each loop as you don't actually care about the results.</p>
","4","2","45264","13013"
"33007","<p>There are quite a few ways to measure the difference between two distributions. Take a look at this <a href=""https://en.wikipedia.org/wiki/Statistical_distance"" rel=""nofollow noreferrer"">overview article on wikipedia</a>.</p>

<p>One very common way that is used often in Machine and Deep Learning is the <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""nofollow noreferrer"">Kullback-Leibler (KL) Divergence</a>. It is most commonly used in minimising the cross-entropy between the distribution of your training data and the expected (generalised) distribution of the problem you are analysing. In general, a value close to <strong>0</strong> indicates that two distributions are expected to show similar behaviour, while a large value indicates the distributions behave very differently - so knowing the first distribution doesn't help you know anything about the second.</p>

<p>A more general class of measures of dissimilarity between distributions is the so called $f$-divergence, ""an average, weighted by the function $f$ of the odds ratio given by [probability measures] $P$ and $Q$"". The Wikipedia <a href=""https://en.wikipedia.org/wiki/F-divergence"" rel=""nofollow noreferrer"">article</a> contains a formal definition and a table with functions $f$ that give some popular dissimilarities/distances, including the KL-divergence.</p>

<blockquote>
  <p>The following table lists many of the common divergences between probability distributions and the $f$ function to which they correspond (cf. Liese &amp; Vajda (2006))</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/5KNhs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5KNhs.png"" alt=""Distribution distance metrics""></a></p>

<p>KL-Divergence is combined with the entropy itself, to define the <em>cross-entropy</em>. Take a look <a href=""http://cs231n.github.io/linear-classify/"" rel=""nofollow noreferrer"">here, under the Information Theory View</a>, for a bit more info.</p>
","4","2","45264","13013"
"33013","<p>In general you are extracting/creating 10 features that scan theoretically recreate the input i.e. the 41 features. The features on their own may not necessarily make a lot of sense and (depending on the dataset) may not be easily interpretable. One could draw a comparison to Principal Component Analysis and the result components as features.</p>

<p>One benefit of using Deep Belief Nets to <em>pre-train</em> your model and encode features is that the data must not be labelled. This leads to the final point, which is that your 10 features must not necessarily have names. You can just call them e.g. <code>feature1, feature2, ..., feature10</code>.</p>

<p>If you are having problems with the actual code and the way to push the 10 features further into the classifier, I would suggest you provide the code that you have so far and add more detail regarding your exact problem.</p>
","0","2","45264","13013"
"33015","<p>Interesting question. I personally haven't seen that for products going into production, but understand the logic. </p>

<p>Theoretically, the more data your deployed model has seen, the better is should generalise. So if you trained the model on the full set of data you have available, it should generalise better than a model which only saw for example train/val sets (e.g. ~ 90%) from the full data set.</p>

<p>The problem with this (and the reason we split data into train/val/test sets in the first place!) is that we want to be able to make statistical claims as to the accuracy on unseen data. As soon as we re-train a model again on <strong>all</strong> the data, it is no longer possible to make such claims.</p>

<h3>[Edit]</h3>

<p>Here is a <a href=""https://stats.stackexchange.com/questions/184095/should-final-production-ready-model-be-trained-on-complete-data-or-just-on-tra"">related question on Cross-Validated</a>, where the accepted answer makes similar points to me and mentions other ways of doing things.</p>

<hr>

<p>We loop over: </p>

<ol>
<li>train a model</li>
<li>assess performance on validation set $\rightarrow$ if satisfactory, go to step 5</li>
<li>change model</li>
<li>go to step 1</li>
<li>assess performance on test set</li>
<li>Present model with test accuracy found in step 5</li>
</ol>

<p>Eventually, if you manage to get a great score on the test set, you can claim it generalises well.  So the question as to whether <em>re-training</em> on the full dataset will improve performance on future unseen data is not strictly something you can test. Empirical evidence of better performance in other related problem sets would be the only source or guidance at the point in time when you must make the decision.</p>

<p>A sanity check would be to test the final re-trained model again on the original test set; expecting that it scores higher than it ever did when the model only saw the train/val set, because it has actually seen the test set during training. This wouldn't make me feel 100% confident that this final model is superior in all future cases, but at least it is as good as it can be with the given data.</p>

<p>Perhaps there are more rigorous arguments against doing what you say (probably academically motovated), however it does seem appealing for practical applications!</p>
","18","2","45264","13013"
"33024","<p>I am not sure I can go as deep as you'd like, but I can give the basics.
The base types in R are C-structs. Taken from Hadley Wickham's Advanced R:</p>
<blockquote>
<h2>Base types</h2>
<p>Underlying every R object is a C structure (or struct) that describes
how that object is stored in memory. The struct includes the contents
of the object, the information needed for memory management, and, most
importantly for this section, a type. This is the base type of an R
object. Base types are not really an object system because only the R
core team can create new types. As a result, new base types are added
very rarely: the most recent change, in 2011, added two exotic types
that you never see in R, but are useful for diagnosing memory problems
(NEWSXP and FREESXP). Prior to that, the last type added was a special
base type for S4 objects (S4SXP) in 2005.</p>
</blockquote>
<p>One layer higher (or at least different in parallel to the C base types), R itself has a few Object Oriented Systems at work. Major data containers that you likely use, such as vectors, dataframe, methods are going to be of type <code>S3</code>. The newer major object system defines <code>S4</code> objects. There is a great overview <a href=""http://adv-r.had.co.nz/OO-essentials.html"" rel=""nofollow noreferrer"">on Hadley's webpage</a></p>
<p>Things such as removing a column if it is set to NULL (as opposed to Panda's approach, of making each column take that value) I would guess are design choices. It is likely more common to drop a column than it is to fill it with null-values - in the context of 99% of the algorithms R packages the null values would simply be ignored anyway. So for convenience; not that <code>DF.drop()</code> in Pandas is a major inconvenience.
This behaviour is called <a href=""https://www.r-bloggers.com/r-na-vs-null/"" rel=""nofollow noreferrer"">an idiom in this blog</a>. They also show that assigning NULL to a vector element also deletes that element. Additionally, <code>NULL</code> and <code>NA</code> behave differently: NA is a missing value, NULL is not!</p>
<p>IMO you should take the time to at least skim some documentation. With the help of search engines and <code>control-F</code>, you rarely need to drudge through pages of docs to find what you are looking for.</p>
","1","2","45264","13013"
"33078","<p>The error here seems to be because you want train and test data (so two data sets), meaning that each class must be present in each of the data sets. This would mean that each class must have at least two samples. It is a design choice of whoever implemented <code>train_test_split</code>. I guess it might not technically be <code>stratified</code> otherwise.</p>

<p>You can see where it is implemented in <a href=""https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/model_selection/_split.py#L1541"" rel=""noreferrer"">the SciKit Learn source code</a>, within <code>class StratifiedShuffleSplit</code>:</p>

<pre><code>classes, y_indices = np.unique(y, return_inverse=True)
n_classes = classes.shape[0]

class_counts = np.bincount(y_indices)

if np.min(class_counts) &lt; 2:
    raise ValueError(""The least populated class in y has only 1""
                     "" member, which is too few. The minimum""
                     "" number of groups for any class cannot""
                     "" be less than 2."")
</code></pre>

<p><a href=""https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.unique.html"" rel=""noreferrer""><code>np.unique</code></a> finds indices of the each the classes in <code>y</code>. Because the option <code>return_inverse=True</code> is passed, it returns an array of indices that will allow full reconstruction of the input array, <code>y</code>. This means, to get the total number of classes that are present, you need to use <a href=""https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.bincount.html"" rel=""noreferrer""><code>np.bincount</code></a>; creating <code>class_counts</code>.</p>

<p>The final check is whether or not <code>class_counts</code> is less than the number of data sets you want to create. If it is, then you cannot create a properly stratified split of your data - so you get an error.</p>

<hr>

<p>As to how you might create your own version: one way I implemented stratified sampling was to use histograms, more specifically NumPy's <code>histogram</code> function. It worked well for continuous labels (i.e. not discrete classes) - and I was not looking at a multi-label problem, so you might have to adjust my suggestion to allow it to accomodate your needs.</p>

<p>The main idea is to split the labels into bins of a histogram and then randomly sample from those bins, with the option to allow for <strong>duplicates</strong>. That is really the part that will solve your specific problem of &lt; 2 labels in a class. I realise this doesn't specifically answer your problem, but perhaps it will give you some new ideas.</p>

<p>If duplicates don't make sense or are strictly not allowed in your experiment, then you could think about merging the smaller classes toether in some way, so they will have > 2 labels per class. This might be more useful than deleting them, but whether or not it is feasible  will depend on your data.</p>
","12","2","45264","13013"
"33080","<p>While your approach sounds logical, it is quite different from how most language-based models are created. You really need to tell us what it is that your want your model to learn and be able to predict, once you have trained it. Why are you interested in the length of the words; what is the significance of the input layer having the same number of neurons as there are characters in your longest word? It sounds like a performance optimisation of some kind.       </p>

<p>Because we are usually interested in the meanings of words, methods have been developed which <em>encode</em> meanings. There are models such as <a href=""https://en.wikipedia.org/wiki/Word2vec"" rel=""nofollow noreferrer"">Word2Vec</a>, which take text as input and produce a <strong>semantic</strong> meaning for each word in the form of a vector. <a href=""https://rare-technologies.com/word2vec-tutorial/"" rel=""nofollow noreferrer"">See a tutorial here</a>.</p>

<p>When we have images as inputs to a model, we are usually interested in things like as ""what is in the picture?"", ""where in the picture is it?"" and ""what is next to what?"". This means that <strong>spatial</strong> information is important.</p>

<p>Word vectors and images are both then in numerical form and can be passed into a network. We still need to to know what we want our model to learn, as to know what we want to optimise; we must decide on a definition for <em>loss</em> or <em>error</em>, as this is what then trains the model via backpropagation.</p>
","0","2","45264","13013"
"33093","<p>The fixed answer of $1/3$ is a result of their decision to use the uniform distribution along with the parameterised arguments, namely $1/\sqrt{n}$.</p>

<p>For a uniform distribution, denoted with lower and upper bounds $a$ and $b$:</p>

<p>$$
U(a, b)
$$</p>

<p>the variance is defined as:</p>

<p>$$
\frac{1}{12} (b - a)^2
$$</p>

<p>So in the case of the authors, Glorot and Bengio, the two bounds are simply the square root of the number of neurons in the layer of interest (generally referring to the preceding layer, as they put it). This size is called $n$, and they set the bounds on the uniform distribution as:</p>

<p>$$
a = - \frac{1}{\sqrt{n}}
$$
$$
b = \frac{1}{\sqrt{n}}
$$</p>

<p>So if we plug these values into equation 15, we get:</p>

<p>$$
Var = \frac{1}{12}(\frac{1}{\sqrt{n}} - -\frac{1}{\sqrt{n}})^2
$$</p>

<p>$$
Var = \frac{1}{12}(\frac{2}{\sqrt{n}})^2
$$</p>

<p>$$
Var = \frac{1}{12} * \frac{4}{n}
$$</p>

<p>And so finally:</p>

<p>$$
n * Var = \frac{1}{3}
$$</p>
","0","2","45264","13013"
"33096","<p>This is really just like a convention that appears in some places because we normallt want to take the derivative of the cost function (i.e. compute gradients), which means the power of 2 would be taken to the front.</p>

<p>If we put the $\frac{1}{2}$ at the front to begin with, it just looks nicer once we have finished. I have seen this written somewhere before in a paper, but can't find a reference right now.</p>

<p>Because the nominal values of cost themselves (the scale of the values) is not of importance, we can scale it as we like really. Multiplying by a constance of $0.5$ does not change the algebraic behaviour.</p>
","3","2","45264","13013"
"33103","<p>You can visualise the activation maps of the various layers, to show how a deep CNN decomposes your input into more and more abstract building blocks. There is a good discussion on <a href=""https://stats.stackexchange.com/questions/213325/neural-network-meaning-of-weights"">Cross Validated SE here</a>. This explains the main idea and intuition as to how CNNs work and normally gets <em>layman</em> motivated to listen and learn more  :-)</p>

<hr>

<p>There is another great idea names <a href=""https://arxiv.org/pdf/1312.6034.pdf"" rel=""nofollow noreferrer"">saliency maps - [Simonyan et al.]</a>. As well as reading that paper, I suggest watching <a href=""https://youtu.be/6wcs6szJWMY"" rel=""nofollow noreferrer"">Lecture 12 of CS231n</a> (<a href=""http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf"" rel=""nofollow noreferrer"">slides here</a>). The general idea of 'how to do this' is, I believe, can really be applied to most deep neural networks.</p>

<p>Here is a sample from the linked paper:</p>

<p><a href=""https://i.stack.imgur.com/JaWkj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JaWkj.png"" alt=""Saliency map of dog photo""></a></p>

<p>The steps are something like this:</p>

<ol>
<li>take a trained network architecture</li>
<li>choose the layer that you are interested in (be it the final layer or an intermediate layer</li>
<li>set all weights in layers prior to your chosen layer to zero</li>
<li>perform backpropagation rom your selected layer back to your input</li>
</ol>

<p>Now you will have a set of weights that match the dimensions of your input, but the values will be those of the weights at your selected layer, simply <em>extrapolated</em> backwards onto your input.</p>

<p>You can then use these weights with an input image, perhaps simply add these value to one or all of the colour channels, and you will have an image which has certain regions highlighted. The highlighted regions signify where the corresponding neurons of your selected layer are activated - i.e. what they focus on to make their contribution towards the final prediction.</p>

<hr>

<p>Yet another approach is to systematically occlude (cover up) parts of your images, and track how the predictions fluctuate. The intuition is that, when you cover up a certain part of an image, and the prediction becomes terrible, you know your network really looks for that part, which you just covered up. The first time this was really published (as far as I can find) was in the <a href=""https://arxiv.org/abs/1311.2901"" rel=""nofollow noreferrer"">2013 paper from Zeiler and Fergus</a>.</p>

<p>In the image below (from that linked paper), you can see how the grey square was shifted over and image to create a final heatmap, which highlights the critical portions of the image to getting the correct image classification.</p>

<p><a href=""https://i.stack.imgur.com/JfJlg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JfJlg.png"" alt=""enter image description here""></a></p>
","1","2","45264","13013"
"33112","<p>One important factor to take into account is how you use the numerical representation of words / embeddings from either TF-IDF or Word2Vec to then compute sentiments. Without knowing how you do this, it is difficult to give a concrete answer. Also, which task are you working on, what does a result of 90% mean?</p>

<p>Regardless of how you compute TF-IDF (there are several definitions - shown below), it is essentially assigning a numerical value to a word, thus creating a mappng of sorts. Word2Vec technically created an <em>embedding</em>, as it maps individual words into a vector space.</p>

<p><a href=""https://i.stack.imgur.com/LABih.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LABih.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/lxzI2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lxzI2.png"" alt=""enter image description here""></a></p>

<p><a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">Images taken from here</a></p>

<p>The final TF_IDF is simply the multiplication of the term frequency and the inverse document frequency.</p>

<p>I won't go into details as to how the vectors in Word2Vec are computed, but they also define a way to assign a numerical vector (an embedding) to a single word. In essence, both of these are saying how important a word is, in the context of your documents (your corpus), with Word2Vec also having the interpretability of comparison between word vectors. For example, doing this with the associated vectors of the words actually works really well:</p>

<pre><code>King - Man + Woman = Queen
</code></pre>

<p>Perhaps the sentiments you are computing, based on either of these methods , does something similar to taking an average over many words appearing in a sentence, and you end up with a normalised and similar results.</p>

<hr>

<p>TF-IDF takes a more intuitive approach, looking at how many times a word appears in general, in how many of the documents does it appear and how many times. Word2Vec instead looks at which words often appear together (there is a related quote that normally gets brought up here: <em>""You can judge a person by the company they keep""</em>). So the intuition behind each on is slightly different, but you have numerical values for each word. Perhaps a closer comparison of TF-IDF would be to look at Doc2Vec.</p>

<p>There is also the GLoVE embedding model, which scores well in many NLP tasks - on the same level as Word2Vec embeddings.</p>
","1","2","45264","13013"
"33113","<p>I am afraid it is not that simple - Have a look at <a href=""https://blog.waya.ai/deep-residual-learning-9610bb62c355"" rel=""nofollow noreferrer"">this pretty good walkthrough</a>.</p>

<p>The table you posted is a kind of overview that doesn't contain all the details of how the ""blocks"" are linked. Other details such as max pooling after conv layers.</p>

<p>Your model therefore also doesn't yet contain the main idea, which is residual mappings, as shown by the black arrows jumping over several conv layers in this snippet of the diagram from <a href=""https://arxiv.org/pdf/1512.03385.pdf"" rel=""nofollow noreferrer"">the paper</a>:</p>

<p><a href=""https://i.stack.imgur.com/1oa3l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1oa3l.png"" alt=""snippet of a resnet""></a></p>

<p>Here is the code for one of the blocks (taken from the blog I linked above):</p>

<pre><code>def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):
    shortcut = y

    # down-sampling is performed with a stride of 2
    y = layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)
    y = layers.BatchNormalization()(y)
    y = layers.LeakyReLU()(y)

    y = layers.Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)
    y = layers.BatchNormalization()(y)

    # identity shortcuts used directly when the input and output are of the same dimensions
    if _project_shortcut or _strides != (1, 1):
        # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)
        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2
        shortcut = layers.Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)
        shortcut = layers.BatchNormalization()(shortcut)

    y = layers.add([shortcut, y])
    y = layers.LeakyReLU()(y)

    return y
</code></pre>

<p>You can see they use the functional API (meaning they use <code>Model</code> instead of <code>Sequential</code>, from <code>keras.models</code>). Also note how the <code>shortcut</code> variable is stored at the beginning of the block as copy (a residual mapping) of the blocks input, which is then used to introduce the actual magic, gre-supply those residuals back to the network at the end of the block. </p>

<p>From the paper:</p>

<blockquote>
  <p>The operation F + x is performed by a shortcut connection and element-wise addition</p>
</blockquote>

<p>which is basically what is going on after the <code>if</code> block in the line:</p>

<pre><code>y = layers.add([shortcut, y])
</code></pre>

<p>The full code example at the end of the linked blog should help to give some further context.</p>
","2","2","45264","13013"
"33120","<p>If you can keep adding new data (based on a main concept such as area i.e. the ZIP code) <strong>and</strong> the performance of your model improves, then it is of course allowed... assuming you only care about the final result.</p>

<p>There are metrics that will try to guide you with this, such as the <a href=""https://en.wikipedia.org/wiki/Akaike_information_criterion"" rel=""noreferrer"">Akaike Information Criterion</a> (AIC) or the comparable <a href=""https://en.wikipedia.org/wiki/Bayesian_information_criterion"" rel=""noreferrer"">Bayesian Information Criterion</a> (BIC). These essentially help to pick a model based on its performance, being punished for all additional parameters that are introduced and that must be estimated. The AIC looks like this:</p>

<p>$${\displaystyle \mathrm {AIC} =2k-2\ln({\hat {L}})}$$</p>

<p>where $k$ is the number of parameters to be estimated, i.e. number of features you apply, because each one will have one coefficient in your logistic regression. $\hat{L}$ is the maximum value of the Maximum Likelihood (equivalent to the optimal score). BIC simply uses $k$ slightly differently to punish models.</p>

<p>These criteria can help tell you when to stop, as you can try models with more and more parameters, and simply take the model which has the best AIC or BIC value.</p>

<p>If you still have other features in the model, which are not related to the ZIP, they could potentially become overwhelmed - that depends on the model you use. However, they may also explain things about the dataset which simply cannot be contained in the ZIP information, such as a house's floor area (assuming this is relatively independent from ZIP code).</p>

<p>In this case you might compare these to something like Principal Component Analysis, where a collection of features explain one dimention of the variance in data set, while other features explain another dimension. So no matter how many ZIP-related features you have, you may never explain importance of floor area.</p>
","6","2","45264","13013"
"33146","<p><strong>[EDIT:]</strong>
Your problem is definitely in the generators, in that you do not set the target size, and its default it (256, 256) - as seen in <a href=""https://keras.io/preprocessing/image/#imagedatagenerator-class"" rel=""nofollow noreferrer"">the documentation fro <code>flow_from_directory</code></a>:</p>

<pre><code>flow_from_directory(directory, target_size=(256, 256), color_mode='rgb', ...)
</code></pre>

<blockquote>
  <p>target_size: Tuple of integers (height, width), default: (256, 256). The dimensions to which all images found will be resized.</p>
</blockquote>

<p>Try setting the target size parameter to (150, 150) and I think it will work. That default seems to be overwriting your preprocessing.</p>

<hr>

<p>It must be in your generators - I ran the following code and a model trained as expected:</p>

<pre><code>from keras import models, layers, metrics
import numpy as np

model = models.Sequential()
   ...: model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(150, 150,
   ...:  3)))
   ...: model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
   ...: model.add(layers.Conv2D(64, (3, 3), activation='relu'))
   ...: model.add(layers.MaxPooling2D((2, 2)))
   ...: model.add(layers.Conv2D(64, (3, 3), activation='relu'))
   ...: model.add(layers.MaxPooling2D((2, 2)))
   ...: model.add(layers.Conv2D(128, (3, 3), activation='relu'))
   ...: model.add(layers.MaxPooling2D((2, 2)))
   ...: model.add(layers.Flatten())
   ...: model.add(layers.Dense(300, activation='relu'))
   ...: model.add(layers.Dense(10, activation='softmax'))

In [10]: model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 146, 146, 32)      2432      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 14, 14, 128)       73856     
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 300)               1881900   
_________________________________________________________________
dense_2 (Dense)              (None, 10)                3010      
=================================================================
Total params: 2,016,622
Trainable params: 2,016,622
Non-trainable params: 0
_________________________________________________________________
In [17]: model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics = ['
    ...: acc',metrics.categorical_accuracy])

# Create some fake data to match your inputs. Each label seems to be 10 points: (1, 10)
In [11]: fakes = np.random.randint(0, 255, (100, 150, 150, 3))
In [24]: labels = np.random.randint(0, 2, (100, 10))

In [25]: model.fit(fakes, labels, validation_split=0.2)

Epoch 1/10
80/80 [==============================] - 2s - loss: 62.8913 - acc: 0.1125 - categorical_accuracy: 0.1125 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00
Epoch 2/10
80/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00
Epoch 3/10
80/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00
Epoch 4/10
80/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00
Epoch 5/10
80/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00
Epoch 6/10
80/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00
Epoch 7/10
80/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00
Epoch 8/10
80/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00
Epoch 9/10
80/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00
Epoch 10/10
80/80 [==============================] - 0s - loss: 67.0916 - acc: 0.0000e+00 - categorical_accuracy: 0.0000e+00 - val_loss: 71.7255 - val_acc: 0.0000e+00 - val_categorical_accuracy: 0.0000e+00
</code></pre>
","4","2","45264","13013"
"33149","<p>I have done this before and didn't find a default implementation - the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"" rel=""nofollow noreferrer"">StratifiedKFold</a> and <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold"" rel=""nofollow noreferrer"">RepeatedStratifiedKFold</a> are only documented to work with classes.</p>

<p>The way I ended up doing it was not quite as you are thinking with quartiles/deciles, but rather using a histogram (it matched my needs).</p>

<p>NumPy has a nice method for doing this, with many different formulas for computing the bin sizes that you can play around with to be match your data i.e. if it is normally distributed or not. I can't post the entire method code, but here is the gist:</p>

<pre><code>samples_per_bin, bins, = np.histogram(data, bins='doane')    # Doane's method worked best for me

min_bin_size = samples_per_bin.min()

# compute the maximum batch size possible, using all samples from the bin with lowest population
n_bins = len(samples_per_bin)
max_batch = min_samples_single_bin * n_bins
</code></pre>

<p>I then put the data into a Pandas DataFrame and added a column indicating which bin each data point was in - finally doing something like this to perform the sampling from each of the bins:</p>

<pre><code>df.groupby('bin_name', group_keys=False).apply(
            lambda x: x.sample(n_per_group, replace=True))
</code></pre>

<p>Obviously you can allow duplicates in a batch or not by changing the <code>replace</code> argument. It might be necessary if you want a larger batch size while forcing stratification. It is definitely a limitation of my approach, which you might be able to overcome by using quartiles etc as you suggest.</p>
","4","2","45264","13013"
"33155","<p>One of the simplest ways to get to some predictions would be to use a model like ARIMA, which looks at recent previous observations to predict a number of steps ahead. ARIMA stands for Autoregressive Integrated Moving Average:</p>

<ul>
<li><strong>Autoregressive</strong> means that something looks at its own (auto) historical values</li>
<li><strong>Integrated</strong> refers to the _differencing, a step tp help remove non-stationarity</li>
<li><strong>Moving Average</strong> just means the model also takes the moving average into account (helps keep predictions in reasonable ball park)</li>
</ul>

<p>Here is a good little explanation of <a href=""https://www.quora.com/What-is-ARIMA"" rel=""nofollow noreferrer"">the main points in more detail</a>.</p>

<p>In your case, given there are factors such as weather and time of data to be used, you would probably benefit from using <em>seasonality</em>. For this there is an extended ARIMA, called SARIMA - where S stands for seasonal.
There is an implementation in Python's statsmodels package - or if you want to use R, then maybe take a look at the <a href=""https://github.com/robjhyndman/forecast"" rel=""nofollow noreferrer""><code>forecast</code> package by the great Rob Hyndman</a>, or <a href=""https://cran.r-project.org/web/packages/sarima/sarima.pdf"" rel=""nofollow noreferrer"">the <code>sarima</code> package</a>.</p>

<hr>

<p>If you want to use something a little more modern and cutting edge, then you are really talking about Recurrent Neural Networks (RNNs). I am not sure how familiar you are with there? One key term you should understand: LSTM - a type of unit that considers past data, current data and maintains a <em>state</em> of your model at a specific point in time.</p>

<p>Have a look at <a href=""http://adventuresinmachinelearning.com/keras-lstm-tutorial/"" rel=""nofollow noreferrer"">a walkthrough (example)</a> to see if it makes sense to you.</p>

<hr>

<p>ARIMA type models are easy to get running and the results are easy to interpret, so you really know what is going on. The performance of things like RNNs should be better, but it is rarely a short walk to success... of course it will depend on your data.</p>
","1","2","45264","13013"
"33190","<p>If you want to get quite involved and be able to specify names for each of the panels you create, you could look at the h5 file format.</p>

<p>This allows you to <em>group</em> datasets in named containers. You can then read them from disk later on one by one i.e. you don't need to read the whole dataset into memory.</p>

<p>Here is an example of a function that would save such a dataset:</p>

<pre><code>def save_h5(h5_filename, data, labels, descr=None,
            data_dtype='float32', label_dtype='float32'):
    """"""Create a compressed .h5 file containing:
    data    : numpy array
    labels  : numpy array
    descr   : text description ofthe data contained (must be a string)
    """"""

    if os.path.exists(h5_filename):
        # prevent overwriting a file
        sys.exit('File already exists!')

    h5_fout = h5py.File(h5_filename)

    h5_fout.create_dataset(
        name='data',
        data=data,
        compression='gzip', compression_opts=4,
        dtype=data_dtype)

    h5_fout.create_dataset(
        name='labels',
        data=labels,
        compression='gzip', compression_opts=4,
        dtype=label_dtype)


    if descr is not None:
        h5_fout.create_dataset(
            'description', data=descr)

    h5_fout.close()
</code></pre>

<p>For the meaning of the parameters, <a href=""http://docs.h5py.org/en/latest/quick.html"" rel=""nofollow noreferrer"">head over the documentation</a>.</p>

<p>You can write a similar function to make accessing the saved h5 file. This really is a flexible way to save data, and it can be compressed with one of the best known (widely-spread) algorithms in the open-source world: gzip! There are also other possibilities implemented.</p>

<hr>

<p>On a side note, if you want to minimise the possibility of corruption, you could consider saving each panel/DataFrame (whichever method you go for) into separate files, and then make copies/backups.</p>

<p>Additionally, you said: </p>

<blockquote>
  <p>I tried using pickle and csv, but once a line was corrupted I lost the whole file.</p>
</blockquote>

<p>... the beauty of a simple <code>csv</code> file is that you can actually open it in notepad or a spreadsheet and usually find the line which is ""broken"" and fix/delete it. Pickle, on the other hand, is a little more complicated to debug.</p>
","3","2","45264","13013"
"33195","<p>This comes from some standard definitions really. There is a similar question on <a href=""https://stats.stackexchange.com/questions/123320/mse-decomposition-to-variance-and-bias-squared"">Cross Validated SE</a> that has good answers. There are related questions there that might be worth looking through too, <a href=""https://stats.stackexchange.com/questions/152882/question-about-bias-variance-tradeoff?rq=1"">like this one</a>.</p>

<hr>

<p>The $\sigma^2_{e}$, which is basically the <em>noise</em> that comes with a <strong>random variable</strong>. Perhaps there isn't much of it, but we normally just write it at the end of such equations.</p>

<p>In the context of a real world machine learning problem, I also sometimes think of that term as accounting for information that I just do not have the possibility to explain, with the data that I have. So in that particular project, it is as good as irreducible error.</p>
","1","2","45264","13013"
"33199","<p>If your error stays at about 20%, it sounds like your features are not really helping.</p>

<p>It is likely the case that your relationship in data/features is not simple, so you need to allow your SVM model more flexibility and/or train for longer. This will of course bring in the danger of overfitting, but should improve things... you'll need to try out a few different things.</p>

<p>If you are using SciKit Learn, this might equate to using a radial basis function with a high value of argument <code>C</code> (giving much more flexibility to the model) and also try a higher value for the argument <code>gamma</code>, which will reduce the <em>radius of influence</em> of each individual data point.
<a href=""http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html"" rel=""nofollow noreferrer"">Check out this example for further explanation.</a></p>

<p>Think of the classic example of flipping an unbiased coin: we would expect to get 50% heads and 50% tails. So in this binary prediction, any time you make a prediction, you have 50-50 odds and so a model with more than 50% error is worse than a random guess!</p>

<p>If you are getting 20% error, it sounds like your model is just predicting black. A simple way to see and understand this - and why it really makes sense perhaps - would be to plot the data.</p>

<p>If in your feature speace, the points all look like a big mixture, a cloud, where the majority of points are black, then even a human would likely just predict black:</p>

<p><a href=""https://i.stack.imgur.com/mWj2V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mWj2V.png"" alt=""enter image description here""></a></p>

<p>So two options:</p>

<ol>
<li>Allow the model higher flexibility (as described above), which will let it (over-)fit a line that cleanly cuts out the sparse red points from the black</li>
<li>Get new features, or preprocess your features in such a way that a plot might end up looking something more like thi the image below, which will allow a simpler model to classify the red from the black with e.g. a straight line.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/HdZuW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HdZuW.png"" alt=""enter image description here""></a></p>
","3","2","45264","13013"
"33202","<p>There are general rules of thumb for different models, both in terms of the number of samples required as well as the number of features. I will try to just put some concrete numbers on things, with the caveat that is is indeed problem specific, and you usually have to make compromises.</p>

<p>If you are looking at something like a simple linear regression e.g. with 5 features, then for the basic tests to be considered statistically relevant, you need at least 40 samples.</p>

<p>In your case with 500,000 samples, 50 features is definitely acceptable - you could even have more. There are, however, two things I would be wary of:</p>

<p>Firstly, you make life hard for your model is many of the features are really describing the same thing. The technical term is <a href=""https://en.wikipedia.org/wiki/Multicollinearity"" rel=""nofollow noreferrer"">multicollinearity</a>, which, if your data has it, means that your features themselves are highly (linearly) correlated. It is hard for the model to know which feature to rely on, and your results may be improved by simply removing one. A simple example: if I predict your shoe size based on your height, and I have two features: your height in inches and your height in centimetres, you can see that I only really need one of these, as they are perfectly correlated measures.</p>

<p>Secondly, <em>The more the better</em> is theoretically true, as long as you have a model which is able to decide when to include features and when to ignore them - and that your data is good enough to facilitate this. There are ideas like <em>Occam's razor</em>, saying that a simple model should be selected over a complex model with the same performance; i.e. you should also consider interpretability. There are measures defined that quantify this, such as the <a href=""https://en.wikipedia.org/wiki/Akaike_information_criterion"" rel=""nofollow noreferrer"">Akaike Information criterion</a>. It is a model selection tool - so you could fit the model using 10, 20, 30, and 50 of your features, compare the results of this creiterion and pick the model which scores best. Here is <a href=""https://datascience.stackexchange.com/questions/33117/is-this-a-good-practice-of-feature-engineering"">a recent question which touches on that topic</a>.</p>

<p>There are functions that will remove features, based on a correlation threshold with other features in the dataset. It is explained well in <a href=""https://topepo.github.io/caret/pre-processing.html#corr"" rel=""nofollow noreferrer"">the Caret package</a>, for R, but the same thing can be done in Python fairly easily (not sure if there is a standard implementation).
<strong>[EDIT]</strong> - the Python equivalent looks to be <a href=""http://scikit-learn.org/stable/modules/feature_selection.html"" rel=""nofollow noreferrer"">the VarianceThreshold</a> class.</p>
","1","2","45264","13013"
"33216","<p>There is a kind of bias that you are introducing, yes. You are basically extracting some statistics (i.e. the mean) from your hold-out set and using that to train, which makes your final claims of accuracy a little weaker (some people might say they are useless).</p>

<p>The general approach is to compute the mean of your training data, then you may subtract that from <strong>all</strong> of the data, including hold-out data.</p>

<p>You can do the mean subtraction, in general, using something like the <a href=""https://keras.io/preprocessing/image/#imagedatagenerator-class"" rel=""nofollow noreferrer"">ImageDataGenerator</a>. The mean to be subtracted can be computed using all or some of the training data. That class also offers other augmentation functionalities, such as normalising the dataset too, adding rotations etc.</p>

<p>you mentioned you read features from a CSV file, so if you are not talking about images, as long as you can use e.g. NumPy, you can perform is manually on all data at the beginning.</p>
","1","2","45264","13013"
"33218","<p>I haven't seen the specifical cases you mention , but I know of machine learning within insurance, and the types of models I've seen being used always depended on the <em>criticality</em> of the data and use of the results.</p>

<p>So a more general answer: if they need to justify all modelling decisions (as is often the case in insurance), they stick to more traditional models, such as logistic regression, tree-based methods and the like. More cutting-edge models are considered black-boxes.</p>

<p>If the project if more about analysing ad-hoc topics and non-core businesses, but which could perhaps <em>support</em> core business decisions (or otherwise), then anything is game: from stacked bidirectional LSTMs on word embeddings of their contracts to capsule networks on images scraped from social media websites.</p>

<p>I suppose you could model Moral Hazard similarly to fraud, in which case there are plenty of examples to be found online. I think there was even a Kaggle competition on that.</p>
","1","2","45264","13013"
"33288","<p>From the code and task as your present it, a confusion matrix wouldn't make sense. This is because it shows how well a model is <strong>classifying</strong> samples i.e. saying which category they belong to. Your problem (as the author in your link states) is a <strong>regression</strong> problem, because you are predicting a continuous variable (temperature). <a href=""https://en.wikipedia.org/wiki/Confusion_matrix"" rel=""nofollow noreferrer"">Have a look here for more information</a>.</p>

<hr>

<p>In general, if you do have a classification task, printing the confusion matrix is a simple as using the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"" rel=""nofollow noreferrer""><code>sklearn.metrics.confusion_matrix</code></a> function.</p>

<p>As input it takes your predictions and the correct values:</p>

<pre><code>from sklearn.metrics import confusion_matrix

conf_mat = confusion_matrix(labels, predictions)
print(conf_mat)
</code></pre>

<hr>

<p>You could consider altering your task to make it be a classification problem, for example by grouping the temperatures in to classes of a given range.</p>

<p>You could say transform the target temperature to be a <code>new_target_class</code>, then change your code to use the <code>[RandomForestClassifier][3]</code>.</p>

<p>I have done a quick and dirty conversion on the same data linked in that article, <a href=""https://docs.google.com/spreadsheets/d/1_U7nMvJl_Hr_aVpPBpe3NHbKSvFrILUVzmIaYT4Xnmo/edit?usp=sharing"" rel=""nofollow noreferrer"">check it out here</a>. I basically use the minimum and maximum values of the target variable to set a range, then aim for 10 different classes of temperature and create a new column in the table which assign that class to each row. The top it looks like this (click on picture to enlarge):</p>

<p><a href=""https://i.stack.imgur.com/xcNQn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xcNQn.png"" alt=""example of converting continuous to discrete target variable""></a></p>

<p>If you can get those predictions going using the <code>RandomForestClassifier</code>, you can then run the confusion matrix code above on the results.</p>
","4","2","45264","13013"
"33298","<p>My preferred way to transpose a <code>data.frame</code> (or <code>data.table</code>) is to use the <code>transpose</code> function found in <a href=""https://github.com/Rdatatable/data.table/wiki"" rel=""nofollow noreferrer"">the data.table package</a>.</p>

<p>It means you might have to install it: <code>install.packages(""data.table"")</code>.
This give you a function that will do what you want. Here is a demo how to use it:</p>

<pre><code>library(data.table)    # makes the transpose function available

col_names &lt;- colnames(worldcup)      # keep track of original column names
wc_2 &lt;- colMeans(worldcup)           # compute the means
wc_2 &lt;- transpose(as.data.frame(wc_2))              # this gives you generic column names
colnames(wc_2) &lt;- col_names          # reapply the column names
</code></pre>

<p>Or combining it with your example (not fully tested):</p>

<pre><code>library(magrittr)    # to import the pipe operator: %&gt;%

wc_2 &lt;- worldcup %&gt;% 
  select(Time, Passes, Tackles, Saves) %&gt;%
    colMeans() %&gt;% as.data.frame() %&gt;% transpose()       # you might need to put a dot (`.`) in the empty brackets to pass the argument before the pipe operator
</code></pre>

<p>then maybe add the names you would like:</p>

<pre><code>colnames(wc_2) &lt;- col_names
</code></pre>

<p>If you like the sound of the package, I recommend going through the mini intro, built in to the package:</p>

<pre><code>install.packages(""data.table"")        # install it
library(data.table)                   # load it
example(data.table)                   # run the examples section of ?
</code></pre>
","1","2","45264","13013"
"33300","<p>Ordinal data deals with categories, which themselves are ordered by meaning somehow, but we cannot strictly talk about the distance between the categories. In the example below, you can see that the ordinal data runs from <code>awesome</code> to <code>terrible</code>, with 5 categories explaining an entire ordinal range:</p>

<p><a href=""https://i.stack.imgur.com/HJVjA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HJVjA.png"" alt=""ordinal example""></a></p>

<p>Having a look at the paper, it seems the define a uniform distribution $U(a, b)$ (probably continuous), which they then divide into some random categories, let's say 5, to follow the table above. So if we set the bound of the distribution, $a = 0$ and $b = 1$, we might randomly create five categories, such as:</p>

<pre><code>Category :  a      b
----------------------
Awesome  : 0.00 - 0.09
Great    : 0.10 - 0.44
OK       : 0.45 - 0.46
Bad      : 0.47 - 0.59
Terrible : 0.60 - 1.00
</code></pre>

<p>We can see that they are <em>ordinal</em>, because they numerical values do signify some kind of meaningful order, but the size of the categories is not necessarily uniform. The authors generated them randomly.</p>
","0","2","45264","13013"
"33307","<p>For the problem of an imbalanced dataset, you can look into <strong>stratified sampling</strong>, or stratified-cross-validation (as mentioned <a href=""https://stats.stackexchange.com/questions/91922/splitting-an-imbalanced-dataset-for-training-and-testing"">here</a>). One idea might be to create stratified batches from the data.</p>

<p>I would probably make all attempts to get <em>train/val/test</em> splits, because you will otherwise face issues when claiming a final test accuracy, as the model might well have seen your entire dataset.</p>

<p>One could imagine splitting to have e.g. 300/9750 (pos/neg) in the training dataset, and during training, you create stratified batches from those 1050 images, so each batch e.g. of 50 images, might contain, 10 positives and 40 negatives. This is still somewhat imbalanced, but you are pushing the balance into a more favourable direction in that the model should be able to learn more efficiently.</p>

<p>In medical research it is often the case that there are too few samples (in addition to class imabalnces), and so there is usually a huge effort that goes into data augmentation, which you might also be able to make use of it. Here is some <a href=""https://openreview.net/forum?id=rkBBChjiG"" rel=""nofollow noreferrer"">related literature</a>  (extremely fresh - edited one week ago!).</p>

<p><a href=""https://arxiv.org/abs/1306.3706"" rel=""nofollow noreferrer"">Here is another approach</a>, whereby the authors (William Fithian &amp; Trevor Hastie) devise a subsampling method, which uses the features of the samples to accept/reject them. They design it for the simplist case (logistic regression), but perhaps it might give you ideas:</p>

<blockquote>
  <p>... using a pilot estimate to preferentially select examples whose responses are conditionally rare given their features. </p>
</blockquote>

<p>Something to be especially aware of when using the ideas I mentioned above is <a href=""https://stats.stackexchange.com/questions/131233/neural-network-over-fitting"">overfitting</a>. Cross-validation is probably what can best help you out in this respect.</p>
","1","2","45264","13013"
"33366","<p>I have noticed this when training on one or more GPUs. I think it is due to Tensorflow having to acquire the resources (in the background it blocks the entire GPU memory). Perhaps some amount of data e.g. the first batch is copied over to the GPU.</p>

<p>I didn't find a way to reduce the waiting time dramatically, but you could try the tensorflow options to allocate less memory and allow memory growth as required. Check out <a href=""https://www.tensorflow.org/programmers_guide/using_gpu#allowing_gpu_memory_growth"" rel=""nofollow noreferrer"">the official docs</a> and <a href=""https://github.com/keras-team/keras/issues/1538"" rel=""nofollow noreferrer"">this issue for the Keras version</a>.</p>

<p>Here is the latest recommendation form that issue: setting Tensorflow options befor eimporting Keras.</p>

<pre><code>-------------------------- set gpu using tf ---------------------------
import tensorflow as tf
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.Session(config=config)
-------------------  start importing keras module ---------------------
import keras.backend.tensorflow_backend as K
import keras......
</code></pre>
","1","2","45264","13013"
"33380","<p>Depending on the shape of your plots, it might make sense to create four separate plots. I suggest this just because it makes sense to show the axes and labels on each individual page, otherwise it could be that the reader would have to always flips back to e.g. page 1 to see what the scales are.</p>

<p>I would probably do something like a 6 (rows) by 5 (columns) on each page, assuming portrait page orientation. Then you need to just adjust your code to do something like this (as pseudo-code):</p>

<pre><code>page1 &lt;- ggplot(steps[0:30]), aes(x=factor(edu))) + geom_bar(aes(y = (..count..), group = id_study,)) + facet_wrap(~id_study,)

page2 &lt;- ggplot(steps[30:60]), aes(x=factor(edu))) + geom_bar(aes(y = (..count..), group = id_study,)) + facet_wrap(~id_study,)

page3 &lt;- ggplot(steps[60:90]), aes(x=factor(edu))) + geom_bar(aes(y = (..count..), group = id_study,)) + facet_wrap(~id_study,)

page4 &lt;- ggplot(steps[90:114]), aes(x=factor(edu))) + geom_bar(aes(y = (..count..), group = id_study,)) + facet_wrap(~id_study,)
</code></pre>

<p>Then continue to export the images as you require, for example:</p>

<pre><code>ggsave('./first_page_plots.pdf', page1, dpi=600)
</code></pre>

<p>check out more <a href=""http://ggplot2.tidyverse.org/reference/ggsave.html"" rel=""nofollow noreferrer"">save options here</a>.</p>
","0","2","45264","13013"
"33384","<p>You are correct to keep punctuation if you want to be able to predict it.</p>

<p>Tokenization of your input should actually work for any character, be it a letter or punctation. In fact there have already been exmaples of people modelling mathematics using Word2Vec and then generating very realistic maths, via $\LaTeX$ !  Look at the subsection called Algebraic Geometry in <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow noreferrer"">Karpathy's now famous blog post</a>.</p>

<p>There is a good <a href=""https://stackoverflow.com/questions/46290238/how-to-deal-with-punctuations-in-machine-translation"">note on the matter here</a>, whcih a specific example given in seq2seq learning (basically translation within the realm of NLP). Be sure to read the comments on the accepted answer there.</p>

<p>to answer your final question, I don't think it would be possible to use your generated model to place to required punctuation back into your model, as something like an LSTM would not have a representation for, say a comma, as it had never seen one.</p>
","0","2","45264","13013"
"33385","<p>I would suggest trying to do it in batches. The underlying issue could well still be memory related in some way, as the <code>merge</code> method makes copies of its input and so is not memory efficient at all.</p>

<p>As an example, you could read in 10 files, create the desired output, as you have done already. Repeat this for files 10-20, then 20-30 then 30-40. The finally four the four files that you have created. It is a bit of an annoyance, but sometimes these little workarounds get the job done.</p>

<hr>

<p><strong>[EDIT]</strong></p>

<p>Another option might be to use the more involved memory management during reading, option via <a href=""http://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking"" rel=""nofollow noreferrer"">the <code>chunksize</code> argument</a> of <code>pd.read_csv()</code>. This will read parts of the file into memory in chunks, as the name suggests. If you do this in a loop, it should put an upper limit on the memory usage. For example (untested):</p>

<pre><code>chunksize = 50e6        # 50 Mb

for single_file in list_of_file_paths:
    for i, chunk in enumerate(pd.read_csv(single_file, chunksize=chunksize):
        if i == 0:
            result = chunk
        else:
            result = pd.merge((result, chunk), copy=False, how= ...)
</code></pre>

<p>You may need to do something with the indivdual chunks before merging them.</p>

<p>Additionally, note that I set the <code>copy</code> argument in <code>merge</code> to <code>False</code>, which might help - <a href=""http://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer"">the documentation</a> is a little vague as to how it saves memory.</p>
","2","2","45264","13013"
"33414","<p>I would say that option 1 will not work out too well: in my experience, the model will either only be good for the first or last model you train, depending on how much freedom you give the algorithm to change weights as time goes on (e.g. with the learning rate).</p>

<p>You really need to decide what you are going to be predicting. Is it the pollution level for a single city: Which features do you have for each city?</p>

<p>It could can make sense to train all cities at the same time if the features you have are also general ones that really can explain the target variable. So if you have temperature, humidity, some transport statistics for that city etc. then training everything together could make sense.</p>

<p>I would think about each sample leading to one target pollution level, and if that sample has enough information (based on the features) to distinguish itself from samples of the other cities, the model should pick up on and leverage those subtleties in the data.</p>
","2","2","45264","13013"
"33420","<p>You are correct in saying that it would be unfair - and if avoidable, you shouldn't do it.</p>

<p>In order to truly be able to claim (in a statistical sense) that a model achieves e.g. 90% accuracy, the test must be performed on unseen data. That is where your test data should be used.</p>

<p>Training a neural network requires the validation data (as you mentioned, within the <code>fit_generator</code> method of a Keras model), in order to compute errors and steer the weights in the right direction. The final accuracy you report needs to be on data to which the the training pipeline has never been exposed.</p>

<p>Creating a training/validation/test split is advised where possible; however, it can sometimes be a challenge, due to things like lack of data and <a href=""https://www.quora.com/What-is-an-imbalanced-dataset"" rel=""nofollow noreferrer"">imbalanced datasets</a>. You can try things such as cross-validation - here is <a href=""https://www.kaggle.com/stefanie04736/simple-keras-model-with-k-fold-cross-validation"" rel=""nofollow noreferrer"">an example using Keras</a>. Her is <a href=""https://datascience.stackexchange.com/questions/11747/cross-validation-in-keras"">another example</a>, in a similar question.</p>
","1","2","45264","13013"
"33443","<p>The first answer in your <a href=""https://datascience.stackexchange.com/a/27282/45264"">commented link</a> answers one point about how region proposals are selected. It is the Intersection Over Union (more formally <a href=""https://en.wikipedia.org/wiki/Jaccard_index"" rel=""nofollow noreferrer"">the Jaccard Index</a>) metric. So how much of your anchor overlaps the label. There is usually a lower limit set for this metric to then filter out all the useless proposals, and the remaining matches can be sorted, choosing the best.</p>

<hr>

<p>I recommend reading through this <a href=""https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46"" rel=""nofollow noreferrer"">excellently explained version of a proposal network</a> - Mask-R-CNN (Masked Region-based CNN).
If you prefer looking at code, there is <a href=""https://github.com/matterport/Mask_RCNN/"" rel=""nofollow noreferrer"">the full repo here</a>, implemented in Keras/Tensorflow (there is also a PyTorch implementation linked somewhere).</p>

<p>There is even <a href=""https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/inspect_model.ipynb"" rel=""nofollow noreferrer"">an explanatory Jupyter notebook</a>, which might help make things click for you.</p>
","4","2","45264","13013"
"33445","<p>The <code>&lt;unk&gt;</code> tags can simply be used to tell the model that there is <em>stuff</em>, which is not semantically important to the output. This is a choice made via the selection of a dictionary. If the word is not in the dictionary we have chosen, then we are saying we have no valid representation for that word (or we are simply not interested).</p>

<p>Other tags are commonly used to groups such thing together, not only <strong>(j)unk</strong>.</p>

<p>For example <code>&lt;EMOJI&gt;</code> might replace any token that is found in our list of deined emojis. We are keeping some information, i.e. that there is a symbol representing emotion of some kind, but we are neglecting exactly which emotion. You can think of many more examples where this might be helpful, or you just don't have the right (labelled) data to make the most of the contents semantically.</p>
","1","2","45264","13013"
"33600","<p>So it seems that you would like to compute the <a href=""https://en.wikipedia.org/wiki/Sensitivity_and_specificity"" rel=""nofollow noreferrer""><strong>specificity</strong></a> of your model:</p>

<p><span class=""math-container"">$${\displaystyle \mathrm {TNR} ={\frac {\mathrm {TN} }{N}}={\frac {\mathrm {TN} }{\mathrm {TN} +\mathrm {FP} }}}$$</span></p>

<p>So you would require a function that can take your predictions, compute the number of true negative and false positive, then compute the <em>specifictiy</em> using the equation above. The body of this function is <a href=""https://stackoverflow.com/a/48087308/3126298"">borrowed from here</a> and simply modified for two classes.</p>

<pre><code>import numpy as np
import keras.backend as K

def compute_binary_specificity(y_pred, y_true):
    """"""Compute the confusion matrix for a set of predictions.

    Parameters
    ----------
    y_pred   : predicted values for a batch if samples (must be binary: 0 or 1)
    y_true   : correct values for the set of samples used (must be binary: 0 or 1)

    Returns
    -------
    out : the specificity
    """"""

    check_binary(K.eval(y_true))    # must check that input values are 0 or 1
    check_binary(K.eval(y_pred))    # 

    TN = np.logical_and(K.eval(y_true) == 0, K.eval(y_pred) == 0)
    FP = np.logical_and(K.eval(y_true) == 0, K.eval(y_pred) == 1)

    # as Keras Tensors
    TN = K.sum(K.variable(TN))
    FP = K.sum(K.variable(FP))

    specificity = TN / (TN + FP + K.epsilon())
    return specificity
</code></pre>

<p><strong>Edit:</strong> this function gives results equivalent to a numpy version of the function and is tested to work for 2d, 3d, 4d and 5d arrays.</p>

<p>As the link you added suggests, you must also create a wrapper function to use this custom function as a loss function in Keras:</p>

<pre><code>def specificity_loss_wrapper():
    """"""A wrapper to create and return a function which computes the specificity loss, as (1 - specificity)

    """"""
    # Define the function for your loss
    def specificity_loss(y_true, y_pred):
        return 1.0 - compute_binary_specificity(y_true, y_pred)

    return specificity_loss    # we return this function object
</code></pre>

<p>Note the the specificity <strong>loss</strong> is returned from the wrapper function as <span class=""math-container"">$1 - specificity$</span>. This could have been performed in the first function too - it should matter, I just separated the computation of specificity from that off the loss.</p>

<p>This can then be used like this:</p>

<pre><code># Create a Keras model object as usual
model = my_model()

# ... (add layers etc)

# Create the loss function object using the wrapper function above
spec_loss = specificity_loss_wrapper()

# compile model using the return los function object
model.compile(loss=spec_loss)

# ... train model as usual
</code></pre>

<hr>

<p>Additionally, you could try importing Tensorflow itself and use its <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/confusion_matrix"" rel=""nofollow noreferrer"">built-in tf.confusion_matrix operation</a>.</p>
","2","2","45264","13013"
"33612","<p>I haven't seen this discussed anywhere really, so an interesting thought.</p>

<p>I think the answer will be that it doesn't matter. I think you should stick to one format (not important which, but don't mix them). Once the images are read into memory for processing and training a model, they are going to just be numbers in an array, regardless of the format they were loaded from.</p>

<p>Perhaps the numbers will differ slightly, but we usually normalise the input to something like the range [0, 1], so that wouldn't matter.</p>

<p>If your file formats change the image in a way, such that some features of the image are different (like the hue), then perhaps results will differ slightly, but the contents and spatial positioning of objects remain the same relative to themselves.</p>

<p><strong>EDIT:</strong></p>

<p>I should mention that converting between ""image types"" is nothing more than using a different compression algorithm to store the data. So when a photo is taken, the camera software compresses the raw sensor data into the target format (jpg, png, ...) and that format will have characteristic artefacts. Perhaps we don't recognise them, but they are there, and there is research done trying to reverse the information loss due to such compression algorithms. Here just <a href=""http://www.cns.nyu.edu/pub/lcv/simoncelli97b.pdf"" rel=""nofollow noreferrer"">a random example</a>.</p>
","1","2","45264","13013"
"33623","<p>The results you are getting are the following:</p>

<pre><code>[[1.85572928e-04]  =  0.000185572928         ~ 0
 [9.99755942e-01]  =  0.999755942            ~ 1
 [5.21248255e-09]  =  0.000000000521248255   ~ 0
 [9.99767481e-01]  =  0.999767481            ~ 1
 [9.99963580e-01]  =  0.999963580            ~ 1
 [2.07334909e-04]] =  0.000207334909         ~ 0
</code></pre>

<p>These are indeed very close to your expected results. You are computing and predicting floating point numbers, and not binary zeros and ones. You could for example add a simply rule that will accepts values below a threshold to be zero and above the threshold to be one.</p>
","1","2","45264","13013"
"33640","<p>We are going backwards in the sense that we are <em>upsampling</em> and so doing the opposite to a standard conv layer, like you say, but we are more generally still moving forward in the neural network. For that reason I would add the bias after the convolution operations. This is standard practice: apply a matrix dot-product (a.k.a affine transformation) first, then add a bias before finally applying a non-linearity.</p>

<p>With a transpose convolution, we are not exactly reversing a forward (<em>downsampling</em>) convolution - such an operation would be referred to as the inverse convolution, or <a href=""https://en.wikipedia.org/wiki/Deconvolution"" rel=""nofollow noreferrer"">a deconvolution,  within mathematics</a>. We are performing a (transpose) convolution operation that returns the same input dimensions that produced the activation map in question, with no guarantee that the actual values are identical to the original input.</p>

<p>You can see from the <a href=""https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md"" rel=""nofollow noreferrer"">animations of various convolutional operations here</a>, that the transpose convolution is basically a normal convolution, but with added dilation/padding to obtain the desired output dimensions. The trick is to retain the mappings of localisation between the pixels.</p>

<p>In <a href=""https://arxiv.org/pdf/1603.07285.pdf"" rel=""nofollow noreferrer"">the paper from which those animations are taken</a>, they explain how a transpose convolution is essentially the convolution steps performed in reverse:</p>

<blockquote>
  <p>..., although the kernel <strong>w</strong> defines a convolution whose forward and
  backward passes are computed by multiplying with $\textbf{C}$ and $\textbf{C}^T$ respectively, it also defines a transposed convolution whose forward and backward passes are computed by multiplying with $\textbf{C}^T$ and $(\textbf{C}^T)^T = \textbf{C}$ respectively.</p>
</blockquote>

<p>One other source to back up my opinion: in <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.ConvTranspose2d"" rel=""nofollow noreferrer"">the PyTorch implementation</a> it seems the bias is added the output of the convolution's result.</p>
","2","2","45264","13013"
"33642","<p>I couldn't think of a situation, where <code>np.array</code> would return a map, however if your input <code>ranks</code> is a map, it seems you do get an array of <code>map</code> objects back!</p>

<p>Here is an example:</p>

<pre><code>In [1]: ranks = map(lambda x, y: x + y, [1, 2, 3, 4], [2, 3, 4, 5])

In [2]: ranks
Out[2]: &lt;map at 0x7f79d79c0e10&gt;

In [3]: np.array(ranks)
Out[3]: array(&lt;map object at 0x7f79d79c0e10&gt;, dtype=object)

In [4]: 1 * ranks
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-34-ff69339d7f75&gt; in &lt;module&gt;()
----&gt; 1 1 * ranks

TypeError: unsupported operand type(s) for *: 'int' and 'map'
</code></pre>

<p>I create a map object (just adds the two lists element-wise), then create a numpy array of that map. If I multiply it by an integer, I get your error.</p>

<p>You need to therefore make sure that your input argument <code>ranks</code> is not a map object. If you provide more information as to what it is, maybe I can help convert it as necessary.</p>
","1","2","45264","13013"
"33646","<p>I assume you have some training data with labels, i.e. data where the titles are already linked to a given class? This is then <em>supervised learning</em> (as opposed to <em>unsupervised learning</em>), and so you could folow the following steps:</p>

<p><strong>Step 1:</strong> you have words as input, so you will need a method to create numerical representation (vectors). For that you could look into algorithms such as <a href=""https://datascience.stackexchange.com/questions/20076/word2vec-vs-sentence2vec-vs-doc2vec"">Word2Vec, Doc2Vec</a>, <a href=""https://nlp.stanford.edu/projects/glove/"" rel=""nofollow noreferrer"">GLoVE</a> or something like <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""nofollow noreferrer"">TF-IDF</a>. If you go for the first, you might consider trying <a href=""https://spacy.io/usage/vectors-similarity"" rel=""nofollow noreferrer"">the spaCy library</a> in python. Here is <a href=""https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/"" rel=""nofollow noreferrer"">a tutorial on Word2Vec using spaCy</a>.</p>

<p><strong>Step 2:</strong> once you have your numerical representations for each of your titles, you need to somehow classify them. You could do this a few ways. Perhaps the simplest would be something like a clustering algorithm, e.g. the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"" rel=""nofollow noreferrer"">DB-Scan algorithm in SciKit Learn</a> - here is a <a href=""http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py"" rel=""nofollow noreferrer"">demo</a>.
You could try more complicated methods, such as Support Vector Machines or Neural Networks, but probably best to start with a method that will get you to some results more quickly. You are classififying titles, so be sure to form your problem as a <a href=""https://medium.com/simple-ai/classification-versus-regression-intro-to-machine-learning-5-5566efd4cb83"" rel=""nofollow noreferrer"">classification as opposed to a regression problem</a>.</p>

<p><strong>Step 3:</strong> assess your results and try changing a part of the loop above.</p>

<hr>

<p>In the above, I assumed you are talking about the <strong>semantic meaning</strong> of the conference titles, and not similarity between literal word/letter combinations. That could of course be computed analytically, without the use of a model that <em>learns</em>.</p>

<hr>

<p><strong>In response to OP's comment:</strong>
From my experience, using TF-IDF or something called <em>minimal new sets</em> might be a good way to get your titles into representations that allow clustering. Once clusters are formed, it would be up to you to then interpret them and assign labels. If you know that there are e.g. only 10 conference, it shouldn't be too difficult to reach results. <a href=""http://publications.lib.chalmers.se/records/fulltext/253923/253923.pdf"" rel=""nofollow noreferrer"">Have a look at this master thesis</a> that does a similar thing - instead of conferences, they want to detect topics. <em>Disclaimer: I supervised that thesis</em>.</p>
","1","2","45264","13013"
"33664","<p><strong>Some question you might want to think about:</strong></p>

<p>Is your dataset big enough? What kind of data is it? Time series? Should you be shuffling? What learning rate are you using? Can you change it and see the effect on the learning curves: <code>plot(history)</code> output?</p>

<hr>

<p>With regards to your model, you usually leave the last connected layers joined only by a linear activation (i.e. don't use an activation function, just an identity matrix). You have used the sigmoid all the way through, which is fine, but not for final layer! I have corrected this by not including such a non-linearity, rather the softmax activation, which will squash all values into the range of [0, 1], so they can be interpreted as probabilities. </p>

<p>I have increased the number of layers and neurons in the initial layers, and swapped in the preferred non-linearity: <a href=""https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"" rel=""nofollow noreferrer"">the Rectified Linear Unit (ReLU)</a>. I would recommend having a quick read of <a href=""http://cs231n.github.io/neural-networks-1/"" rel=""nofollow noreferrer"">this intro from Stanford's CS231n course</a>, which covers some of the best practices. Also, have a <a href=""https://keras.rstudio.com/articles/examples/cifar10_cnn.html"" rel=""nofollow noreferrer"">look here for a sample of a larger network</a> performing classification, to see how R Keras is best used.</p>

<p>Give the following code a test, and also plot the history to get more intuition as to how the training is progressing: how many epochs might be necessary, whether you are over- or underfitting, etc.</p>

<pre><code>model &lt;- keras_model_sequential() %&gt;%
    layer_dense(units = 200, activation = ""relu"", input_shape = 39) %&gt;%
    layer_dense(units = 100, activation = ""relu"") %&gt;%
    layer_dense(units = 100, activation = ""relu"") %&gt;%
    layer_dense(units = 50, activation = ""relu"") %&gt;%
    layer_dense(units = 2, activation = ""softmax"")


model %&gt;% compile(
              optimizer = ""rmsprop"",
              loss = ""categorical_crossentropy"",
              metrics = c(""accuracy"")
          )

history &lt;- model %&gt;% keras::fit(
                                x_train,
                                y_train,
                                epochs = 500,
                                batch_size = 16,
                                validation_split = 0.7, shuffle = T
                            )


plot(history)                           # Are we overfitting?
</code></pre>

<p><strong><em>Disclaimer:</em></strong> If you don't have much data, this model might be even worse that your original one - or you will massively overfit and get 100% training accuracy and terrible validation/test accuracy.</p>

<p>It could well be the case that your data is just better modelled with a simpler model, or that you do not have enough data to get a neural network to hone in on a nice optimum in its optimisation.</p>
","2","2","45264","13013"
"33686","<p>Theoretically, you do indeed have a linear model, yes.</p>

<p>You have a linear relationship between the dependent variable and your parameters.
You would still be going through the process of performing linear regression, fitting a line through points via a linear combination of some regressors. However you have a non-linear <em>equation</em> due to the higher-order regressors you have manually inserted (<code>x1x2</code> and <code>x2^3</code>).</p>

<p>This is enough to still call it a linear model generally speaking - see <a href=""https://stats.stackexchange.com/questions/32477/can-i-use-a-variable-which-has-a-non-linear-relationship-to-the-dependent-variab"">some useful answers in this thread</a>.</p>

<p>In general I wouldn't say you have a linear model in the <strong>strictest</strong> sense at the end, as you are modelling linear relationship between your dependent variable <code>y</code> and non-linear combinations of your regressors: <code>x</code>, <code>x2</code>, and <code>x3</code>, but perhaps there could be an underlying feature to be observed which is exactly equal to <code>x1x2</code>, call it <code>x4</code>, and then you would have removed one of the non-linear covariates. Tyhe same goes for the final non-linear term.</p>
","1","2","45264","13013"
"33727","<p>Yes, there is a french model free and ready to use via the spaCy package!</p>

<p><a href=""https://spacy.io/models/fr"" rel=""nofollow noreferrer"">Here are the small amd medium sized models, that should be ready to go</a>.</p>

<p>Here is the basic summary of the dataset, shown at the spaCy website:</p>

<p><a href=""https://i.stack.imgur.com/ADqKj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ADqKj.png"" alt=""french spacy model details""></a></p>
","4","2","45264","13013"
"33735","<p>If you are talking about time-series analysis, your test data should be the three columns you want to predict, but shifted by some amount. So the first batch (e.g. first 5 rows) predicts the 6th row. Here is a small diagram showing what I mean:</p>
<p><a href=""https://i.stack.imgur.com/2t024.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2t024.png"" alt=""5 rows predict the following row"" /></a></p>
<p>Notice that the timestamp column not in the target, and that we predict the three features in the following row, given an input of 5 rows.</p>
","0","2","45264","13013"
"33760","<p>If you goal is simply to halve the size of your filters, you could think about using some different methods other than padding, such as dilated convolutions. <a href=""https://arxiv.org/pdf/1603.07285.pdf"" rel=""nofollow noreferrer"">Have a look at this paper for some ideas and nice explanations with pictures.</a> Just thinking about your dimensions quickly, I am not sure you could go from 14 to 7 very easily. getting to 8 or 5 is simple enough though.</p>

<hr>

<p>One thing that isn't always obvious if you just started learning KEras, is that you can mix in Tensorflow operations directly from the Tensorflow library in with your Keras code.</p>

<p>In TF there is <a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""nofollow noreferrer"">a function called <code>pad</code></a>, which allows you to specify the padding manually on each side of a tensor. There are also options to say whether the padding is done with zeros, or if the values inside your original tensor are repeated/mirrored (using the <code>mode</code> argument).</p>

<p>You could try using this to pad the layers. I can show you how to pad the tensors to get the effect you want:</p>

<pre><code>from keras.models import Sequential, Model
from keras.layers import (Input, Conv2D, MaxPooling2D, 
                          Flatten, Dense, Reshape, Lambda)
import tensorflow as tf

input_shape = (448, 448, 3)
batch_shape = (None,) + input_shape
raw_input = tf.placeholder(dtype='float16', shape=batch_shape)
paddings = tf.constant([[0, 0],   # the batch size dimension
                        [3, 3],   # top and bottom of image
                        [3, 3],   # left and right
                        [0, 0]])  # the channels dimension

padded_input = tf.pad(raw_input, paddings, mode='CONSTANT',
                      constant_values=0.0)  # pads with 0 by default

padded_shape = padded_input.shape     # (454, 454, 3) because we add 2*3 padding

input_layer = Input(padded_shape, batch_shape, tensor=padded_input)
layer0 = Conv2D(192, kernel_size = (3,3), padding='valid')(input_layer)
layer1 = MaxPooling2D(pool_size=(2, 2), strides = 2)(layer0) 
layer2 = Conv2D(192, kernel_size = (3,3), padding='valid')(layer1)
layer3 = MaxPooling2D(pool_size = (2,2),strides = 2)(layer2)
layer4 = Conv2D(512, kernel_size = (3,3), padding='valid')(layer3)
layer5 = MaxPooling2D(pool_size = (2,2), strides = 2)(layer4)
layer6 = Conv2D(256, kernel_size = (1,1), padding='valid')(layer5)
# layer6.shape --&gt; [Dimension(None), Dimension(55), Dimension(55), Dimension(256)]

layer6_padded = tf.pad(layer6, paddings, mode='CONSTANT')
layer6_output = Input(input_shape=layer6_padded.shape)(layer6_padded)

# This will end up giving this error at compilation:
# RuntimeError: Graph disconnected: ...,

layer7 = Conv2D(1024, kernel_size=(3, 3), strides=2)(layer6_output)
layer8 = Flatten()(layer7)
layer9 = Dense(4096)(layer7)
layer10 = Dense(7*7*30)(layer8)
output_layer = Reshape((7, 7, 30))(layer10)

# The following both fail to get the graph as we would like it
model = Model(inputs=[input_layer], outputs=[output_layer])
#model = Model(inputs=[input_layer, layer6_output], outputs=[output_layer])

model.compile('adam', 'mse')
model.summary()
</code></pre>

<p>I have been unable to then bring this tensor back into the Keras model (as a layer, which is required) because the standard way of using the <code>Input</code> object forces it to be the entry point of the computational graph, but we want the padded tensors to form an intermediary layer.</p>

<p>If you don't force the padded tensors into a Keras layer, attributes will be missing:</p>

<pre><code># AttributeError: 'Tensor' object has no attribute '_keras_history'
</code></pre>

<p>Which you can hack by just adding the attribute from the layer before we padded:</p>

<pre><code>#layer6_output._keras_history = layer6._keras_history
</code></pre>

<p>Unfortunately, I still ran into other errors.</p>

<p>Perhaps you can post <a href=""https://stackoverflow.com/questions/tagged/tensorflow+keras"">a new question on StackOverflow asking how to to this, if you can find anything</a>. I did have a quick try using <a href=""https://blog.konpat.me/tf-connecting-two-graphs-together/"" rel=""nofollow noreferrer"">the idea of creating two graphs and then joining them</a>, but didn't succeed.</p>
","2","2","45264","13013"
"33805","<p>Perhaps you could frame this as predicting a distribution (I hope I haven't mis-understood your question!).</p>

<p>You could try using Markov Chain Monte Carlo (don't be scared by the name!). What this essentially does, it to sample a posterior distribution from your input data and see how likely it was that is is related to your prior distribution (your data). If it thinks the sample fits with your data, it keeps puts it records that sample, otherwise it discards it. It keeps searching like this until some stop-criterion is reached. All the accepted points that were recorded then represent your target distribution, which can be N-dimensional (and so hopefully matches your use-case).</p>

<p>Here is a <a href=""https://www.youtube.com/watch?v=12eZWG0Z5gY"" rel=""nofollow noreferrer"">quick video introduction</a> to the topic in general, and here is a <a href=""https://www.youtube.com/watch?v=h1NOS_wxgGg"" rel=""nofollow noreferrer"">deeper video explanation</a>. Here is a <a href=""https://people.duke.edu/~ccc14/sta-663/MCMC.html"" rel=""nofollow noreferrer"">more thorough text-based introduction</a>.</p>

<p>You can get started doing this with packages like:</p>

<ul>
<li>PyMC3 (<a href=""https://github.com/pymc-devs/pymc3"" rel=""nofollow noreferrer"">code</a> and <a href=""http://docs.pymc.io/"" rel=""nofollow noreferrer"">docs</a>) if you use Python. There are <a href=""https://mybinder.org/v2/gh/pymc-devs/pymc3/master?filepath=%2Fdocs%2Fsource%2Fnotebooks"" rel=""nofollow noreferrer"">interactive examples here, e.g. the PyMC3_tips_and_heuristic notebook</a> too.</li>
<li>If you use R, then there is the <a href=""https://cran.r-project.org/web/packages/mcmc/index.html"" rel=""nofollow noreferrer"">mcmc package</a>.</li>
</ul>
","1","2","45264","13013"
"33815","<p>How many images are you using to train?  You might need more... let's say  at least 1000 for a single category - depending on how big your MobileNet is (there are different variants).</p>

<p>Are you using a pretrained MobileNet, or are you training it from scratch. If Mac'n Cheese was not a category in the original data used for a pre-trained model, then you might need to train from scratch, or only keep the first few layers of the model frozen, as they learn very general features from images that will liekly be useful for your target class (and many other). If you are unsure, search for <strong>transfer learning</strong>, to see what is possible.</p>

<p>The nominal value of the loss is quite hard to interpret, so values of 0.3 or 1.0 don't mean a lot without context. You can use them for comparison - so if you change something and loss values then start dropping to 0.005, you know that you are likely onto a good idea.</p>

<p>You can check out this <a href=""https://github.com/tensorflow/models/blob/5bb9e6f349e22270420dd637f3fa89260ab5b441/research/slim/nets/mobilenet/mobilenet_example.ipynb"" rel=""nofollow noreferrer"">tutorial notebook on MobileNet</a>, which might give you more ideas about how to play with the network</p>
","1","2","45264","13013"
"33832","<p>There are a few things that you need to be careful with here.</p>

<p>You can do certain things when preprocessing data or performing data augmentation that can be applied across an entire dataset (train and validation). <strong><em>The main idea is not to allow the model to gain insight from the test data.</em></strong></p>

<hr>

<h3>Time-series example</h3>

<p>Missing data can be managed in many ways, such as simple imputation (filling the gaps). This is very common in time-series data. In your training data, you can fill the gaps using the previous value, the following value, the average of the data or something like the moving average. Where you must be careful is with violating the information flow through time. For example, in your test data, you should not fill gaps using a method that looks at data points <em>in front</em> of the emtpy time slot. This is because, at that point in time, you will not be able to do the same as you shouldn't know the future values.</p>

<h3>Image data example</h3>

<p>Looking instead at image data, there are data-preprocessing steps such as normalisation. This means just scaling the image pixel values to a range like $[-1, 1]$. To do this, you must compute the population mean and variance, which you then use to perform the scaling. When computing these two statistics, it is important not to include the test data. The reason is that you would be <em>leaking</em> information the dataset that is then used to train a model. Your technically knows things that it shouldn't; in this case, clues regarding the mean and variance of the target distribution.</p>

<hr>

<p>People might also consider ""missing data"" to include imbalanced datasets; i.e. there are cases that you know of, but just don't appear in your dataset very often. There are some tricks to help with this, such as stratified sampling or cross-validation. The optimal solution would, of course, be to gather a dataset that more closely represents the problem at hand.</p>
","1","2","45264","13013"
"33833","<p>Regarding your second question:</p>

<blockquote>
  <p>The users who did not have and tweet for the time range, is this okay to assign difference in mean zero, or I should exclude them from the samples?</p>
</blockquote>

<p>You essentially have missing data in this case. How you can deal with this will depend on the model you are using, and if it is robust to missing data. If the model can ignore $\mu = 0, \sigma = 0$ values, then try it out. Otherwise, you might want to leave them out as you suggest, or perhaps even <strong>impute</strong> them with their previous known values.
If you are e.g. using something like an ARIMA model, then it keeps track of a moving average. In this case, using zero values will have an undesired impact (assuming zeros are not common in general).</p>

<hr>

<p>I'm not sure I understand what you are asking in your first question. What have you tried already? Have you got some results?</p>
","0","2","45264","13013"
"33834","<p>You can simply make your training data a shape that <strong>is</strong> divisible by 50.
    In [1]: sample_size = 63648</p>

<pre><code>In [2]: to_remove = sample_size % 50

In [3]: end_index = sample_size - to_remove

In [4]: model.fit(trainX[:end_index], trainY[:end_index], ...
</code></pre>

<hr>

<p>What are the dimensions of <code>trainX</code> and <code>trainY</code>? You are not specifying a validation dataset, is that intentional?</p>

<p>You say you want to reset the state after a mini-batch, but you are resetting the state after each epoch (one loop through your entire dataset).</p>

<p>One more comment: the <code>n_epochs = 10000</code> seems excessive.</p>
","0","2","45264","13013"
"33858","<p>I think you might be a little confused with the terminology here.</p>

<p>Whether you train locally on your own computer <strong>or</strong> in the <em>cloud</em> (via AWS, Microsoft Azure, GoogleCloud, FloydHub, ...) there are only <s>two</s> three options:</p>

<ol>
<li>using a CPU $\rightarrow$ whatever is in your computer!</li>
<li>using a GPU $\rightarrow$ probably an <a href=""https://developer.nvidia.com/cuda-gpus"" rel=""nofollow noreferrer"">Nvidia card supportung CUDA</a></li>
<li><a href=""https://cloud.google.com/tpu/docs/quickstart"" rel=""nofollow noreferrer"">using a TPU</a> $\rightarrow$ a type of GPU to accelerate Machine Learning)</li>
</ol>

<p>Your deep learning framework of choice (Pytorch, Tensorflow, CNTK, ...) will make use of the hardware with which you provide it.</p>

<p>All cloud services will offer <em>instances</em> with or without GPUs (they will almost vertainly have a CPU).</p>

<p>Training on a GPU is of course much faster thatn a CPU, as it can parallelise many of the operations that are performed (especially for CNNs, compared to e.g. LSTMs). It is of course possible to offload certain parts on training onto each of them, so you would be using both together.</p>

<p>A GPU in your computer at home and GPU in a big server-rack in an AWS data-center would both perform as equally as can be hoped for. The only differences will be cost, but that is a different discussion.</p>
","1","2","45264","13013"
"33994","<p>Residual errors are the errors that remain after a model has tried fitting to some data. It is the error which <em>resides</em>.</p>

<p>People use that term, along with just <em>error</em> or <em>residuals</em> interchangeably, but after a model has been tested, it just means how much of the data cannot be explained by the model.</p>

<p>The letter $\epsilon$ is commonly used to explain stochastic noise inherent in (co)-variates of a model, i.e. noise.error that we cannot explain with the given data.</p>
","1","2","45264","13013"
"34034","<h3>One way to look at this is through the idea of <strong>under-/overfitting</strong></h3>

<p>First off, here is a sketch of the generally observed relationship between bias and variance, in the context of model size/comlpexity:</p>

<p><a href=""https://i.stack.imgur.com/JIYOI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JIYOI.png"" alt=""bias-variance trade-off""></a></p>

<p>Say you have a model which is learning quite well, but your test accuracy seems to be pretty low: 80%. The model is essentially not doing a great job of mapping input features to outputs. We have a high bias. But for a wide variety of input (assuming a good test set), we consistently obtain this 20% error; we have a low variance. We are <strong>underfitting</strong></p>

<p>Now we decide to use a bigger model (e.g. a deep neural network), which is able to capture more details of the feature space and so maps inputs to outputs more accurately. We now have an improved test accuracy: 95%. At the same time, we notice that several runs of the model produce different results; sometime we have 4% error, and sometimes 6%. We have introduced a higher amount of variance. We are <em>perhaps</em> somewhere around the <strong>optimum model complexity</strong> shown on the graph above.</p>

<p>You say ok... let's create a monolithic neural network. It totally nails training and ends with a perfect accuracy: 100%. However, the test accuracy now drops to 90%! So we have zero bias, but a large variance.
We are <strong>overfitting</strong>. The model is almost as good as a look-up table for training data, but doesn't generalise at all when it sees new samples. Intuitively, that 10% error corresponds to a difference in distribution between the training and test sets used <span class=""math-container"">$\rightarrow$</span> the model knows the training distribution in extreme detail, some of which do not apply to the test set (i.e. the reality). </p>

<p><strong>In summary:</strong> The bias <em>tends</em> to decrease faster than the variance increases, because you can likely still make a more competitive model for your dataset; the model is <em>underfitting</em>. It is like the low-hanging fruit that you can easily get - so an incremental improvement on the red curve above gives a big decrease in bias (increase in performance). Obviously that pattern cannot go on indefinitely, with each increment in model complexity, you get a lower increase in performance; i.e. you have diminishing returns. Furthermore, as you begin to <em>overfit</em>, the model becomes less able to generalise and so exhibits larger errors on unseen data; variance is creeping in.</p>

<hr>

<p>For some more intuition between bias/variance in machine learning, I'd recommend <a href=""https://www.youtube.com/watch?v=F1ka6a13S9I"" rel=""nofollow noreferrer"">this talk by Andrew Ng</a>. There is also <a href=""https://github.com/thomasj02/DeepLearningProjectWorkflow"" rel=""nofollow noreferrer"">a text summary</a> of the talk, for a quicker overview.</p>

<p>For a brief but more mathematical explaination, head over to <a href=""https://stats.stackexchange.com/questions/336433/bias-variance-tradeoff-math?rq=1"">this post of Cross-Validated</a>. The second answer there is very recent and is perhaps better than the (old) accepted answer.</p>
","7","2","45264","13013"
"34037","<p>To begin with, you can think of the batch size as a way to control the smoothness of the learning curve. With a huge batch size, you are taking the average of many errors for each update, and this average loss (on average), doesn't have great variance.</p>

<p>Using a batch size of 1, your cost on each iteration is solely dependent on the single sample that you fed the network. Each sample is hopefully a little different from the others, which will lead to a very <em>noisy</em> loss curve.</p>

<p>Assuming model parameters are well-suited and the model converges irrespective of batch size, you should reach <em>similar</em> results. However, there are works that analyse batch size and show other trade-off, which are usually training-times, memory consumption and the like.</p>

<p><a href=""https://arxiv.org/abs/1804.07612"" rel=""noreferrer"">This recent paper (Masters, Luschi)</a> analyses the trade-offs of batch size on a standard dataset (CIFAR10). Here is Figure 15 from the paper that shows this pretty succinctly:</p>

<p><a href=""https://i.stack.imgur.com/xpRNi.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xpRNi.png"" alt=""performance based on batch-size""></a></p>

<p><em>Warning: all models and datasets could behave differently, however, so don't take any results like these as fact!</em></p>

<hr>

<p>I know that it is possible to train a model that has varying input shapes by using a technique called <strong>Adaptive Pooling</strong> (<a href=""https://pytorch.org/docs/stable/nn.html?highlight=adaptiveav#torch.nn.AdaptiveAvgPool2d"" rel=""noreferrer"">or adaptive average pooling, in PyTorch</a>, but you would likely have to come up with your own function that is able to do such a thing within the constraints of a <em>stateful</em> LSTM.</p>

<p>So as the shape of your dataset must be divisible by the batch size, there are a few ways to make that a reality:</p>

<ol>
<li><a href=""https://datascience.stackexchange.com/questions/32831/batch-size-of-stateful-lstm-in-keras/32935#32935"">using the highest common multiple</a> of your datasets: so with your example datasets having 4000 days (set A) and 500 days (set B), you would compute the</li>
</ol>

<p>This of course limits the possible choices of sequence length, but for the two examples of 4000 and 500, you could choose from these: <em>1, 2, 4, 5, 10, 20, 25, 50, 100</em></p>

<ol start=""2"">
<li><a href=""https://datascience.stackexchange.com/questions/33831/mini-batch-size-and-reset-states/33834#33834"">Trimming the different datasets</a> so that you get a nice sequence length that works for both</li>
</ol>

<p>This means possible leaving out some of your data, which is undesireable, but it might not be much.</p>

<p>Which of those two possiblities you might go for will depend on your specific dataset sizes. It might be optimal to use both - so trimming a few datasets to allow the computation of a nice <em>Highest Common Factor</em>.</p>

<p>You can compute the HCF in Python using something like this:</p>

<pre><code>def hcf(a, b):
    while b:
        a, b = b, b % a
    return a
</code></pre>

<hr>

<p><strong>One final idea from my comment below:</strong></p>

<p>... in order to change the batch size between datasets, you could copy them model weights from a trained model (from your first dataset), then compile a new model with the batch size required for the next dataset, but set the weights equal to those from the first model. This is almost like a manual way of implementing stateful behaviour. <a href=""https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/"" rel=""noreferrer"">Have a look here for a simple example</a>. </p>
","8","2","45264","13013"
"34045","<p>Since you were working with RegEx, I will ofer a RegEx solution.</p>

<p>I will also show that you need to also take care to first remove punctuation. (I will not go down the rabbit-hole of re-sinserting the punctuation back where it was!)</p>

<h3>A RegEx solution:</h3>

<pre><code>import re
sentence = 'I need need to learn regex... regex from scratch!'

# remove punctuation
# the unicode flag makes it work for more letter types (non-ascii)
no_punc = re.sub(r'[^\w\s]', '', sentence, re.UNICODE)
print('No punctuation:', no_punc)

# remove duplicates
re_output = re.sub(r'\b(\w+)( \1\b)+', r'\1', no_punc)
print('No duplicates:', re_output)
</code></pre>

<p>Returns:</p>

<pre><code>No punctuation: I need need to learn regex regex from scratch
No duplicates: I need to learn regex from scratch
</code></pre>

<ul>
<li><code>\b</code> : matches word boundaries</li>
<li><code>\w</code> : any word character</li>
<li><code>\1</code> : replaces the matches with the second word found - the group in the second set of parentheses</li>
</ul>

<p>The parts in parentheses are referred to as groups, and you can do things like name them and refer to them later in a regex. This pattern should recursively catch repeating words, so if there were 10 in a row, they get replaced with just the final occurence.</p>

<p>Have a look <a href=""https://www.rexegg.com/regex-quickstart.html"" rel=""noreferrer"">here for more detailed definitions</a> of the regex patterns.</p>

<h3>The more pythonic (looking) way</h3>

<p>It has to be said that the <code>groupby</code> method has a certain <strong><a href=""https://www.python.org/dev/peps/pep-0020/"" rel=""noreferrer"">python-zen</a></strong> feel about it! Simple, easy to read, beautiful.</p>

<p>Here I just show another way of removing the punctuation, making use of the <code>string</code> module, translating any punctuation characters into None (which removes them):</p>

<pre><code>from itertools import groupby
import string

sentence = 'I need need to learn regex... regex from scratch!'

# Remove punctuation
sent_map = sentence.maketrans(dict.fromkeys(string.punctuation))
sent_clean = sentence.translate(sent_map)
print('Clean sentence:', sent_clean)

no_dupes = ([k for k, v in groupby(sent_clean.split())])
print('No duplicates:', no_dupes)

# Put the list back together into a sentence
groupby_output = ' '.join(no_dupes)
print('Final output:', groupby_output)

# At least for this toy example, the outputs are identical:
print('Identical output:', re_output == groupby_output)
</code></pre>

<p>Returns:</p>

<pre><code>Clean sentence: I need need to learn regex regex from scratch
No duplicates: ['I', 'need', 'to', 'learn', 'regex', 'from', 'scratch']
Final output: I need to learn regex from scratch
Identical output: True
</code></pre>

<h3>Benchmarks</h3>

<p>Out of curiosity, I dumped the lines above into functions and ran a simple benchmark:</p>

<p><strong>RegEx</strong>:</p>

<pre><code>In [1]: %timeit remove_regex(sentence)
8.17 µs ± 88.6 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre>

<p><strong>groupby</strong>:</p>

<pre><code>In [2]: %timeit remove_groupby(sentence)
5.89 µs ± 527 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre>

<p>I had read that regex would be faster these days (using Python3.6) - but it seems that sticking to beautiful code pays off in this case!</p>

<p><em>Disclaimer: the example sentence was very short. This result might not scale to sentences with more/less repeated words and punctuation!</em></p>
","6","2","45264","13013"
"34158","<p>It would make sense that the time-series data sticks together - and so forms these lines you are seeing. In normal time-series analysis where the variables are assumed to be random (e.g. modelled on Brownian motion), the best prediction for tomorrow is just the same as today. t-SNE finds the closest points withing your feature-space and embedding them into a 2D space. It is quite impressive that it picks it out and ends up with your plot!</p>

<p>While you often get circular looking plots, it is not true that you always get circles/spheres. 
t-SNE is maximising the distance between clusters and at the same to minimising the distance between points within a single cluster... for the sake of efficiency, circles arise very often. You can observe this in nature: the shape of planets, of bubbles in water... circles are efficient!</p>

<p>t-SNE does not allow you to directly interpret the distance between clusters back to the input units (e.g. a line twice as high as another doesn't mean the values are twice as big). It would perhaps be interesting to plot the individual time-series lines themselves there (within a single cluster) next to the input time-series data of the same feature, then look for any correspondance.</p>

<p>For more understanding, I would recommend reading <a href=""https://distill.pub/2016/misread-tsne/"" rel=""nofollow noreferrer"">this great walkthrough/visualisation article</a> covering t-SNE. There are a few examples that show non-circular results: </p>

<p><a href=""https://i.stack.imgur.com/9newj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9newj.png"" alt=""almost linear clusteres""></a></p>

<p><strong><em>Wattenberg, et al., ""How to Use t-SNE Effectively""</em></strong></p>
","2","2","45264","13013"
"34225","<p>Like all models, the final outcome will depend heavily on the data you use to train.</p>

<p>Given you don't have many samples for each patient and that the samples are temporally irregular, I imagine you might be lumping all recorded data (from all patients) into a single training set... ? In that case, it will be important to either perform random sampling or something like stratified sampling - both without replacement - to make sure patients don't appear in both train and test sets. Stratification simply means to create training/test splits, which do their best to preserve class balance. So if you select one feature as most important, stratification will make sure that training and test sets each have a proportional number of samples exhibiting that feature.</p>

<p>If you are using R, look into <a href=""https://topepo.github.io/caret/available-models.html"" rel=""nofollow noreferrer"">the Caret package</a> - here is <a href=""https://gist.github.com/Bergvca/c1df8e579005e3cd82e8d3c8b009403a"" rel=""nofollow noreferrer"">an example of usage</a>. That is a very powerful package in general and worth investigating (if you use R or not!).
If you are using Python, have a look at the <a href=""http://scikit-learn.org/stable/modules/classes.html#splitter-classes"" rel=""nofollow noreferrer"">Splitter Classes</a>, which include: Stratified K-folds and Stratified Shuffle Split, among others.</p>

<p>If temporal ordering does play a big role, taking the most recent sample is usually considered best practice. For example, measurements driven by general information flow, e.g. finance, would also consider the most recent measurement as the most valueable for prediction.</p>

<p>This is the case in many applications, but perhaps not with your measurements - in medical data, behavioural/environmental conditions of the patient around the time of measuring may outweigh the recentness of the data.</p>

<p>One more observation, in the you are trying to predict some feature that is very specific to a single patient (and you say you may only have one observation from each patient!), then you might have a hard time making accurate predictions.</p>
","1","2","45264","13013"
"34227","<p>You can pass the entire month's data as a single sample, which should predict the sales of that month. It would look like an index with multiple indexes. Something like <a href=""https://pandas.pydata.org/pandas-docs/stable/advanced.html#creating-a-multiindex-hierarchical-index-object"" rel=""nofollow noreferrer"">MultiIndex in python's Pandas library</a>.</p>

<p>Another idea would be to still try predicting the sales every day, then just sum up all predictions at the end to create your monthly aggregation.</p>

<p>Using something like a neural network may be overkill - it depends how much data you have - but will offer the flexibility to easily train a model with multiple inputs and a single output, as is your case.</p>
","1","2","45264","13013"
"34234","<p>Using a stateful model, am I correct in assuming that you have time-series data?</p>

<p>If so, it would perhaps make sense (at least for your test accuracy) to always validate against the time-step that immediately follows a test batch. This is what you would actually want to be doing with the model once it is trained.</p>

<p>Have a <a href=""https://datascience.stackexchange.com/a/30908/45264"">look at this answer</a>, where I explain the idea in a lot more detail. The main idea is to split data into a kind of <em>rolling forecast</em> pattern, whereby you would have a test batch e.g. with something like:</p>

<pre><code>window_size = 100     #whatever makes sense for your data
val_size = 15

train_batch1 = data[0:window_size]
val_batch1 = data[window_size:window_size + val_size]

train_batch2 = data[1:window_size+1]
val_batch2 = data[window_size + 1:window_size + val_size + 1]

...
</code></pre>

<p>That is just pseudo code to make it as clear as possible. Here is the diagram I created for the post linked above, which visualises the same notion:</p>

<p><a href=""https://i.stack.imgur.com/rfSAF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rfSAF.png"" alt=""rolling window validation""></a></p>

<p>In the image, it just has train/test sets... you could of course add validation into the mix.</p>

<p>If this doesn't seem to make sense for you data, temporal relationships must not matter too much. In that case, perhaps a stateful model is also not entirely useful.</p>
","0","2","45264","13013"
"34257","<p>It's a great question and one I have thought about a lot too.</p>

<p>Are you are talking about a network structure that evolves/mutates during training, or one that simply chooses whether or not to use all available resources? While I would never give a straight-up <strong>No</strong> to such a questions (especially the former), I can't say I have really seen it in publications. The cloest thing that comes to mind is <a href=""https://arxiv.org/abs/1707.06203"" rel=""nofollow noreferrer"">a recent paper from DeepMind</a>, which utilises a combination of ""model-free and model-based aspects""; however, this more high-level than network architectures specifically, as you refer to them.</p>

<p>Perhaps you are considering a model, which is forced to choose between one resource and another? An example of the latter would be whether or not to use a skip connection <em>or</em> a dense layer (assuming shapes all match). Yann LeCun says (the video linked below) that such <em>paths</em> in a network not only provide more flexibility/complexity, but additionally ease the optimisation problem and provide a source of regularisation.</p>

<p>In any case, I am sure people are doing that, making use of <a href=""https://pytorch.org/"" rel=""nofollow noreferrer"">dynamic networks created using PyTorch</a>, the <a href=""https://www.tensorflow.org/api_guides/python/control_flow_ops"" rel=""nofollow noreferrer"">control flow operators in Tensorflow</a>, the new tensorflow <a href=""https://www.tensorflow.org/programmers_guide/eager"" rel=""nofollow noreferrer"">eager library</a> (which aims to replicate dynamic networks like PyTorch produces).</p>

<p>One could also argue that providing a monolithic network to a model affords it the flexibility to use the resources (i.e. the weights) that it chooses. It doesn't have to use them all. Just as <a href=""https://arxiv.org/abs/1502.03167"" rel=""nofollow noreferrer"">batch normslisation layers</a> allow the model to parametrically adjust the extent to which they are used.</p>

<p>There are <a href=""http://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html"" rel=""nofollow noreferrer"">great discussions on what is required</a> in the long run, heading toward Artificial General Intelligence (a recent overview in the context of RL <a href=""https://thegradient.pub/why-rl-is-flawed/"" rel=""nofollow noreferrer"">part1</a> &amp; <a href=""https://thegradient.pub/how-to-fix-rl/"" rel=""nofollow noreferrer"">part2</a>. If we should be training models end-to-end, how much information should we give an algorithm learning to play a game and </p>

<p>The tools that would allow people to do that, like you mention, are coming along, but I haven't yet seen a paper that explicitly outlines how that is done. It would be great is someone else gives you a more satisfying answer! ;-)</p>
","1","2","45264","13013"
"34266","<p>You could have a look at an R package called mboost <a href=""https://cran.r-project.org/web/packages/mboost/mboost.pdf"" rel=""nofollow noreferrer"">(documentation)</a>, which performs standard boosting (fitting a linear model using some features you give it) and the performs a coefficient update for only the feature that contributes to the largest reduction in error.</p>

<p>All coefficients start at zero, so after many iterations, this results in some coefficient with large values, some with small coefficients and normally some with coefficients equal to zero... they were not selected at all. This means you have inherent feature selection during training.</p>

<p><a href=""https://cran.r-project.org/web/packages/mboost/vignettes/mboost_tutorial.pdf"" rel=""nofollow noreferrer"">Check out the tutorial paper, which is very helpful in getting started</a>.</p>

<p>Here is an image, which shows the coefficient development during training:
it shows the names of the features on the right... you can see that some values are still at zero once training has finished.
<a href=""https://i.stack.imgur.com/i1nBZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i1nBZ.png"" alt=""enter image description here""></a></p>

<p>The package has built in functionality for cross-validation, plotting and so on.</p>

<p><strong>EDIT:</strong></p>

<p>You can think of the training process as follows:</p>

<ol>
<li>run a regression on the data</li>
<li>measure which feature was best able to fit (e.g. had smallest error)</li>
<li>this feature ""won the round"" and gets its coefficient in the final equation increased by an amount (e.g. 0.001)</li>
<li>repeat steps 1-3 until a threshold/criterion is met</li>
<li>All features that didn't win a single round can be removed</li>
</ol>

<hr>

<p>How many features do you have? If it isn't too many, you can simply run the model many times, adding/removing a single feature each time. You could also try using some metrics such as BIC (Bayesian Information Criterion) to decide which model explains the data best with the given features.</p>
","3","2","45264","13013"
"34377","<p>If you follow the linked literature (down the rabbit hole for a few levels), you end up at <a href=""https://www.irisa.fr/vista/Themes/Demos/Debruitage/ImageDenoising.html"" rel=""nofollow noreferrer"">a paper from 2005 from Kervann and Boulanger</a>
- at least that's as deep as I got.</p>

<p>In that linked webpage, they define the patch-based image noise reduction methods as such:</p>

<blockquote>
  <p>The main idea is to associate with each pixel the weighted sum of data points within an adaptive neighborhood.</p>
</blockquote>

<p><strong>So a patch is an area of a single image, like a convolutional kernel, but it doesn't convolve</strong>.</p>

<p>They talk about adaptive patches, meaning that you need to (perhaps randomly) select a pixel, then adapt the patch size used in order to include enough surrounding information to reproduce a homogenous patch as the output.</p>

<p>It seems as though training with clean images, to which noise is added (additive Gaussian white noise),  are used to train. This will help with robustness of final models by reducing the variance, but doing so also must introduce a bias to re-create areas where the noise is somehow uniform. The first link above, if you scroll down, shows many examples of typical images to be <em>de-noised</em>. The noise is not always so uniform.</p>

<p>Here is a picture taken from that 2005 paper, where they show patch-regions (marked in yellow). Page 5 gives a nice short description of the general idea. Patch sizes in their work were typically $7 x 7$ or $9 x 9$ in pixel size.</p>

<p><a href=""https://i.stack.imgur.com/SIeAP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SIeAP.png"" alt=""patch examples on noisy image""></a></p>
","1","2","45264","13013"
"34378","<p>I would recommend <a href=""https://web.stanford.edu/~hastie/Papers/ESLII.pdf"" rel=""nofollow noreferrer"">Elements of Statistical Learning</a>, by Trevor Hastie and Rob Tibshirani. That link gets the book directly from their Stanford website.</p>

<p>In addition, <a href=""https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/"" rel=""nofollow noreferrer"">there are also accompanying videos</a> (they follow the structure of a similar book), which are really helpful!</p>

<p>They offer a simpler set of books and online courses, which <a href=""https://web.stanford.edu/~hastie/lectures.htm"" rel=""nofollow noreferrer"">you can find listed here.</a></p>
","1","2","45264","13013"
"34421","<p>I can imagine a value of <code>-1</code> consumes all available resources as and when they become available. Depending on which function you are talking about, it seems data is copied for each of the jobs, which can lead to memory problems if the dataset is large enough. Here is a snippet of information from the docstring of <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"" rel=""noreferrer"">GridSearchCV</a>:</p>

<blockquote>
<pre><code>If `n_jobs` was set to a value higher than one, the data is copied for each
point in the grid (and not `n_jobs` times). This is done for efficiency
reasons if individual jobs take very little time, but may raise errors if
the dataset is large and not enough memory is available.  A workaround in
this case is to set `pre_dispatch`. Then, the memory is copied only
`pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *
n_jobs`.
</code></pre>
</blockquote>

<p>So it might be a good idea to use <code>pre_dispatch</code> to put an upper limit on your memory consumption.</p>

<p>Otherwise, why are you setting it to <code>-1</code>? You should just set it to the number of physical cores on your machine, or maybe 2 times that number, if the task can be multi-threaded. </p>

<p><strong>EDIT:</strong></p>

<p>It seems setting <code>n_jobs=-1</code> does indeed just select all physical cores and maximises their usage. Have a look at <a href=""https://stackoverflow.com/questions/32761556/python-scikit-learn-n-jobs"">the comments in this answer on StackOverflow</a>.</p>

<p>If you have not set <code>pre_dispatch</code>, it will of course try to copy a lot. This is why you run out of memory. If you have 4 cores there will be, by default, 8 copies of the dataset made (as described above in the quote).</p>

<p>Here is <a href=""https://stackoverflow.com/questions/45562465/python-n-jobs-for-large-scale-prob"">another thread</a>, which looks more at the performance side</p>
","8","2","45264","13013"
"35515","<p>You can get a list of all possible words (including the nouns, verbs, adjectives and so on) by using a <em>comprehension</em>. This loops over all elements of a sequence, as with a <code>for-loop</code>, but <a href=""https://stackoverflow.com/questions/39518899/comparing-list-comprehensions-and-explicit-loops-3-array-generators-faster-than/39518977"">can be faster</a>.</p>

<p>In you case, you have a dictionary, so we loop over each <strong>key-value</strong> pair. The keys are <strong>n</strong> (noun), <strong>a</strong> (adjective), <strong>v</strong> (verb) and <strong>r</strong> (??). For each of these keys, we have a list of words, which we can append to your final desired output list, <em>if</em> they aren't already in the list.
Because we loop over a dictionary, and then over each of the lists, with becomes nested:</p>

<pre><code>from word_forms.word_forms import get_word_forms
word_forms = get_word_forms(""review"")

desired_words = []
for word_type, word_list in word_forms.items():
    for word in word_list:
        if word not in desired_words:
            desired_words.append(word)

print(desired_words)
# ['reviewers', 'review', 'reviewer', 'reviews', 'reviewing', 'reviewed']
</code></pre>

<p>This can be compressed into a nested comprehension:</p>

<pre><code>desired_words = []
{desired_words.append(word) for word_type, word_list in word_forms.items() for word in word_list if word not in desired_words}

print(desired_words)
# ['reviewers', 'review', 'reviewer', 'reviews', 'reviewing', 'reviewed']
</code></pre>

<p>We didn't need to save the resulting object, we only want to side-effect of appending to <code>desired_words</code></p>

<p>Another way to do this would be to simply put all the words into a list, then convert it into a <code>set()</code> object, which would remove all duplicates and order the list alphabetically:</p>

<pre><code>desired_words = []
# the 'if' statement is removed in the dictionary comprehension:
{desired.words.append(word) for word_type, word_list in word_forms.items() for word in word_list}
# Set removes duplicates - we then wrap it back into a list
desired_words = list(set(desired_words))
</code></pre>
","1","2","45264","13013"
"35517","<p>This question is highly subjective, like asking what colour keyboard is best for programming faster.</p>

<p>A large number of Computer Science research at universities is done using Unix based operating systems (normally a Linux variant like Ubuntu) because it is completely open source and allows the user to tweak things as they please. Windows never allowed this officially. Apple's OSX doesn't either. However, OSX is based on Unix like Linux, and so there are many more similarities in every day usage between Linux and OSX than Linux and Windows. Many of the poster-boys/girls from data science have an academic background, where they likely used Linux.</p>

<p>Because many of the latest research ideas come from universities (where I claim most researchers use Linux), it is common to see open-source projects that have only been tested and packaged for Linux usage. Making them run on OSX is a smaller jump than going for Windows, as OSX and Linux are more alike.</p>

<p>Other guesses as to why many pick Apple products, are that they generally have higher build quality, they have a reputation for many thing <em>just working</em> (though IMO, Microsoft and Linux have become almost as good in that respect) and they are marketed as being luxury consumer products - and so are visually appealing. I'm sure there are books written on how social psychology with respect to this. All this ends up with the price being somewhat inflated for the actual hardware parts that you get.</p>

<p>In the end, someone who wants to get work done and has the necessary skills  will manage it on a 10 year old netbook running Windows. Likewise, owning a Linux server with 8 GPUs and a Macbook Pro to remotely connect to that server is no guarantee of good work. I for one have worked on all three platforms, for data science as well as other tasks, and although I have my preferences, the operating system rarely poses the biggest problem.</p>
","7","2","45264","13013"
"35532","<p>I think it is necessary to perform <strong>all</strong> operations using the backend versions, allowing Keras to perform backpropagation on every step of the function. You use the common <code>+</code> for example.... try <code>K.add</code>, which should work (based on the available <a href=""https://www.tensorflow.org/api_guides/python/math_ops#Arithmetic_Operators"" rel=""nofollow noreferrer"">arithmetic operation of the tensorflow backend</a>).</p>

<p>Also, have a look at <a href=""https://datascience.stackexchange.com/questions/33587/keras-custom-loss-function-as-true-negatives-by-true-negatives-plus-false-posit/33600#33600"">a related question</a>, where some of the mechanics around creating a custom loss function in Keras are discussed.</p>

<p>The two main loops in your function that compute the gradients should be candidates for vecotisation, where you could compute the differences in one operation. Try something along these lines:</p>

<pre><code>diffs = K.subtract(u_true[i+1, j], u_true[i, j])
</code></pre>

<p>and then the quotient in a second operation:</p>

<pre><code>quot = K.divide(diffs, dx)
</code></pre>

<p>and then of course the same for <code>v_true</code> and <code>dy</code>.</p>
","1","2","45264","13013"
"35582","<p>You will have to address a varying sequence length,  one way or another. will likely have either perform some padding (e.g. using zeros to make all sequences equal to a max. sequence length).</p>

<p>Other approaches, e.g. used within NLP, to make training more efficient, are to splice series together (sentences in NLP), using a clear break/splitter (full-stops/periods in NLP).</p>

<p>Using a convolutional network sort of makes sense to me in your situation, predicting a binary output. As the convolutions will be measuring correlations in the input space, I can imagine the success of the model will be highly dependend on the nature of the problem. For some intuition of conv nets uses for sequences, have a look at <a href=""http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"" rel=""nofollow noreferrer"">this great introductory article</a>. If each of your six sequences are inter-related, it allows the convolutions to pick up on those cross-correlations. If they are not at all related, I would proably first try Recurrent Networks (RNNs), such as the LSTM you mentioned.</p>

<p>Getting you head around the dimensions of a multi-variate LSTM can be daunting at first, but once you have addressed theissue of varying sequence length, it becomes a lot more manageable.</p>

<p>I don't know what framework you are using, but as an example in Keras/Tensorflow, the dimensions for you problem would be something like:</p>

<pre><code>(batch_size, sequence_length, num_sequences)
</code></pre>

<p><code>batch_size</code> can be set to <code>None</code> to give flexibility around your available hardware. <code>sequence_length</code> is where you need to decide on a length to use/create via padding/trimming etc. <code>num_sequences = 6</code>  :-)</p>

<p>If helpful, check out these threads, where I explained that stuff in more detail.</p>

<ul>
<li><a href=""https://datascience.stackexchange.com/questions/27563/multi-dimentional-and-multivariate-time-series-forecast-rnn-lstm-keras/27572#27572"">Multi-dimentional and multivariate Time-Series forecast (RNN/LSTM) Keras</a></li>
<li><a href=""https://datascience.stackexchange.com/questions/27533/keras-lstm-with-1d-time-series/27535#27535"">Keras LSTM with 1D time series</a></li>
</ul>
","0","2","45264","13013"
"35585","<p>If you read the entire file without skipping jumbled lines, does it stil work? What values appear in those jumbled cells?</p>

<p>You could go down the road of <strong>imputation</strong>, i.e. filling the missing gaps, based on something you can deduce from the data that is there.</p>

<p>Example are:</p>

<ul>
<li><strong>fill-forward</strong>: fill will preceding non-missing value</li>
<li><strong>fill-backwards</strong>: fill with following non-missing value</li>
<li><strong>average-filling</strong>: if numerical, fill gaps with the mean/median etc. of that column. You could also use something like a moving average.</li>
<li><strong>model-based</strong>: fill missing values using e.g. a regression model on available values (this needs a target variable, or at least timestamps)</li>
</ul>

<p>One last snity-check: I assume it isn't possible for you to try creating the CSV file again and addressing the problem at its source?</p>
","1","2","45264","13013"
"35616","<p>One reason for normalising the inputs is to make gradient descent more stable, as gradients spend more time in a comfortable region with meaningful updates and less neurons 'die' during trainings - getting stuck at one of the tails of e.g. the sigmoid non-linearity.</p>

<p>Normalising the output distribution is perhaps not the best idea, as you are by definition altering the defition of the target. This means you are essentially predicting a distribution that doesn't mirror your real-world target (at least without some reverse non-linear transforms later on).</p>

<p>On this you could do would be to <em>scale</em> the target, instead of normalising. The shape of the distribution should remain almost identical (thinking about the <em>shape</em> of the distribution), but the values themselves might be more easily attainable and therefore faster to optimise for; they are all closer in magnitude to the gradients that are being computed.</p>
","5","2","45264","13013"
"35618","<p>From wikipedia:</p>

<blockquote>
  <p>..., the binomial distribution with parameters <strong>n</strong> and <strong>$\rho$</strong> is the discrete probability distribution of the number of successes in a sequence of <strong>n</strong> independent experiments, each asking a yes–no question, and each with its own boolean-valued outcome: a random variable containing a single bit of information: success/yes/true/one (with probability $\rho$) or failure/no/false/zero (with probability $\rho = 1 − \rho$).</p>
</blockquote>

<p>So if you know that logistic regression is performed in order to model a binary output variable to some modelling question (i.e. to give 0 or 1, yes or no, etc.), it would make sense to base any probabilistic assumptions on a distribution, which shares this feature. Therefore, a binomial distribution may make sense compared to a continuous distribution, such as a Gaussian or Cauchy.</p>
","0","2","45264","13013"
"35656","<p>As Ankit said in comments, this is not possible in your concrete case, as you cannot just expand 1d point coordinates to 2d without inventing some information. </p>

<p>In 1d, where each point it represented by one single number, it would only be possible to find the 3rd point, equidistant to two original points. If you imagine the two points on a line, the third point would then just be the centre of those two points, which also happens to be the mean of those two points.</p>

<p>For three points, you would require a 2d space, i.e. each point must be a vector of length two: so basically an (x, y) coordinate pair.</p>

<p>If you did have (x,y) coordinates for three unique points, they would form a triangle, and the equidistant position (i.e. your fourth point) is called the <strong>circumcenter</strong>, and it found by finding the centre of each of the sides of the triangle, then drawing a line through each, which is perpendicular to its corresponding side of the triangle. These three lines will cross at the circumcenter - and it is not necessarily inside the triangle!</p>

<p>Example:</p>

<p><a href=""https://i.stack.imgur.com/10ujb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/10ujb.png"" alt=""Equidistant point example for triangle""></a></p>
","2","2","45264","13013"
"35794","<p>As is <a href=""https://stats.stackexchange.com/questions/73646/how-do-i-test-that-two-continuous-variables-are-independent"">discussed in the link to Cross-Validated SO</a> from Mephy, this is isn't an easy thing to do.</p>

<p>If they are independent, you might expect a correlation between pairs of variables to be close to zero. That would mean that knowing anything about one of the two variables doesn't give you any insight as to the behaviour of the second. To this end, there is <a href=""https://stackoverflow.com/questions/33997753/calculating-pairwise-correlation-among-all-columns"">a nice answer here</a>, which shows how to compute pairwise pearson correlation (with corresponding p-values) for all columns in a Pandas DataFrame.</p>

<p>The Pearson correlation does assume your random variables to be normally distributed, so keep that in mind when interpreting results. Alternatively, you could swap out that <code>pearsonr</code> function for the Spearman Rank correlation function: <code>spearmanr</code>, which does not assume normality of your variables.</p>

<p>Another (perhaps simpler) way using just a Pandas DataFrame is to use the built in method <code>corr</code>: This takes a keyword <code>method</code>, which allows you to specify one of three:</p>

<blockquote>
  <p>method : {‘pearson’, ‘kendall’, ‘spearman’}</p>
</blockquote>

<hr>

<p>If you random variables are time-series (you didn't mention it), another possible tool to look at would be <a href=""https://en.wikipedia.org/wiki/Granger_causality"" rel=""nofollow noreferrer"">Granger Causality</a>. This could also be performed pairwise (or batch-wise) across variables. It tests to see if a the future value of variable can be better predicted when historic values of the a different variable are included in the model. For example, if the price of <em>StockA</em> can be predicted with an accuracy of 52% using its own prices of the previous 5 days, the Granger test would have a null hypothesis that including some lags from <em>StockB</em> would not improve the accuracy. So if the accuracy does indeed jump up to 53% when including lagged prices of StockB, (and the test is significant), the null hypothesis is rejected and we say that StockB Granger-causes StockA.</p>

<p>This is implemented in the <a href=""https://www.rdocumentation.org/packages/vars/versions/1.5-2"" rel=""nofollow noreferrer""><code>vars</code> package in R</a> (there are others too). As a bonus, this version can also perform the Wald test for correlation in error processes of predictor and target variables.</p>
","3","2","45264","13013"
"35800","Questions mostly concerned with managing data, without focus on pre-processing or modelling.","0","4","45264","13013"
"35801","<p><em>Note: this question was asked and removed just before I posted my answer below, so am repeating the general idea here</em></p>

<hr>

<p>People often refer to pipelines when talking about models, data and even layers in a neural network. What can be meant by a <strong>pipeline</strong>?</p>
","4","1","45264","13013"
"35802","<p>A pipeline is almost like an algorithm, but at a higher level, in that it lists the steps of a process. People use it to describe the main stages of project. This could include everything from gathering data and pre-processing it, right through to post-analysis of predictions. The <strong>pipeline</strong> is essentially a large chain of modules, which can be individually examined/explained.
Here is an example image (source: <a href=""https://databricks.com/blog/2017/09/06/build-scale-deploy-deep-learning-pipelines-ease.html"" rel=""noreferrer"">DataBricks</a>)</p>

<p><a href=""https://i.stack.imgur.com/wruHK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wruHK.png"" alt=""deep learning pipeline""></a></p>

<p>There is actually a nifty module (class, actually) in Scikit Learn for building your own machine learning pipelines, which is literally called <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"" rel=""noreferrer"">Pipeline</a>. You can specify processing steps, models and other transformations, then wrap them into a pipeline, which carries out the start-to-finish process for you. This makes it much easier to work in a modular way and alter parameters, while keeping things organised.</p>

<p>In the documentation, they use an ANOVA (analysis of variance) model to select variables, which are then fed into an SVM (support vector machine) to perform classification. </p>

<hr>

<p>In the context of what might be considered a single model, a pipeline may refer to the various transformations performed on data. This might include <a href=""https://en.wikipedia.org/wiki/Dimensionality_reduction"" rel=""noreferrer"">dimensionsality reduction</a>, <a href=""https://www.quora.com/What-is-word-embedding-in-deep-learning"" rel=""noreferrer"">embeddings</a>, encoding/decoding (<a href=""https://www.cc.gatech.edu/~hays/7476/projects/Avery_Wenchen/"" rel=""noreferrer"">GAN example</a>), <a href=""http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/"" rel=""noreferrer"">attention</a> mechanisims, and so on.</p>

<p>Here is an example of what might be referred to as a pipeline: the <a href=""https://arxiv.org/abs/1506.02025"" rel=""noreferrer"">Spatial Transformer Network</a>:</p>

<p><a href=""https://i.stack.imgur.com/eWD5o.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eWD5o.png"" alt=""STN""></a></p>

<p>Images are passed through a pipeline with three parts:</p>

<ol>
<li>a localisation network</li>
<li>a grid generator</li>
<li>a sampling mechanism</li>
</ol>

<p>these three parts might be akin to one of the parts in the MLlib Pipeline displayed above.</p>

<hr>

<p>Another area in which <strong>pipeline</strong> is used extensively is within <strong>data management</strong>. In this case, it refers to how and where data is transferred, and perhaps by which frequency. There are large packages dedicated to building such pipelines, e.g. :</p>

<ol>
<li><a href=""https://spark.apache.org/"" rel=""noreferrer"">Apache Spark</a> - can do a lot more than just pipelines these days (<a href=""https://medium.com/walmartlabs/how-we-built-a-data-pipeline-with-lambda-architecture-using-spark-spark-streaming-9d3b4b4555d3"" rel=""noreferrer"">use case example</a>)</li>
<li><a href=""https://github.com/spotify/luigi"" rel=""noreferrer"">Luigi</a> - manages complex batch processes (developed by Spotify)</li>
</ol>

<hr>

<p><em>Note: this question was asked and removed just before I posted my answer, so am providing the content via a self-answered question</em></p>
","5","2","45264","13013"
"35805","<p>Section 3.3 simply gives the equation for the negative log-likelihood.
They say that it takes the form of cross-entropy (because it just looks like a cross-entropy equation, perhaps?), but mathematically it seems to come from the fact that they define the model in equation 2 to follow a <a href=""https://en.wikipedia.org/wiki/Bernoulli_distribution"" rel=""nofollow noreferrer"">Bernoulli distribution</a>, which can be 0 or 1 with probabilities p or q respectively:</p>

<p>$$
\Pr(X=1)=p=1-\Pr(X=0)=1-q
$$</p>

<p>The probability mass function of the Bernoulli looks like this:</p>

<p>$$
f(x;p)=px+(1-p)(1-x)\!\quad {\text{for }}x\in \{0,1\}
$$</p>

<p>and the maximum likelihood for the Bernoulli looks like this:</p>

<p>$$
L(p) = \prod_{i+1}^n p^{x_1} (1 - p)^{1 - x_1}
$$</p>

<p>Taking the log (to get the log-likelihood that they mention), gives something of the form we see in equation 3:</p>

<p>$$
\log{L(p)} = \log{p}\sum_{i=1}^n x_i + \log{(1-p)}\sum_{i=1}^n (1-x_i)
$$</p>

<p>Then you can takes the sum out the front as it is over the image patches:</p>

<p>$$
\sum_{i}^{w^2_m} (\tilde{m}_i ln\hat{m}_i + (1-\tilde{m}_i)ln(1- \hat{m}_i))
$$
So here 
This sums over all the pixel-wise output units in each image patch (map: <em>m</em>), hence why the summation goes to $w_m^2$; the width of the image patch squared. $\tilde{m}$ is a realisation (possible outcome) of $\hat{m}$, just as $x$ is a realisation of distribution $p$ in the above equations.</p>

<p>This looks like the binary cross-entropy loss function, which is why I think they say:</p>

<blockquote>
  <p>For the model given in Equation 2 the negative log likelihood
  takes the form of a cross entropy between the
  patch $\tilde{m}$ derived from the given map and the predicted
  patch $\hat{m}$</p>
</blockquote>
","2","2","45264","13013"
"35829","<p>As far as I can make out, this is simply the error aggregated at <code>layer_1</code>, meaning it is the sum of the errors from the current hidden layer, plus the error from all <em>future</em> hidden layers (in this case, that is only one layer: <code>layer_2</code>) plus the error from the output (prediction) layer - <em>sigmoid_out</em>.</p>

<p>I think that line of code (#99) is performing the loss update with respect to a chosen layer and all layers that are <em>ahead</em> in time - so closer to the network's output layer, but as we are moving backwards during backpropagation. It corresponds then, I believe, to equation 3.24 and 3.25 in <a href=""http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf"" rel=""nofollow noreferrer"">Ilya Sutskever's PhD Thesis</a> (on page 35).</p>

<p>So you are summing up the gradients of all layers ahead of the current layer, as well as the gradient of the output layer (the sigmoid). Equation 3.25 from the link above looks like this:</p>

<p>$$
\frac{\delta L}{\delta W} = \sum_{t=1}^{T-1} \bigg( W'^T (r_{t+1} \odot (1 - r_{t+1}) \odot \frac{\delta L}{\delta r_{t+1}}) \bigg) v_t^T + 
\sum_{t=1}^{T-1} \frac{\delta log \hat P (v_t | r_{t-1})}{\delta W}
$$</p>

<blockquote>
  <p>... where , where $v_t$ are the input variables and $r_t$ are the RNN’s hidden variables (all of which are deterministic). <strong>(from page 34)</strong></p>
</blockquote>

<p>The explanation given in the thesis:</p>

<blockquote>
  <p>The first summation in eq. 3.25 corresponds to the use of $W$ for computing $r_t$, and the second summation arises from the use of $W$ as RBM parameters for $log \hat P(v_t |r_{t−1})$, so each $∂log \hat P(v_{t+1}|r_t)/∂W$ term is computed with CD <strong>(contrastive divergence)</strong>. Computing $∂L/∂rt$ is done most conveniently with a single backward pass
  through the sequence. It is also seen that the gradient of the RTRBM <strong>( Recurrent Temporal Restricted Boltzmann Machine)</strong> would be computed exactly if CD were replaced with the derivatives of the RBM’s log probability.</p>
</blockquote>

<p><em>bold text added by me</em></p>

<p>The main leap here to relate to your example blog is that the it equates the log probabilities of a Restricted Boltzmann Machine to the loss of a generative RNN. If you are interested in more details, I would recommend reading either all of Chapter 3, or perhaps just sections 3.9 and 3.10.</p>

<hr>

<p>I posted that equation because I could see a version that so closely matches the code in that blog in either of the original papers that propose backpropagation through time. Additionally, none seem to be free to read.
You can find them link on <a href=""https://en.wikipedia.org/wiki/Backpropagation_through_time"" rel=""nofollow noreferrer"">the relevant Wikipedia page</a>.</p>

<p>Have a look at this related paper, written by one of the original authors of BPTT. <a href=""http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/pdfs/Werbos.backprop.pdf"" rel=""nofollow noreferrer"">Backpropagation Through Time: What It Does and How to Do It</a>.</p>
","1","2","45264","13013"
"35862","<p>Because I did not have your exact data (I don't have the csv file), I created a dummy dataframe (<a href=""https://www.r-bloggers.com/r-sorting-a-data-frame-by-the-contents-of-a-column/"" rel=""nofollow noreferrer"">swiped from this tutorial</a>):</p>

<pre><code>numPeople = 10
sex=sample(c(""male"",""female""),numPeople,replace=T)
age = sample(14:102, numPeople, replace=T)
income = sample(20:150, numPeople, replace=T)
minor = age&lt;18

population = data.frame(sex=sex, age=age, income=income, minor=minor)
population

&gt; population
      sex age income minor
1    male  34    130 FALSE
2    male  48     86 FALSE
3  female 101     36 FALSE
4    male  64     78 FALSE
5  female  97     92 FALSE
6    male  19     45 FALSE
7  female  42    124 FALSE
8  female  24     68 FALSE
9  female  24     96 FALSE
10 female  91     30 FALSE
</code></pre>

<p>This means you will have to adjust the variable names and so on to match your example, but can learn how to order a list of matrices. More specifically, you really have a list of <code>data.frame</code> objects, after using the <code>split</code> function. it is important to notice the difference between a data.frame and a matrix, as they can behave differently.</p>

<p>I will split the <code>data.frame</code> by the <code>sex</code> column:</p>

<pre><code>l = split(population, population$sex)
</code></pre>

<p>You can test that each element of the resulting list is indeed a <code>data.frame</code> (and not a matrix) by running:</p>

<pre><code>&gt; is.data.frame(l[[1]])
[1] TRUE
</code></pre>

<p>Notice that you have to use the doublesquare brackets to access the actual element of the list. Single square brackets will return something else:</p>

<pre><code>&gt; is.data.frame(l[1])
[1] FALSE
</code></pre>

<p>Tt is a named list-element and needs to be referenced by using (in my case) one of the names of the group that we split on: male or female
Below I comment the code heavily to explain what each step does.</p>

<pre><code>&gt; l[1]
$female                          # this is the name of the list element
     sex age income minor
1 female  20     42 FALSE
6 female  75    103 FALSE
7 female  30    148 FALSE
</code></pre>

<p>So we can access the actual element two ways:</p>

<ol>
<li><p>by name:</p>

<pre><code>&gt; l[1]$female
     sex age income minor
1 female  20     42 FALSE
6 female  75    103 FALSE
7 female  30    148 FALSE
</code></pre></li>
<li><p>by double square brackets:</p>

<pre><code>&gt; l[[1]]
 sex age income minor
1 female  20     42 FALSE
6 female  75    103 FALSE
7 female  30    148 FALSE
</code></pre></li>
</ol>

<p>That all being said, he is a working example of sorting a list (vector, holding lists) containing <code>data.frame</code>s:</p>

<pre><code>l = split(population, population$sex)    # population is defined as shown above
N = length(l)

results &lt;- vector(""list"", N)

for(i in 1:N) {

    results[[i]] &lt;- l[[i]][order(l[[i]]$age),]
}
</code></pre>

<p>I specify the length of the vector when I create it, as we know how long the resulting list will be in advance.
... and check the results:</p>

<pre><code>&gt; results[[1]]
    sex age income minor
8 female  66     90 FALSE
5 female  69    131 FALSE
2 female  70     52 FALSE
9 female  95     43 FALSE

&gt; results[[2]]
    sex age income minor
6  male  18     87 FALSE
4  male  61     97 FALSE
3  male  64    131 FALSE
7  male  72     93 FALSE
1  male  82    107 FALSE
10 male  92     42 FALSE
</code></pre>

<p><em>The results depend on random choice when creating <code>population</code> at the beginning, using <code>sample()</code>.</em></p>
","0","2","45264","13013"
"35863","<p>In the model, you will decide how best to get uncertainty. If you used <a href=""https://github.com/fmfn/BayesianOptimization"" rel=""nofollow noreferrer"">Bayesian optimization</a> (that's a great package for it in Python), for example, you get a covariance matrix along with your expected values, and so inherently get an uncertainty measure. In this case, you can make predictions as to the underlying function of your data, and the (co-)variance will provide levels of uncertainty, as shown by the width of the green bands around the line below:</p>
<p><a href=""https://i.stack.imgur.com/qmKcd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qmKcd.png"" alt=""enter image description here"" /></a></p>
<p>So the red points show where we have some sample data... notice that we have none e.g. at <code>X = 4</code> and <code>X = -1</code>, which is why we have high uncertainty; the 95% confidence interval is very large.</p>
<hr />
<p>If you use a <em>standard</em> deep neural network TPO perform classification, for example, there is no inherent measure of uncertainty. All you really have is your <strong>test accuracy</strong>, to let you know how well the model performs on hold-out data.  I cannot remember where it is explained, but I believe it is not actually feasible to interpret the class prediction values in terms of uncertainty.</p>
<p>For example, if you are predicting <code>cat</code> or <code>dog</code> for an image, and the two classes receive (normalized) logit values <code>[0.51, 0.49]</code> respectively, you cannot assume this means very low certainty.</p>
","5","2","45264","13013"
"35872","<p><strong>TL:DR</strong>, I would suggest not to optimise over the random seed. A better investment of the time would be to improve other parts of your model, such as the pipeline, the underlying algorithms, the loss function... heck, even optimise the runtime performance!  :-)</p>

<hr>

<p>This is an interesting question, even though (in my opinion) should not be a parameter to optimise.</p>

<p>I can imagine that researchers, in their struggles to beat current state-of-the-art on benchmarks such as ImageNet, may well run the same experiments many times with different random seeds, and just pick/average the best. However, the difference should not be considerable.</p>

<p>If your algorithms has enough data, and goes through enough iterations, the impact of the random seed should tend towards zero. Of course, as you say, it may have a huge impact. Imagine I am categorising a batch of images, into <code>cat</code> or <code>dog</code>. If I have a batch size of 1, and only 2 images that are randomly sampled, and one is correctly classified, one is not, then the random seed governing which is selected will determine whether or not I get 100% or 0% acuracy on that batch.</p>

<hr>

<p><strong>Some more basic information:</strong></p>

<p>The use of a random seed is simply to allow for results to be as (close to) reproducible as possible. All random number generators are only <em>pseudo-random</em> generators, as in the values appear to be random, but are not. In essence, this can be logically deduced as (non-quantum) computers are deterministic machines, and so if given the same input, will always produce the same output. Have a <a href=""https://softwareengineering.stackexchange.com/questions/124233/why-is-it-impossible-to-produce-truly-random-numbers"">look here for some more information</a> and relative links to literature.</p>
","6","2","45264","13013"
"35879","<p>I assume that you want to keep the newlines in the strings for some reason after you have loaded the csv files from disk. Also that this is done again in Python. My solution will require Python 3, although the principle could be applied to Python 2.</p>

<h2>The main trick</h2>

<p>This is to replace the <code>\n</code> characters before writing with a weird character that otherwise wouldn't be included, then to swap that weird character back for <code>\n</code> after reading the file back from disk.</p>

<p>For my weird character, I will use the <em>Icelandic thorn</em>: Þ, but you can choose anythin that should otherwise not appear in your text variables.
Its name, as defined in the standardised Unicode specification is: <strong>LATIN SMALL LETTER THORN</strong>. You can use it in Python 3 a couple of ways:</p>

<pre><code>weird_name = '\N{LATIN SMALL LETTER THORN}'
weird_char = '\xfe'             # hex representation
weird_name == weird_char        # True
</code></pre>

<p>That <code>\N</code> is pretty cool (and works in python 3.6 inside formatted strings too)... it basically allows you to pass the <strong>N</strong>ame of a character, as per Unicode's specification.</p>

<h2>Replacing <code>\n</code></h2>

<p>Now we use this weird character to replace <code>'\n'</code>. Here are the two ways that pop into my mind for achieving this:</p>

<ol>
<li><p>using a list comprehension on your list of lists: <code>data</code>:</p>

<pre><code>new_data = [[sample[0].replace('\n', weird_char) + weird_char, sample[1]]
             for sample in data]
</code></pre></li>
<li><p>putting the data into a dataframe, and using replace on the whole <code>text</code> column in one go</p>

<pre><code>df1 = pd.DataFrame(data, columns=['text', 'category'])
df1.text = df.text.str.replace('\n', weird_char)
</code></pre></li>
</ol>

<p>The resulting dataframe looks like this, with newlines replaced:</p>

<pre><code>               text              category
0         some text in one line      1   
1  text withþnew line character      0   
2    another newþline character      1   
</code></pre>

<h2>Writing the results to disk</h2>

<p>Now we write either of those identical dataframes to disk. I set <code>index=False</code> as you said you don't want row numbers to be in the CSV:</p>

<pre><code>FILE = '~/path/to/test_file.csv'
df.to_csv(FILE, index=False)
</code></pre>

<p>What does it look like on disk?</p>

<blockquote>
  <p>text,category</p>
  
  <p>some text in one line,1</p>
  
  <p>text withþnew line character,0</p>
  
  <p>another newþline character,1</p>
</blockquote>

<h2>Getting the original data back from disk</h2>

<p>Read the data back from file:</p>

<pre><code>new_df = pd.read_csv(FILE)
</code></pre>

<p>And we can replace the <code>Þ</code> characters back to <code>\n</code>:</p>

<pre><code>new_df.text = new_df.text.str.replace(weird_char, '\n')
</code></pre>

<p>And the final DataFrame:</p>

<pre><code>new_df
               text               category
0          some text in one line      1   
1  text with\nnew line character      0   
2    another new\nline character      1   
</code></pre>

<p>If you want things back into your list of lists, then you can do this:</p>

<pre><code>original_lists = [[text, category] for index, text, category in old_df_again.itertuples()]
</code></pre>

<p>Which looks like this:</p>

<pre><code>[['some text in one line', 1],
 ['text with\nnew line character', 0],
 ['another new\nline character', 1]]
</code></pre>
","8","2","45264","13013"
"35899","<p><strong>TL:DR</strong> The most common and straight-forward approach would be to scale all the numerical data to be within a given range. This makes the currency differences irrelevant, as all fluctation remains constant, relative to the original scale.</p>

<h2>Rescaling</h2>

<p>Here is an example function to scale the numerical columns of your dataset:</p>

<pre><code>def rescale(data, new_min=-1, new_max=1):
    """"""Rescale the columns of Dataframe to be in the range [new_min, new_max].

    Parameters
    ----------
    data    : a Pandas DataFrame
    new_min : the target minimum for each of the columns, optional
    new_max : the target maximum for each of the columns, optional

    Returns
    -------
    out : the rescaled input data, with each column now in the range [new_min, new_max]

    """"""
    return (data - data.min()) / (data.max() - data.min()) * (new_max - new_min) + new_min
</code></pre>

<p>You can then do the following to see nice descriptions of each column in your dataframe, ensuring the min/max values are as desired:</p>

<p><a href=""https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.describe.html"" rel=""nofollow noreferrer""><code>your_dataframe.describe()</code></a></p>

<p>One thing to perhaps try out, would be whether or not to scale **all* data to be within a fixed range, say [0, 1], or whether to scale each individual currency to the range [0, 1]. That equates to applying the above function on either single columns of your dataframe, or the entire dataframe.</p>

<h2>Considerations</h2>

<p>Things to keep in mind/check for would be:</p>

<ol>
<li><p>if some currencies are nominal huge compared to others (e.g. <a href=""https://www.google.de/search?num=30&amp;ei=HIZVW4iRFoTfwAKY_KOwBQ&amp;q=euro%20indonesian%20rupiah%20&amp;oq=euro%20indonesian%20rupiah%20&amp;gs_l=psy-ab.3..0i7i30k1l6j0i30k1l2j0i7i5i30k1l2.34370.41101.0.41401.10.10.0.0.0.0.89.816.10.10.0....0...1c.1.64.psy-ab..1.9.728...0j0i67k1j0i7i10i30k1j0i8i13i30k1j0i8i30k1j35i39k1.0.2ktVAZGfrc4"" rel=""nofollow noreferrer"">1 euro = 17,000 Indonesian Rupiah</a>), the smaller currencies will have time-series values all very close to zero - this makes computation/optimisation more difficult - especially if you use methods such as gradient descent.</p></li>
<li><p>Scaling the currencies individually to a range would lose the relativity of their nominal values. To see this, plot the currencies before and after scaling.</p></li>
</ol>

<hr>

<h2>Unify the Currencies</h2>

<p>If you have (or could get) exchange rate the data, another alternative would be to convert all of the currencies to the one one your choice.</p>

<p>As the exchange rate changes over time, this would also inherently introduce new information into the dataset, namely the relative economic health of each of the countries. Depending on your use case, this could be something that really helps, or you might really want to avoid. If you have to time, to do it, it could definitely make for some interestinf research!</p>

<p>You can get <a href=""https://www.quandl.com/data/CUR-Foreign-Exchange-Rates"" rel=""nofollow noreferrer"">currency data from Quandl</a> - they have a Python API too, which is simple enough to use.</p>
","0","2","45264","13013"
"35911","<p>The answer to such a question is opinion based and the question itself is very broad.</p>

<p>I have used an HP Envy in the past, with a 4GB Nvidia 950M GPU, which worked well with Linux installed.</p>

<p>In general:</p>

<ul>
<li><p>The higher the <strong>compute capability</strong> of the GPU the better (look here at the <a href=""https://developer.nvidia.com/cuda-gpus"" rel=""nofollow noreferrer"">list for Nvidia GPUs</a> under GeForce products for notebooks). </p></li>
<li><p>Another option worth considering would be to build compute - you will lose portability, but get a vast improvement of power for the same money. </p></li>
<li><p>Finally, you could buy a cheap laptop and then use the rest of your budget on cloud services, such as Google Cloud, Amazon, etc. This might be the best option to get started (with some free credit they usually offer for new accounts) - also if you think you might only be using the GPU for your thesis, and not training huge models for days at a time after that period. If you know you'll be training large models over long periods of time, it's generally more cost effective to buy the hardware. </p></li>
</ul>
","1","2","45264","13013"
"35932","<p>What do you want to do with the chat bot? How you parse it will depend on the final use case and, believe it or not, many people get the job done by simply collecting the POS they want and using some filtering.</p>

<p>If you want to try to maintain more of the data and perhaps abstract it makes sense to try clustering of some kind, perhaps using hierarchical methods, such as the (relateively new) hdbscan. The features on which you cluster will again depend on what you want to achieve.</p>

<p>If you haven't already, check out <a href=""https://spacy.io/usage/examples"" rel=""nofollow noreferrer"">the spaCy examples</a> for some inspiration!</p>

<p>Once you have a corpus with word all tagged, you can try training models that might be able to answer questions, based on user input. This will involve steps such as creating encoding of the words (or entire user questions), using embeddings such as Word2Vec, GLoVe, or simple sparse one-hot encodings. You basically need to transform words into numerical input somehow.</p>

<p>I hope this gives you some keywords to help you on your search  :-)</p>
","1","2","45264","13013"
"35995","<p>Do I understand correctly that you do actually have the data for all segments? Or do you only have the mean and standard deviation of each subset? In this case, it is possible to compute the estimate standard deviation over the entire collection of subsets!</p>

<p>As is explained in <a href=""https://stats.stackexchange.com/a/276035/37863"">the second answer to that question on CrossValidated</a> (and as you noted), computing the mean over your entire segment is as simple as taking the mean of the means i.e. the simple average of all your $\mu$-values.</p>

<p>If the former is true (you have all the data), then it is possible to compute the variance (i.e. the standard deviation squared) of the combined data, in a similar way in which you compute the mean.</p>

<p>Think about what it means to compute the sample variance. In words:</p>

<blockquote>
  <p>First compute the mean of the population. Next, sum up the squared difference between every single sample and the population mean. Divide this by the number of samples in the population (minus 1).</p>
</blockquote>

<p>The formula is as follows:</p>

<p>$$\bar{\sigma}^2 = \frac{1}{N - 1} \sum_{i=1}^{N} (x_i - \bar{\mu})^2$$</p>

<p>where $\bar{\mu}$ is the sample mean (over one of your subsets) and $\bar{\sigma}^2$ is the sample variance (over one of your subsets).</p>

<p><a href=""https://math.stackexchange.com/questions/2238086/calculate-variance-of-a-subset"">Have a look at this answer on Math Overflow</a>.</p>

<p>To scale up the variance estimate over multiple subsets, however, you must account for the difference in variances between groups. The way we can do this is to use the mean of the global population $\mu_{global}$ (over all your subsets). Imagine two of your <code>subSegments</code> are $A$ and $B$, then the formula to compute the aggregated variance would be:</p>

<p>$$\sigma_{A+B}^2 = \frac{1}{N - 1} \left( \sum_{i=1}^{N_A} (A_i - \mu_{global})^2 + \sum_{i=1}^{N_B} (B_i - \mu_{global})^2 \right)$$</p>

<p>where the $\mu_{global}$ is what you already compute, the average of the means, here it'd be:</p>

<p>$$\mu_{global} = \frac{1}{2} (\mu_A + \mu_B)$$</p>

<p>To get the standard deviation, just take the square root of the $\sigma_{A+B}$ value above.</p>

<p>To relate this back to the description of the variance computation above, all we are changing now is the mean value that we are subtracting!</p>

<hr>

<p>If you are curious as to why we divide by $N-1$ when computing averages here: it helps ensure the estimate we compute is not biased. It is called <a href=""https://en.wikipedia.org/wiki/Bessel%27s_correction"" rel=""nofollow noreferrer"">Bessel's correction</a>. One downside, is that it will increase the mean squared error of the estimate you compute, which becomes clear when you think about a tiny subset of data, where that $-1$ makes a large impact.</p>
","0","2","45264","13013"
"36177","<p>If you know that the multiple prices on a single day are chronological (and you don't have the actual timestamps), I would suggest simply taking the last price. This is then as close as you can get to using the <em>Closing Price</em>, which is the most common. Often you have <em>Open, High, Low, Close</em>, for example, but just use the <em>Close</em> prices.</p>

<p>Interpolating, or <em>imputing</em>, the data can be done in many ways. One factor that might help decide on a method, is how many of the points are missing? You have ~500 data points: how many days in total? how many of those require imputing?</p>

<p>For a relatively low number of missing values, scattered more or less randomly over your time-series, I would suggest going for simpler imputation methods, rather than model-based one.</p>

<p>A model-based approach can introduce other biases/problems that would potentially be harder to debug and more difficult to understand, so I would hold off there initially.</p>

<h3>Forward-filling</h3>

<p>This is my <em>go to method</em>, to just get started. If only a few percent of the entire time-series are missing, something as simple as fill-forward might be acceptable. Even the gaps in data are randomly or evenly spaced along the timeline, this is also a reasonable approach.</p>

<p>A nice property of this method is that it agrees with the theory on stochastic price-paths. These are usually modelled as Gaussian random variables, following the principles or Brownian motion. This all culminates in the idea of a <a href=""https://en.wikipedia.org/wiki/Martingale_(probability_theory)"" rel=""nofollow noreferrer"">Martingale Process</a>, which states:</p>

<blockquote>
  <p>at a particular time in the realized sequence, the expectation of the next value in the sequence is equal to the present observed value even given knowledge of all prior observed values.</p>
</blockquote>

<h3>Random sampling</h3>

<p>One way to fill a seemingly stochastic price-path, would be to compute the mean and variance of the dataset, then draw random samples from that distribution to fill missing values. This should preserve the mean and variance of the dataset's distribution quite well (and fairly).</p>

<p>Here is is important to realise that you may inadvertently be inducing a bias within that dataset, namely that you have use population statistics to alter the data. This means, depending on your model testing method later on, that you violate information flow through time. E.g. when predicting the 100th step, using the 99 preceding steps, it could be the case that the 99th step was one you imputed. Using the mean and variance of the entire dataset to do so has inherently encoded some information from the 100th step (and all following steps) into that 99th step. This is a subtle detail, but something to be aware of.</p>

<hr>

<p>If you can put the time-series in a Pandas DataFrame, it might give you some more ideas by skimming the <a href=""https://pandas.pydata.org/pandas-docs/stable/missing_data.html"" rel=""nofollow noreferrer"">DataFrame's built-in methods to handle missing data</a>.</p>

<p>Here is <a href=""http://www.stat.columbia.edu/~gelman/arm/missing.pdf"" rel=""nofollow noreferrer"">a more in-depth analysis of possibilities</a> with some good explanations.</p>
","0","2","45264","13013"
"36248","<p>The output of a neural network will never, by default, be binary - i.e. zeros or ones. The network works with continuous values (not discrete) in order to optimise the loss more freely in the framework of gradient descent.</p>

<p>Have a look here at <a href=""https://stackoverflow.com/questions/46040656/binary-keras-lstm-model-does-not-output-binary-predictions"">a similar question</a> that also shows some code. </p>

<p>Without any kind of tweaking and scaling, the output of your network is likely to fall somewhere in the range of your input, in terms of its nominal value. In your case, that seems to be roughly between 0 and 2.</p>

<p>You could now write a function that turns your values above into 0 or 1, based on some threshold. For example, scale the values to be in the range [0, 1], then if the value is below 0.5, return 0, if above 0.5, return 1.</p>
","10","2","45264","13013"
"36253","<p>As you point out, with $\delta_k = y_k - t_k$, the author is stating the relationship between the final units' output and the target. So Equation 5.54 is simply stating:</p>

<blockquote>
  <p>the error on the $k^{th}$ output unit is the difference between its output and the target.</p>
</blockquote>

<p>I believe that $\delta_k$ refers to the error simply at the output, whereas $\delta_j$ is the derivative of the output at unit $n$ with respect to any neuron back in the network, $j$. If this is the case, doing your derivative as you did, for $\delta_k$ (instead of $\delta_j$) means you are computing the error gradient between the output and the final layer. Over this layer, the activation must be linear (we do not apply a non-linearity at the final layers output). This would mean your 
$\dfrac{\partial h(a_k)}{\partial a_k}$ term would actually fall away in this case (to a constant), using the $k$ subscript - and you have the answer from the author: $y_t - t_k$.</p>

<hr>

<p>The network that is considered in your second question seems to only really discuss a single fully-connected layer, so yes, two actual layers of neurons - input and output only with no activation (so a linear activation, as you mention).</p>
","0","2","45264","13013"
"36277","<p><strong>TL:DR</strong> - If you know the posterior distribution of the complex process (i.e. the <em>output</em> distribution), and that distribution is one which can be modelled with reasonable accuracy, then sampling from it should reasonably represent the responses of the complex system.</p>

<hr>

<p>An example could be a complex decision process, whereby many decisions are made consecutively, maybe with some conditional or temporal relationships along the way (basically any process that is considered to be complex). Now imagine, after all this complexity, there are e.g. two possible categorically outputs: a zero or a one. Well this something we might be able to model with a <a href=""https://en.wikipedia.org/wiki/Bernoulli_distribution"" rel=""nofollow noreferrer"">Bernoulli distribution</a>, assuming we can estimate a reasonable parameter $p$ (specific to the Bernoulli distribution).</p>

<p>By sampling from the distribution, we would hope to draw samples, which are representative of the complex process. I hope this covers your main question.
I think <strong>point 4.</strong> is also clear to understand as well from my trivial example. I am unsure as to what <em>cost</em> refers exactly in <strong>point 3</strong>. </p>

<p>As for <strong>point 1.</strong>, there are examples where (random) sampling, for example in a grid search of model/optimisation parameters can improve results, as it results in an improved exploration of the parameter space compared to other methods such as grid-search.</p>
","1","2","45264","13013"
"36373","<p>The function <code>set_weights</code> on a Keras layer requires the shape of the inputs to match the shape of the weights which you are replacing. You can find out which dimensions these are by calling the <code>get_weights</code> method on the layer in which you are interested.</p>

<p>In that tutorial, it would look something like the following. We only need the first element returned by get_weights(), hence the <code>[0]</code>. We then see the shape of it:</p>

<pre><code>In [7]: model.layers[0].get_weights[0].shape
(4, 4, 1, 1)
</code></pre>

<p>So the person who wrote that tutorial needed to match that shape and, as it is only considering one layer of the defined model, it is able to be hard-coded in their example.</p>

<hr>

<p>Here are the docstrings for the two main functions I mentioned above:</p>

<p><strong>set_weights()</strong></p>

<pre><code>In [8]: l1.set_weights?

Signature: l1.set_weights(weights)
Docstring:
Sets the weights of the layer, from Numpy arrays.

# Arguments
    weights: a list of Numpy arrays. The number
        of arrays and their shape must match
        number of the dimensions of the weights
        of the layer (i.e. it should match the
        output of `get_weights`).

# Raises
    ValueError: If the provided weights list does not match the
        layer's specifications.
</code></pre>

<p><strong>get_weights()</strong></p>

<pre><code>In [9]: l1.get_weights?
Signature: l1.get_weights()
Docstring:
Returns the current weights of the layer.

# Returns
    Weights values as a list of numpy arrays
</code></pre>
","2","2","45264","13013"
"36375","<p>The Adaline (Adaptive Linear Element) and the Perceptron are both linear classifiers when considered as individual units. They both take an input, and based on a threshold, output e.g. either a 0 or a 1.</p>

<p>The main difference between the two, is that a <strong>Perceptron</strong> takes that binary response (like a classification result) and computes an error used to update the weights, whereas an <strong>Adaline</strong> uses a continous response value to update the weights (so before the binarized output is produced).</p>

<p>The fact that the Adaline does this, allows its updates to be more repesentative of the actual error, before it is thresholded, which in turn allows a model to converge more quickly.</p>

<hr>

<p>Have a look at this <a href=""http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/"" rel=""nofollow noreferrer"">really interesting history of neural networks</a>, which contains a small section on Adalines, along with <em>memistors</em> - resistors with memory, as the neurons where figuratively perceived back in the 1960's.</p>

<p>There are also some other answers to a similar question <a href=""https://www.quora.com/Artificial-Neural-Networks-what-is-the-difference-between-perceptron-and-adaline-in-recognition-+-and-X-images"" rel=""nofollow noreferrer"">here</a>.</p>
","3","2","45264","13013"
"36396","<p>Python is by default single-threaded. Read about the Global Interpreter Lock (GIL). Although some modules with utilise multi-threading, asyncronous processing and multicore solutions, these will not take effect when you are simply loading modules and data.</p>

<p>Unless you are using some distributed system, or perhaps something like Tensorflow-serving, you will not be able to train/exectute your 20 LSTM models in parallel in a single Python script, so perhaps you could just load one at a time. Alternatively, just make 20 copies of the script - one for each model - then launch all 20 scripts in different terminals (or via a multi-threaded or multicore script).</p>

<p>Asking on StackOverflow would reach an audience that is more adept in the sorts of problems.</p>
","1","2","45264","13013"
"36414","<p>Using <code>valid</code> will essentially use as much of your input as possible, such that the dimensions continue to work. This means there is a chance some input will be <em>trimmed</em> (removed).</p>

<p><code>same</code> on the other hand, will add padding to allow e.g. the number of filters/convolutions you specify to be applied.</p>

<p>The reason not to simply have <em>padding=True</em> as an option, is that there are more than two options (see snippet from the docs below). Also, that would not be very explicit: using <code>same</code> or <code>valid</code> makes it crystal clear, what the result should be using the given approach.</p>

<p>This is what the documentation says:</p>

<blockquote>
  <ul>
  <li>padding: One of ""valid"", ""causal"" or ""same"" (case-insensitive).  <strong>""valid""</strong> means ""no padding"".  <strong>""same""</strong> results in padding the input such that the output has the same length as the original input</li>
  </ul>
</blockquote>

<p>Take a look at this nice <a href=""https://stackoverflow.com/questions/47213051/what-are-padding-valid-in-keras-conv2d-and-how-to-disable-padding"">answer with a simple example</a>.</p>
","1","2","45264","13013"
"36416","<p>It sounds like you essentially have a sparse input problem, similar to doing something like a recommendation system. Imagine trying to recommend a film to somebody based on films they have already watched and rated. There would be many many films they have not seen, so you (in a sense) have missing data. This was the case in <a href=""https://en.wikipedia.org/wiki/Netflix_Prize"" rel=""nofollow noreferrer"">the Netflix Prize</a> challenge.</p>

<p>Facorisation methods such Single Value Decomposition are common approaches in these cases. There is a nice <a href=""http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/"" rel=""nofollow noreferrer"">summary of such methods</a> used in the Netflix Prize.</p>

<p>Using a neural network Auto-Enconder will be reliant on relatively large numbers of samples, which is something to bear in mind when assessing performance.</p>

<hr>

<p>Just one other idea that might be worth trying out, depending on how many features you have and the general sparsity, could be to replace the missing values with fixed values on a feature-level. This could be as simple as replacing the missing values e.g. in ""relationship status"" to <strong><code>&lt;RELATIONSHIP&gt;</code></strong>, and then in general ""feature X"" to just be <strong><code>&lt;X&gt;</code></strong>. This is a kind of trick that is often used in NLP (natural language processing) to allow filler-words to encode some useful information.</p>

<p>In your case, this might at least help any model distinguish between missing values across features. I have not tested something like this myself, so unfortunately I cannot cite references nor point to results.</p>
","0","2","45264","13013"
"36422","<h2>How</h2>

<p>Adding a dropout layer to a neural network has a few functions. First of all, here is some simple code that implements it:</p>

<pre><code>In [1]: import numpy as np

In [2]: weights = np.random.randint(0, 100, (8, 8))    # example weights 

In [3]: weights
Out[3]: 
array([[10, 48,  9, 87, 23, 18, 60, 83],
       [29, 48, 88, 43, 34, 56, 52, 82],
       [35,  0, 70,  3,  8, 88,  6, 15],
       [39, 16, 59, 91, 33, 13, 53, 73],
       [20, 56, 70, 35, 16, 12, 80,  6],
       [21, 17, 23, 21, 72, 93, 58, 56],
       [26, 86, 38, 90, 91, 87, 65,  0],
       [67,  6, 10, 94, 19, 25, 49, 61]])

In [4]: dropout_mask = np.random.randint(0, 2, (8, 8))    # either 0 or 1

In [5]: dropout_mask
Out[5]: 
array([[1, 1, 0, 0, 0, 1, 0, 1],
       [1, 0, 1, 1, 0, 0, 0, 0],
       [1, 0, 0, 1, 0, 1, 1, 0],
       [0, 0, 0, 0, 1, 0, 1, 0],
       [0, 1, 0, 0, 0, 0, 1, 0],
       [0, 1, 1, 1, 0, 0, 0, 1],
       [0, 1, 1, 0, 1, 1, 1, 1],
       [1, 0, 0, 1, 0, 1, 1, 1]])

        # we make a copy here just to use helpful variable names
In [6]: masked_weights = weights.copy()

        # where the mask is ""on"" (==1), we set those weights to 0
In [7]: masked_weights[dropout_mask == 1] = 0 

In [8]: masked_weights
Out[8]: 
array([[ 0,  0,  9, 87, 23,  0, 60,  0],
       [ 0, 48,  0,  0, 34, 56, 52, 82],
       [ 0,  0, 70,  0,  8,  0,  0, 15],
       [39, 16, 59, 91,  0, 13,  0, 73],
       [20,  0, 70, 35, 16, 12,  0,  6],
       [21,  0,  0,  0, 72, 93, 58,  0],
       [26,  0,  0, 90,  0,  0,  0,  0],
       [ 0,  6, 10,  0, 19,  0,  0,  0]])
</code></pre>

<p>So we have selected roughly half of weights and <em>turn them off</em> for a pass through the network.</p>

<p><strong>EDIT</strong>:</p>

<p>Droput randomly drops neurons on each pass in training, as shown above, but during test time (a.k.a. inference), the dropout layers are deactivated by default. This means that all neurons are available and are used. There is still, however, the issue of scaling - so while all neurons are active during inference, their outputs are scaled to reflect the layers overall output such that the expected overall sum remains unchanged. This can be performed a few different ways - see comments on this answer for related links.</p>

<p>There are of course nice implementations of <em>dropout</em> in all deep learning frameworks (PyTorch, Tensorflow, CNTK, and so on). As is discussed and <a href=""https://github.com/keras-team/keras/issues/5357"" rel=""nofollow noreferrer"">shown here</a>.</p>

<h2>Intuition</h2>

<p>One intuitive explanation, is that is ensures each neuron increases its <strong>robustness</strong> and manages to contribute to the network in its own right. What I mean by this, is that dropout prevent chain or clusters of neurons being strongly dependent on one another. It helps focus the neurons to be able to deliver some useful information in as many different scenarios as possible, e.g. when it only receives signal from a subset of the neurons in the preceding layer.</p>

<p>It can be helpful to think of dropout in terms of <strong>regularisation</strong>. We add regularisation terms to models in order to penalise them again certain behaviour. An example would be in linear regression via the <a href=""https://en.wikipedia.org/wiki/Bayesian_information_criterion"" rel=""nofollow noreferrer"">Bayesian Information Criterion</a>, whereby we increase a model's error if it uses more and more covariates. We also choose the form of error/regularisation when computing values in backpropagation e.g. we select the sum of squared errors (the $L_2$ loss), or just the raw error (the $L_1$ loss). Dropout, on the other hand, applies a change the network itself! We are constricting it by simply removing access to a random selection of neurons during one single pass through the network.</p>

<h2>Maths</h2>

<p>There actually is not a lot of maths involved, with <a href=""http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf"" rel=""nofollow noreferrer"">the original paper</a> explaining the concept in no more than four lines: </p>

<p><a href=""https://i.stack.imgur.com/Q6IM4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q6IM4.png"" alt=""Dropout equations""></a></p>

<p>The magic really happens in that second line, with the element-wise multiplication of a Bernoulli distributed random variable $\textbf{r}$ and the vector outputs of the preceding layer, $\textbf{y}$. This performs exactly the same operation as <code>IN [7]</code> in my code example above.</p>

<p>I believe the reason is that it was inspired by the intuitive idea, easy to implement and finally proven to work exceptionally well in practice. The above equations can then be applied to various problems - I would suggest reading the paper for some examples.</p>

<p>For further understanding of dropout (and everything around neural network concepts!), I heartily recommend reading <a href=""http://neuralnetworksanddeeplearning.com/index.html"" rel=""nofollow noreferrer"">Michael Nielsen's book</a>. Exaplantions of dropout with excellently helpful diagrams can be found in <a href=""http://neuralnetworksanddeeplearning.com/chap3.html"" rel=""nofollow noreferrer"">Chapter 3</a>.</p>
","4","2","45264","13013"
"36426","<p>Here is an example where I create a new row called <strong>Other</strong>, which contains the sum of all values below a given <code>threshold</code>.</p>

<p>I then remove the rows that were below that threshold (and so included in the sum), so the final Pandas Series only has values above the <code>threshold</code>, plus the new <strong>Other</strong> row.</p>

<p>This is all performed on a randomly generated Pandas Series, as follows:</p>

<pre><code>In [1]: import pandas as pd, numpy as np

In [2]: X = pd.Series(np.random.random(20))

In [3]: X
Out[3]: 
0     0.507151
1     0.366259
2     0.444196
3     0.027280    this will be in the sum
4     0.132785    this will be in the sum
5     0.993170
6     0.614712
7     0.942894
8     0.516255
9     0.107436
10    0.710416
11    0.512221
12    0.502685
13    0.753515
14    0.894735
15    0.780213
16    0.998315
17    0.971558
18    0.504525
19    0.224767
dtype: float64

In [4]: threshold = 0.2

In [5]: X.loc['Other'] = X[X &lt; threshold].sum()

In [6]: X.drop(X[X &lt; threshold].index, inplace=True)

In [7]: X
Out[7]: 
0        0.507151
1        0.366259
2        0.444196
5        0.993170
6        0.614712
7        0.942894
8        0.516255
10       0.710416
11       0.512221
12       0.502685
13       0.753515
14       0.894735
15       0.780213
16       0.998315
17       0.971558
18       0.504525
19       0.224767
Other    0.267501
dtype: float64
</code></pre>

<p>Notice that the indices still match the original values, i.e. they are not continguously numbered - indeices <code>3</code> and <code>4</code> have been <em>dropped</em>/removed.
If you want to get continuously numbered indices back, you can do the following:</p>

<pre><code>X.reset_index()
</code></pre>

<p>This creates a new index and puts the original one shown above in a new column called <code>index</code>, which can then itself be dropped if desired:</p>

<pre><code>X.drop('index', axis=1)
</code></pre>

<p>That will remove the label <strong>Other</strong>, which was in the original index.</p>
","1","2","45264","13013"
"36454","<p>The inclusion of the word <strong>stochastic</strong> simply means the <em>random</em> samples from the training data are chosen in each run to update parameter during optimisation, within the framework of <strong>gradient descent</strong>.</p>

<p>Doing so not only computed errors and updates weights in faster iterations (because we only process a small selection of samples in one go), it also often helps to move towards an optimum more quickly. Have a <a href=""https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent"">look at the answers here</a>, for more information as to why using stochastic minibatches for training offers advantages.</p>

<p>One perhaps downside, is that the path to the optimum (assuming it would always be the same optimum) can be much noisier. So instead of a nice smooth loss curve, showing how the error descreases in each iteration of gradient descent, you might see something like this:</p>

<p><a href=""https://i.stack.imgur.com/fCNJb.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fCNJb.png"" alt=""noisy loss curve""></a></p>

<p>We clearly see the loss decreasing over time, however there are large variations from epoch to epoch (training batch to training batch), so the curve is noisy.</p>

<p>This is simply because we compute the mean error over our stochastically/randomly selected subset, from the entire dataset, in each iteration. Some samples will produce high error, some low. So the average can vary, depending on which samples we randomly used for one iteration of gradient descent.</p>
","8","2","45264","13013"
"36467","<p>the <code>arima</code> function from the <strong>stats</strong> library does not take an argument called <code>model</code>, which is why you are receiving an error. Here is the function signature:</p>

<pre><code>arima(x, order = c(0L, 0L, 0L),
      seasonal = list(order = c(0L, 0L, 0L), period = NA),
      xreg = NULL, include.mean = TRUE,
      transform.pars = TRUE,
      fixed = NULL, init = NULL,
      method = c(""CSS-ML"", ""ML"", ""CSS""), n.cond,
      SSinit = c(""Gardner1980"", ""Rossignol2011""),
      optim.method = ""BFGS"",
      optim.control = list(), kappa = 1e6)
</code></pre>

<p>The returned object, does contain the <code>model</code>. Read <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/arima.html"" rel=""nofollow noreferrer"">the related documentation</a> for more details.</p>

<hr>

<p>I think your workflow is perhaps a little confused. You fit a model first on 20 data points, which is fine (more data would be nice!). You make some forecasts and plot it, which is also good - you can see if the model learned much and if there is perhaps some systematic error, e.g. simply predicting the previous time-step and not a more intelligent trend.</p>

<p>The final step, however, should be to once again make predictions on your hold-out data; the last 4 data points. You should not <strong>fit</strong> another model to the hold-out data! Just predict what you model would say for that data, which it has never before seen.</p>

<p>The reason we work like this, is so we can assess the model's performance independently from the data that was used to train it. We want to know what will happen in the future, when you get your 25th datapoint.</p>

<hr>

<p>Have a look at <a href=""https://www.datascience.com/blog/introduction-to-forecasting-with-arima-in-r-learn-data-science-tutorials"" rel=""nofollow noreferrer"">this nice tutorial</a>, which explains the mains concepts of ARIMA and has a working example. Here is a very <a href=""https://www.youtube.com/watch?v=m4b5yYx8oWw"" rel=""nofollow noreferrer"">similar tutorial</a>, but it is a video.</p>
","0","2","45264","13013"
"36513","<p>As you realise, you are introducing some form of redundancy by using both indicator values and variables; however, it might be a good starting point. Many models such as neural networks and boosted regression will be able to pick up on this, and will likely end up making use of one of the two primarily. This is because, in your particular case, the variable will be 100% correlated with the imputed indicator value.</p>

<p>Using both will give your model as much help as possible to learn about the missing values and their relation to the rest of the data.</p>

<p>My advice would be to as many combinations for which you have time. So try:</p>

<ul>
<li>indicator value and indicator variable</li>
<li>indicator value</li>
<li>imputation via one or more methods</li>
</ul>

<p>You can then compare the results of each of these and (assuming you have enough data to make things statistically trustworthy), you will be able to assess, which method is indeed the best in your case.</p>

<p>You could try thinking about how to encode perhaps a little more information into either the value or variable, in order to distinguish them from one another. E.g. you could vary the value (don't always make it 500, as per your example), depending on a metric e.g. a function of how long it has been since the previous missing value.</p>
","0","2","45264","13013"
"36515","<p>One way to do this is to use the <em>PIPE</em> operator to pass the column of the dataframe into your function, then assign it back to the dataframe.</p>

<p>I created a dataframe with one column holding your example date string five times:</p>

<pre><code>&gt; x &lt;- ""2014-04-06T18:42:05.823Z""
&gt; DF &lt;- as.data.frame(c(x, x, x, x, x), colnames=""original"")
&gt; colnames(DF) &lt;- ""original""
&gt; str(DF)
'data.frame':   5 obs. of  1 variable:
 $ original: chr  ""2014-04-06T18:42:05.823Z"" ""2014-04-06T18:42:05.823Z"" 
</code></pre>

<p>I use this <em>PIPE</em> symbol, which allows you to chain methods together, passing the output from one into the input of the following. Using a <code>.</code> symbolises the output from the previous method.</p>

<p>To make the <em>PIPE</em> operator available, load in the <code>magrittr</code> package (I believe it is also included in the <em>tidyverse</em>, so perhaps via the <code>dplyr</code> package:</p>

<pre><code>library(maggritr)
</code></pre>

<p>Now I apply the <code>as.POSIXct</code> function to the <code>original</code> column, and save it back to the dataframe in a columnn called <code>new</code>:</p>

<pre><code>DF$new &lt;- DF$original %&gt;% as.POSIXct(., tz = ""UTC"", ""%Y-%m-%dT%H:%M:%OS"")
&gt; DF
                  original                     new
1 2014-04-06T18:42:05.823Z 2014-04-06 18:42:05.822
2 2014-04-06T18:42:05.823Z 2014-04-06 18:42:05.822
3 2014-04-06T18:42:05.823Z 2014-04-06 18:42:05.822
4 2014-04-06T18:42:05.823Z 2014-04-06 18:42:05.822
5 2014-04-06T18:42:05.823Z 2014-04-06 18:42:05.822

&gt; str(DF)
'data.frame':   5 obs. of  2 variables:
 $ original: chr  ""2014-04-06T18:42:05.823Z"" ""2014-04-06T18:42:05.823Z"" ...
 $ new     : POSIXct, format: ""2014-04-06 18:42:05.822"" ""2014-04-06 18:42:05.822"" ...
</code></pre>

<p>You can remove the <code>original</code> column if you like, using:</p>

<pre><code>df$original &lt;- NULL
</code></pre>
","0","2","45264","13013"
"36541","<h2>An activation function</h2>
<p>This the name given to a function, which is applied to a neuron that just had a weight update as a result of new information. It can refer to any of the well known activation funtions, such as the Rectified Linear Unit (ReLU), the hyperbolic tangent function (tanh) or even the identity function! Have a look at somewhere like the <a href=""https://keras.io/activations/"" rel=""nofollow noreferrer"">Keras documentation</a> for a nice little list of examples.</p>
<p>We usually define the activation function as being a non-linear function, as it is that property, which gives a neural network its ability to approximate <a href=""https://en.wikipedia.org/wiki/Universal_approximation_theorem"" rel=""nofollow noreferrer""><strong>any</strong> equation (given a few constraints)</a>. However, an
activation function can also be linear e.g. the identity function.</p>
<h2>A squashing function</h2>
<p>This can mean one of two things, as far as I know, in the context of a neural network - the tag you added to the question - and they are close, just differently applied.</p>
<p>The first and most commonplace example, is when people refer to the <strong>softmax function</strong>, which <em>squashes</em> the final layer's activations/logits into the range [0, 1]. This has the effect of allowing final outputs to be directly interpreted as probabilities (i.e. they must sum to 1).</p>
<p>The second and newest usage of these words in the context of neural networks is from the relatively recent papers (<a href=""https://arxiv.org/pdf/1710.09829.pdf"" rel=""nofollow noreferrer"">one</a> and <a href=""https://openreview.net/pdf?id=HJWLfGWRb"" rel=""nofollow noreferrer"">two</a>) from Sara Sabour, Geoffrey Hinton, and Nicholas Frosst, which presented the idea of <strong>Capsule Networks</strong>. What these are and how they work is beyond the scope of this question; however, the term &quot;squashing function&quot; deserves special mention. Paper number one introduces it followingly:</p>
<blockquote>
<p>We want the length of the output vector of a capsule to represent the probability that the entity represented by the capsule is present in the
current input. We therefore use a non-linear &quot;squashing&quot; function to ensure that short vectors get shrunk to almost zero length and long vectors get shrunk to a length slightly below 1.</p>
</blockquote>
<p>That description makes it sound very similar indeed to <a href=""https://en.wikipedia.org/wiki/Softmax_function"" rel=""nofollow noreferrer"">the softmax!</a></p>
<p>This <em>squashing function</em> is defined as follows:</p>
<p><span class=""math-container"">$$
v_j = \frac{||s_j||^2}{1 + ||s_j||^2} \cdot \frac{s_j}{||s_j||}
$$</span></p>
<blockquote>
<p>where <span class=""math-container"">$v_j$</span> is the vector output of capsule <span class=""math-container"">$j$</span> and <span class=""math-container"">$s_j$</span> is its total input.</p>
</blockquote>
<p>If this is all new to you and you'd like to learn more, I'd recommend having a read of those two papers, as well as perhaps a nice overview blog, <a href=""https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b"" rel=""nofollow noreferrer"">like this one</a>.</p>
","3","2","45264","13013"
"36638","<p>A while back, when presenting ML basics to experts of other domains, I also looked into the formal definitions; Mitchell's seems to be the most official/prominent is usage. However, I found that the interpretation can be also somewhat subjective. In addition, the lines between Machine Learning, Deep Learning, Artificial Intelligence and Statistics in general, do sometimes become a little hazy in various fields of application - the terms are also often abused and used as buzz-words (just like <em>Big Data</em>). I think the biggest guinine struggle is perhaps the interpretation of each of the terms that Mitchell introduces: Experience (E), Tasks (T) and Performance (P).</p>

<p>Perhaps it would be worth keeping another definition of <em>machine learning</em> in mind (my current favourite); that of Kevin Murphy from his book Machine Learning: a Probabilistic Approach (page 1):</p>

<blockquote>
  <p>... we define machine learning as a set of methods that can automatically detect patterns in data, and then use the uncovered patterns to predict future data, or to perform other kinds of decision making under uncertainty (such as planning how to collect more data!).</p>
</blockquote>

<p>This is certainly a lot broader, and may help accept the greater everyday toolbox as machine learning, instead of tight constraints.</p>

<hr>

<p>For these reasons, I would say my answer below is more of a <em>discussion</em> than a factual answer.</p>

<p><strong>I'd happily be corrected by readers possessing better informed opinions or facts!</strong>  :-)</p>

<hr>

<h2>Inverted Deduction</h2>

<p>Of the examples you list, I am least familiar with <strong>inverted deduction</strong>. It is described by Mitchell himself (in Section 10.6 of <a href=""https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf"" rel=""nofollow noreferrer"">his book</a>) and   this helpful <a href=""https://www.quora.com/What-does-it-mean-that-the-symbolists-have-inverse-deduction-as-their-master-algorithm"" rel=""nofollow noreferrer"">Quora post</a>, as logically equivalent to <strong>induction</strong>! The latter link includes one paragraph describing this <em>symbolist</em> method, which makes the idea quite accessible to someone more similar to modern <em>connectionist</em> approaches:</p>

<blockquote>
  <p>The most successful form of symbolic AI is expert systems, which use a network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols.</p>
</blockquote>

<p>At this point, it helped me distinguish between <a href=""https://math.stackexchange.com/questions/2547472/is-maths-inductive-or-deductive"">mathematical <em>induction</em> and mathematical <em>deduction</em></a>. The quote above sounds a bit like a expert designed recommender system. Given that <em>deduction</em> implies to refer a universal hypothesis from axioms or known facts (I imagine Sherlock Holmes!), we start with some rules that come from experience (instead of starting with just data). The idea with inverted deduction is that we have some observations and can <em>induce</em> relationships/rules that support the data/observations, and from that we can improve our system. We accept this to be weaker than pure mathematical deduction, and the starting point differs to Mitchell's own definition of machine learning, but if machine learning is an iterative cycle, is our starting point in that cycle critical to the definition?</p>

<p>Putting inverted deduction in the framework of a <em>machine learning</em>  problem (as per our definition with T, E and P), we have the following definition from Mitchell to help: </p>

<blockquote>
  <p>... assume as usual that the training data D is a set of training examples, each of the form $\langle x_i, f(x_i) \rangle$. Here $x_i$ denotes the $i^{th}$ training instance and $f(x_i)$ denotes its target value. Then learning is the problem of discovering a hypothesis $h$, such that the classification $f(x_i)$ of each training instance $x_i$ follows deductively from the hypothesis $h$, the description of $x_i$ and any other background knowledge B known to the system.</p>
</blockquote>

<p>He explicitly writes that the <em>Task</em> is to extract the hypothesis (the target function) rules that explain the system as a whole. <em>Experience</em> is the observations <em>plus</em> background knowledge (including the experience of the experts who create/define the rules). <em>Performance</em> remains as the accuracy or strength of the final set of rules. It is especially the interpretation of experience as data as well as expert/background knowledge, which differs slightly from simple observations.</p>

<p>As a facit: <strong>induction</strong> allows us to make statements about relationships, mapping input to outputs on the known range of our observations, but it is not as watertight as <strong>deduction</strong>, which by definition must be a universal truth (and therefore becomes a <em>theorem</em>). This makes <em>induction</em> sound a lot like machine learning (and therefore <em>inverted deduction</em>!)- just as neural networks learn relationships very well, but only over their input domain i.e. not universally.</p>

<h2>Genetic Programming</h2>

<p>Straight off the bat, I would say this does fall into machine learning by the given definition. Models are iteratively assessed on their ability to perform a predefined task, then further training data is used to improve the performance on that task.</p>

<p>Given the usual approach of allowing many models to train, then selecting the best models and allowing them to somehow be fused (equivalent to genetic cross-pollination in <em>biological</em> reproduction). The resulting child-model would thereafter (hopefully) contain the best traits of the two best parent-models. Just as can be the case in biology, or within Reinforcement Learning and exploration step (as opposed to exploitation), the resulting child-model also is sprinkled with a bit of <strong>luck/randomness</strong> in the form of a <strong>mutation</strong>. Here is <a href=""https://blog.sicara.com/getting-started-genetic-algorithms-python-tutorial-81ffa1dd72f9"" rel=""nofollow noreferrer"">a summary article</a>. The resulting child-model may well be degenerate i.e. less performant than the parents.</p>

<blockquote>
  <p>OP: What is experience in this case? Maybe ""iterations"" / ""epochs""? </p>
</blockquote>

<p>Given the above rough description, I would call a training case as all the work done between the consolidatory steps (breeding and mutation: producing the child-model). This is also similar to the paradigm used within meta-learning (<a href=""https://youtu.be/9EN_HoEk3KY?t=12m55s"" rel=""nofollow noreferrer"">a great lecture from Ilya Sutskever</a>), whereby a task is defined as the sample dataset, plus information about the task itself. This is inspired by the idea of allowing algorithms to ""learn to learn"". For me, this is very close to the idea of genetic algorithms and evolutionary models.</p>

<h2>PCA</h2>

<p>I'm not convinced PCA falls into a machine learning algorithm; however, it is such a commonly used pre-processing method, I would be surprised if it didn't get a mention in any machine learning book.</p>

<p>If I had to force PCA into Mitchell's definition, this would be my best shot: The Task, T, would be simply that of PCA; to reduce the dimensionality of the dataset. The Performance metric, P; I would interpret as the percentage of the variance that is contained with the first $k$ principal components - e.g. just fix $k = 3$, and run the algorithm with more and more data. More data would allow this to be performed more accuractely, i.e. with higher levels of confidence and the variance should be described more densely be the denser set of input points. So, let's say P is given by:</p>

<p>$$
P = \sum_{i=1}^{k} C_i
$$</p>

<p>where $k = 3$, for example and $C_i$ is the $i^{th}$ principal component's normalised coefficient. This sum therefore integrates to 1 over all $k$.</p>

<p>If the value of P actually decreases after adding more data, I would perhaps argue that we have actually learned more about the sample distribution with the extra data, even though our performance metric technically indicates worse results.</p>

<p>Despite this attempt to shoehorn PCA into the definition - I am still uneasy about calling this machine learning (see the second paragraph in my conclusion). </p>

<h2>Clustering</h2>

<p>I agree with you that the only real <em>experience</em> is the number of samples. To add to any doubts, clustering (like PCA), is an unsupervised method - there are typically no labels on the data.</p>

<p>If experience, E, is then the number of data points, the metric, P, would be the inherent error measured used to iteratively improve the clustering result. For example, in the case if KNN, it could be the sum of squared distances between points one cluster and those of a different clusters. Allowing the model to train with additional data points (assuming the ability to overfit), it would not produce a worse value for P: either the same or better.</p>

<p>In my opinion, clustering is included in many machine learning books, not because it falls nicely into Mitchell's definition, but because it is a widely used and extremely useful tool. That being said, it can also be used in the a non-parametric setting, e.g. using a Dirichlet process  as a prior and allowing the number of clusters to grow as more data is provided. I find that idea to be easier to merge into Mitchell's notion of a model improving a metric given more experience!</p>

<p>The fact that clustering is indeed defined as un-supervised learning, i.e. without any sample labels, just makes the idea here more interesting to me.</p>

<hr>

<h2>Conclusion</h2>

<p>I agree with @Anony-Mousse, in that PCA and clustering really don't give the impression of a learning algorithm. I think it is because I conflate <em>statistical</em> learning with <em>human</em> learning. General discussion around machine learning and AI tends to use <strong>anthropomorphism</strong>: e.g. ""AlphaZero learned from scratch how to play a wide range of Atari games""... If we replace AlphaZero with <em>Alice</em> or <em>Bob</em>, it is clear we are talking about a human. This way of describing algorithms leaks into methods like PCA and clustering - people say how PCA <em>learns</em> representations of the variance in the data, even though we struggle to align Mitchell's definition of machine learning to PCA. After the explosion of machine and deep learning since ca. 2012, we see there are still many undisputed terminologies floating around - something that is unavoidable for a rapidly devloping area.</p>

<p>To address the discssion in the comments to Anony-Mousse's answer: Arthur Samuel coined the term <em>machine learning</em>, as a computer being able to learn without being explicitly programmed. Here, in its original context, I believe it meant <strong>to learn</strong> as a human does, i.e. to exhibit the use of intelligence (paraphrased from quotes in <a href=""http://www.genetic-programming.com/machineintelligence.html"" rel=""nofollow noreferrer"">this nice article</a>). Humans learn a great deal from observations, even without knowing the correct answer (the case without labels). Given this, and drawing the direct comparison to <em>unserupervised learning</em>, one could argue that any process by which a machine is able to improve some metric is indeed a learning machine. So clustering as an example, is learning, as it improves a metric and derives new insights into data (without labels). In the case of PCA, the metric is unclear - it must be derived from the usefulness of the results for the task at hand: beauty is the eye of the beholder.</p>

<p>In the end, it doesn't matter too much to me, as long as we know what we are talking about and can make progress in research and applications thereof. My only concern is that this confusion makes teaching the subject more difficult and raises the barrier to entry for newcomers, who have to deal with sometimes contradictory terminology.</p>
","1","2","45264","13013"
"36657","<p>Yes, for most intents and purposes, they can do the same job. From <a href=""https://discuss.pytorch.org/t/equivalent-of-np-reshape-in-pytorch/144"" rel=""nofollow noreferrer"">this link</a>, an example:</p>

<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; t = torch.ones((2, 3, 4))
&gt;&gt;&gt; t.size()
torch.Size([2, 3, 4])
&gt;&gt;&gt; t.view(-1, 12).size()
torch.Size([2, 12])
</code></pre>

<hr>

<p>If you are concerned with memory allocation, here is <a href=""https://stackoverflow.com/questions/42479902/how-view-method-works-for-tensor-in-torch"">another answer on StackOverflow</a> with a little more information. PyTorch's <code>view</code> function actually does what the name suggests - <a href=""https://stackoverflow.com/questions/42479902/how-view-method-works-for-tensor-in-torch"">returns a <em>view</em></a> to the data. The data is not altered in memory as far as I can see.</p>

<p>In <em>numpy</em>, the <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html#numpy.reshape"" rel=""nofollow noreferrer""><code>reshape</code></a> function does not guarantee that a copy of the data is made or not. It will depend on the original shape of the array and the target shape. Have a <a href=""https://stackoverflow.com/questions/36995289/when-will-numpy-copy-the-array-when-using-reshape"">look here</a> for further information.</p>
","1","2","45264","13013"
"36844","<p>In the cs231n course, as far as I remeber, you spend most the time implementing neural networks yourself using nothing but NumPy! that was definitely an amazing learning experience for me.</p>

<p>After that, in the last assignments, you definitely need to be looking at either <strong>TensorFlow</strong> (<a href=""https://github.com/aymericdamien/TensorFlow-Examples"" rel=""noreferrer"">examples</a>) or <strong>Pytorch</strong> (<a href=""https://github.com/pytorch/examples"" rel=""noreferrer"">examples</a>) to build more complicated networks. These frameworks were built by people like those creating courses like CS231n - researchers and industry experts.</p>

<p>The <a href=""http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neural_network"" rel=""noreferrer"">SciKit Learn neural network module</a> consists of feed-forward networks for either classification or regression, but nothing fancier, such as convolutional networks (CNNs), recurrent networks (RNNs) or other more exotic components, such as separate activation functions. </p>

<p>I agree with Djib2011, that that Keras is a great alternative to get started - and will let you choose between TensorFlow, CNTK or Theano as a backend. Keras is a nice uniform wrapper around all of the three monster frameworks, so let's you get things up and running very quickly. Here is a reletively recent and <a href=""https://deepsense.ai/keras-or-pytorch/"" rel=""noreferrer"">useful comparison of Keras with Pytorch</a></p>

<p>Once you are familiar with a tool like Keras, it will be faster to use that than the simple offerings in SciKit Learn.</p>

<hr>

<p>I know you didn't ask about PyTorch, but I thought I'd mention it, as one of the original creators of CS231n, Andrej Karpathy, says it is the best framework(<a href=""https://twitter.com/karpathy/status/868178954032513024?lang=en"" rel=""noreferrer"">source 1</a>, <a href=""https://twitter.com/karpathy/status/829518533772054529?lang=en"" rel=""noreferrer"">source 2</a>).</p>
","5","2","45264","13013"
"36845","<p>the regularisation values are by default computed in the loss and so you cannot see the regularisaiton values separately (as far as I know). For a brief explanation of an example using the cross-entropy loss with regularisation, have a look at <a href=""http://neuralnetworksanddeeplearning.com/chap3.html#regularization"" rel=""nofollow noreferrer"">the example in Michael Nielsen's book</a>.</p>

<p>You can see what is returned and available by saving the results from calling <code>model.fit()</code>.</p>

<p>You can train a model then also check out:</p>

<pre><code>print(history.history.keys())
</code></pre>

<p>which will show you all available metrics for analysis after training the model.</p>

<p>You could perhaps define a <a href=""https://keras.io/metrics/#custom-metrics"" rel=""nofollow noreferrer"">custom function</a> that compute the regularisation terms you are using, and execute that as either your own metric or as a callback function in a Keras model. If you use your own metric, those are by default recorded separately from the training/validation loss value.</p>
","3","2","45264","13013"
"36876","<p>It is common to say the error term follows a standard Guassian distribution. If you assume that to be true, then your squared errors follow a <a href=""https://en.wikipedia.org/wiki/Chi-squared_distribution"" rel=""nofollow noreferrer"">Chi-squared distribution</a>:</p>

<blockquote>
  <p>In probability theory and statistics, the chi-squared distribution (also chi-square or χ2-distribution) with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables.</p>
</blockquote>

<p>Have a <a href=""https://stats.stackexchange.com/questions/78079/confidence-interval-of-rmse"">look here for some ideas</a> about how to implement a quasi-confidence metric, based on your (mean squared) errors. It assumes the errors follow a chi-aquared distribution and then uses the normalised RMSE to define a set of confidence boundaries for a given confidence level,  $\alpha$, as follows:</p>

<p>$$
\left[\sqrt{\frac{n}{\chi_{1-\frac{\alpha}{2},n}^{2}}}RMSE,\sqrt{\frac{n}{\chi_{\frac{\alpha}{2},n}^{2}}}RMSE\right]
$$</p>

<p>See the link for the steps involved. Here is the coded simulation <em>taken from that post</em>, with some added comments (requires python 3):</p>

<pre><code>from scipy import stats
import numpy as np

s = 3                           # a constant to scale the random distribution
n = 4                           # number of samples/states per prediction
alpha = 0.05                    # confidence interval

# distribution with confidence intervals ɑ = 0.05
c1, c2 = stats.chi2.ppf([alpha/2, 1-alpha/2], n)

# we will take this many samples (this pre-allocates the y-vector)
y = np.zeros(50000)

# Loop over each sample and record the result mean sample
# This would be your prediction vector - here it is random noise
for i in range(len(y)):
    y[i] = np.sqrt(np.mean((np.random.randn(n)*s)**2))

# Use the chi-squared distributed confidence intervals to see when predictions fall
# finds percentage of samples that are inside the confidence interval
conf = mean((sqrt(n/c2)*y &lt; s) &amp; (sqrt(n/c1)*y &gt; s))

print(""1-alpha={:2f}"".format(conf))
</code></pre>

<p>Here is another <a href=""https://stats.stackexchange.com/questions/247551/how-to-determine-the-confidence-of-a-neural-network-prediction"">answer on CrossValidated</a>, which gives more information around the area. </p>

<hr>

<p>Additionally, if you assume your predictions lie within a Gaussian distribution, you could use the variance of your predictions as the confidence (welcome to Bayesian learning!). </p>

<p>There are packages that will help you do this, such as <a href=""https://github.com/fmfn/BayesianOptimization"" rel=""nofollow noreferrer"">BayesOptimization</a>. There are lots of examples on that webpage. Essentially, you will be able to make predictions and automatically get robust estimates for condifence... and some cool plots to show where your model is quite sure, and where it isn't:</p>

<p><a href=""https://i.stack.imgur.com/cxAVL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cxAVL.png"" alt=""BayesianOptimization example plot""></a></p>
","2","2","45264","13013"
"36890","<p>You should break this down one step further: retaining <strong>local</strong> structure and retaining <strong>global</strong> structure.</p>

<hr>

<ul>
<li><p>Other well-understood methods, such as <strong>Principal Component Analysis</strong> are great at retaining <strong>global</strong> structure, because it looks at ways in which a dataset's variance is retained, <em>globally</em>, across the entire dataset.</p></li>
<li><p><strong>t-SNE</strong> works differently, by looking at <strong>locally</strong> appearing datapoints. It does this by computing a metric between each datapoint and a given number of neighbours - modelling them as being within a t-distributed distribution (hence the name: t-distributed Stochastic Neighbourhood Embedding). It then tries to find an embedding, such that neighbours in the original n-dimensional space, are also found close together in the reduced (embedded) dimensional space. It does this by minimising the <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""nofollow noreferrer"">KL-divergence</a> between the before and after datapoint distributions, $\mathbb{P}$ and $\mathbb{Q}$ respectively.</p></li>
</ul>

<p>This method has the benefit of retaining local structure - so clusters in the low dimensional space should be interpretable as datapoints that were also very similar in the high dimensional space. t-SNE works remarkedly well on many problem, however there are a few things to watch out for:</p>

<ol>
<li><p>Because we know have some useful local structure retained, we essentially trade that off for ability in retaining global structure. This equates to you not being able to really compare e.g. 3 clusters in the final embedding, where 2 are close together and 1 is far away. This <strong>does not</strong> mean they were also far away from each other in the original space. </p></li>
<li><p>t-SNE can be very sensitive to its <em>perplexity</em> parameter. In fact, you might get different results with the three-cluster example in point 1, using an only slightly different perplexity value. This value can indeed be roughly equated to ""how many points shall we inlude in the t-distribution to find neighbours of a datapoint"" - it essentially gives the area which is encompassed in the t-distribution.</p></li>
</ol>

<hr>

<p>I would recommend <a href=""https://www.youtube.com/watch?v=RJVL80Gg3lA"" rel=""nofollow noreferrer"">watching this lecture</a> by the author of t-SNE, Laurens van der Maaten, as well as getting some intuition for t-SNE and it's parameters using <a href=""https://distill.pub/2016/misread-tsne/"" rel=""nofollow noreferrer"">this great visual explanation</a>.</p>

<p>There are also some good answers <a href=""https://stats.stackexchange.com/questions/238538/are-there-cases-where-pca-is-more-suitable-than-t-sne"">here on CrossValidated</a> with a little more technical information.</p>
","5","2","45264","13013"
"36903","<blockquote>
  <p>...  the reward is now a function of action taken in state given the sampled goal.</p>
</blockquote>

<p>I believe the action taken is that from the <strong>original</strong> goal, not from the newly sampled goal (as you say you understand). Otherwise I think you have everything more or less correct.</p>

<hr>

<p>We see in the first block of the algorithm, that each action $a_t$, given the current goal, <em>g</em>, results in the reward, $r_t$ (as usual). This is stored, along with the new state $s_{t+1}$ concatenated with the current goal (shown by the <code>||</code> symbol). This is highlighted as being <strong>standard experience replay</strong>.</p>

<p>In the second block, using the sampled (<em>virtual</em>) goals $g'$, we receive a <em>virtual</em> reward for our performance using the same action as previously $a_t$. This is repeated for some number of simulated goals, selected by a sampling strategy, of which several are discussed in Section 4.5.</p>

<p>I myself was wondering how many replays are sampled, as it seems that the key there is to sample enough, so that the buffer itself sees the right balance of additional goals (to reduce the reward density), but not so much that the virtual HER recordings from the second for-loop do not outnumber the <em>real</em> performed goal-action pairs from the first loop. In the paper (Section 4.5), this seems to be around the $k=8$ mark, where $k$ is the ratio of sampled/virtual goals to the original goals.</p>

<p>So I believe the sampled goals that are indeed visited states from the original goal, would indeed receive a non-negative reward.</p>

<hr>

<p>I think the following is a key statement to help explain the intuition:</p>

<blockquote>
  <p>Notice that the goal being pursued influences the agent's actions but not the environment dynamics and therefore we can replay each trajectory with an arbitrary goal assuming we have an off-policy RL algorithm like DQN ...</p>
</blockquote>

<p>This is very true in life. Imagine you try to throw a frisbee straight across a field to a friend. It doesn't make it, instead flying off to the right. Although you failed, you could <em>learn</em> that the wind is perhaps blowing left to right. If that had just so happened to be the task at hand, you would've received some positive reward for it!
The authors sample many additional goals, which in my analogy, may be the flight dynamics of that particular frisbee, the air density/humidity etc.</p>

<p>The main contribution of this paper, is a method to increase the density of the reward function i.e. to reduce how sparse the reward is for the model while training. Sampling these additional goals after each attempt (failed or otherwise) gives the framework the opportunity to teach the model something in each episode.</p>

<hr>

<p>In the grid-based example, if for example the agent doesn't reach the final goal (as its <em>original</em> goal), it records -1 to the replay buffer. Then other goals are sampled from the possible next steps according to a sampling strategy, $\mathbb{S}$. For If you were close to the goal, it would make sense that sampling from <em>future</em> states selected at random from the same episode - <em>after</em> the transition - that you would likely end up at the goal. It is important here to realise that the <strong>goal</strong> has changed, which allows reward to be received. I point this out because the goal usually doesn't change in grid-based games; however, the experiments in the paper were performed on a robotic arm with 7-DOF in <em>continuous</em> space (only the reward was discrete).</p>

<p><strong>EDIT</strong></p>

<p>Below is a sketch of an example path, where we reach the final goal after 10 transitions (blue arrows). I set $k = 4$, so in each of the states $s_t$, we also have 4 randomly selected goals. We then take the corresponding action $a_t$ for the current state, which is the blue arrow. If the randomly sampled goal, $g'$, happens to be the same as $s_{t+1}$, we get a non-negative reward - these are the orange arrows. Otherwise, a negative reward is returned: the green arrows.</p>

<p><a href=""https://i.stack.imgur.com/RRFLG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RRFLG.png"" alt=""enter image description here""></a></p>

<p>This is an example of the <em>random</em> sampling strategy, as my sampled goals $G$, are states that have been encountered in <strong>the whole training procesdure</strong> (not just the current episode), even though you cannot see it in my sketch.</p>

<p>So here we see there are 4 sampled goals, whch do indeed return non-negative reward. That was chance. The authors do say that:</p>

<blockquote>
  <p>In the simplest version of our algorithm we repla each trakectory with the goal $m(s_T)$, i.e. the goal which is achieved in the final state of the episode.</p>
</blockquote>

<p>In that case, it would mean $k=1$ and is always simply where the episode ended. This would mean negative rewards in the HER portion of the algorithm for all time steps exluding the final, $t=T$, where we would reach the sampled goal.</p>

<p>That would indeed equate to the model having learned from an otherwise <em>failed</em> episode. In every single episode!</p>
","1","2","45264","13013"
"36934","<p>This will all depend on the dimensions of each of your images. Medical images are usually quite large. Below are some examples of how you can determine this and see how much memory each image (or an array thereof) is occupying in memory (RAM).</p>

<p>Here is an example of 400 images, each of with <code>shape = (400, 400, 3)</code>:</p>

<pre><code>In [1]: import numpy as np
In [2]: from sys import getsizeof
In [3]: a = np.random.rand(400, 400, 400, 3)

In [4]: getsizeof(a)
Out[4]: 1536000144

In [5]: getsizeof(a) / 10**6
Out[5]: 1536.000144               # in Mb
</code></pre>

<p>So you see it should not take more that ~ 1.6 Gb. And this is the worst case, where you have data type <code>np.float64</code>:</p>

<pre><code>In [6]: a.shape
Out[6]: (400, 400, 400, 3)

In [7]: a.dtype
Out[7]: dtype('float64')

Make a copy of the data, this time in `np.float32` format:

In [8]: b = a.astype(np.float32)

In [9]: getsizeof(b) / 10**6
Out[9]: 768.000144                    # in Mb
</code></pre>

<hr>

<p>I would recommend creating two numpy matrices: one for the images and one for the labels. Then save them separately too.</p>

<p>Using the methods above, you should be able to compute how many images you can compute in each step, then simply break down your loop into several smaller ones.</p>
","1","2","45264","13013"
"36935","<p>I understand your pain!</p>

<p>Can you not simply use tools to read the filenames and filter those as required, thereby letting your development language do all the escaping etc. for you?</p>

<p>In Python, for example, that'd mean using something like: <code>os.walk()</code> or <code>os.listdir()</code>.</p>
","1","2","45264","13013"
"36988","<p>At the basic level, CNNs work by finding spatially-linked correlations i.e. places in the input which often appear close together. For this reason, creating cartesian projections of your data from the polar information sounds like the natural way to go about it.</p>

<p>If there is some inherent <strong>structure</strong> to your raw data, it might be possible to use something like a CNN, but you'd have to think carefully about the architecture.</p>

<hr>

<p>Here are a couple of other thoughts that cross my mind that might help you brainstorm or find related research papers:</p>

<ul>
<li><p>Casting the polar data into images, as you describe your method, not only increases the dimensions of the data, but usually the sparsity of the data. This makes it very difficult to train something like a CNN, which generally works best for dense chunks of information, such as a photograph. If your data is sparse, you might consider some other pre-processing such as adding blur to the projections.</p></li>
<li><p>Polar coordinates with a distance metric is essentially the same as a point cloud. If you represent a series as scans, instead of a set of images, but rather as a 3d pointcloud, you could look into models such a <a href=""https://github.com/charlesq34/pointnet"" rel=""nofollow noreferrer"">PointNet, PointNet++, VoxelNet</a> (see the example projects at the bottom of that linked webpage). There are examples of object detection or segmentation in 3d pointclouds, which might give you other ideas for your case - all available openly in Tensorflow or another DL framework.</p></li>
<li><p>How well do you understand your data? Perhaps some further exploration or visualisation might help spark some ideas or at least provide a better feeling for approaches that could work. Try plotting the cartesian coordinates of your data (if you can map many frames to a global origin), using something like PyntCloud (<a href=""https://github.com/daavoo/pyntcloud/blob/master/examples/%5Bvisualization%5D%20Polylines.ipynb"" rel=""nofollow noreferrer"">an example</a>).</p></li>
</ul>
","3","2","45264","13013"
"36998","<p>You generally normalise the inputs e.g. to the range [-1, 1], such that the neural network predicts within the same range.</p>

<p>If you are predicting multiple output of various scales, you could just retain the scaling factors used above when scaling the input, and use them to scale the predicted outputs back to the same scale as the input.</p>
","0","2","45264","13013"
"37015","<p>There can be some other factors that affect this, such as using <a href=""https://en.wikipedia.org/wiki/Simulated_annealing"" rel=""nofollow noreferrer""><em>simulated annealing</em></a> (in a <a href=""https://pdfs.semanticscholar.org/7889/592eb7b44b631bcb242e210b7c8b1de7d197.pdf"" rel=""nofollow noreferrer"">NN context</a>) or other learning rate schedules. <strong>Are you using a specific LR schedule?</strong></p>

<p>A schedule might be that the LR decreases by 50%, every time the validation loss of 5 epochs in a row does not decrease. This will help get closer and closer to a minimum of the loss. However, we know it is possible to get stuck within a local minimum, which may be far from optimal, so we can shake things up by increasing the LR once again, which will essentially throw the algorithm our of tyhe local minima and on its way to a new minima (at least that is the hope). This kind of schedule often produces loss curves like the ones you see.</p>

<p>Another alternative is simple that your batch sizes are quite small, and every once ina  while, you get a batch that consists of example which your model really struggles with, so the loss for that batch (and so the epoch) would <em>spike</em> in comparison to other epochs.</p>

<p>A final idea - thinking more about your data - if it is time-series e.g. of stock prices or weather - there could be a <a href=""https://en.wikipedia.org/wiki/Regime_shift"" rel=""nofollow noreferrer"">regime change/shift</a>. Meaning that the underlying function or system suddenly switches to a new pattern. Something like this could throw your model off the scent for a while, and so produce bumps in the loss curve.</p>

<hr>

<p>A small point on terminology: LSTM and GRU architectures are themselves RNNs. A recurrent network is one in which connections do not <em>only</em> move forward in a network, but can also go sidewards across a layer or indeed backwards. So it is a more general term, whereas LSTM/GRU layers are specific examples of RNNs.</p>

<p>If you can say a little more about your three model architectures, perhaps it might be clearer which names to use - and maybe even better understand these loss curves  :-)</p>
","3","2","45264","13013"
"37017","<p>I'm not sure why/where you want to apply the noise, but if you want to add some Gaussian noise to a variable, you can do this:</p>

<pre><code>import numpy as np

target_dims = your_target.shape
noise = np.random.rand(target_dims)
noisy_target = your_target + noise
</code></pre>

<p>Now use the <code>noisy_target</code> as input to your model.</p>
","0","2","45264","13013"
"37024","<p>The function of dropout is to increase the robustness of the model and also to remove any simple dependencies between the neurons.</p>
<p>Neurons are only removed for a single pass forward and backward through the network - meaning their weights are synthetically set to zero for that pass, and so their errors are as well, meaning that the weights are not updated.
Dropout also works as a form of <strong>regularisation</strong>, as it is penalising the model for its complexity, somewhat.</p>
<p>I would recommend having a read of <a href=""http://neuralnetworksanddeeplearning.com/chap3.html"" rel=""nofollow noreferrer"">the Dropout section in Michael Nielsen's Deep Learning book</a> (freely available), which gives nice intuition and also has very helpful interactive diagrams/explanations. He explains that:</p>
<blockquote>
<p>Dropout is a radically different technique for regularization. Unlike L1 and L2 regularization, dropout doesn't rely on modifying the cost function. Instead, in dropout we modify the network itself.</p>
</blockquote>
<p>Here is a <a href=""https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5"" rel=""nofollow noreferrer"">nice summary article</a>. From that article:</p>
<blockquote>
<p>Some Observations:</p>
</blockquote>
<blockquote>
<ul>
<li>Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.</li>
</ul>
</blockquote>
<ul>
<li>Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less.</li>
<li>With H hidden units, each of which can be dropped, we have
2^H possible models. In testing phase, the entire network is considered and each activation is reduced by a factor p.</li>
</ul>
<h3>Example</h3>
<p>Imagine I ask you to make me a cup of tea - you might always use your right hand to pour the water, your left eye to measure the level of water and then your right hand again to stir the tea with a spoon. This would mean your left hand and right eye serve little purpose. Using dropout would e.g. tie your right hand behind your back - forcing you to use your left hand. Now after making me 20 cups of tea, with either one eye or one hand taken out of action, you are better trained at using everything available. Maybe you will later be forced to make tea in a tiny kitchen, where it is only possible to use the kettle with your left arm... and after using dropout, you have experience doing that! You have become more robust to unseen data.</p>
","36","2","45264","13013"
"37028","<p>With your three labels: positive, neutral or negative - it seems you are talking more about <strong>sentiment analysis</strong>. This answer the question: <em>what are the emotions of the person who wrote this piece of text?</em></p>

<p><strong>Semantic analysis</strong> is a larger term, meaning to analyse the meaning contained within text, not just the sentiment. It looks for relationships among the words, how they are combined and how often certain words appear together.</p>

<p>To gain a deeper insight into your text, you could read about topics such as:</p>

<ul>
<li>Semantic Analysis in general might refer to your starting point, where you parse a sentence to understand and label the various parts of speech (POS). A tool for this in Python is <a href=""https://spacy.io/usage/linguistic-features"" rel=""noreferrer""><strong>spaCy</strong></a>, which words very nicely and also provides visualisations to show to your boss.</li>
<li>Named Entity Recognition (NER) - finding parts of speech (POS) that refer to an entity and linking them to pronouns appearing later in the text. An example is to distinguish between <em>Apple</em> the company, and <em>apple</em> the fruit.</li>
<li>Embeddings - finding <em>latent</em> representation of individual words e.g. using <strong>Word2Vec</strong>. Text is processed to produce a single embedding for individual words in the form of an n-dimensional vector. You can then compute <em>similarity measures</em> (e.g. cosine similarity) between the vectors for certain words to analyse how they are related.</li>
<li>Lemmatisation - this method reduces many forms of words to their base forms, which means they appear more regularly and we don't consider e.g. verb conjugations as separate words. As an example, <code>tracking</code>, <code>tracked</code>, <code>tracker</code>, might all be reduced to the base form: <code>track</code>.</li>
</ul>

<p>Your next step could be to search for blogs and introductions to any of those terms I mentioned.</p>

<p>Here is an example parse-tree from spaCy:</p>

<p><a href=""https://i.stack.imgur.com/OyxO3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/OyxO3.png"" alt=""example parse tree""></a></p>

<hr>

<h3>Reducing dimensions</h3>

<p>This is something that would then refer to the vectors, which describe each of your words. Generally, the <a href=""https://en.wikipedia.org/wiki/Word2vec"" rel=""noreferrer""><strong>Word2Vec</strong></a> vectors are something like 300-dimensional. You might want to visualise the words, plotting them in 2d space. You can try a method like <a href=""https://lvdmaaten.github.io/tsne/"" rel=""noreferrer""><strong>t-SNE</strong></a>, which will map the 300d vectors to 2d space, allowing nice plots showing relationships, while retaining as much of the original relationships described in the 300d space. There will, of couse, be some information loss, but you could not have visualised the 300d vectors in the first place!</p>

<p>Using the vectors for your words, you can compute things like the similarity (on a scale between 0 and 1) between <code>president</code> and <code>CEO</code> is something like 0.92 - meaning they are almost synonyms!</p>
","5","2","45264","13013"
"37035","<p>Why do you need that? You shouldn't need to do anything like that using the backend, as Keras will take strings as arguments, or you can use a regular print function.</p>

<p>There are certain special formatting options in Keras, for example, saving model weights during training, inserting the current epoch and validation loss into the name to help give them meaning. This is shown in the documentation for <a href=""https://keras.io/callbacks/#modelcheckpoint"" rel=""nofollow noreferrer"">the ModelCheckpoint callback</a></p>

<p>If you really want to do something special, the Tensorflow backend does have a <a href=""https://www.tensorflow.org/api_docs/python/tf/Print"" rel=""nofollow noreferrer""><code>tf.Print</code> function</a>. I am not sure if you can call this directly from <code>keras.backend.Print</code> or whether you need to directly use tensorflow, with <code>tf.Print</code> - either way it would be the same operation being performed.</p>
","0","2","45264","13013"
"37077","<p>Try aggregating your dataframe on the <code>session id</code> column <em>against</em> the <code>item id</code> column - and pass the function you want, namely <code>mean</code>:</p>

<pre><code>newDF &lt;- aggregate(`item id` ~ `session id`, data, mean)
</code></pre>

<p>Those backticks should be necessary as your column names contain spaces.</p>

<p>Here are some <a href=""https://stackoverflow.com/questions/1660124/how-to-sum-a-variable-by-group"">other alternative methods</a> to get the job done.</p>

<p>–––</p>

<p>You could rename them using this:</p>

<pre><code>names(data) &lt;- c(""col1"", ""col2"") 
</code></pre>
","0","2","45264","13013"
"37086","<h3>Examples</h3>

<p>Here are two adapted functions from the first of the links below that should get some weights out for a given input sample for you and plot the activations for some filters of that layer:</p>

<pre><code>def getActivations(layer,sample):
    units = sess.run(layer,feed_dict={x:np.reshape(sample, [1,784], order='F'), keep_prob:1.0})
    plotNNFilter(units)

def plotNNFilter(units):
    filters = units.shape[3]
    plt.figure(1, figsize=(20,20))
    n_columns = 6
    n_rows = math.ceil(filters / n_columns) + 1
    for i in range(filters):
        plt.subplot(n_rows, n_columns, i+1)
        plt.title('Filter ' + str(i))
        plt.imshow(units[0,:,:,i], interpolation=""nearest"", cmap=""gray"")
</code></pre>

<p>This is the result, showing the second layer of activations from their model used for MNIST classification - the sample used was clearly a <strong>7</strong>:</p>

<p><a href=""https://i.stack.imgur.com/eunYG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eunYG.png"" alt=""example activations from layer 2""></a></p>

<p>Rather than paste the entire code here, I suggest you read their instructions/explanations in context. The first link contains the function above.</p>

<ul>
<li><a href=""https://gist.github.com/awjuliani/acde9d491658265c3fbf6a74b91518e3#file-deep-layer-visualization-ipynb"" rel=""nofollow noreferrer"">Inspect activations at given layer using Tensorflow</a> - (<a href=""https://medium.com/@awjuliani/visualizing-neural-network-layer-activation-tensorflow-tutorial-d45f8bf7bbc4"" rel=""nofollow noreferrer"">blog version</a>)</li>
<li><a href=""https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59"" rel=""nofollow noreferrer"">Visualising layer activations using Keras</a></li>
<li>Using the <a href=""https://www.tensorflow.org/api_docs/python/tf/summary/image"" rel=""nofollow noreferrer""><code>tf.summary.image</code></a> operator on a <code>Variable</code>, which will log the given <code>Variable</code>, allowing you to view it in Tensorboard. <a href=""https://stackoverflow.com/questions/33783672/how-can-i-visualize-the-weightsvariables-in-cnn-in-tensorflow""><strong>Examples here</strong>.</a></li>
</ul>

<h3>Some background</h3>

<p>You can do this a couple of ways:</p>

<ol>
<li><p>extract the activations for a given sample and plot them - you will get plots of varying sizes as you move through the network, correspoding to the dimensions of the weight matrix.</p></li>
<li><p>select your target layer, freeze all layers before that layer, then perform backbrop all the way to the beginning. This essentially extrapolates the weights back to the input, allowing you to display them over an input sample - giving insight as to what a particular layer is focusing on.</p></li>
</ol>

<p>Here is <a href=""https://www.youtube.com/watch?v=ghEmQSxT6tw"" rel=""nofollow noreferrer"">a lecture presented by Matt Zeiler</a>, who co-authored the <a href=""https://arxiv.org/abs/1311.2901"" rel=""nofollow noreferrer"">original paper</a> that showed the second method above. Their approach gives some results like this:</p>

<p><a href=""https://i.stack.imgur.com/2qymZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2qymZ.png"" alt=""Layer three of the network used in the linked paper""></a></p>

<p>They use a ""deconvolutional model"" - which is also named the <em>transpose convolution</em>, which describes it more accurately. A <em>deconvolution</em> is actually a mathematical operator, which has a different meaning, so many people choose not to use the term.</p>

<p>Another approach to this, which goes a bit beyond what you asked, was proposed a few months later, by <a href=""https://arxiv.org/pdf/1312.6034.pdf"" rel=""nofollow noreferrer"">Simonyan et al.</a>, which looks at the saliency maps of weights/images - this is ""the spatial support of a given class in a given image"". IT can help localise objects - read the paper if that sounds good. In <strong>Section 4</strong> of their paper, they claim their method to be a generalisation of the approach from Zeiler and Fergus.</p>
","1","2","45264","13013"
"37121","<p>I don’t know if the seasonality will be achievable in your predictions, due simply to the timeframes you we using. If you see a <strong>daily</strong> up—down movement, but only provide 20 minutes for a prediction, how will the Model know whether or not it is at a turning point? You would perhaps need to include other features that contain that information - perhaps even the time stamp would suffice.</p>

<p>It is odd, that the model always predicts a downward movement — I would have expected it to simply continue on the current path (up or down), assuming you have both directoins in your training data...?</p>

<hr>

<p>Perhaps you could look into some ideas used in common timeseries analysis methods, like separating the seasonality, trend and noise and feeding them separately to the model. Search for terms SARIMA, ARIMA, seasonality and cycles. (S)ARIMA stands for “seasonal autoregressive integrated moving average”, and represents a common way to look at data over time using previous values (autoregressivej, the differences of current value to previous values (integrated) and a moving average of past time steps (moving average).</p>

<p>The terms generated would of course catch the phases where the value changes direction and so be able to model seasonality fairly well.</p>
","1","2","45264","13013"
"37157","<p>Try a one-hot encoding of the elements in your training set (H, C, I etc.) and the same for the chemical descriptors ('acid', 'oxalic' etc.). Then try feeding the data to a simple feed-forward neural network, mapping the one-hot encodings of the descriptors as your <code>x_train</code> and <code>x_val</code> to the chemical elements, which are your <code>y_train</code> and <code>y_val</code>.</p>

<p>For the encodings, take a look at the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""nofollow noreferrer"">Scikit learn encoders. </a></p>

<p>I think such a simple approach may work, as the chemical names are logically named, so any experienced chemist knows exactly which elements to expect in a compound, given its name. Your problem doesn't require further information in the predictions, e.g. the actual chemical makeup, like $CH_3CH_2OH$ (ethanol). </p>

<p>Regarding a small amount of data: look into something like K-fold cross validation. Using this, you select some portion of the data to be your validation data, and train the model. You then repeat this process, selection a different portion of the data. This will help make the most of a limited dataset, although it may introduce overfitting because your model will eventually have seen all the data!</p>

<p>Here is a schematic of this method:</p>

<p><a href=""https://i.stack.imgur.com/qzrok.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qzrok.png"" alt=""k-fold cross validation""></a></p>

<p><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"" rel=""nofollow noreferrer"">Scikit Learn has a class</a> that implements it for you.</p>
","1","2","45264","13013"
"37187","<p>In my opinion, this is subjective and problem specific.  You should use whatever is the most important factor in your mind as the driving metric, as this might make your decisions on how to alter the model better focussed.</p>

<p>Most metrics one can compute will be correlated/similar in many ways: e.g. if you use MSE for your loss, then recording MAPE (mean average percentage error) or simple $L_1$ loss, they will give you comparable loss curves.</p>

<p>For example, if you will report an <em>F1-score</em> in your report/to your boss etc. (and assuming that is what they really care about), then using that metric could make most sense. The F1-score, for example, takes <em>precision</em> and <em>recall</em> into account i.e. it describes the relationship between two more <em>fine-grained</em> metrics. </p>

<p>Bringing those things together, computing scores other than normal loss may be nice for the overview and to see how your final metric is optimised over the course of the training iterations. That relationship could perhaps give you a deeper insight into the problem,</p>

<p>It is usually best to try several options, however, as optimising for the validation loss may allow training to run for longer, which eventually may also produce a superior <em>F1-score</em>. Precision and recall might sway around some local minima, producing an almost static F1-score - so you would stop training. If you had been optimising for pure loss, you might have recorded enough fluctuation in loss to allow you to train for longer.</p>
","6","2","45264","13013"
"37190","<p>I believe it is, in theory, always a good idea to use this method. I say <em>in theory</em> because the theory of gradient descent points to the fact that a minimum can only be reached when the learning rate approaches 0. Otherwise, with a permanent large learning rate, the model's performance (i.e. the loss metric) will <em>bounce</em> around the minimum – always overshooting it.</p>

<p>That being said, it is important to think practically and be aware of the dangers of using this method. There can be phases of training where you don't improve your metric by much, and so the learning rate is reduced. But this can happen prematurely - depending on the plateau-parameters you selected - and so actually impede/prevent the model from ever getting far enough down the loss curve! If you set a minimum learning rate, you may always get down to a minimum (local or otherwise), but it could take a lot longer if your learning rate was reduced prematurely.</p>

<p>In the end, trying out as many combinations as possible is likely your best best.</p>

<p>Alternatively, you could try combing this with <strong>simulated annealing</strong>, whereby the learning rate is boosted upwards again, according to a predefined rule, such as after a number of epochs or a %-reduction in the learning rate itself.</p>
","2","2","45264","13013"
"37192","<p>I would reommend parsing the sentences using something like <a href=""https://spacy.io/usage/spacy-101"" rel=""nofollow noreferrer""><strong>spaCy</strong></a>, which will be able to build such relationships for you.</p>

<p>It will then be a matter of extracting the realtionships and deciding how you want to label them. Let's work through a small example. Imagine your big block of text contains many sentence. We first parse the big block, then could (optionally) extract just the sentence that we care about:</p>

<pre><code>import spacy

nlp = spacy.load('en_core_web_sm')      # this defines our 'parser' for English
doc = nlp(big_block_of_text)            # you may want to clean the text beforehand
</code></pre>

<p>Assume we only want sentences that contain the word 'went':</p>

<pre><code>sents = [sent for sent in doc.sents if 'went' in sent.string]
</code></pre>

<p>Here is an example of <em>grammatically walking</em> down the parse tree. I say semantically because we are using the grammar parts-of-speech to conditionally navigate (using e.g. nouns):</p>

<pre><code>for sent in sents:                          # loop over each sentence
    for word in sent:                       # analyse each word
        if word.pos_ == 'NOUN':             # if it is a noun
            for child in word.children:     # find children (go down the tree)
                if child.pos_ == 'NOUN':    # select only nouns from the children
                    print(word, child)
</code></pre>

<p>I believe this snippet would extract the pairs in your given example, so the output would hopefully indeed be:</p>

<pre><code>""He movie""
""they school""
</code></pre>

<p>There are many other parts-of-speech that you will be able to use to be more specific or match other use cases - check out <a href=""https://spacy.io/api/annotation"" rel=""nofollow noreferrer"">the relevant spaCy docs</a>.</p>

<hr>

<p>How to actually encode the relationships between your target words will depend on their final purpose, i.e. what you eventually want to do with the encodings. You now have the noun pairs. If you want to retain the knowledge of where they actually came from (from which sentences), you could indeed build a hierarchy, that keeps a code for the origin sentence and then a code for each pair of words.</p>
","1","2","45264","13013"
"37308","<p>It seems as though there is a lot of advice on the <a href=""https://developer.yahoo.com/faq/?guccounter=1"" rel=""nofollow noreferrer"">Yahoo Developer Site</a>.</p>

<p>Basic steps will be:</p>

<ol>
<li>get yourself a standard Yahoo account</li>
<li>using that account, get yourself an <em>Application ID</em> (basically an API key)</li>
<li>(probably) get a unique password to go with your API key</li>
</ol>

<p>This will all allow you to authenticate yourself when making requests for data.</p>

<p>There are daily rate limits for data requests, they say:</p>

<blockquote>
  <p>Each service may have different limits on the number of results returned by each query, as well as other variations in supported parameters. The documentation for each service specifies the default number and maximum number you can receive.</p>
</blockquote>

<p>I should also point out that the service is not intended for commercial use, so if you plan to make a product and sell it, based on Yahoo data, you need to probably discuss the API usage with them.</p>

<hr>

<p>The results will be in a form that requires parsing, such as XML, as it seems to be a standard web requests API. This means you can look at tools such as <a href=""https://www.crummy.com/software/BeautifulSoup/bs4/doc/"" rel=""nofollow noreferrer"">BeautifulSoup4</a> or the xml <a href=""https://www.crummy.com/software/BeautifulSoup/bs4/doc/"" rel=""nofollow noreferrer"">ElementTree</a>. You can then choose the specific parts of the received data from Yahoo's API.</p>
","1","2","45264","13013"
"37311","<p>It might not be directly possible to shoehorn the output of your CNN directly into an LSTM (at least without being a lot more thorough with your dimensions).</p>

<hr>

<p>Another approach is to have a look at the Keras wrapper layer: <strong><a href=""https://keras.io/layers/wrappers/#timedistributed"" rel=""nofollow noreferrer"">TimeDistributed</a></strong>. You essentially extract features using your Conv layers as usual, but do that over time-steps, not just a random mini-batch. The features the come out then are e.g. over 5 consecutive timeframes -> this would be one single sample for the LSTM. If you perform a minibatch of say 10 samples, this means 10 * 5 = 50 input frames.</p>

<p>Straight from the linked documentation:</p>

<blockquote>
  <p>This wrapper applies a layer to every temporal slice of an input.</p>
  
  <p>The input should be at least 3D, and the dimension of index one will
  be considered to be the temporal dimension.</p>
  
  <p>Consider a batch of 32 samples, where each sample is a sequence of 10
  vectors of 16 dimensions. The batch input shape of the layer is then
  (32, 10, 16), and the input_shape, not including the samples
  dimension, is  (10, 16).</p>
  
  <p>You can then use TimeDistributed to apply a Dense layer to each of the
  10 timesteps, independently:</p>
</blockquote>

<pre><code># as the first layer in a model
model = Sequential()
model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))
# now model.output_shape == (None, 10, 8)
</code></pre>

<hr>

<p>As a hint for dimensions, make use of <code>model.summary()</code> on a compiled model in order to inspect the dimensions of each layer through the model. You will likely have to get a pen and paper out to work through the dimensions, as things start to become a little complicated when using such a wrapper layer.</p>

<p>Unfortunately I don't have any links/tutorials to show how this is exactly done in Keras, but hopefully the description above gives you enough info for some useful Bing! searches. Just kiddin... Google searches  ;-)</p>
","1","2","45264","13013"
"37312","<p>Have a look at the <a href=""https://github.com/tensorflow/models/blob/master/research/syntaxnet/g3doc/syntaxnet-tutorial.md#detailed-tutorial-building-an-nlp-pipeline-with-syntaxnet"" rel=""nofollow noreferrer"">documentation from the Authors</a> of SyntaxNet themselves (Google). That link shows all the main steps that you might want to do.</p>

<p>Looking at <a href=""https://github.com/tensorflow/models/tree/master/research/syntaxnet"" rel=""nofollow noreferrer"">the general setup required</a>, it seems like quite an involved process! I can't actually find any nice walkthroughs on how to really run and understand the models... only that first link above!</p>

<hr>

<p>If you would like to simply parse sentences and are not tied to SyntaxNet, it might be worth looking into similar and (in my opinion) <em>easier to use</em> tools, such as <a href=""https://spacy.io/usage/linguistic-features"" rel=""nofollow noreferrer""><strong>spaCy</strong></a>. You can <a href=""https://spacy.io/usage/"" rel=""nofollow noreferrer"">install it easily</a> using one of the commands from their website:</p>

<p><a href=""https://i.stack.imgur.com/bEQ3T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bEQ3T.png"" alt=""enter image description here""></a></p>
","1","2","45264","13013"
"37358","<p>You are correct: a probability cannot be larger than 1.</p>

<p>At the final layer, the activations (also known as <em>logits</em>), are passed through the final a <a href=""https://en.wikipedia.org/wiki/Softmax_function"" rel=""nofollow noreferrer""><em>softmax</em> function</a> in order to fulfill this constraint. The standard neural network does not have an implicit mechanism by it can ensure that constraint is met during training etc.</p>

<hr>

<p>By non-mutual classification, we could be talking about something like classifying cat and dog images - in which case the label for each image either cat <strong>or</strong> dog. So they are mutually <strong>exclusive</strong>. This is a very common case - almost any form of image classification falls into this category.</p>

<p>You do not use a <em>sigmomid</em> function (or any other non-linearity for that matter) after the final layer, as there are no neurons following them, making a non-linearity somewhat redundant. Using a non-linearity for the purpose of fitting a non-linear model is different to the purpose of a final softmax function. This has exactly the purpose of <em>scaling</em> the final logits/activations into the nice range of <code>[0, 1]</code> that can be interpreted as probabilities. That allows us to make simple rules on how to classify the outputs - e.g. if <code>p = [0.51, 0.49]</code> then that sample was a cat, whereas <code>p =[0.49, 0.51]</code> is a dog.</p>

<p>I used those values in the example to highlight a further point; namely that you cannot interpret them as pure probabilities. Those examples don't mean the model was really unsure in both cases because all four ""probabilities"" we close to 0.5. The model gives more weight to the option is believes is correct - the relative magnitudes of those values are not directly interpretable.</p>
","0","2","45264","13013"
"37361","<p>The softmax function is simply a generalisation of the logistic function, which simply squashes values into a given range.</p>

<p>At the final layer of a neural network, the model produces its final activations (a.k.a. <em>logits</em>), which we would like to be able to interpret as probabilities, as that would allow is to e.g. create a classification result.</p>

<p>the reason for using the softmax is to ensure these logits all sum up to 1, thereby fulfilling the constraints of a probability density. If we are try to predict whether a medical image contain cancer or not (simply <code>yes</code> or <code>no</code>), then the probability of a positive result (<code>yes</code>) and a negative result (<code>no</code>) must sum up to one. o the model produces a probability vector for each outcome, in pseudo-code: <code>p = [yes, no]</code>.</p>

<p>If the final logits in this binary classification example were <code>p = [0.03, 1.92]</code>, we can see that they sum to <code>1.95</code>. This is not interpretable as a probability, although we can see the value is much higher for <code>no</code>. In other examples where there might be 1000s of categories, not just two), we can no-longer assess so easily, which is the largest logit. The softmax gives us some perspective and (quasi-) interpretable probabilities for the categories.</p>

<p><strong>EDIT</strong></p>

<p>As described by <a href=""https://datascience.stackexchange.com/questions/37357/why-is-the-softmax-function-often-used-as-activation-function-of-output-layer-in/37361?noredirect=1#comment43757_37361"">@Neil Slater</a> in his comment below, using the <strong>softmax followed by a log loss</strong> function does indeed lead to a model that predicts values in the range of <em>true</em> probabilities, thus making them interpretable, as well as providing some nice statistical properties (see the <a href=""https://en.wikipedia.org/wiki/Maximum_likelihood_estimation"" rel=""nofollow noreferrer""><em>Properties</em> section of Maximum Likelihood Estimation</a>).</p>

<p>Also note: <strong>minimising log-loss</strong> is equivalent to <strong>maximising the Maximium Liklihood Estimation</strong>. See <a href=""https://stats.stackexchange.com/questions/152368/can-we-express-logistic-loss-minimization-as-a-maximum-likelihood-problem"">a mathematical explanation</a> here.</p>

<p>Implementations of the log-loss include:</p>

<ul>
<li>the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html"" rel=""nofollow noreferrer"">log_loss</a> from Scikit Learn</li>
<li><a href=""https://keras.io/losses/#binary_crossentropy"" rel=""nofollow noreferrer"">binary_crossentropy</a> in Keras (for binary problems)</li>
<li><a href=""https://keras.io/losses/#categorical_crossentropy"" rel=""nofollow noreferrer"">categorical_crossentropy</a> in Keras (for cases with > 2 classes)</li>
</ul>

<p>These first two method names also highlight that log-loss is also the same as cross-entropy (in the general machine learning context of computing error rates between 0 and 1).</p>
","2","2","45264","13013"
"37369","<p>If I understand your meaning: you are modelling by proxy. In other words, it could be seen as using a common <em>autoregressive</em>, simply with the autoregressive part itself removed. I won't get into the details of RNNs and Keras, but let me first explain with a common model used for time-series analysis, ARIMA:</p>

<p><a href=""https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average"" rel=""nofollow noreferrer""><strong>ARIMA</strong></a>X stands for:</p>

<ul>
<li><strong>A</strong>utoreg<strong>r</strong>essive - model a variable using its own historic values</li>
<li><strong>I</strong>ntegrated - use <em>differences</em> between values over timesteps (not the raw values)</li>
<li><strong>M</strong>oving <strong>A</strong>verage - include historic values of the actual forecast errors</li>
<li><strong>X</strong> - common final letter to show we use e<strong>x</strong>ogenous variables, variable apart from our target</li>
</ul>

<p>ARIMA is characterised by three parameters, which tell us how many previous timesteps of the first three terms above are used: ARIMA(p, d, q). <code>p</code> is for <strong>AR</strong> terms, <code>d</code> for <strong>I</strong> terms and <code>q</code> for <strong>MA</strong>.</p>

<p>Within this framework, your problem would be equivalent to setting <code>p = 0</code>. We can see on the Wikipedia site linked above, <a href=""https://people.duke.edu/~rnau/411arim.htm"" rel=""nofollow noreferrer"">or here</a>, that doing so makes the ARIMA model essentially an exponential smoothing model!</p>

<hr>

<p>You could start by analysing how correlated your variables <code>X1</code> and <code>X2</code> are with the target variable <code>X3</code>, in order to assess how well (or badly) they might explain the target.</p>

<p>If you follow some tutorials on setting up an experiment for ARIMA, it will only be technicalities to feed the same data into an RNN, or anything else you might build using Keras. You need to feed blocks of data into the model, leaving out previous timesteps of the target variable itself... although if you have them and can use them for validation/testing, I would wonder why you aren't using them to improve the model!</p>
","1","2","45264","13013"
"37370","<p>I am unsure there will be a formal way to show which is best in which situations - simply trying out different combinations is likely best!</p>

<p>It is worth noting that Dropout actually does a little bit more than just provide a form of regularisation, in that it is really adding robustness to the network, allowing it to try out many many <em>different</em> networks. This is true because the randomly deactivated neurons are essentially removed for that forward/backward pass, thereby giving the same effect as if you had used a totally different network! Have a look at <a href=""https://datascience.stackexchange.com/questions/37021/why-does-adding-a-dropout-layer-in-keras-improve-machine-learning-performance-g/37024#37024"">this post for a few more pointers regarding the beauty of dropout layers</a>.</p>

<p>$L_1$ versus $L_2$ is easier to explain, simply by noting that $L_2$ treats outliers a little more thoroughly - returning a larger error for those points. Have a look here for <a href=""http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/"" rel=""noreferrer"">more detailed comparisons</a>.</p>
","22","2","45264","13013"
"37391","<p>The scikit-learn <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"" rel=""nofollow noreferrer"">DecisionTreeClassifier</a> takes a parameter called <code>random_state</code>. If this is set to an integer, your model should produce the same results every time.</p>

<p>The person suggesting you run the model many times would be correct, assuming you allow for no set random state. This means the results should be slightly different every time, because there is some random selection going on in the algorithm. Here is an example from <a href=""https://github.com/scikit-learn/scikit-learn/blob/14031f65d144e3966113d3daec836e443c6d7a5b/sklearn/tree/_splitter.pyx"" rel=""nofollow noreferrer"">the splitter classes</a>:</p>

<pre><code># Draw a feature at random
f_j = rand_int(n_drawn_constants, f_i - n_found_constants,
               random_state)
</code></pre>

<p>If are are not setting that random state (or any other kind of random seed), I am not sure off the top of my head, why or how the results would always be identical.</p>
","1","2","45264","13013"
"37392","<p>This is not really a data science specific question, so you might want to ask elsewhere; in any case, you need to provide more information!</p>

<p>What error message are you receiving? Are you running out of memory? What size are the images?</p>

<hr>

<p>Unless your images are really really large, I would not expect you to be running out of memory, given you have 32Gb. however, if you have built a massive network, where e.g. many ConvNet filters are then passed to an extremely wide FullyConnected layer (Dense layer), you will have a huge number of weights, which may result in a memory error.</p>

<p>Try creating your model (or a similar one) using Keras instead of pure Tensorflow, and then using <code>model.summary</code> on the compiled model. This will show you a nice overview of the number of weights in each layer.</p>
","2","2","45264","13013"
"37416","<p>There are a couple of things to know around this topic:</p>

<h2>Keras Backends</h2>

<p>It might be difficult to get identical results using Keras. This is because it is a wrapper around lower-level libraries, such as <a href=""https://datascience.stackexchange.com/questions/30973/why-choose-tensorflow/30977#30977"">Tensorflow, Theano and CNTK</a>.</p>

<p>Using these backends, a static graph is built that represent all the computation steps in your network. This then allows automatic differentiation (and so backpropagation) to be performed. The graph that is built may be in several blocks. For example, in Tensorflow, you can use <em>context managers</em> to separate when and how weights are updated (essentially using <code>with</code> blocks.</p>

<p>If your model does have these (under the hood or otherwise!), you would need to set a random seed in each of those block. You can a little more on <a href=""https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed"">this topic, here</a>.</p>

<h2>Catastrophic cancellation</h2>

<p>In addition to the above, there are operators in Tensorflow (and probably other frameworks), which use approximations/simplifications for the sake of efficiency and so speed. <a href=""https://www.tensorflow.org/api_docs/python/tf/reduce_sum"" rel=""nofollow noreferrer""><code>tf.reduce_sum</code></a> is an example that introduces non-deterministic deviations that could lead to your variations in accuracy. This operator is used to add up the errors of your model and will do so in a parallelised way where we cannot know the order (or set it with a seed). The problem arises because summation of numbers, as used in that operator, are not <a href=""https://en.wikipedia.org/wiki/Commutative_property"" rel=""nofollow noreferrer"">commutative</a>.</p>

<p><strong>Example:</strong></p>

<p>If I add the numbers <code>1 + 2 + 7</code> or the numbers <code>7 + 1 + 2</code>, both give us the result of 10 - because addition is <em>commutative</em>. However, in floating point addition, where we are adding numbers like <code>1.2223427 + 7.0195516 + 1.9719819</code>, (or actually numbers with much more decimal places) there will be a loss in accuracy as we cannot retain all information... one could imagine it like <strong>rounding errors</strong>. It is also referred to as <em>catastrophic cancellation</em>. See <a href=""http://www.drdobbs.com/floating-point-summation/184403224"" rel=""nofollow noreferrer"">more details</a> here.</p>

<p>In this case, the order in which we add up the numbers will matter! As I mentioned earlier, the parallelisation of operations will mean that we cannot know the order of the operations, and so we cannot guarantee identical answers for the same runs of an algorithm, while still enjoying the parallel computations!</p>

<h2>Practicality</h2>

<p>Although this might cause a headache for some people, because reproducibility is a big - both in academic research as well as industry applications - the variation in results due to this pseudo-randomness and parallelisation/summation errors is really negligible in the bigger picture.</p>

<p>Changing a layer in a deep NN, altering the learning rate or the regularisation are all factors that are much more important and will make larger differences in results. They also encode knowledge and decisions made by you, as a practitioner. I would suggest spending time thinking about these things and not at all worry about these small blips.</p>

<hr>

<h2>Bonus</h2>

<p>There is a nice post from Python Guru &amp; Core DEv: Raymond Hettinger, where he shows how to <a href=""http://code.activestate.com/recipes/393090/"" rel=""nofollow noreferrer"">maintain full precision for summations of floating point numbers</a>. It involves keeping track of sub-totals, which can be used to ensure the final sum did not cause any loss of precision.</p>
","0","2","45264","13013"
"37478","<h2>1. Mini introduction to <a href=""https://en.wikipedia.org/wiki/Generative_adversarial_network"" rel=""nofollow noreferrer"">Generative Adversarial Networks</a> (GANs)</h2>

<p>During the training phase, a GAN model essentially makes two neural networks compete against one another; the generator and the discrimator. As always we start with a training dataset, e.g. of images. Then:</p>

<ul>
<li>The <strong>discrimator</strong> network is given images and must deicde whether or not they are real images</li>
<li>The <strong>generator</strong> learns how to generate new images, with the aim of tricking the discriminator into thinking the image is real</li>
</ul>

<p>Here is a simple sketch from this article:</p>

<p><a href=""https://i.stack.imgur.com/xpWKA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xpWKA.png"" alt=""basic idea of a GAN""></a></p>

<p>Because both networks are fighting over the same metric (the loss of the discrimator network), this idea is also described as a min-max problem. The discriminator is minimising it's own error, while the generator is maximising the same error.</p>

<p><strong>More resources:</strong></p>

<p>To see where GANs all started, have a skim of <a href=""https://arxiv.org/abs/1406.2661"" rel=""nofollow noreferrer"">the seiminal paper by Ian Goodfellow</a>. For a nice video introduction:</p>

<ul>
<li>A general <a href=""https://www.youtube.com/watch?v=Sw9r8CL98N0"" rel=""nofollow noreferrer"">overview from Numberphile</a></li>
<li>A more <a href=""https://www.youtube.com/watch?v=9JpdAg6uMXs"" rel=""nofollow noreferrer"">detailed explanation from Ian Goodfellow himself</a>, explaining GAN-related concepts, such as:

<ul>
<li>mode collapse, where the generator produces copies of the same image</li>
<li>using GAN to generate super-resolution images</li>
</ul></li>
</ul>

<h2>2. Inference using GANs</h2>

<p>Using the process outlined above, we have trained two neural networks. Performing inference could actually mean using either one of those (or a combination of both, I suppose). In your case, it would seem you only require the generator part:</p>

<blockquote>
  <p>I am using DCGAN ( Deep Convolution GAN ) to generate images</p>
</blockquote>

<p>This means you would take your trained <strong>generator</strong> model and use it to generate random samples by providing different random noise states as input (as is shown in the diagram above). Hopefully the learned distribution of your generator during training will be diverse enough to produce different samples. If it ends up always producing the same image, given different random noise samples as input, you are seeing an example of <strong>mode collapse</strong> (see linked video above).</p>

<h2>3. GANs on embedded devices</h2>

<p>To end up with a generator that is really of any practical use, you will need to do a lot of training, with many examples. State-of-the-art models use databases of images such as ImageNet, which contains 1 million images over 1000 categories.</p>

<p><strong>Inference:</strong> </p>

<p>To perform this kind of <strong>training from scratch</strong>, I really don't think you will have any luck on an embedded or micro device, such as a Jetson.
To perform <strong>inference</strong>, however, you might be able to do it. This would simply require loading the pre-trained generator model onto the device (along with the deep learning framework required e.g. Tensorflow). This might work, and I have seen people manage to do this - there were frame rates of around 1 images per second on a conservative convolutional network.</p>

<p>Memory constraints are just a problem as computation itself. On an embedded device, this will likely be a painfully slow process, but could work out if you are able to optimise many parts of the network and training process. You could e.g. compress images to smaller scales, use a final output layer which has fewer neurons, or even go down the path of using less floating point precision in your calculations! See <a href=""https://arxiv.org/abs/1805.11046"" rel=""nofollow noreferrer"">this paper</a> on 8-bit training (as opposed to standard 32-bit). There are also smaller versions of well-known models such as <a href=""https://pjreddie.com/darknet/yolo/"" rel=""nofollow noreferrer"">tiny-YOLO</a>.</p>

<p><strong>Training:</strong></p>

<p>Another approach to edge-training (online training) you might take would be to start with <a href=""http://cs231n.github.io/transfer-learning/"" rel=""nofollow noreferrer""><strong>transfer learning</strong></a>. You could take a model that was already trained on a task similar to your own, which you will have to search for in open-source projects. This model could then be loaded onto the embedded device and made to work as was done for inference, however we want to train. We do this by keeping the weights from the <em>pre-trained</em> model, freezing the majority of the layers and just allowing the final one or two layers to actually be fine-tuned with your new <strong>edge data</strong>.</p>
","1","2","45264","13013"
"37507","<h1>EarlyStopping</h1>

<p>The final weights will be saved, not the weights where your <code>patience</code> parameter is triggered.</p>

<p>Looking at <a href=""https://keras.io/callbacks/#earlystopping"" rel=""nofollow noreferrer"">the documentation for EarlyStopping</a>, it seems not to be involved with saving weights at any point - it isn't mentioned.</p>

<p><strong>EDIT</strong></p>

<p>Upon further investigation (reading the source code), it seems you can indeed save <strong>the best</strong>, using the EarlyStopper callback</p>

<p>The class can be initialised with the aprameters <code>restore_best_weights</code>, <a href=""https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L503"" rel=""nofollow noreferrer"">as seen here</a>. Then at the end of training, when your waiting period has overshot the patience parameter, the model's weights are returned to be the best weights (weights of the model at the time of loewst validation loss:</p>

<pre><code>if self.restore_best_weights:
    if self.verbose &gt; 0:
        print(""Restoring model weights from the end of the best epoch"")
    self.model.set_weights(self.best_weights)
</code></pre>

<p>It does this by tracking a chosen metric and comparing it to the recorded best value. By default this will be the validation loss. Check out the course code and it's <a href=""https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L458"" rel=""nofollow noreferrer"">description of the class there</a>.</p>

<p>If these parameters don't work with your code, you will need to upgrade to the latest version (master branch from GitHub). This new parameter was only <a href=""https://github.com/keras-team/keras/commit/1fc585adb57f20a2acf69f0cd08b731259b8d2f8"" rel=""nofollow noreferrer""><strong>added 10 days ago</strong></a>!</p>

<h1><strong>ModelCheckpoint</strong></h1>

<p>If you want to use a <em>callback</em> to prevent overfitting, have a look at the <a href=""https://keras.io/callbacks/#modelcheckpoint"" rel=""nofollow noreferrer""><strong>ModelCheckpoint callback class</strong></a>. This has options to save the model weights at given times during training, and will allow you to keep the weights of the model at the end of the epoch specifically where the <em>validation loss</em> was at its minimum. This is selected using the the <code>save_best_only</code> parameter - use it like this:</p>

<pre><code>from keras.callbacks import EarlyStopper, ModelCheckpoint

checkpointer = ModelCheckpoint(filepath, monitor='val_loss', verbose=0,
                               save_best_only=False, save_weights_only=False,
                               mode='auto', period=1)

# your early stopper as before
early_stopper = ...
</code></pre>

<p>All callbacks must be placed in a list:</p>

<pre><code>my_callbacks = [checkpointer, early_stopper]

# pass to model along with other parameters as you did already
model.fit(..., callbacks=my_callbacks, ...)
</code></pre>

<p>If you don't use <code>save_best_only</code>, the default behaviour is to save the model at the end of every epoch.</p>

<p>You can set the <code>filepath</code> by using certain dynamic variables available during training, so that the filenames have some useful information:</p>

<p><strong>E.g.</strong> <code>filepath is weights.{epoch:02d}-{val_loss:.2f}.hdf5</code></p>

<blockquote>
  <p>the model checkpoints will be saved with the epoch number and the validation loss in the filename.</p>
</blockquote>

<p>Have a look at the linked documentation for how to use the other parameters of <code>ModelCheckpoint</code>.</p>
","3","2","45264","13013"
"37517","<p>I will first list some examples of models you could look into.</p>

<blockquote>
  <p>You can suggest me things I should learn about too.</p>
</blockquote>

<p>You can look at more classical regression type models, or go <em>deeper</em> in to deep neural networks, utilising Long Short Term Memory cells, which model relationships over time.</p>

<h2>Classical</h2>

<p>The main term that will help you search for related tutorials/documentation will be <strong>autoregressive</strong>, which is a fancy name for saying a target value is predicted from its recent history. How recent, is a parameter you can tune your model for, called <strong>lags</strong>.</p>

<p>Have a look at the options in the <a href=""https://www.statsmodels.org/stable/index.html"" rel=""nofollow noreferrer""><strong>statsmodels</strong></a> module for Python. More specifically, you might want to try out the <a href=""https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima_model.ARIMA.html?highlight=arima"" rel=""nofollow noreferrer""><strong>ARIMA</strong></a> model class. <a href=""https://datascience.stackexchange.com/questions/37363/help-framing-a-sequence-prediction-problem/37369#37369"">For more details on ARIMA, check out this thread</a>.</p>

<p>There are many <em>traditional</em> models that you could use for a time-series problem. Terms you might consider searching for include:</p>

<ul>
<li>Here is an introduction article about these <a href=""https://towardsdatascience.com/generalized-linear-models-8738ae0fb97d"" rel=""nofollow noreferrer"">Generalised Linear Model</a> (GLM) - </li>
<li>Here is a variant, called General Additive Models (GAM) with a <a href=""https://petolau.github.io/Analyzing-double-seasonal-time-series-with-GAM-in-R/"" rel=""nofollow noreferrer"">good walkthtough using R</a></li>
</ul>

<h2>Deep Learning</h2>

<p>This is a much newer topic and is overall a fair bit complexer than the classical models discussed above. If you go this route, you would probably make progress quickest by using a Deep Learning framework such as Keras, which allows you to build complex models without too much time investment. It is a wrapper library that uses either Tensorflow, Theano or CNTK in the background (these are the available <strong>backends</strong>).</p>

<p>In a deep learning approach involving time-series analysis, you almost certainly want to start with an LSTM model. This stands for Long Short Term Memory, and uses a complex cell to monitor various states as you pass in your time-series data. It is a really big topic, so I will just provide:</p>

<ul>
<li><a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""nofollow noreferrer"">an introduction to the logic behind LSTM</a>, and</li>
<li><a href=""https://www.kaggle.com/amirrezaeian/time-series-data-analysis-using-lstm-tutorial"" rel=""nofollow noreferrer"">a nice tutorial on how to get things working</a></li>
</ul>

<p>If you decide to go this way (as opposed to classical models), then <a href=""https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b"" rel=""nofollow noreferrer""><strong>yes, you should understand backprop</strong></a>.</p>

<hr>

<p>This is a really broad problem that you have described and there are many many approaches. It is also likey very active in research within the fields of (self-)localisation.</p>
","1","2","45264","13013"
"37575","<p>As interesting as these kind of questions are; <strong>it will always be subjective and task oriented</strong>. There are a multitude of comparisons between these languages: <a href=""https://www.quora.com/What-are-the-advantages-of-Julia-over-Python-and-R-besides-the-speed-in-the-long-run-for-data-analysis"" rel=""noreferrer"">here</a>, <a href=""https://www.codementor.io/zhuojiadai/julia-vs-r-vs-python-simple-optimization-gnqi4njro"" rel=""noreferrer"">here</a>, <a href=""https://www.infoworld.com/article/3241107/python/julia-vs-python-julia-language-rises-for-data-science.html"" rel=""noreferrer"">here</a> and <a href=""https://discourse.julialang.org/t/julia-vs-r-vs-python/4997"" rel=""noreferrer"">many viewpoints here</a>.</p>

<hr>

<p>I won't list the technical differences, but just offer my perspective on each language.</p>

<h2>Julia</h2>

<p>If you want to learn something new and upcoming, perhaps even influence the development in the long term, Julia might be a nice candidate, because (after ca. 10 years of development) it recently made its first 1.0 release and may gain maintstream use over the coming years. Starting now would potentially make you one of relatively few experts in a few years. It is geared towards scientific computing, combining the strengths of more established languages and I agree, some features do sound too good to be true!</p>

<h2>R</h2>

<p>If you want to have a very well-founded language with more focus on the intracacies of statistics, proven robustness over decades and a strong community, maybe <strong>R</strong> is for you. There are packages for almost anything related to modelling, forecasting, Bayesian methods, ... the list goes on. It can be fairly painless to learn and provides a short path to results, as libraries oftentimes have a single function that does lots of work for you. R also provides hands-down the nicest plotting experience, whose importance is not to be underestimated!</p>

<h2>Python</h2>

<p>If you want a ""Swiss Army Knife"", able to analyse data, build a deep learning model and then perhaps write a webserver to stream API data, from a robot, finally pushing live results to a dynamic website (and so on). There are libraries and tutorials available on seemingly endless topics. Syntax is generally quite friendly, so the path to being productive is just as short as the others. Python isn't as well established (historically) within statistics and machine learning compared to R, but I rarley find myself lacking anything. Python is growing in popularity too, with the majority of academic research I see within Data Science appearing e.g. on GitHub in Python. So it is a mature language and is also expanding.</p>

<hr>

<p>I think other people could offer their perspectives to give a fuller comparison, but that's my two cents. I personally switched from R to Python for almost all tasks - and haven't yet found time to take Julia for a full test-drive (but want to!).</p>

<p>Coming back to my starting point: all the languages are <em>Turing complete</em>, meaning you can achieve anything with any of the languages... but why open a can of beans with a spoon if there is also can-opener available?</p>

<p><strong>For most practical cases</strong>: if you are more comfortable with one language - use that one. If a task requires a certain package from one language, or your company requires the usage of one language - your decision is made for you.</p>

<p>Having motivation to learn a language and get some actual work done should, in my opinion, be the main factor. If you try all three, it is likely one will feel more natural to your way of thinking.</p>
","5","2","45264","13013"
"37625","<p>I would recommend breaking the problem down a little bit to reduce the memory usage at any one time.</p>

<p>The first part of your main function gets the IDs using <code>getIDList</code>. That seems fine, so leave it there.</p>

<p>I would then break that list down into smaller chunks, calling <code>get3Dmatrix</code> on each chunk in turn. Altering your code, it might look something like this:</p>

<pre><code># Get number of entries in ID list
N = len(ID_list)

# break it down into a number of chunks e.g. 4, based on your progress bar
import numpy as np    # should already be imported

N = len(ID_list)
num_chunks = 4           # you can play with this number, making it larger until you don't get emmory errors
chunks = np.linspace(0, N, num_chunks)

for i in range(len(chunks) - 1):
    this_sublist = ID_list[chunks[i] : chunks[i + 1]]
    sub_data_set = get3Dmatrix(this_sublist)

    # At this point, either save this sub_data_set, or try appending it to another list toi make one final numpy matrix at the end before saving

...

print(""Begin saving in numpy file"")
np.save(output_path+'np_ds(10000)-25-25-25(zyx)_one_hot.npy', Data_set)
print(""%s time takes in seconds"" % (time.time() - start_time))
</code></pre>

<p>Even from the traceback you added, it is hard to say where exactly in your code that is happening.</p>

<p>Roughly looking at the dimensions you mention, it also doesn't seem plausible the a 16Gb machine is running out of memory - so I must not completely understand just how many images/patches are being saved.</p>
","1","2","45264","13013"
"37648","<p>The <code>sample_weight</code> parameter is used both to weigh the splits as well as to weigh the errors.</p>

<h3>Errors</h3>

<p>For example, using the LeastSquaresError, the implementation uses the sample weights thusly:</p>

<pre><code>def __call__(self, y, pred, sample_weight=None):
    if sample_weight is None:
        return np.mean((y - pred.ravel()) ** 2.0)
    else:
        return (1.0 / sample_weight.sum() *
                np.sum(sample_weight * ((y - pred.ravel()) ** 2.0)))
</code></pre>

<p>So if we provide large weights to certain samples, their individual error will have a bigger influence on the overall error for that sample set. The squared prediction error for each sample is scaled by by the corresponding weight, and the sum is normalised by the sum of the sample weights.</p>

<p>If no weights are provided, (leaving the default <code>sample_weight=None</code>), then they are initialised to a vector of ones, <a href=""https://github.com/scikit-learn/scikit-learn/blob/f0ab589f/sklearn/ensemble/gradient_boosting.py#L981"" rel=""nofollow noreferrer"">example here</a>.</p>

<h3>Splits</h3>

<p>In the case that you set the parameter <code>min_weight_fraction_leaf</code> upon class instantiation, requiring each leaf to obtain a minimum fraction of the total sum of weight to indeed become a leaf, then <code>sample_weight</code> is also used to scale that value.  You can <a href=""https://github.com/scikit-learn/scikit-learn/blob/f0ab589f/sklearn/ensemble/gradient_boosting.py#L1059"" rel=""nofollow noreferrer"">see it here</a>:</p>

<pre><code>    # Set min_weight_leaf from min_weight_fraction_leaf
    if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
        min_weight_leaf = (self.min_weight_fraction_leaf *
                           np.sum(sample_weight))
</code></pre>
","0","2","45264","13013"
"37651","<p>You can think of each feature running along its own axis on a graph. Just because we store all feature e.g. in a single DataFrame – one feature per column – it doesn't mean the data's structure is just 2d (rows and columns). This is not the case only in NLP, but in most contexts involving statistics and modelling.</p>

<p>We can see this with your example data. There are text blocks, which you should match to a category (as far as I can tell).</p>

<p>The initial dataset contains other meta-data, such as a desctiption of the dataset, the <em>names</em> of the target categories and also the location of each sample's file. We don't really care about these for the pure modelling part. So there are only text blocks, called <code>data</code>, and the target categories, called <code>target</code>. Your input is then 1d - the text blocks.</p>

<p>I will show how to put that into a dataframe, being very verbose about dimensions and features:</p>

<pre><code>from sklearn.datasets import fetch_20newsgroups
groups = fetch_20newsgroups()

import pandas as pd                    # needed to use a dataframe

# Get the desired parts from ""groups""
desired = ['data', 'target']           # we don't care about the 'filenames' ona so on

# make a new dictionary with only desired key-value pairs
only_data = {k: v for k, v in groups.items() if k in desired}
</code></pre>

<p>Now we put this into a dataframe</p>

<pre><code>df = pd.DataFrame.from_dict(only_data)

# Check the shape of the dataframe
df.shape
(11314, 2)
</code></pre>

<p>So there are <code>11314</code> samples of 1 feature, to 1 target variable. This is therefore 1-dimensional input data (we don't count the target variable).</p>

<hr>

<p>When we have e.g. 50 features, explaining some target variable, it may be referred to as a 50-dimensional input space. People then may use <em>dimensionality reduction</em> techniques, such as Principal Components Analysis, which will attempt to squeeze the 50 dimensions into a lesser number (you can choose how many to use).</p>

<p>In your data, you will likely pre-process the text samples to create more features. These will just be new columns in the dataframe, whose <code>shape</code> could become e.g. <code>(11314, 40)</code> if you add another 38 features, by doing things like counting words or constructing some word-embeddings.</p>
","1","2","45264","13013"
"37713","<p>Although my money would be on the interpolation function taking a fair while, here are a few other ideas:</p>

<p><strong>1.</strong> Is that <code>resize_factor</code> matrix going to be different for every single image? You could otherwise pre-compute it one for all images of a given size and then just use it, instead of re-computing it for every image.</p>

<p><strong>2.</strong> One aspect, which may answer <a href=""https://datascience.stackexchange.com/questions/37604/out-of-memory-error-when-consrtucting-2d-list-from-2-numpy-arrays/37625?noredirect=1#comment44195_37625"">your other related question on DS</a>), is memory usage, that can take time if the arrays are large enough. Those matrix divisions that you perform will return a new matrix, that means a new chunk of memory. If it turns out (from the profiling idea below), that those division operations are costly, you could instead try pre-allocating an array to hold all your results (assuming you know the relaevant shapes), then filling the array with the output.</p>

<p><strong>3.</strong> ``scipy.interpolation.zoom<code>returns an</code>ndarray<code>, so you don't need to call</code>np.array<code>on it again in your return statement of the</code>resample` function.</p>

<hr>

<p>I would suggest <em>profiling</em> your script. This will allow you to see which function calls are called most often and which take the most time. You can then focus on making making changes that will have the biggest effect on the runtime. Here is <a href=""https://www.youtube.com/watch?v=8qEnExGLZfY"" rel=""nofollow noreferrer"">a nice example tutorial</a> if you're new to profiling in Python.</p>

<p>You can use <a href=""https://docs.python.org/3.7/library/profile.html"" rel=""nofollow noreferrer""><code>cProfile</code></a>, a built-in python module. You can do this in the terminal quite simply (if your script is designed to be run that way) by running:</p>

<pre><code>python -m cProfile -o profiling_results.prof your_script.py
</code></pre>

<p>In words (flags shown in bold): python run the <strong>module</strong> <code>cProfile</code>, to run <code>your_script.py</code>, producing the <strong>output</strong> file <code>profiling_results.prof</code>.</p>

<p>The generic version of this command it:</p>

<pre><code>python -m cProfile [-o output_file] [-s sort_order] (-m module | myscript.py)
</code></pre>

<p>You can open and read the output file in a normal text editor. The suffix is a common convention, because you can pass it to a tool like <a href=""https://jiffyclub.github.io/snakeviz/"" rel=""nofollow noreferrer""><strong>SnakeViz</strong></a>, which will allow you to visualise the results and introspect the <em>method tree</em> of your script; the order in which functions call each other.</p>

<hr>

<p>There is <a href=""https://www.kaggle.com/arnavkj95/candidate-generation-and-luna16-preprocessing/notebook"" rel=""nofollow noreferrer"">a notebook on Kaggle</a>, which you may find interesting, as it performs similar tasks on the same LUNA16 dataset.</p>
","1","2","45264","13013"
"37715","<p>From the <a href=""https://en.wikipedia.org/wiki/Discounted_cumulative_gain"" rel=""nofollow noreferrer"">wikipedia page</a>:</p>

<blockquote>
  <p>Previously there has not been any theoretically sound justification for using a logarithmic reduction factor<a href=""https://i.stack.imgur.com/H7777.png"" rel=""nofollow noreferrer"">2</a> other than the fact that it produces a smooth reduction. But Wang et al. (2013)[3] give theoretical guarantee for using the logarithmic reduction factor in NDCG. The authors show that for every pair of substantially different ranking functions, the NDCG can decide which one is better in a consistent manner.</p>
</blockquote>

<p>The work showed that the log function is able to converge to consistent results for different ranking functions. It distinguishes the methods nicely even at high numbers of documents (<em>at thelimit</em>).</p>

<p>For example, here is a graph of the ranking measures using <strong>NDCG@k</strong>, where only the top <code>k</code> entries in the ranked list are considered:</p>

<p><a href=""https://i.stack.imgur.com/H7777.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H7777.png"" alt=""enter image description here""></a></p>
","1","2","45264","13013"
"37718","<p>So if you want to impute some missing values, based on the <strong>group</strong> that they belong to (in your case <em>A, B, ...</em>), you can use the <code>groupby</code> method of a Pandas DataFrame. So make sure your data is in one of those first.</p>

<pre><code>import pandas as pd
df = pd.DataFrame(your_data)              # read documentation to achieve this
</code></pre>

<p>Then, it is just a case of chaining a few steps together:</p>

<pre><code>df[""Value""] = df.groupby(""Brand"")[""Value""].transform(lambda x: x.fillna(x.mean()))
</code></pre>

<ul>
<li><a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>df.groupby</code></a> simply groups the dataframe into sub-dataframes (groups), such that each group only contains one <code>Brand</code></li>
<li><a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.transform.html"" rel=""nofollow noreferrer""><code>transform()</code></a> will apply a function to a dataframe - so to each of the individual groups created in <code>groupby</code></li>
<li>the <em>nameless</em> function (a lambda function) calls the DataFrame's <code>fillna()</code> method on each dataframe, using just the <code>mean()</code> to fill the gaps</li>
</ul>

<p>You can simply substitute the <code>mean()</code> method for anything you like. You could also create a more complicated function, ifyou need it, and replace that <em>lambda function</em>. It would simply need to take a dataframe as input and return a dataframe with a comparable <code>index</code>.</p>
","2","2","45264","13013"
"37729","<h3>TL;DR</h3>

<p>This is an interesting idea and probably best to be tested with your specific problem; however it is generally understood that <strong>you will get better results by not using both Dropout (including DropConnect) and Batch-Norm together</strong>, given their overall effects during training. <a href=""https://arxiv.org/pdf/1801.05134.pdf"" rel=""nofollow noreferrer"">Recent evidence/tests</a>. Having said that, I think it would balance out in the end anyway, as you propose.</p>

<h3>More considerations</h3>

<p>Looking at the <a href=""https://cs.nyu.edu/~wanli/dropc/"" rel=""nofollow noreferrer"">explanation of the base implementation</a>, you could be correct in saying that batch normalisation computations of mean and variance could be influenced - but surely it depends <em>in which order</em> you compute your batch statistics?</p>

<p>There is an ongoing debate (see e.g. <a href=""https://stats.stackexchange.com/questions/327620/dropout-before-batch-normalization"">this question</a>) as to which order to apply layers such as batch-norm, dropout and activations themselves to the weights. There is the argument (pointed out above) that using both together isn't a good idea. There are some more <a href=""https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout-in-tensorflow"">great points made here</a>.</p>

<p>If you have a look at <a href=""https://nickcdryan.com/2017/06/13/dropconnect-implementation-in-python-and-tensorflow/"" rel=""nofollow noreferrer"">the three implementations summarised here</a>, you can see that there isn't a big difference between <em>Dropout</em> and <em>DropConnect</em>... it is just about scaling the values correctly, so that the (expected) sum remains consistent.</p>

<h3>To each their own!</h3>

<p>It almost gets philosphical! I believe it depends on your own personal perspective on why we use each of these layers. If you see Dropout as a means of restricting the network and preventing the co-adaptation of neurons, we are trying to block flow of information through certain paths of the network, so it might make sense to also remove that information from the batch-norm computations.
If you see batch-norm as being a surgically precise method to man-handle the input distribution to a layer, you might want to compute batch-norm stats before the DropConnect sets any weights to zero and use those on the entire outbound batch.</p>
","1","2","45264","13013"
"37759","<p>When creating an RNN, we generally assume there is some temporal correlation, for example: that the data has a time-series nature, like the price of a stock. We might consider a window of 30 days as the timestep for a single sample.</p>

<p>When creating input data for an LSTM layer, you need to consider how many timesteps are included in one of your samples, and put the data into that shape. The input shape should be (number_of_sample, num_timesteps, num_features):</p>

<p>Take a <a href=""https://datascience.stackexchange.com/questions/27563/multi-dimentional-and-multivariate-time-series-forecast-rnn-lstm-keras/27572#27572"">look at this answer for more information</a>.</p>

<p>Your data then might be able to be reshaped:</p>

<pre><code>new_shape = data.reshape((None, 100, 4))       # new input shape to LSTM
</code></pre>

<ul>
<li><code>None</code> means you let the number of samples be whatever it must in order to fit your original data shape.</li>
<li><code>100</code> means you pass 100 sequential datapoints/timesteps as a single sample</li>
<li><code>4</code> is your number of independent variables (features)</li>
</ul>

<p>That <code>reshape</code> might throw an error if your data cannot be reshaped to make the dimensions you want. You might need to trim your data (i.e. leave some of teh oldest samples out)</p>
","3","2","45264","13013"
"37768","<p>I would try both and see how big the difference is. It will depend on how clusttered your images are and how you otherwise threshold the mAP score. Will you for example ignore any bounding boxes that are tiny, compared to the image size?</p>

<p>Secondly, I would say it also depends on what you are validating. If you are interesting in the model, tweaking it during training and training further, I'd probably not ignore any proposals. If you are validating results for a real-world deployment, where you know that the thershold of 0.25 will be used, then I would probably just also validate without the <em>thresholded</em> proposals. That would provide a validation metric that might best respresent the expected performance when used in the wild.</p>
","0","2","45264","13013"
"37774","<p>Welcome to Data Science SE! As a short answer has been given, I'll give a longer overview of the problem, starting with a analogous human approach. But first, a <em>formal</em> definition of machine learning from Mitchell:</p>

<blockquote>
  <p>""A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.""</p>
</blockquote>

<p>This essentially says: ""Do something, get it wrong, <em>*perform magic*</em>, try again, repeat until happy""</p>

<p>It doesn't detail the magic part... and I guess that is what you are interested in. Well, one main inspiration for that magic within machine learning (and artificial intelligence) is humans - and how we learn. We generally follow the phrase ""learning by doing"".  Let's use a stupid <em>human example</em> to begin with:</p>

<p><strong>GOAL:</strong> you want to get a cookie from a jar</p>

<p><strong>STEPS:</strong></p>

<ol>
<li>you naïvely stretch out your arms, but you can't reach the jar... no cookie - you are <strong>unhappy</strong></li>
<li>you learn a trick e.g. getting a chair to stand on. Now you can reach the jar - you are <strong>happy</strong></li>
<li>you look in the jar and see 3 different cookies - you want chocolate chip but pick a raisin cookie... don't like the taste - <strong>unhappy</strong></li>
<li>you pick a different cookie... chocolate chip - <strong>happy</strong></li>
</ol>

<p>In this example above, <em>steps 1 and 2</em> sound like <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow noreferrer"">reinforcement learning</a>.... iteratively learning how to complete a task, exploring possibilities. <em>Steps 3 and 4</em> are most like <a href=""http://cs231n.github.io/classification/"" rel=""nofollow noreferrer"">image classification</a>... classifying and labelling images (types of cookies).</p>

<p>Comparing the main steps of learning - first the human brain view, then the machine learning equivalent as the second point:</p>

<ol>
<li><p>Initialisation</p>

<ul>
<li>Your brain is the model, holding lots of information about your experience and how happy your are. It is combined to tell you what to do based on what you <em>sense</em> (input).</li>
<li>a model consisting of a combination of weights/coefficients define  conditional behaviour. Inputs are combined and <em>weighted</em> with these to produce an output.</li>
</ul></li>
<li><p>Doing</p>

<ul>
<li>You get feedback (cookie or not) - this tells you how to adjust your brain's model of the poblem at hand.</li>
<li>The machine learning model measures a numerical error based on its output and the expected result - putting a number on how bad was it's attempt was.</li>
</ul></li>
<li><p>Learning</p>

<ul>
<li>the (average) human cannot now further explain <em>how</em> it improves. It tries something different... goes for a cookie with smaller dark coloured dots, the bigger cookie, etc. This is the <em>magic</em> part that gets into neuroscience.</li>
<li>an ML model can be explained. It uses an optimisation method to make sense of those errors... it changes the weights/coefficients in a way it thinks will reduce the error it measures. </li>
</ul></li>
</ol>

<p>We generally speak about <a href=""https://hackernoon.com/gradient-descent-aynk-7cbe95a778da"" rel=""nofollow noreferrer""><em>gradient descent</em></a> (<a href=""https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3"" rel=""nofollow noreferrer"">more help</a>) as a way to train the model - <strong>to help it learn</strong>. It is called <em>gradient descent</em>  because we imagine the errors/cost of the model are like a hill that we want to <em>descend</em> i.e. we want to get to the bottom of it:</p>

<p><a href=""https://i.stack.imgur.com/81Sss.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/81Sss.png"" alt=""gradient descent curve. Going down the gradient""></a></p>

<p>This curve shows how the iterative process of computing our error, updating the weights and repeating, takes us down the gradient of the error/cost. We do this iteratively, in little steps. The weights of the model are stored between stages - the weights are persistent between iterations. Once the model is happy, measuring small/acceptable errors, we are finished training and can use that hold-out test dataset to validate the model's performance. </p>

<p>Gradient descent (and e.g. backpropagation) are then really where <em>learning</em> happens. They're the methods by which we update the weights. They basically close the loop from (1) getting an error –how unhappy we are– to (2) being able to adjust those weights slightly in a way that would make us better at <em>that</em> task next time. This really equates to adding or subtracting a small amount from the existing weights. Exactly how much we add/subtract is controlled using the <em>learning rate</em> parameter. If it is big, we add/subtract big amounts - so learning fast. But that has its own problems - we cannot learn consistently and can get completely lost, so it is usually a small number ~ 0.001.</p>

<p>The weights we start with in a neural network are most commonly just random numbers. We ask the model to perform a task, then use gradient descent with the errors to slowly improve those random weights with tiny alterations. At the end, it is these weights that we save to disk on the computer. They are intellgient combinations of simply decimal numbers.</p>

<hr>

<p>To get your head around machine learning and what it means, at the highest level, you might want to read <a href=""https://towardsdatascience.com/introduction-to-machine-learning-db7c668822c4"" rel=""nofollow noreferrer"">an article like this</a>.</p>

<p>I would suggest reading <a href=""http://neuralnetworksanddeeplearning.com/"" rel=""nofollow noreferrer""><strong>Michael Nielsen's eBook</strong></a> as a nice intuitive introduction - is a great place to start.</p>
","3","2","45264","13013"
"37839","<p>It means that you randomly select 40% of the neurons and set their weights to zero for the forward and backward passes i.e. for one iteration.</p>

<p>Have a look here for some of <a href=""https://datascience.stackexchange.com/questions/37021/why-does-adding-a-dropout-layer-in-keras-improve-machine-learning-performance-g/37024#37024"">the reasons and benefits</a>.</p>

<p>Have a look here for <a href=""https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"" rel=""nofollow noreferrer"">all the details of standard Dropout</a>. Important is to notice that the remaining weights are commonly scaled by the value <code>p</code>, as to keep the expected mean value of the weights to be roughly consistent over many iterations. Different deep learning frameworks scale the weights at different different points, but the reason it the same. </p>

<hr>

<p>From the <a href=""https://keras.io/layers/core/#dropout"" rel=""nofollow noreferrer"">relevant Keras documentation</a>:</p>

<blockquote>
  <p>Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.</p>
</blockquote>
","5","2","45264","13013"
"37845","<p>As far as I understand the notation, it looks as though it is normal least squares - you are predicting the state, based on the output of $F$ being applied to previous states, constraining the magnitude of the weights.
Perhaps you should distinguish your $w^T$ variables; for example using $w^T$ for the sampled weights and ${w^{T}}^{*}$ for your estimates, i.e. the one within the approximation function, $F$.</p>

<p>The fact that you have $\alpha_j$ in your example approximator should linearly scale the weights arbitrarily to counter any (linear) constraints you apply to $w$. Here I am assuming that $\alpha_j$ is a scalar and learnable, so can acts dynamically with $w$.</p>

<hr>

<p>Did you make a mistake in your subscripts though, going one too far? Should the argument of $F$ not be: $\mathbf{w}^T x(t_j)_{j=0, i}$ (without: $_{+1}$). Otherwise it would seem you actually use the current state to predict the current state, which any good model would exploit for 100% accuracy. I agree the output of $F$ would be e.g. $y(t_j)_{j=i+1}$</p>
","0","2","45264","13013"
"37912","<p>There are a few approaches to this off the top of my head:</p>

<h3>1. Trivial case</h3>

<p>You can first check if the images are identical by simply using numpy's functions, either <code>numpy.array_equal(A, B)</code> or <code>numpy.allclose(A, B)</code>. Where <code>A</code> and <code>B</code> are the two images stored as numpy arrays.</p>

<p>To check if images are semantically similar, you need to compute a <em>similarity measure</em>. First let me back up a bit...</p>

<h3>2. Latent representations</h3>

<p>In deep learning, we can represent data such as images, voice recordings, text etc. in a <em>latent space</em>. This is an unobservable (""not real"") space, where we attempt to capture the essence of the data. For words, we might create a 300-dimensional vector that should encode the information that a given word represents. For images, we could use an <strong>Auto Encoder</strong>.</p>

<p>The idea is to distill the images into a matrix that really contains all the core information in an image. This would be your <em>latent representation</em> of an image,  and this is where you compare images. Here is a schematic of such a model, where the ""bottleneck"" in the middle is where you have achieved a dense representation of an input image:</p>

<p><a href=""https://i.stack.imgur.com/DNKrp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DNKrp.jpg"" alt=""Autoencoder""></a></p>

<p>The model is trained end to end, by trying to expand that condensed middle section into an array the same as the input array (input image). </p>

<p>You create the latent, encoded representation of two images to be compared by running them just through the first half of your trained model (i.e. just the encoder), and calculate a distance or similarity metric between the resulting arrays. You could perhaps use a simple cosine-similarity (of flattened arrays) or even just the $L_2$ norm (sum of squared differences).</p>

<p>Hopefully that all provides enough guidance and keywords for you to find relevant tutorials :-) </p>
","3","2","45264","13013"
"38001","<blockquote>
  <p>Which one of the two models is better?</p>
</blockquote>

<p>It depends what you care about.</p>

<blockquote>
  <p>Why?</p>
</blockquote>

<p>If you just want the highest $R^2$, $R_{adj.}$ or $R_{pred}$, obviously Model 1 is better. In fact, in your example, all performance metrics would tell you to go for Model 1! But there are other aspects to picking a model...</p>

<p>Many people care about the <em>density of explanability</em> (I made that term up); think of it as the explanatory power normalised by number of degrees of freedom (also referred to as the <em>elegance</em> of the model). If that was your preference, then Model 2 look more attractive, as its metrics drop by only 1% and 1% again, compared to Model 1's 10% and 10% drop.</p>

<h2>Some relevant background</h2>

<p>There is the term <a href=""https://en.wikipedia.org/wiki/Minimum_description_length"" rel=""nofollow noreferrer""><strong>Minimum Description Length</strong></a>, which formalises <em>Ockham's Razor</em>, stating:</p>

<blockquote>
  <p>the best hypothesis (a model and its parameters) for a given set of data is the one that leads to the best compression of the data.</p>
</blockquote>

<p>This can help decide which of two models is <em>best</em>, by assessing how <em>dense</em> the explanability of each model. In simple terms: assume models <code>A</code> and <code>B</code> both produce 80% accuracy, but <code>A</code> uses an extra parameter, then <code>B</code> is the more elegant model as it uses its input more efficiently. In other words, its compression of the information is higher.</p>

<p>Here is <a href=""https://towardsdatascience.com/when-bayes-ockham-and-shannon-come-together-to-define-machine-learning-96422729a1ad"" rel=""nofollow noreferrer"">a really nice blog post</a> (published today!) that explains how Ockham's Razor, Bayes' and Shannon Entropy all come together to equally define how thie density of information in a model is a really important criterion. <em>Spoiler:</em> all three boil down to the same guiding principal.</p>

<p>Another way people try to decide which regression model is best, is to use another metric such as the <a href=""https://en.wikipedia.org/wiki/Akaike_information_criterion"" rel=""nofollow noreferrer"">Akaike Information Criterion (AIC)</a> or the <a href=""https://en.wikipedia.org/wiki/Bayesian_information_criterion"" rel=""nofollow noreferrer"">Bayesian Information Criterion (BIC)</a>, both of which try to find a compromise between the performance of a model and the number of degrees of freedom (i.e. number of parameters) that were used. We pick the model with the <strong>lowest</strong> score. For example, the AIC is defined as:</p>

<p>$$
AIC = 2k-2 ln({\hat {L}}),
$$</p>

<p>where $k$ is the number of parameters and $\hat{L}$ is the performance metric (here the Maximum Likelihood is implied). So in plain English, the AIC gets larger and alrger as we include more parameters and out model becomes more complex. However, if the model fits the data more accuractely with each parameter, that is subtracted and so reduces the AIC value. The BIC essentially works the same way. I think it is intuitive what we are trying to do here.</p>

<h2>Summary</h2>

<p>Coming back to my first answer: ""It depends what you care about"" - it hopefully now clear that the selection is subjective. If you can live with high model complexity, you might just go for the best $R_{pred.}$ and so Model 1. On the other hand, if you enjoy an elegant model with high information density, the deltas of 1% between your three metrics will lead you to select Model 2.</p>

<h3>EDIT</h3>

<p>A term I was searching for came back to me: <strong>parsimony</strong>. A parsimonious model is one that reaches accuracy expectation and does so with a few parameters as possible. It is all about maximum information density and explanatory efficiency. Check out this <a href=""https://stats.stackexchange.com/questions/17565/choosing-the-best-model-from-among-different-best-models"">thread that is relevant to your questions</a>.</p>
","2","2","45264","13013"
"38005","<p>From the notation section starting on page $xix$, the subscript $_{\pi}$ seems to be read:</p>

<blockquote>
  <p>... under the policy $\pi$</p>
</blockquote>

<p>So $\mathbb{E}$ is the expectation, whereas</p>

<blockquote>
  <p>$\mathbb{E_\pi}$ is <strong>the expectation under the policy $\pi$</strong>.</p>
</blockquote>

<p>We could compute the expectation of a set of random numbers selected from 1 to 10. If the probabilities of selecting each of the numbers in that range are all equal, we can simple take the weighted mean. This would be equal to 5.5.</p>

<p>However, if we base the selection on some unequal weights, so non-random action (just like a policy would give us), we have unequal weights. Now the answer is not going to be the simple mean, but rather a value skewed towards the heavier weights i.e. the more likely selections. </p>

<hr>

<p>In those specific equations, if I am not mistaken, the authors simply put the subscript there in $\mathbb{E}_{\pi}$ to make it clear that we are working <strong>under the policy $\pi$</strong>. The subscript appears only on the third line, because the policy is removed from the <strong>conditional part</strong> of the expection - the part after the vertical bar. So we remove the relevant policy term: $A_t = \pi'(s)$, and just indicate we are working with the policy via the subscript.</p>
","0","2","45264","13013"
"38006","<p>I am not sure why your <code>MinMaxScaler</code> didn't work, but here is a function that should scale your data into the desired range:</p>

<pre><code>def rescale(data, new_min=0, new_max=1):
    """"""Rescale the data to be within the range [new_min, new_max]""""""
    return (data - data.min()) / (data.max() - data.min()) * (new_max - new_min) + new_min
</code></pre>

<p>Looking at the documentation of the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"" rel=""nofollow noreferrer""><code>MinMaxScaler</code></a>, it seems my function above it the same as their method.</p>

<p>You could break your code down a little to explicitly comppute each step on its own line. This might help find the origins of your problem. I tried it out and got the expected results:</p>

<pre><code>In [1]: import numpy as np

In [2]: from sklearn.preprocessing import MinMaxScaler

In [3]: x = np.random.randint(0, 10, (10, 10)).astype(np.float)

In [4]: x                                    # generate random data in range [0, 9]
Out[4]: 
array([[ 1.,  4.,  5.,  4.,  6.,  1.,  8.,  1.,  8.,  9.],
       [ 3.,  1.,  4.,  4.,  6.,  2.,  5.,  1.,  0.,  8.],
       [ 2.,  0.,  6.,  1.,  5.,  2.,  5.,  8.,  8.,  4.],
       [ 8.,  9.,  2.,  8.,  5.,  6.,  0.,  5.,  0.,  5.],
       [ 1.,  3.,  2.,  2.,  3.,  2.,  4.,  1.,  7.,  5.],
       [ 7.,  0.,  8.,  8.,  3.,  6.,  6.,  6.,  4.,  3.],
       [ 4.,  3.,  4.,  4.,  7.,  6.,  4.,  5.,  6.,  7.],
       [ 9.,  0.,  8.,  9.,  7.,  1.,  2.,  2.,  4.,  6.],
       [ 7.,  4.,  2.,  8.,  6.,  5.,  2.,  9.,  9.,  9.],
       [ 7.,  6.,  9.,  2.,  9.,  0.,  1.,  5.,  7.,  3.]])


In [5]: scaler = MinMaxScaler()              # defaults to range [0, 1]

In [6]: scaler.fit(x)                        # compute the scaling factors
Out[6]: MinMaxScaler(copy=True, feature_range=(0, 1))

In [7]: scaled_data = scaler.transform(x)    # scale the data

In [8]: scaled_data.shape                    # still the same shape
Out[8]: (10, 10)

In [9]: scaled_data.min()                    # min and max are 0 and 1 as expected
Out[9]: 0.0

In [10]: scaled_data.max()
Out[10]: 1.0
</code></pre>
","1","2","45264","13013"
"38009","<p>If I understand your question correctly, I would advise you not to spend time on this.</p>

<p>If you have data that has a structural order to it, for example time-series data where each sample follows from the previous, then shuffling would be throwing that information away.</p>

<p>If your data is not sequential, on the other hand, then computing all possible permutations and training the model on each of them is really just a waste of time. We shuffle the data e.g. to prevent a powerful model from trying to learn some sequence from the data, which doesn't exist.</p>

<p>Training a model on all permutations might be a way to uncover the <em>correct</em> order of the data, is you know it has an order, but it was already shuffled. Otherwise, this experiment would be as useful as training the same model $(n^2)!$ times, just using a different random seem each time.</p>

<p>Have a look here for some <a href=""https://stats.stackexchange.com/questions/245502/shuffling-data-in-the-mini-batch-training-of-neural-network"">more points on shuffling data</a> and why we might do it. Here is more discussion on <a href=""https://datascience.stackexchange.com/questions/24511/why-should-the-data-be-shuffled-for-machine-learning-tasks"">shuffling input samples</a>.</p>
","1","2","45264","13013"
"38029","<h2>Parts of Speech (POS)</h2>

<p>This is what it is called when you label each of the words (often called <em>tokens</em>) of a sentence or many sentences. Usually they are labelled with grammatical descriptions, such as <strong>Noun, Adjective, Adverb</strong>. They can often get quite specific, also distinguishing e.g. between types of nouns (<strong>proper nouns</strong> etc).</p>

<p>You can then use these descriptions of the tokens as input to a model or to filter the tokens to extract only the parts you are interested in. </p>

<p>POS are usually parts of the output when we <em>parse</em> a block of text using an NLP toolkit, such as <a href=""https://spacy.io/usage/linguistic-features"" rel=""nofollow noreferrer""><strong>spaCy</strong>. Have a look here for their available POS</a>.</p>

<p>Here is a snippet of  <em>parse tree</em> of the sentence: <em>Apple is looking at buying a UK startup for $1 billion.</em></p>

<p><a href=""https://i.stack.imgur.com/ws3ws.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ws3ws.jpg"" alt=""start of parse tree""></a></p>

<p>Apple has been recognised as a proper noun (<code>NNP</code>) as well as being the subject of the first verb (shown by the arrow labelled <code>nsubj</code>). </p>

<p>For a nice introduction to POS among many other terms within NLP, <a href=""https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72"" rel=""nofollow noreferrer"">check out this article.</a>. </p>

<h2>Sentiment Analysis Perspective</h2>

<p>There are many many reasons to include POS in a sentiment model (some examples below), but they really all boil down to one overarching reason: <a href=""https://www.thoughtco.com/polysemy-words-and-meanings-1691642"" rel=""nofollow noreferrer""><strong>polysemy</strong></a>. The definition of which is:</p>

<blockquote>
  <p>the coexistence of many possible meanings for a word or phrase.</p>
</blockquote>

<p>So essentially saying, that words in different contexts can have different meanings. This is of course a massive gain in information that we can pass to a model!</p>

<p>The word <strong>duck</strong> can be a noun (the bird) or a verb (the motion, to crouch down). If we can tell a model which one of these it is in a given sentence, the model can learn to make a lot more sense out of the sentence. </p>

<p>Beyond distinguishing between <em>meanings</em> of single words, we can also simply uses them on their <em>usage</em>, or placement. One example use would be to use the adverb: <strong>however</strong>.</p>

<p>If our parser is good enough to tell us that it used in a particular sentence as a <em>contrasting conjunction</em> (which technically, would be grammatically incorrect!). An example sentence could be:</p>

<blockquote>
  <p>I really love muffins, however, I hate strawberries.</p>
</blockquote>

<p>We have two clauses: a positive one before <em>however</em> and one after. The first clause is positive, the latter negative. If we have a scale of <code>-5</code> ro <code>+5</code> for sentiment for each clause (perhaps the mean of each word in that clause) we could imagine scores such as <code>+3</code> for the positive clause and <code>-3</code> for the negative.</p>

<p>This is where I have seen some models (Vader, SentiStrength, etc.) using POS to scale those base scores. In our example, perhaps <em>however</em> would be used to increase the magnitude of the negative clause's score by 10%, giving it a final score of <code>-3.3</code>. Whether or not that makes sense depends on the use case, the data and probably the developers general experiences.</p>

<h2>Summary</h2>

<p>There are many uses for POS, you can imagine quite a few, whether to hand-tailor a sentiment model of just to produce more features. In any case, it is a process that <em>extracts</em> more information from the original raw text, applying langage models (like grammar!) that have been tested and are known to be robust for any official form of writing.</p>
","9","2","45264","13013"
"38034","<p>You can make sure that command is executed for every terminal (meaning Anaconda will be found) by adding it to your user's <strong>bash profile</strong>.</p>

<p>Open a terminal and follow these steps:</p>

<ol>
<li><p>open the terminal profile: <code>gedit ~/.bashrc</code></p></li>
<li><p>at the end of the file, add: <code>export PATH=~/anaconda3/bin:$PATH</code></p></li>
<li><p>save the file (<code>control+s</code>) and close it</p></li>
<li><p>load the changed profile: <code>source ~/.bashrc</code></p></li>
</ol>

<p>Now this terminal window (and any <strong>new</strong> ones) should find Anaconda! See which version of Python is now the default, using <code>which -a python</code>. First in the list should be something like <code>/home/username/anaconda3/bin/python</code></p>

<p>The Anaconda setup/installation usually asks you if you want to prepend Anaconda to start of your path, so whoever installed it must have said no or skipped that step.</p>
","4","2","45264","13013"
"38046","<p>Do you only have one single model? If you were only mixing the data up (and not trying different parameter values) then any plot might not be very really useful. If performance is on the y-axis, what would be on the x-axis? The fold number itself doesn't really give any insight regarding the results (assuming random batch selection).</p>

<p>Are there some statistics you could compute for each of the 10 folds? If there is any metric you can compute for the x-axis, then using a simple line chart would help see how performance varies with that metric. For example, if you have images, you could compute the average pixel value, or the number of true positives/negatives in that batch (if that idea applies to your problem?). An <a href=""http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html"" rel=""nofollow noreferrer"">example from Scikit Learn is this</a>, where the gamma parameter of an SVM is altered on the x-axis:</p>

<p><a href=""https://i.stack.imgur.com/XNvoB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XNvoB.png"" alt=""Varying kernel parameter on x-axis""></a></p>

<p>Additionally, if you have several models (e.g. CNNs with varying numbers of layers / depth), you could use a nice box-whisker plot. <a href=""https://seaborn.pydata.org/generated/seaborn.boxplot.html"" rel=""nofollow noreferrer"">Seaborn offers some nice variations</a>. Here is one example that would show four different models:</p>

<p><a href=""https://i.stack.imgur.com/Gww24.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Gww24.png"" alt=""enter image description here""></a></p>

<p>You could make a box plot for a single model too, which wouldn't be so appealing, but might look like this:</p>

<p><a href=""https://i.stack.imgur.com/rSNF8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rSNF8.png"" alt=""enter image description here""></a></p>

<p>Again, I'm not really sure what you would be putting on the axes.</p>
","1","2","45264","13013"
"38047","<p>Word embeddings are generally used as input features, which like you noticed for image based models, do not get modified during training.</p>

<p>It is in fact quite difficult to update embeddings during training (or at all after they have been computed!), because they are often in some latent space that holds information regarding their relationships to one another. It is for this reason that adding new vocabulary to existing embeddings is extremely challenging, and updating computed embeddings is also difficult without using all the data that was originally used to create them.</p>

<p>I am afraid I don't have an example I can point to where embedding are used during training and simultaneously updated/improved upon. I can imagine that there are ways to store them, however, such that it is technically feasible (although perhaps memory intensive!)</p>
","3","2","45264","13013"
"38073","<h2>TL;DR</h2>

<p>If you have unlimited time and use a 64-bit version of Excel, you can get as far with Excel as any other data analysis tool.</p>

<h2>Time</h2>

<p>I mention <strong>time</strong> as my first factor, because Excel only has basic funcitonality built in, such as summing, random number generation, lookups etc. These correspond to a kind of <em>standard library</em>, which Python and R also have. Using these basic functions, with enough time, you can build up pretty much any analysis tool out there. Don't expect good runtime performance. In Python and R, however, there are many many packages that people have already created, which perform well and have been tested by lots of people and so are trusted.</p>

<h2>Memory</h2>

<p>My second point about 64-bit Excel is because that allows a lot more <strong>memory</strong> to be used by a single instance of Excel. It allows many more cells to be filled. Using 32-bit excel will limit you to projects of around 2Gb. That is a fair amount of data, but it is a hard limit.</p>

<p>Then steps in 64-bit Excel, which basically means no more memory limits - only those that come from your hardware, and that means Python and R will also be stopped in their tracks.</p>

<p>To provide some numbers, we can simply compute the number of bits able to be stored in each version. Here in Python's interactive prompt:</p>

<pre><code>In [1]: (2**32) / 10**9          # 10^9 means the result is 4.3 Gb
Out[1]: 4.294967296

In [9]: (2**64) / 10**18         # 10^15 means the result is 18.4 Eb
Out[9]: 18.446744073709553
</code></pre>

<p><code>Eb</code> means <em>exa-bytes</em>. This means 18.4 million million million gigabytes.</p>

<p>I notice the computation shows 4Gb for 32-bit, while I said 2 Gb above. <a href=""https://support.office.com/en-us/article/choose-between-the-64-bit-or-32-bit-version-of-office-2dee7807-8f95-4d0c-b5fe-6c6f49b8d261"" rel=""nofollow noreferrer""><strong>I read</strong></a> there is a kind of hard limit on the 32-bit version. I don't know or care why that is... I use Python and R  ;-)</p>

<p>In any case, I hope that is enough to convince you that memory is not an issue, if you are a brave person willing to invest all your time building tools from the ground up!</p>

<h2>Summary</h2>

<p>If you have complicated business logic, where the actual analysis is mathematically simple, stick to Excel. Business people will love you for it.</p>

<p>If you want to do more than linear regression, use Python or R.</p>

<h3>Caveats</h3>

<p>As far as I know you cannot run remote or distributed tasks using Excel, whereas that is relatively easy using Python and (a little less so in my opinion) R. So at that point, I would give up on Excel. You'd likely have to implement your own tools in C# or C++ using the .Net framework.</p>
","2","2","45264","13013"
"38075","<p>To get the exact answer you provided, included entries for negative cases, you will have to create a dataframe in advance that is  all possible <code>Id</code> and <code>Categories</code> values. No pain, no gain!</p>

<p>Let's walk through my solution, starting with imports:</p>

<pre><code>In [1]: from itertools import product   # will compute Id/Category possibilities

In [2]: import pandas as pd
</code></pre>

<p>Create your example dataframe</p>

<pre><code>In [3]: df1 = df1 = pd.DataFrame(data={'Id': [1, 1, 2, 2, 2], 'CategoryId': ['A', 'B', 'A', 'E', 'F
   ...: ']})[['Id', 'CategoryId']]

In [4]: df1
Out[4]: 
   Id CategoryId
0   1          A
1   1          B
2   2          A
3   2          E
4   2          F
</code></pre>

<p>Here you must provide the possible values for the ""Id"" and ""CategoryId"" columns</p>

<pre><code># the ids you showed
In [5]: ids = range(1, 3)            # gives [1, 2]

# Either manually create the category values...
In [6]: cats = ['A', 'B', 'C', 'D', 'E', 'F']

# Or get jiggy with some Python to be more flexible:
In [7]: cats = [chr(c) for c in range(ord('A'), ord('F') + 1)]
</code></pre>

<p>Knowing the possible values for each column, we can now compute all possible combinations of those:</p>

<pre><code>In [8]: possibilities = list(product(ids, cats))

In [9]: possibilities
Out[9]: 
[(1, 'A'),
 (1, 'B'),
 (1, 'C'),
 (1, 'D'),
 (1, 'E'),
 (1, 'F'),
 (2, 'A'),
 (2, 'B'),
 (2, 'C'),
 (2, 'D'),
 (2, 'E'),
 (2, 'F')]
</code></pre>

<p>Next we can pre-allocate a results table using the possible <code>Id</code> and <code>Category</code> values:</p>

<pre><code>In [8]: results = pd.DataFrame(index=ids, columns=cats).fillna(0)

In [9]: results
Out[9]: 
   A  B  C  D  E  F
1  0  0  0  0  0  0
2  0  0  0  0  0  0
</code></pre>

<p>I pre-filled the dataframe with <code>0</code> values – you could use <code>'N'</code>.</p>

<p>Now it is a simple matter of checking to see if each possible combination appears or not and filling the coresponding cell in the <code>results</code> dataframe with your desired value (I use a <code>1</code> – you could make it <code>'Y'</code>)</p>

<pre><code>In[10]: for i in list(df1.itertuples()):
            if (i.Id, i.CategoryId) in possibilities:
                results.loc[i.Id, i.CategoryId] = 1
</code></pre>

<p>Check the output:</p>

<pre><code>In [11]: results
Out[11]: 
   A  B  C  D  E  F
1  1  1  0  0  0  0
2  1  0  0  0  1  1
</code></pre>

<p><a href=""https://www.youtube.com/watch?v=q5pESPQpXxE"" rel=""nofollow noreferrer""><strong>That's a bingo!</strong></a></p>
","0","2","45264","13013"
"38104","<p>You can change whatever you like!</p>

<p>The benefits will depend on your data and what exactly you are comparing to. As you didn't say exactly what <em>sequence</em> you have as your starting point, I can't compare anything exactly, but provide a simple outline.</p>

<p>Generally, you follow the following recipe:</p>

<ul>
<li><p><strong>Input layer</strong>: perform any reshaping or normalisation of your data</p></li>
<li><p><strong>Multiplication layer</strong>: either fully-connected layer, a convolutional layer or something else</p></li>
<li><strong>Non-linearity</strong>: <code>relu</code>, <code>tanh</code> - gives the network its power to fit non-linear functions</li>
<li><strong>regularisation layer</strong>: <code>batchnorm</code>, <code>dropout</code> - help models converge and prevent overfitting</li>
</ul>

<p>This scheme is then repeated as many times as you feel is necessary.</p>

<hr>

<p>Doing something like applying several ReLU layers, one after another, definitely doesn't make sense, as it would actually change anything! A ReLU simply filters out negative values, setting them to zero. Applying it a second time would therefore have no effect.</p>
","1","2","45264","13013"
"38122","<p>Pandas dataframes have many many more high level functions integrated right into the base classes that store the data for you.</p>

<p>Some of the commandline tools can be pretty powerful for manipulating text efficiently (Perl in particular), but I would argue that the learning curve is quite steep and the interactive experience is not as friendly. For one thing, it isn't easy to simply get a glimpse of your data or create an attractive plot.</p>

<p>While I admit that I am not a pro awk/sed or Perl user, I am pretty sure it will be a little less intuitive in those tools/languages to do something like this hypothetical computation, which involves numerical data and text:</p>

<pre><code>In [1]: import pandas as pd
In [2]: import numpy as np

# Create a DataFrame holding some data over a time range

In [3]: df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
                                  'foo', 'bar', 'foo', 'foo']*4,
                           'B' : ['one', 'one', 'two', 'three',
                                  'two', 'two', 'one', 'three']*4,
                           'C' : np.random.randn(32)},
                          index=pd.date_range('01.01.2018', periods=32))

In [4]: df.head()
Out[4]:
              A      B         C
2018-01-01  foo    one  0.965554
2018-01-02  bar    one  0.053814
2018-01-03  foo    two  1.075539
2018-01-04  bar  three -0.999941
2018-01-05  foo    two -1.940361
</code></pre>

<p>Now imagine we want to group the rows so we have just rows with column <code>A</code> contains <code>foo</code> in one table, and another with just the rows containing <code>foo</code>.</p>

<p>From those two tables, we only care about column <code>C</code>. We want to compute the moving average over a 5 day time-frame. The moving average will leave some <code>NaN</code> values at the beginning, so we want to drop those time-steps.</p>

<p>Oh, and we want to visualise that!</p>

<pre><code>In[5]: df.groupby('A')['C'].rolling(5).mean().dropna().plot(grid=True, legend=True)
</code></pre>

<p>From that one line of code, we get this:</p>

<p><a href=""https://i.stack.imgur.com/tMTCz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tMTCz.png"" alt=""Not pretty""></a></p>

<p>The above also highlights the abundance of other powerful and specialised packages avavilable within the Python environment - here I used <a href=""http://www.numpy.org/"" rel=""nofollow noreferrer""><code>numpy</code></a> in conjunction with Pandas.</p>

<hr>

<p>For manipulating text files, perhaps cleaning up scraped text and parsing large amounts of text using regular expressions, it might be faster to use one of the commandline options, but as soon as you want to do any data science, I would really recommend using some specialised tools, like Pandas.</p>
","1","2","45264","13013"
"38204","<h1>Data exploration</h1>

<p>I would suggest exploring the data a little further, which might help decide what would be the best approach for this bird-song dataset.</p>

<p>For example, have a look at the spectrogram of each bird (there are only 66 different genus types), to see how you might extract more data from the samples. Here is the spectrogram of a sample <a href=""https://www.kaggle.com/rtatman/how-to-create-spectrograms-from-flac-files"" rel=""nofollow noreferrer"">taken from here</a>:</p>

<p><a href=""https://i.stack.imgur.com/t40Cp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t40Cp.png"" alt=""bird call spectrogram""></a></p>

<p>We can see that there is clearly a repeating pattern! We can see those tall light green blocks intermittently appearing throughout. So although the sample is indeed just over 70 seconds of sound, the call of the bird seems to really only last about 2 seconds!</p>

<p>Either with a simple filtering algorithm or even building a model to find those chunks, you could extract those chunks and only work on those, perhaps along with the data regarding the gaps between those chunks.</p>

<p>That is just one example of data-specific pre-processing; I am sure there are many other ways to improve the information density.</p>

<h1>Samplerate</h1>

<p>This is another degree of freedom you could look at. One idea would be to accept different samplerates within your input to a model. One could adjust the sample rate to ensure the final samples all are all of the same length.</p>

<p>My idea would be to use the length of the shortest sample and then perform regular sampling of all longer sound snippets, such that they the resulting snippets are all of the same length as your shortest sample.</p>

<p>This method will obviously be compromising the quality of the data (irregularly across samples), but if the starting samplerate is sifficiently high, you might be able to get away with it!</p>

<p>Have a look at <a href=""https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a"" rel=""nofollow noreferrer""><strong>this useful article</strong></a> that describes many methods in (pre-processing) sound waves.</p>

<h2>Two models</h2>

<p>In your particular case, if you really only have two possible lengths: <code>8637686</code> and <code>3227894</code>... it might be feasible to simple create two models, one for each sample length. It definitely isn't an optimal solution; however, it would allow for very quick development and model iterations, as you could use the same model and would only need to change on parameter to use both parts of the data.</p>

<h1>Basics</h1>

<p>As well as truncating your longer samples (cutting them to match the length of the shorter/shortest samples - you could using <em>padding</em> to simple make the shorted samples match the length of the longest sample.</p>

<p>Typically, this is done by simply adding zeros to the end of the vectors. You could also try adding zeros at the beginning and end to keep the information centered within each sample.</p>

<p>If you create a neural network using Keras, you would probably be best by first looking at <a href=""https://keras.io/layers/convolutional/#zeropadding1d"" rel=""nofollow noreferrer"">the ZeroPadding1d layer</a>.</p>
","2","2","45264","13013"
"38206","<p>Your two columns <code>X0</code> and <code>X4</code> are constants, i.e. they contain a single value throughout.</p>

<p>The model will be trying to find a constant during its fit, so probably has a check that you're not including one.</p>

<p>Once you have your entire dataframe (e.g. called <code>df</code>), you can remove constant columns like the two above by using:</p>

<pre><code>df_no_constants = df.loc[:, (df != df.iloc[0]).any()]
</code></pre>

<p>Then try putting that into your model as before.</p>
","2","2","45264","13013"
"38208","<p>There is not a consensus that can be proved across all model types.</p>

<p>Thinking of <em>dropout</em> as a form of regularisation, how much of it to apply (and where), will inherently depend on the type and size of the dataset, as well as on the complexity of your built model (how big it is).</p>
","7","2","45264","13013"
"38209","<p>First of all, having this situation isn't very common. If would imply that your training and test data do not originate from the same underlying distribution! That being said, here are a few other thoughts on the matter:</p>

<ol>
<li><p>If the volatile training accuracies <strong>and</strong> the good test accuracy occur consistently i.e. they are reproducible - then I would probably be happy with that. Then start adding different types of regularisation to smooth the training loss curves a little. This could be in the form of batch normalisation or dropout, for example.</p></li>
<li><p>If the test accuracies are all over the place, also varying dramatically, then I would definitely want to sort out the training curves first. Try more data pre-processing, different batch-sizes (perhaps yours is too smnall, hence the volatility) as well as adding regularisation as mentioned above.</p></li>
<li><p>Another approach might be to try some form of dimensionality reduction of your input data. Mapping the features you have to a latent space, where the information may be more dense and (hopefully) more uniform. Which method to try will greatly depend on your type of data: text, images, videos, sound, temperature, stock-prices and so on. Have a look at things like <a href=""https://lvdmaaten.github.io/tsne/"" rel=""nofollow noreferrer"">t-SNE</a>, which will work for most data types, or <a href=""https://en.wikipedia.org/wiki/Word2vec"" rel=""nofollow noreferrer"">Word2Vec</a> for text.</p></li>
</ol>
","1","2","45264","13013"
"38250","<h2>First possibility:</h2>

<p>How well is your dataset balanced? Are there mostly <code>0</code>s? If I understand your graph well, it would seem that is the case.</p>

<p>If you in fact have 90% <code>0</code> and 10% <code>1</code> in your dataset, then your results would indicate the model is actually just performing random selection. This means it hasn't really learny anything about the data.</p>

<h2>Second possibility:</h2>

<p>If your training accuracy is high, but your test accuracy is low, this generally does point to <strong>overfitting</strong>, as you suspect.</p>

<p>Your model is essentially memorising the training data set, making it able to simply recall the correct result quite reliably, but without learning the underlying function that describes the dataset.</p>

<p>Here are a few things that might be behind this:</p>

<ul>
<li>training for too many epochs</li>
<li>using a model that too big and complex</li>
<li>not using enough regularisation</li>
</ul>

<hr>

<p>I would recommend first checking the class balances of your dataset. If it highly imbalanced, you could look into performing <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html"" rel=""nofollow noreferrer"">stratified sampling</a> to create your train/test split. This ensures that each batch has roughly the same number of <code>0</code> and <code>1</code> samples.</p>

<p>Secondly, you might consider using a slightly larger network. In the first instance, this could be as simple as increasing the number of neurons in each of your <code>Dense</code> layers. I would suggest something like <code>(100, 50, 1)</code> to begin with. That will immediately tell you if the size of the model is restricting learning.</p>
","0","2","45264","13013"
"38252","<p>Try using a <code>trisurf</code> plot! It is very simple to get a nice surface plot. </p>

<p>Have a look at <a href=""https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html"" rel=""nofollow noreferrer"">this Matplotlib 3d documentation</a> that includes many examples.</p>

<p>Here is one of the examples, where I additionally print the size of the three arrays used to plot. We see that they are all single vectors of the same length, therefore your data should just work fine using the code below:</p>

<p><a href=""https://i.stack.imgur.com/uVX5a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uVX5a.png"" alt=""enter image description here""></a></p>

<pre><code>from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import numpy as np


n_radii = 8
n_angles = 36

# Make radii and angles spaces (radius r=0 omitted to eliminate duplication).
radii = np.linspace(0.125, 1.0, n_radii)
angles = np.linspace(0, 2*np.pi, n_angles, endpoint=False)

# Repeat all angles for each radius.
angles = np.repeat(angles[..., np.newaxis], n_radii, axis=1)

# Convert polar (radii, angles) coords to cartesian (x, y) coords.
# (0, 0) is manually added at this stage,  so there will be no duplicate
# points in the (x, y) plane.
x = np.append(0, (radii*np.cos(angles)).flatten())
y = np.append(0, (radii*np.sin(angles)).flatten())

# Compute z to make the pringle surface.
z = np.sin(-x*y)

# Inspect the array shapes
print(x.shape)                    # gives: (289,)
print(y.shape)                    # gives: (289,)
print(z.shape)                    # gives: (289,)

# Create the plot and show it

fig = plt.figure()
ax = fig.gca(projection='3d')

ax.plot_trisurf(x, y, z, linewidth=0.2, antialiased=True)

plt.show()
</code></pre>
","1","2","45264","13013"
"38384","<p>I don't believe there is a well-known method to deal with this.</p>

<h2>Simple pre-processing</h2>

<p>While I haven't done this with images/videos, I know from general time-series analysis that you basically have to interpolate the lower frequencies or you need to down-sample the higher frequencies. If you think about, what else is there to do...?</p>

<h2>Modelling solution</h2>

<p>Nvidia released <a href=""https://arxiv.org/abs/1712.00080"" rel=""nofollow noreferrer"">a research paper</a> with an <a href=""https://www.youtube.com/watch?v=MjViy6kyiqs"" rel=""nofollow noreferrer"">accompanying video</a> showing how they were able to train a model, which could estimate the <strong>frames between frames</strong> - effectively interpolating video and increasing its frame rate. This would essentially be the equivalent of interpolation between frames and allow you to scale up your lower frequency videos to match the higher frequency ones. The paper is named:</p>

<blockquote>
  <p>Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation</p>
</blockquote>

<p>... sounds like something worth reading.</p>

<p>There are older algorithms that try to do the same thing (e.g. ""twixtor""), but I read they have problems with things such as rotating objects. Another thing to keep in mind is the usual GIGO: garbage in garbage out. There are still some artefacts of interpolation in the Nvidia video, but that likely comes from blurry input images used during training when e.g. objects were moving faster than the recording frame rate could handle.</p>

<p>It seems that they train two models: the first encodes the optical flow between frames and the second model uses that, along with the base images to perform the interpolation. Please read the paper for more details. It also outlines how they train the model (learning rates, number of epochs, augmentation steps, etc.).</p>

<p>Here is the sketch of their model for flow computation/interpolation:</p>

<p><a href=""https://i.stack.imgur.com/iTQLz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iTQLz.png"" alt=""enter image description here""></a></p>

<p>We can see that it is an encoder/decoder-looking model, introducing a bottleneck that condenses the information, before upsampling again. This is based on <a href=""https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/"" rel=""nofollow noreferrer"">the U-net model architecture</a>: an encoder/decoder that also introduces skip connections between layers of different scales.</p>
","0","2","45264","13013"
"38388","<p>Where exactly in the computations are these underflows manifesting? See here for <a href=""https://stackoverflow.com/questions/39109674/the-output-of-a-softmax-isnt-supposed-to-have-zeros-right"">a brief explanation around the extremes of the softmax</a>.</p>

<p>Quick fixes could be to either increase the precision of your model (using 64-bit floats instead of, presumably, 32 bit floats), or just introduce a function that caps your values, so anything below zero or <em>exactly zero</em> is just made to be close enough to zero that the computer doesn't freak out. For example, use <code>X = np.log(np.max(x, 1e-9))</code> before going into the softmax.</p>

<p>In any case, the softmax shouldn't have problems with your input, as the final activations are exponentiated:</p>

<p>$$
\sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}}  
$$</p>

<p>This means all values will now be in the range <code>[0, 1]</code>.</p>

<p>The cross-entropy equation should also be able to deal with the output of this.</p>

<p>If none of this helps your specific issue, could you share a specific example of your problem?</p>
","6","2","45264","13013"
"38451","<p>In principal, they are exactly the same.</p>

<p>A numpy array holds the RGB values of an image saved on disk in a memory container (<code>numpy.ndarray</code>). This container offers certain built-in functions, such as the ability to do some <em>fancy slicing</em>. An example would be to flip an image across the vertical axis, giving a mirror image:</p>

<pre><code>flipped =image[:, ::-1]         # memory efficient and therefore fast
</code></pre>

<p>Numpy arrays aren't able to do everything we need for modelling, especially on GPUs using Tensorflow or PyTorch, for example. So we pass the numpy arrays to these frameworks and they put another wrapper on them, making them <em>tensor</em> objects. </p>

<p>These objects have special methods and properties that are tailored to our needs for deep learning. They can do things such as store gradient information or deduce the shapes of the tensor before/after operations - all things that make our lives easier. </p>

<p>Loading images directly into a deep learning framework, using one of their own tools, would skip the numpy step and get straight to the tensors. That is fine when your pipeline is already set up and you don't need to perform any pre-processing.</p>

<p>PyTorch has its basic tensor object; however, it allows you to do most operations you could do on a standard numpy array, which many see as a big advantage of that framework. </p>
","2","2","45264","13013"
"38453","<p>This entirely depends on your data! </p>

<p><strong>Generally, the more augmentation, the more situations your model will be exposed to during training and therefore the more robust it will be when being tested on unseen data.</strong></p>

<p>However, what if we for example were working on a model for self driving cars? Using the <code>vertical_flip</code> just doesn't make sense, because the car will (hopefully!) be er be driving along on its roof.</p>

<p>I would suggest starting with no augmentation and slowly adding one possibility at a time. For example, you record an accuracy of 80% with no augmentation. Then add<code>featurewise_normalizatiom</code> and <code>featurewise_std_normalization</code> giving you an accuracy of 85%. Then adding <code>horizontal_flip</code> gets you to 90%. Finally you try adding <code>zca_whitening</code> and that send you back down to 86%.</p>

<p>The reverse approach may also work well for you, starting with all augmentation parameters turned on and removing them one by one. In any case, it is completely dependent on your specific problem and your available data.  <a href=""https://keras.io/preprocessing/image/#imagedatagenerator-class"" rel=""nofollow noreferrer"">Keras' ImageDataGenerator has a long list of parameters,</a> so having a think about what makes sense will save you a lot of time. </p>
","4","2","45264","13013"
"38505","<p>If you are training the model based on the aggregated values anyway then it wouldn't make any difference on the final dataset that is fed to the model.</p>

<p>You might consider aggregating earlier in your pipeline as to reduce the amount of work that has to be done afterwards (whatever steps you go through).  Aggregating means less data points, so following steps will run faster.</p>
","0","2","45264","13013"
"38653","<p>Imagine you have 20 coefficients to test for and also have target accuracy (or whatever metric you're interested in) that you aim to beat. It acts as a threshold. </p>

<p>One tradeoff could be that performing ""backwards regression"" means you would in theory start with you maximum accuracy and be decreasing each time you remove a parameter. You might cut your threshold after removing only 3 parameters! So don't necessarily need to continue. </p>

<p>Conversely, starting with a single parameter and working your way up, you would have to try 17 models 5o reach your threshold.</p>

<p>This scenario could of course be reversed, so it is important to understand your data and make an informed decision about what suits your situation best. </p>
","1","2","45264","13013"
"38655","<p>This turns out to be fairly simple! There is a handy method called <code>repeat</code> on a datetime index. Here are the steps:</p>

<pre><code>import pandas as pd
</code></pre>

<p>Define a date range, supplying start and end</p>

<pre><code>jan = pd.date_range(start=""1-Jan-2018"", end=""31-Jan-2018"")    # could specify any year
</code></pre>

<p>Now provide how many times you want to repeat each date and create the repeated column</p>

<pre><code>num_repeats = 24
repeated_jans = jan.repeat(num_repeats)
</code></pre>

<p>Let's create the random dummy dataframe as a base</p>

<pre><code>total_dates = num_repeats * len(jan)    # 24 x 31 = 744
df = pd.DataFrame(np.random.randint(0, 10, total_dates))
</code></pre>

<p>This is how we add a column - the name of the column can be anything</p>

<pre><code>df['repeated_jans_lalala'] = repeated_jans
</code></pre>

<p>Have a look at some dates:</p>

<pre><code>print(df.iloc[[0, 24, 48, 71, 72]])    # multiples of 24...we can see one repeated date

    0 repeated_jans_lalala
0   7    2018-01-01
24  4    2018-01-02
48  3    2018-01-03
71  6    2018-01-03
72  3    2018-01-04
</code></pre>

<p>If try adding the column to a dataframe with a different number of rows, we get an error:</p>

<pre><code>df_error = pd.DataFrame(np.random.randint(0, 10, 743))    # require 744!
#df_error['repeated_jans'] = repeated_jans                         # raises ValueError
</code></pre>

<p>If you want to change the way the dates look, you can use <a href=""https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior"" rel=""nofollow noreferrer"">the <code>strftime</code> method</a> on the dates</p>

<pre><code>jans_fancy = jans.strftime('%d-%B-%y')
df['fancy_jans'] = jans_fancy

df.head()

0   repeated_jans_lalal  fancy_jans
0   7   2018-01-01       01-January-18
1   2   2018-01-01       01-January-18
2   8   2018-01-01       01-January-18
3   9   2018-01-01       01-January-18
4   7   2018-01-01       01-January-18
</code></pre>

<p>If you don't want to show the actual year, just leave out the <code>%y</code> part!</p>
","2","2","45264","13013"
"38857","<p>There are two main steps involved to get where you want. First we need to get the timestamps that you are expecting (going up in seconds without gaps).</p>

<p>Scondly we need to assign your data onto those timestamps and fill any missing values with the previously recorded value.</p>

<p>This solution is using python. It might be a good exercise to start with if you are new to Python and data frames, which shows the power of programming compared to spreadsheets. </p>

<h2>Basic setup</h2>

<pre><code>import pandas as pd
import numpy as np
</code></pre>

<p>This just create the example dataframe you privded. The times will be used to make a date-time index, and the values will be out ""data"" - our only column.</p>

<pre><code>times = [""04:23:12"", ""04:23:13"", ""04:23:14"", ""04:23:15"", ""04:23:16"", ""04:24:01"", ""04:24:02""]
values = [2709.082597, 2708.747333, 2708.442548, 2708.229198, 2708.229198, 2708.137763, 2707.924413]
</code></pre>

<p>We create a proper date-time <em>Series</em> to use as our index</p>

<pre><code>index = pd.to_datetime(times)
</code></pre>

<p>Throw it into a dataframe</p>

<pre><code>df = pd.DataFrame(index=index, data=values, columns=['data'])
df.head()
                        data    
2018-09-27 04:23:12  2709.082597
2018-09-27 04:23:13  2708.747333
2018-09-27 04:23:14  2708.442548
2018-09-27 04:23:15  2708.229198
2018-09-27 04:23:16  2708.229198
</code></pre>

<h2>Part one</h2>

<p>Now I will create a second <strong>empty</strong> dataframe, that will however have the desired timestamps in the index i.e. it will go up in seconds <strong>without any gaps</strong>.</p>

<p>We can use the first and last timestamps that we recorded above (or anythin else you need)</p>

<pre><code>new_start = df.index[0]
new_end = df.index[-1]
</code></pre>

<p>We can specify the frequency as seconds, using the parameter <code>freq='s'</code> as shown:</p>

<pre><code>new_index = pd.date_range(new_start, new_end, freq='s')
</code></pre>

<p>Now we create the empty dateframe with the desired index:</p>

<pre><code>new_df = pd.DataFrame(index=new_index)
</code></pre>

<h2>Part two</h2>

<p>Now we combine the two dataframe (the one with your data and the one with the target index), and finally we fill the missing values.</p>

<p>We merge the two dataframes based on their indexes. Using the method=""outer"" means that we keep all values in both indexes, so no timestamps would be lost. In your case we just basically end up with <code>new_index</code> again, as it was already perfect.</p>

<pre><code>combined_df = pd.merge(df, new_df, method=""outer"", left_index=True, right_index=True)
</code></pre>

<p>The last step is very simple: fill missing values with the last recorded value. Here <code>ffill</code> means <strong>forward fill</strong>, which does exactly what you want:</p>

<pre><code>final_df = combined_df.fillna(method='ffill')

final_df
                        data    
2018-09-27 04:23:12  2709.082597
2018-09-27 04:23:13  2708.747333
2018-09-27 04:23:14  2708.442548
2018-09-27 04:23:15  2708.229198
2018-09-27 04:23:16  2708.229198
2018-09-27 04:23:17  2708.229198
2018-09-27 04:23:18  2708.229198
...
...
2018-09-27 04:23:58  2708.229198
2018-09-27 04:23:59  2708.229198
2018-09-27 04:24:00  2708.229198
2018-09-27 04:24:01  2708.137763
2018-09-27 04:24:02  2707.924413
</code></pre>

<hr>

<p>This solution obviously only works for processing such static data, but a better solution might be to implement a short loop during the recording process to simply repeat the last recording if no new one is given, ensuring a datapoint is recorded every second.</p>
","0","2","45264","13013"
"39043","<p>You can use the LeakyRelu <strong>layer</strong>, as in the python class, instead of just specifying the string name like in your example. It works similarly to a normal layer. </p>

<p>Import the LeakyReLU and instantiate a model </p>

<pre><code>from keras.layers import LeakyReLU
model = Sequential()

# here change your line to leave out an activation 
model.add(Dense(90))

# now add a ReLU layer explicitly:
model.add(LeakyReLU(alpha=0.05))
</code></pre>

<hr>

<p>Being able to simply write e.g. <code>activation='relu'</code> is made possible because of simple aliases that are created in the source code. </p>

<hr>

<p>For your second question:</p>

<blockquote>
  <p>what are the best general setting for tuning the parameters of LeakyRelu? And when its performance is significantly better than Relu?</p>
</blockquote>

<p>I can't give you optimal settings for the LeakyReLU, I'm afraid - they will be model/data dependent.</p>

<p>The difference between the ReLU and the LeakyReLU is the ability of the latter to retain some degree of the negative values that flow into it, whilst the former simply sets all values less than 0 to be 0.  In theory, this extended output range offers a slightly higher flexibility to the model using it. I'm sure the inventors thought it to be useful and perhaps proved that to be the case for a few benchmarks. In practice, however, people generally just stick to the ReLU, as the benefits of the LeakyReLU are not consistent and the ReLU is cheaper to compute and therefore models train slightly faster. </p>
","20","2","45264","13013"
"39045","<p>I think you will have to experiment; there isn't generally a <em>one-activation-fits-all</em> for hierarchical models. Give the ones you mentioned a try, but perhaps starting with <code>sigmoid</code> and <code>tanh</code> and also a <code>LeakyReLU</code>.</p>

<p>The reason I'd perhaps leave out a normal <code>ReLU</code> at first is because that too would potential trim out information before the final model, as it simply removes negative weights to zero. A good final model should be able to correct for this, but I think its unnecessary to force it to. </p>
","0","2","45264","13013"
"39079","<p>Welcome to Data Science! You're question needs a little more detail... there are many many ways to make an array into a single number. You should say a little more about what you mean by <strong>meaningful</strong>.</p>

<p>Here are a few examples, using your example array, which may seem outrageously simple, but do indeed form the basis to many of the techniques used in modern research:</p>

<pre><code>In [1]: import numpy as np

In [2]: x = np.array([[129, 155, 191], [123, 150, 185], [120, 149, 183]], dtype=np.float32)

In [3]: x
Out[3]: 
array([[129., 155., 191.],
       [123., 150., 185.],
       [120., 149., 183.]], dtype=float32)
</code></pre>

<p>Now here are a few (randomly selected) ways to create a single number from your array:</p>

<pre><code>In [4]: np.mean(x)    # the mean / average
Out[4]: 153.88889

In [5]: np.sum(x)    # the sum
Out[5]: 1385.0

In [6]: np.std(x)    # the standard deviation
Out[6]: 25.722641

In [7]: np.linalg.norm(x)    # the Frobius norm - a distance measure
Out[7]: 468.07156

In [8]: np.max(x)
Out[8]: 191.0
</code></pre>

<p>These might seem stupidly simple, but if we were to treat your array as a single block from a larger image-array, then these might represent <em>pooling-layers</em> that are used to down-sample arrays as they are passed through a neural network. Just have a look at the available <a href=""http://https://keras.io/layers/pooling/"" rel=""nofollow noreferrer""><strong>pooling layers within the Keras library</strong></a>.</p>

<p>Which method you might want to use will heavily depend on your use case, your dataset and your model.</p>
","2","2","45264","13013"
"39082","<p>As I commented on your post - I think you have a problem with your environment. If you cannot solve that, you should file a bug with the spacy team on their GitHub.</p>

<p>I tested the same code and could only see a difference in results compared to the website when using language models of different sizes. If you installed the English model using the default command: <code>python -m spacy download en</code>, then you get the smallest model by default.  You should be using the medium-sized model to produce their results. I compare the two below</p>

<h3>The script</h3>

<pre><code>import spacy

# Small setup
nlp_small = spacy.load('en')
tokens_s = nlp_small(u'dog cat banana')

# Medium setup
nlp_medium = spacy.load('en_core_web_md')
tokens_m = nlp_medium(u'dog cat banana')

# large setup would use 'en_core_web_lg' ...


print('Results using small model:\n')
for token1 in tokens_s:
    for token2 in tokens_s:
        print(token1.text, token2.text, token1.similarity(token2))

print('\nResults using medium model:\n')
for token1 in tokens_m:
    for token2 in tokens_m:
        print(token1.text, token2.text, token1.similarity(token2))
</code></pre>

<h3>The output</h3>

<pre><code>Results using small model:

dog dog 1.0
dog cat 0.53906965
dog banana 0.28761008
cat dog 0.53906965
cat cat 1.0
cat banana 0.4875216
banana dog 0.28761008
banana cat 0.4875216
banana banana 1.0

Results using medium model:

dog dog 1.0
dog cat 0.8016855
dog banana 0.24327648
cat dog 0.8016855
cat cat 1.0
cat banana 0.28154367
banana dog 0.24327648
banana cat 0.28154367
banana banana 1.0
</code></pre>

<p>The results are clearly better when using the medium sized model - and they match the results on the website.</p>

<hr>

<p>Here is the spacy version as used within a <strong>conda environment</strong> to produce these results:</p>

<pre><code>n1k31t4@n1k31t4~$ conda list | grep spacy
spacy                     2.0.12                    &lt;pip&gt;
</code></pre>

<p>And the Python version information:</p>

<pre><code>n1k31t4@n1k31t4:~$ ipython
Python 3.6.6 | packaged by conda-forge | (default, Jul 26 2018, 09:53:17) 
Type 'copyright', 'credits' or 'license' for more information
IPython 6.5.0 -- An enhanced Interactive Python. Type '?' for help.
</code></pre>

<p>Although I ran the script on Linux (OP used MacOSX) and my Python version is <em>slightly</em> newer... I wouldn't expect these things to explain the difference in results.</p>
","1","2","45264","13013"
"39208","<p>Welcome to Data Science! Here I create your data frame and show one way to create the column you need. I use numpy in addition to pandas:</p>

<pre><code>import pandas as pd
import numpy as np

# Create df
ID = ['bike', 'bike', 'car', 'car', 'car']
Color = ['red', 'black', 'green', 'orange', 'blue']
df = pd.DataFrame(data={'ID': ID, 'Color': Color})
</code></pre>

<p>Add a new column called 'Order', with counts based on each group - these are mini dataframes that contain only one of the ID values only. This makes use of the <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html"" rel=""nofollow noreferrer""><code>groupby</code> method on a Pandas dataframe</a>.</p>

<pre><code>df['Order'] = df.groupby('ID').transform(lambda x: 1 + np.arange(len(x)))
</code></pre>

<p>The <code>transform</code> method takes a function and applies it to each group. I use an <em>anonymous function</em> (just a function with no name), also called <em>lambda functions</em>, using the keyword <code>lambda</code>.</p>

<p>The result:</p>

<pre><code>    Color    ID  Order
0     red  bike      1
1   black  bike      2
2   green   car      1
3  orange   car      2
4    blue   car      3
</code></pre>

<hr>

<p>As <em>anymous.asker</em> mentioned, it would be helpful in the future if you post the code that creates your dataframe!</p>
","2","2","45264","13013"
"39211","<p>The answer will depend on some things such as your hardware and the image you process. Additional, we should distinguish if you are talking about a single run through the network in <em>training mode</em> or in <em>inference mode</em>. In the former, additional parameters are pre-computed and cached as well as several layers, such as dropout, being used, which are simply left out during inference. I will assume you want to simply produce a single prediction for a single image, so we are talking about <em>inference</em> time.</p>

<h2>Factors</h2>

<p>The basic correlation will be:</p>

<ul>
<li>more parameters (i.e. learnable weights, bigger network) - slower than a model with less parameters</li>
<li>more recurrent units - slower than a convolutional network, which is slower than a full-connected network<sup><strong>1</strong></sup></li>
<li>complicated activation functions - slower than simple ones, such as <em>ReLU</em></li>
<li>deeper networks - slower than shallow networks (with same number of parameters) as less run in parallel on a GPU</li>
</ul>

<p>Having listed a few factors in the final inference time required (time taken to produce one forward run through the network), I would guess that MobileNetV2 is probably among the fastest pre-trained model (available in Keras). We can see from the following table that this network has a small memory footprint of only 14 megabytes with ~3.5 million parameters. Compare that to your VGG test, with its ~138 million... 40 times more! In addition, the main workhorse layer of MobileNetV2 is a conv layer - they are essentially clever and smaller versions of residual networks.</p>

<p><a href=""https://i.stack.imgur.com/d0iwP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/d0iwP.png"" alt=""Keras pre-trained models""></a></p>

<h2>Extra considerations</h2>

<p>The reason I included the whole table above was to highlight that with small memory footprints and fast inference times, comes a cost: low accuracies!</p>

<p>If you compute the ratios of top-5 accuracy versus number of parameters (and generally versus memory), you might find a nice balance between inference time and performance.</p>

<hr>

<p><sup><strong>1</strong></sup> Have a look at <a href=""https://stats.stackexchange.com/questions/262220/is-convolutional-neural-network-cnn-faster-than-recurrent-neural-network-rnn"">this comparison of CNNs with Recurrent modules</a></p>
","5","2","45264","13013"
"39353","<p>After you answer in comments:</p>

<p>You can add a new column with either new information (coming from another file or other source), or you can do it based on the existing columns. In fact you are already adding new columns in your code.</p>

<p>I will add a new column called <code>STAGE</code> both ways to give you an example of each.</p>

<h2>From static data</h2>

<p>I assume you have the stages in a list, like this:</p>

<pre><code>stage_data = ['stage 1', 'final stage', 'stage 1', 'final stage',
              'stage 2','stage 1','stage 2',]
</code></pre>

<p>If you have the stages of each of the existing rows, you can add it simply as follows:</p>

<pre><code>dataset['STAGE'] = stage_data
</code></pre>

<p><strong>NOTE:</strong> The length of the list should match the length of your dataset i.e. the length of the pandas DataFrame. Check it with this:</p>

<pre><code>assert len(dataset) == len(stage_data), ""The lengths don't match!!""
</code></pre>

<h2>From a condition on existing columns</h2>

<pre><code># Make new column simply filled with zeros - we will fill it after
dataset['STAGE'] = np.zeros(len(dataset))
</code></pre>

<p>Now you can use conditions on existing columns and fill the rows that match with your corresponding <em>stage</em>:</p>

<pre><code># assume people with BMI above 25 must be `stage 1`
data[dataset.BMI &gt; 25]['STAGE'] = 'stage 1'

# assume people with BMI below 15 must be `final stage`
dataset[dataset.BMI &lt; 15]['STAGE'] = 'final stage`

# Stupid example to fill the gaps and show how you can use two conditions:
dataset[(dataset.BMI &lt;= 25) &amp; (dataset.BMI &gt;= 15)]['STAGE'] = 'stage 2'
</code></pre>
","1","2","45264","13013"
"39381","<p>You can either load the excel files directly into Python, e.g. using the pandas package, or you can use a package that hits the data where it lives i.e. a package that directly modifies the Excel files.</p>

<h2>Python and Pandas</h2>

<pre><code>import pandas as pd

data = pd.read_Excel('/path/to/your/file)
</code></pre>

<p>Data is basically a table that will look like the Excel itself. You can add fields (usually referred to as columns) like so:</p>

<pre><code>data['new field'] = new_data
</code></pre>

<p><code>new_data</code> will contain all values for all rows in the table, e.g. as a list:</p>

<pre><code>new_data = [12, 24, 13, 66, 72, 45]    # assuming there are 6 rows (a.k.a. samples)
</code></pre>

<p>You will have to alter the code for each of your files. You will also need to think about how to get your new data into that <em>list</em> (or a similar construction).</p>

<p><em>Caveat: this way might have problems if you have some very specific formatting or special Excel features in use (more than just conditional formatting - think dropdown lists and hidden/folded rows and columns).</em></p>

<h2>Excel interface</h2>

<p>There are a few good packages that will allow you to speak directly with the excel file and make use of things such as <em>sheets/tabs</em> and references to cells e.g. <code>A1</code> is the top left cell in Excel.</p>

<p>Have a look at the <a href=""https://github.com/jmcnamara/XlsxWriter"" rel=""nofollow noreferrer"">XlsWriter</a> package or the python-excel package. Both let you access the fill and add/remove data as you please. I have used both for simple tasks and they both got the job done quite nicely.</p>

<p>Follow those links to see some nice examples of how to manipulate Excel programmatically.</p>

<hr>

<p>It is worth mentioning that the first option above, using something like Pandas, will offer a lot more functionality and would likely not present a steeper learning curve (in my opinion!). </p>
","0","2","45264","13013"
"39385","<p>Here are a few ways, using some dummy data:</p>

<pre><code>In [1]: import numpy as np

In [2]: a = np.random.randint(0, 10, (10,))

In [3]: b = np.random.randint(0, 10, (5, 10))

In [4]: a
Out[4]: array([4, 1, 0, 6, 3, 3, 6, 6, 1, 8])

In [5]: b
Out[5]: 
array([[9, 0, 6, 1, 1, 1, 4, 7, 4, 7],
       [5, 8, 8, 3, 4, 8, 7, 3, 0, 4],
       [2, 2, 5, 3, 9, 6, 1, 5, 8, 3],
       [2, 0, 4, 3, 5, 3, 3, 4, 3, 3],
       [3, 3, 6, 4, 7, 5, 8, 6, 7, 3]])
</code></pre>

<p>Because of the dimensions you asked for, in order to compute inner products (a.k.a. scalar products and dot products), we need to transpose the matrix <code>b</code> so that the dimensions work out.
With a vector of length 10, numpy gives it shape <code>(10,)</code>. So it seems 10 rows and no columns, however it is kind of ambiguous. Numpy will essentially do what it has to in order to make dimensions work. We could force it into a <code>(10, 1)</code> vector by using <code>a.reshape((10, 1))</code>, but it isn't necessary. The matrix has a defined second dimensions, so we have a shape <code>(5, 10)</code>. In order to multiply these two shapes together, we need to make the same dimensions match in the middle. This means making <code>(10,) * (10, 5)</code>. Performing the transpose on matrix reverses the dimensions to give us that <code>(10, 5)</code>. Those inner <code>10</code>s will then disappear and leave us with a <code>(1, 5)</code> vector.</p>

<hr>

<p>That all being said, we can use any of the following to get equivalent answers:</p>

<ol>
<li><p>The standard standard dot-product:</p>

<pre><code>In [7]: a.dot(b.T)
Out[7]: array([174, 174, 141, 119, 190])
</code></pre></li>
<li><p>The convenient numpy notation:</p>

<pre><code>In [6]: a @ b.T
Out[6]: array([174, 174, 141, 119, 190])
</code></pre></li>
<li><p>The efficient ""<em>Einstein notation</em>"", a subset of Ricci calculus (I leave the interested reader to search online for more information):</p>

<pre><code>In [8]: np.einsum('i,ij-&gt;j', a, b.T)
Out[8]: array([174, 174, 141, 119, 190])
</code></pre></li>
<li><p>Here as in the comments from <em>shadowstalker</em>:</p>

<pre><code>In [9]: np.array([np.dot(a, r) for r in b])
Out[9]: array([174, 174, 141, 119, 190])
</code></pre></li>
</ol>

<hr>

<p>If your matrices are of dimensions <code>(100, 100)</code> or smaller, then the <code>@</code> method is probably the fastest and most elegant. However, once you start getting into matrices that make you wonder if you laptop will handle it (e.g. with shape <code>(10000, 10000)</code>) - then it is time to <a href=""https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.einsum.html"" rel=""nofollow noreferrer"">read the documentation</a> and <strong><a href=""http://ajcr.net/Basic-guide-to-einsum/"" rel=""nofollow noreferrer"">this blog</a></strong> about Einstein notation and the amazing <code>einsum</code> module within numpy!</p>
","2","2","45264","13013"
"39454","<p>Most of the code you provided doesn't do much to help your actual problem of renaming the files. I will just focus on tha tpart by showing you how to get the filenames you want.</p>

<p>You can rename the files, as you describe them, like this - assuming you have a list of filenames that match your description.</p>

<pre><code>filenames = ['1.0.2.34.57_1.png', '1.0.2.34.58_1.png', '1.0.2.34.59_1.png']

for file in filenames:

    # Do what you want with the file...
    # Image processing and modelling, etc.


    # Replace the extension with your custom ending
    final_filename = file.replace('.png', '_mask.png')
</code></pre>

<hr>

<p>Another way to do it, breaking down the removal and addition into two steps</p>

<pre><code>for file in filenames:
    file_without_ext = file.replace('.png', '')

    # Add you custom ending
    final_filename = file_without_extension + '_mask.png'

    # Another variant - joining the extension to the file_without_ext using ""_""
    final_filename = '_'.join(file_without_ext, 'mask.png')
</code></pre>

<p>The three variants of <code>final_filename</code> will be identical.</p>

<p>Now you can save the output of your image processing using the <code>final_filename</code>.</p>
","1","2","45264","13013"
"39669","<p>I think the short mathematical intuition would be to show that <strong>Deeper offers more flexibility</strong>.</p>

<p>Imagine we want to fit an impossibly complicated function, like this:</p>

<p><a href=""https://i.stack.imgur.com/ARexi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ARexi.png"" alt=""random complicated function visualisation""></a></p>

<p>... but in an <em>n-dimensional space</em> - we obviously cannot visualise such a function. But we can agree is is complicated</p>

<h3>Basic math analogy</h3>

<p>The goal of the neural network would be to map the raw input data (e.g. images to a convolutional network) to some output, by approximating the complicated function.</p>

<p>So if we have some input, and apply a non-linear function <span class=""math-container"">$f$</span> to it, we transform it into something else:</p>

<p><span class=""math-container"">$$output_1 = f(input)$$</span></p>

<p>Perhaps that gave us a curvy function, but it doesn't get close to matching the complicated function, so we endow the model with another chance, by applying a second non-linear function:</p>

<p><span class=""math-container"">$$output_2 = g(output_1)$$</span></p>

<p>Giving a second function is almost like offering another degree of freedom (or flexibility) to the model.</p>

<p>We continue like this until the chain of non-linear functions is able to map out the output space sufficiently well. Perhaps we end up with this:</p>

<p><span class=""math-container"">$$output_6 = k(j(i(h(g(f(input)))))$$</span></p>

<p>In this framework, imagine every single non-linear function as one of the layers in a deep network. The deeper the network gets, the more functions we are applying and the more we mould and transform the input to something else; perhaps in different ranges, by different magnitudes and so on.</p>

<p>This should (in a very <em>hand-wavey</em> manner) convince you that more functions applied gives more possibilities in the final output space. So more layers gives us more power to express more and more complicated functions.</p>

<hr>

<p>A practical note: the more layers we add, the more powerful the model and the larger the tendency to either:</p>

<ol>
<li>be very difficult to train</li>
<li>eventual <em>overfit</em> the training data completely</li>
<li>the longer the model takes to train</li>
<li>the greater the level of regularisation that is likely required to obtain a reasonable validation metric on unseen data</li>
</ol>

<hr>

<p><a href=""https://jamesmccaffrey.wordpress.com/2011/10/29/generating-a-3d-plot-of-the-rastrigrin-function-using-scilab/"" rel=""nofollow noreferrer"">image source</a></p>
","4","2","45264","13013"
"39713","<p>From the images alone, assuming the links you posted are representative, I doubt a model will ever be able to predict better than random guessing.</p>

<p>There is a chance the model could effectively learn to read the label, but it seems like a long (and somewhat pointless) path to go down, unless you will only be predicting on labelled bottles in the future!</p>

<p>You will need to introduce different information with each sample, perhaps the price, amount of liquid and other such feature. At this point, you will likely want to ditch your CNN for images and instead use a standard classifier, such as Support Vector Machines or a simple feed-forward neural network.</p>
","0","2","45264","13013"
"39770","<p>You can see in the <code>fit()</code> method on your <code>Model</code> instance, that the input is first sent through a method called <a href=""https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L643"" rel=""nofollow noreferrer""><code>_standardize_user_data</code> at line 643</a>.</p>

<h3>Source of error</h3>

<p>Your error message comes from the checks that happen across <a href=""https://github.com/keras-team/keras/blob/master/keras/engine/training.py#L650"" rel=""nofollow noreferrer"">lines 650</a> to 671 in that file:</p>

<pre><code>if not self.built:
    # We need to use `x` to set the model inputs.
    # We type-check that `x` and `y` are either single arrays
    # or lists of arrays.
    if isinstance(x, (list, tuple)):
        if not all(isinstance(v, np.ndarray) or
                   K.is_tensor(v) for v in x):
            raise ValueError('Please provide as model inputs '
                             'either a single '
                             'array or a list of arrays. '
                             'You passed: x=' + str(x))
        all_inputs += list(x)
    elif isinstance(x, dict):
        raise ValueError('Please do not pass a dictionary '
                         'as model inputs.')
    else:
        if not isinstance(x, np.ndarray) and not K.is_tensor(x):
            raise ValueError('Please provide as model inputs '
                             'either a single '
                             'array or a list of arrays. '
                             'You passed: x=' + str(x))
        all_inputs.append(x)
</code></pre>

<p>They use <code>isinstance()</code> to check the type, and your HDF5 type is not covered anywhere.</p>

<h2>Possible hack</h2>

<p>I linked you GitHub issue as a comment on your post. Hpowever...</p>

<p>You could alter the code above in your local version of Keras to cover your case, essentially converting the received input into a NumPy array, which would then pass then checks and be used.</p>

<p>I would probably just enter a second <code>elif</code> to the conditions above, like this:</p>

<pre><code>elif isinstance(x, dict):
    raise ValueError('Please do not pass a dictionary '
                     'as model inputs.')                   # original code

#### add this snippet #############    
elif isinstance(x, h5py._hl.dataset.Dataset):
    x = np.array(x)      # you might need to find a more elegant way of converting the HDF5 block to a numpy array
###################################

else:
    if not isinstance(x, np.ndarray) and not K.is_tensor(x):   # original code
</code></pre>

<p>You can confirm that the correct type of data for you is that <code>h5py._hl.dataset.Dataset</code> by checking the output of <code>type(keras.utils.io_utils.HDF5Matrix('dataset.h5', 'x_train'))</code>.</p>

<p>This should get things working, although it might cost you some of the other benefits of the HDF5 loading system, such as specifying <code>start</code> and <code>end</code> indices.</p>

<h2>Testing</h2>

<p>Just an example to show how the above transformation should really end up with your data being fed to the model as a numpy array:</p>

<pre><code>import numpy as np
import keras

a = np.arange(0, 75).reshape((5, 5, 3))    # like a 5x5 RGB image
f = h5py.File('tester.h5', 'w') 
f.create_dataset(name='a', data=a)                                     
  # output:  &lt;HDF5 dataset ""a"": shape (5, 5), type ""&lt;i8""&gt;
f.close()

# Later on ...

data = h5py.File('trial.h5', 'r')
data = np.array(data['a'])

np.array_equal(data, a)
  # True
</code></pre>

<hr>

<p>Given <a href=""https://keras.io/utils/#hdf5matrix"" rel=""nofollow noreferrer"">the documentation</a> on the the HDF5 utility:</p>

<blockquote>
  <p><code>keras.utils.HDF5Matrix(datapath, dataset, start=0, end=None, normalizer=None)</code>
  Representation of HDF5 dataset to be used instead of a Numpy array.</p>
</blockquote>

<p>it does feel like there is a bug, or at least a discrepancy between the documentation and the code.</p>

<h2>EDIT</h2>

<p>You can fit via custom generator that would load blocks from your HDF5Matrix.</p>

<pre><code>def generate_arrays_from_file(path):
    while True:
        with h5py.File(path) as f:

            for batch in f:
                # read the data and reshape as necessary
                trainX, trainY, testX, testY = split_batch(batch)
                yield (trainX, trainY)

model.fit_generator(generate_arrays_from_file('dataset.h5'),
                    steps_per_epoch=100, epochs=10)
</code></pre>

<p>You will have to obviously write a generator function yourself that matches you exact h5 format. Perhaps have a look at the <a href=""http://docs.h5py.org/en/latest/high/dataset.html#fancy-indexing"" rel=""nofollow noreferrer""><em>fancy indexing</em></a> options of h5 files.</p>
","0","2","45264","13013"
"39772","<p>If you left out the large blue and yellow peaks, then maybe. Otherwise, no.</p>

<p>With all three distinct peaks, you might call it a multi-modal Guassian - meaning it  is a mixture of three standard Gaussian distributions. This illustrates the idea:</p>

<p><a href=""https://i.stack.imgur.com/ycCvk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ycCvk.png"" alt=""Guassian mixture""></a></p>

<p><strong>Important:</strong> As pointed out by <em>Spacedman</em> in the comments, this comparison would only strictly apply if the data itself could be approximated as a continuous variable. This would mean you x-axis variable (<em>state</em>) should not be discrete. If the values were put in bins and your graph were therefore a histogram of the underlying data. Please have a look at <a href=""https://stats.stackexchange.com/questions/125552/draw-a-histogram-with-normal-distribution-overlay"">this question for more details</a>.</p>

<hr>

<p>We can normally describe the distributions as being <strong>fat-tailed</strong>, when the extremes on the left and right of the curve don't ever really head towards zero, but seeing as your curve really shoots up again at both ends, I don't think it would be a useful description here.</p>
","1","2","45264","13013"
"39775","<p>Try increasing your batch size.</p>

<p>I think it could be because you specify a batch size of 35 and the validation will be tested on a batch_size of 32 by default. Testing a batch of 32 on the weights that were just reached the epoch might indeed lead to a slightly better average performance as all samples in the batch get the newest and best current weights.</p>

<p>If the samples in your train and validation set are extremely similar, I would expect the validation curve to always slightly beat the train score, given you use similar batch sizes (35 vs. 32).</p>

<p>You can see that the train and validation curves do indeed level out over time.</p>
","1","2","45264","13013"
"39778","<p>Simply using the <code>fillna</code> method and provide a <code>limit</code> on how many NA values should be filled. You only want the first value to be filled, soset that it to <code>1</code>:</p>

<pre><code>df.ffill(limit=1)                                                       

  item  month  normal_price  final_price
0    1      1          10.0          8.0
1    1      2          12.0         12.0
2    1      3          12.0         12.0
3    2      1           NaN         25.0
4    2      2          30.0         25.0
5    3      3          30.0          NaN
6    3      4         200.0        150.0
</code></pre>

<p>You can chain together the above with a <code>bfill</code> to then fill the remaining NaN values:</p>

<pre><code>df.ffill(limit=1).bfill(limit=1)

  item  month  normal_price  final_price
0    1      1          10.0          8.0
1    1      2          12.0         12.0
2    1      3          12.0         12.0
3    2      1          30.0         25.0
4    2      2          30.0         25.0
5    3      3          30.0        150.0
6    3      4         200.0        150.0
</code></pre>

<hr>

<p>This would only not be optimal if there are column in your dataframe which you would like to leave unaffected.</p>

<p>In that case you can do them one column at a time - i use the <code>in_place</code> flag so that we do not need to do any of the ugly re-assignments:</p>

<pre><code>df.final_price.ffill(inplace=True, limit=1)                                     

df                                                                     

  item  month  normal_price  final_price
0    1      1          10.0          8.0
1    1      2          12.0         12.0
2    1      3           NaN         12.0
3    2      1           NaN         25.0
4    2      2          30.0         25.0
5    3      3           NaN          NaN
6    3      4         200.0        150.0
</code></pre>

<p>The same idea will work the backward-filling the values, using the <code>bfill</code> method instead of the <code>ffill</code>, as I have done above.</p>
","3","2","45264","13013"
"39823","<p>You have a <em>tuple</em> of two numpy arrays, I'll call it <code>your_tuple</code>.</p>

<p>To round all values in each array to 2 decimal places:</p>

<pre><code>rounded_arrays = [np.round(each_arr, decimals=2) for each_arr in your_tuple]
</code></pre>

<p>The result is a <code>list</code> holding your two arrays. If you want them back in a tuple, just do this:</p>

<pre><code>result = tuple(rounded_arrays)
</code></pre>

<hr>

<p>Only the first array has decimal points, so rounding the second array won't have any effect.</p>

<p>Also, have a look here for <a href=""https://realpython.com/python-lists-tuples/"" rel=""nofollow noreferrer"">differences between lists and tuples</a>.</p>
","0","2","45264","13013"
"39840","<p>The problem is that you're passing a list of numpy arrays to the <code>mode</code> function.
It requires either a single list of values, or a single numpy array with values (basically any single container will do, but seemingly not a list of arrays).</p>

<p>This is because it must make a hash map of some kind in order to determine the most common occurences, hence the <em>mode</em>.  It is unable to hash a list of arrays.</p>

<p>One solution would be to simple index the value out of each array (which then means <code>mode</code> gets a list of integers). Just changing the main line to:</p>

<pre><code>max_voting_pred = np.append(max_voting_pred, mode([a[i][0], b[i][0]]))
</code></pre>

<p>Let me know if that doesn't fix things.</p>

<hr>

<p>If you want something that is perhaps easier than fixing your orignal code, try using the <code>mode</code> function from the <code>scipy</code> module: <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html"" rel=""nofollow noreferrer"">scipy.stats.mode</a>.</p>

<p>This version allows you to pass the whole array and simply specify an axis along which to compute the mode. Given you have the full vectors of predictions from both models:</p>

<ol>
<li><p>Combine both arrays to be the two columns of one single <code>(200, 2)</code> matrix</p>

<pre><code>results = np.concatenate((y_pred, vgg16_y_pred), axis=1)
</code></pre></li>
</ol>

<p>Now you can perform the mode on that matrix across the single rows, but all in one single operation (no need for a loop):</p>

<pre><code>max_votes = scipy.stats.mode(results, axis=1)
</code></pre>

<p>The results contain two things.</p>

<ol>
<li>the mode values for each row</li>
<li>the counts of that mode within that row.</li>
</ol>

<p>So to get the results you want (that would match your original <code>max_voters_pred</code>, you must take the first element from <code>max_votes</code>:</p>

<pre><code>max_voters_pred = max_votes[0]
</code></pre>
","3","2","45264","13013"
"39855","<p>In standard models that try to characterise jumps, such as the Jump Diffusion model from Mertons (<a href=""https://www.csie.ntu.edu.tw/~lyuu/finance1/2015/20150513.pdf"" rel=""nofollow noreferrer"">a short introduction</a>), the model consists of two main parts:</p>

<ol>
<li>Brownian motion; a random walk to account for the random path, perhaps with some <em>drift</em> - when the values head upwards or downwards in a consisten manner. And</li>
<li>An additive <em>Possion process</em>, which with some probability add a jump in a time-step, with a given probability.</li>
</ol>

<p>Drawing from this approach, you could also consider using two models that work independently (or separately) to model the overall market and trend, along with a model that introduces jumps at certain timesteps and itnervals.</p>

<p>You could try using different input data for the second model, such as signals taken from text, such as news feeds or newpapers that discuss current market dynamics/politics, possible decisions from SEC and the like. The first (stable) part could be modelled by your current neural network.</p>

<hr>

<p>This is just a high level idea, and I haven't actually seen any research that already tried it, so unfortunately cannot provide any links to literature.</p>
","1","2","45264","13013"
"39957","<p>Assuming you want to classify the images (and not use bounding boxes to <em>locate</em> classes within each image), a common way it to create a target <em>vector</em> for each image, which holds the information regarding all classes and is what the model would eventually predict.</p>

<p>If you have a dataset with, say 5 classes, and your first example image contains classes 1 and 4, you would create your target vector for that image to be:</p>

<pre><code>example_sample = ... # your image array
example_sample_y = [1, 0, 0, 1, 0]  
</code></pre>

<p>This is a kind of <a href=""https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f"" rel=""nofollow noreferrer"">one-hot encoding</a>, as the vector has a placeholder for each of the 5 classes, but only a <code>1</code> when the class is present.</p>

<p>Have a look at this <a href=""https://towardsdatascience.com/multi-label-image-classification-with-inception-net-cbb2ee538e30"" rel=""nofollow noreferrer"">high-level walkthrough</a>.</p>

<hr>

<p>I think your other suggestion of training an image</p>

<p>You want to learn some kind of joint probability between the classes, and in my opinion, training one the same image with different outcomes (e.g. the sample image above twice, producing either a 1 or a 4) will not only be very inefficient during training, but will also be mathematically confusing. The same input can give 2 possible outputs! This implies your underlying function that maps images to classes is not <a href=""http://mathworld.wolfram.com/Well-Defined.html"" rel=""nofollow noreferrer"">well-defined</a>. That isn't usually a good thing!</p>
","2","2","45264","13013"
"39966","<p>I would use regex. I can only say what would work for the input sentence I can deduce from your desired output:</p>

<pre><code>s = 'to be, or not to be: that is the question!'
</code></pre>

<p>I simply remove all characters that are not letters (upper or lower case) or spaces.</p>

<pre><code>import re

pattern = r'[^A-Za-z ]'
regex = re.compile(pattern)

result = regex.sub('', s).split(' ')

print(result)
</code></pre>

<blockquote>
  <p>['to', 'be', 'or', 'not', 'to', 'be', 'that', 'is', 'the', 'question']</p>
</blockquote>

<hr>

<h2>Edit</h2>

<p>Based on the update comment from OP - my answer can be adjusted to work on each of the words via simple interation of the sentences:</p>

<pre><code>cleaned_sentenced = []    # will become a list of lists

for sentence in sentences:
    temp = [regex.sub('', word) for word in sentence]
    cleaned_sentences.append(temp)
</code></pre>

<p>This uses <code>regex</code> as defined up above.</p>
","0","2","45264","13013"
"40131","<p>Instead of perhaps iterating of each row and filling the gaps as required, I would suggest trying to do it via indexing. The solution is:</p>

<pre><code>df['category'] = df.where(~df.id.isnull())['item'].ffill()
</code></pre>

<hr>

<p>Here I break down my solution to help you understand <em>why</em> it works.</p>

<p>Imagine your dataframe is called <code>df</code>. I created a small version of yours as follows:</p>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame.from_dict(
            {'id':   [1, None, None, 2, None, None, 3, None, None],
             'item': ['CAPITAL FUND', 'A', 'B', 'BORROWINGS', 'A', 'B', 'DEPOSITS', 'A', 'B']})

In [3]: df       # see what it looks like

Out[3]: 

 id          item   
0  1.0  CAPITAL FUND
1  NaN             A
2  NaN             B
3  2.0    BORROWINGS
4  NaN             A
5  NaN             B
6  3.0      DEPOSITS
7  NaN             A
8  NaN             B
</code></pre>

<p>I get the dataframe back where the <code>id</code> column <strong>is not</strong> null (<code>~</code> reverses the <code>isnull()</code>).  On the resulting dataframe, I take only the <code>item</code> column (using <code>[item]</code>) and then fill the missing gaps, using the previous valid value in that column.</p>

<pre><code>In [4]: df['category'] = df.where(~df.id.isnull())['item'].ffill() 

In [5]: df
Out[5]: 
    id          item      category
0  1.0  CAPITAL FUND  CAPITAL FUND
1  NaN             A  CAPITAL FUND
2  NaN             B  CAPITAL FUND
3  2.0    BORROWINGS    BORROWINGS
4  NaN             A    BORROWINGS
5  NaN             B    BORROWINGS
6  3.0      DEPOSITS      DEPOSITS
7  NaN             A      DEPOSITS
8  NaN             B      DEPOSITS
</code></pre>

<p>The trick is to understand this part: <code>df.where(~df.id.isnull())['item']</code></p>

<p>It returns really the whole dataframe, with the values where <code>~df.id.isnull()</code> is <code>True</code>. Then only the <code>item</code> dataframe. The result is this:</p>

<pre><code>In [6]: df.where(~df.id.isnull())['item'] 
Out[6]: 
0    CAPITAL FUND
1             NaN
2             NaN
3      BORROWINGS
4             NaN
5             NaN
6        DEPOSITS
7             NaN
8             NaN
</code></pre>

<p>Now it should be clear why the final <code>.ffill()</code> works as we would like. It <em>forward fills</em> the missing values, using the last known valid value.</p>
","1","2","45264","13013"
"40152","<p>You would usually just scale all of them to be within the same range.</p>

<p>You can do this by using something like the Scikit-Learn <strong><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"" rel=""nofollow noreferrer""><code>MinMaxScaler</code></a></strong>, or just a simple function like this:</p>

<pre><code>def scale(data, new_min=-1, new_max=1)
    """"""Scale values of data to be within the range [new_min, new_max]
    data must be a numpy array or a Pandas Dataframe/Series""""""

    return (data - data.min()) / (data.max() - data.min()) * (new_max - new_min) + new_min
</code></pre>

<p>Between -1 and +1 is just nice, as the data is centered around zero. You could play with those values.</p>

<p>You can think about and perhaps experiment to see if you should scale all variables together (meaning one global min and max in the dataset), or whether to scale the individual columns/features of your dataset, so each one lies in the given range.</p>

<hr>

<p>A tip for financial data is to use the log returns - that means to take your raw prices, compute the logarithm of those values, then take the difference between the closing prices of each day.</p>

<p>The reason for this is to because the resulting values are <em>normally distributed</em>, which is an underlying assumption of many models you will subsequently use (Boosting, ARIMA, GARCH for volatilities etc.). There are also other reasons of convenience - check out <a href=""https://quantivity.wordpress.com/2011/02/21/why-log-returns/"" rel=""nofollow noreferrer"">this article</a></p>
","1","2","45264","13013"
"40173","<p>You are correct that ""stacking LSTMs"" means to put layers on top of one-another as in your second image.</p>

<p>The first picture is a ""<strong>bi-directional LSTM</strong>"" (BiLSTM), whereby we can analyse a point in a series (e.g. a word in a sentence) from both sides. We care about the context of that point.</p>

<p>The most common example I know of is within NLP. Here we want to know the representation of a word in a gap, how it is found between other words. If we have the entire sentence, we can look at the words before <strong>and</strong> the words after our word. In this case, we could use a bi-drectional LSTM to process the sequence in the opposite direction, which your first diagram shows.</p>

<p>Let's play a game, and say you need to guess the missing word in this text snippet:</p>

<blockquote>
  <p>i need to review an __________ ...</p>
</blockquote>

<p>What could it be? ""<em>article</em>"", ""<em>iPad</em>"", ""<em>aerial image</em>"" ?</p>

<p>Here is the solution:</p>

<blockquote>
  <p>i need to review an article, ...</p>
</blockquote>

<p>It was incredibly hard to get that right - perhaps impossible! Well, maybe not if you have some context with it. How about I give you both sides of that snippet:</p>

<blockquote>
  <p>i need to review an ________, for tomorrow's newspaper.</p>
</blockquote>

<p>A BiLSTM would be fed the sentence from both sides, thus letting it see some more context to understand each word.</p>

<p>Have a look <a href=""https://towardsdatascience.com/introduction-to-sequence-models-rnn-bidirectional-rnn-lstm-gru-73927ec9df15"" rel=""nofollow noreferrer"">at this article</a>, which eventually get the bi-directional networks.
Here is a <a href=""https://stackoverflow.com/questions/43035827/whats-the-difference-between-a-bidirectional-lstm-and-an-lstm"">similar question to yours with a few nice answers</a>.</p>

<hr>

<p>In time-series data, such as device readings from IoT devices or the stock market, using such a bi-directional model wouldn't make sense, as we would be violating the temporaneous flow of information i.e. we cannot use information from the future to help predict the present. That isn't a problem in text analysis, voice-recordings or <a href=""https://arxiv.org/abs/1801.02143"" rel=""nofollow noreferrer"">network analysis on sub-network traffic flow</a>.</p>
","4","2","45264","13013"
"40176","<p><em>It seems you may have things working, but maybe I can still help</em></p>

<h1>Catching those pesky non-varying features</h1>

<p>Assuming <code>a</code> is your dataframe, with only numerical types and columns as features, you can try the following, which will return rows filled with <code>NaN</code> values for features of zero variance i.e. the mean value is equal to the minimum value, so the value cannot vary.</p>

<pre><code>a.T.where(a.T['mean'] == a.T['min'])
</code></pre>

<p>They check is slightly differently in the source code of the VAR model which raises your error:</p>

<pre><code>result = (np.ptp(s) == 0.0 and np.any(s != 0.0))
</code></pre>

<p>So they check that the value doesn't have any peaks (<code>ptp</code> = ""peak-to-peak""), so it doesn't vary - and also that the value is not equal to zero.</p>

<p>In any case, there is little tolerance for it. You have done right by trying to prune out features with low (near zero) variance.</p>

<p>Another possiblity which I didn't notice you having already tried, would be to utilise the other arguments to the <code>fit()</code> method of the <code>VAR</code> model class. It has the following signature:</p>

<pre><code>VAR.fit(maxlags=None, method='ols', ic=None, trend='c', verbose=False)
</code></pre>

<p>If you were to change the <code>trend</code> argument to <code>nc</code>, meaning no constant - you may also get around your error. <a href=""https://www.statsmodels.org/dev/generated/statsmodels.tsa.vector_ar.var_model.VAR.fit.html"" rel=""nofollow noreferrer"">Check out the documentation</a>.</p>

<h1>Source code analysis</h1>

<p>Looking through the contents of the file that raised your error, it seems it occurs while processing the ""original input data"" - as defined by the function <code>add_trend</code> (where your error was raised).</p>

<p>This suggest that your reasoning about the error being found on the data after lagged features are produced would be incorrect in this case (although it's a clever idea!)</p>

<h3>Clearing my name...</h3>

<p>You said:</p>

<blockquote>
  <p>... So the original answer suggested in the previous question was not entirely sufficient.</p>
</blockquote>

<p>I would like to point out that <a href=""https://datascience.stackexchange.com/questions/38203/var-model-valueerror-x-already-contains-a-constant"">my answer to your linked question</a> was indeed sufficient, because I had written that there are <strong>constants</strong> - not that the values must all be equal to zero!  :-P</p>
","0","2","45264","13013"
"40214","<p><span class=""math-container"">$\mathbf{D}$</span> is the <em>Discriminator</em> network, but it really is being modelled as a probability distribution - the discriminator:</p>

<blockquote>
  <p>estimates the probability that a sample came from the training data rather than G</p>
</blockquote>

<p>where <span class=""math-container"">$\mathbf{G}$</span> is the <em>Generator</em> network.</p>

<p>In the context of a two-player min-max game, the authors say a unique solution exists in the case that the Generator perfectly learns the underlying distribution of the dataset. When this point is reached, the Discriminator's estimates will not be better than random guesses.</p>

<p>This means that the accuracy of the estimates will just be 50% (assuming a 50-50 split of real versus generated samples), so it doesn't know if the Generator created the image (which would correspond to an estimate of 100%) or definitely did not create that image (an estimate of 0%).</p>

<p>So <span class=""math-container"">$D = 0.5$</span> everywhere means the Generator has perfectly learned the data and can produce images that are indistinguishable from the real images... so the Discriminator is basically clueless and is flipping a coin.</p>
","2","2","45264","13013"
"40231","<p>You must be getting a type error because the elements of <code>text_features</code> and/or <code>author_signatures</code> are not able to be <em>subtracted</em> in the part <code>i - j</code>.</p>

<p>Depending on what they are, you could try converting them. For example, you could try making one or both of them numbers or strings like this:</p>

<pre><code>Diff = [float(i) - float(j) for i,j in zip(text_features, author_signature)]   # both made into numbers

Diff = [str(i) - str(j) for i,j in zip(text_features, author_signature)]   # both made into strings (words)
</code></pre>

<p>To give better advice, you need to give an example of what your two lists are that are in the <code>zip()</code></p>

<h2>EDIT</h2>

<p>Based on the update in the question, I see that it works as expected for me:</p>

<pre><code>In [1]: text_features = [1, 2, 3]                                                         

In [2]: author_signature = [3, 2, 1]                                                      

In [3]: Diff = [i - j for i,j in zip(text_features, author_signature)]                    

In [4]: Diff                                                                              
Out[4]: [-2, 0, 2]
</code></pre>
","0","2","45264","13013"
"40250","<p>I believe current implementations of the best known libraries will allow you to implement any network you come across. So it boils down to the aspects of taste and practicality.</p>

<p>As far as deciding on a framework based on the network you want to create; if the network has branches or separate control flows (data transfer or logical) that are used conditionally during training, <a href=""https://pytorch.org/"" rel=""nofollow noreferrer"">PyTorch</a> is likely more useful. Otherwise, the best starting point is likely to use <a href=""https://keras.io/"" rel=""nofollow noreferrer"">Keras</a> - a high-level API over <a href=""https://www.tensorflow.org/"" rel=""nofollow noreferrer"">Tensorflow</a>.</p>

<h3>The (very) general rule of thumb is:</h3>

<blockquote>
  <p>Use Pytorch for experimentation, and Tensorflow for production</p>
</blockquote>

<p>Now the caveats to the above sweeping statement:</p>

<ol>
<li><strong>Both libraries are making massive efforts to cover ""the other"" library's strengths:</strong></li>
</ol>

<p>The latest release of Pytorch (v1.0) offers several methods of making <em>production ready</em> code - so ways of compiling it to improve performance as well as the use of 16-bit floating point operations (as opposed to the standard 32-bit). They have also started rolling out a C++ API. These are all steps to make full-scale products for deployment. Tensorflow has introduced its <em>eager</em> API, for eager execution, meaning you can essentially build dynamic graphs, just as in PyTorch. The enables faster and more interactive prototyping.</p>

<ol start=""2"">
<li><strong>The experience you have while developing might be more important:</strong></li>
</ol>

<p>If you are a pure Python developer, PyTorch will feel more natural. If you have been a general software engineer in other statically types languages (such as C++ and Java), Tensorflow may feel more logical.</p>

<h3>Summary</h3>

<p>Overall, Tensorflow along with (the now built-in) Keras wrapper, is a more mature library with a larger community and longer history, meaning more support online. PyTorch has a higher level of excitemnet around it, as far as I can tell. It is used very actively in research and newer open-source projects/courses, which indicates it may become a dominant force.</p>
","1","2","45264","13013"
"40472","<p>If I understand correctly, a single patient would be one feature, so one column, and they had interations with hospitals over short periods; followed by no interations (and therefore no data) over longer periods.</p>

<p>Due to the sparsity of data along each single time-series, perhaps you could look into ways to encode the information a little more consicely. I think this approach may lead to fairly good results because you are only aiming to categorise the patients into one of two groups.</p>

<p>This could perhaps be done by converting your base data into more insightful statistics. For example, for each patient, you could compute some standard values:</p>

<ul>
<li>Number of time periods with(-out) hospital interaction</li>
<li>Number of different codes (assuming the code refers to the ailment or something useful?)</li>
<li>Average length of gap between periods of activity</li>
<li>Average length of periods of activity</li>
</ul>

<p>For the above, you'd need to probably heuristcally define what a ""period of activity"" is, e.g. 5 data points within 100 timesteps. Otherwise you're in a ""gap"" between such periods.</p>

<p>This would dramitically reduce the amount of datapoints and lend itself quite well and allow you to use simpler classification methods, such as GLMs, logistic regression (as you only have two classes) or perhaps a simple feed-forward neural network.</p>

<p>The idea is to extract as much of the temporal aspect from the data as possible during this feature engineering/pre-processing, such that formal time-series modelling would become unnecessary.</p>

<hr>

<p>If you want to try normal time-series, such as regression models, you need to remember that they return a numerical value by default (regression problem versus classification), so you would need to decide on a threshold to put patients in one of the two groups.</p>

<p>Try trying to encode the missing data simply as a count of timesteps between datapoints would not work because of the fact that each patient has different timelines, so the resulting data would have strongly varying dimensions, making many models unusable.</p>
","1","2","45264","13013"
"40504","<p>You might be able to get pretty good results on a simple task, but the fact of the matter is that taking random pixels (or indeed just flattening out ALL pixels) essentially destroys any structural information that was contained within the original image.</p>

<p>This was the insight behind convolution networks (<a href=""http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"" rel=""nofollow noreferrer"">from original author Yann LeCun</a>), as they really find areas of correlation in the position/structure of the image input across the dataset. So they understand, for example, the a ""1"" offers high correlations of pixels in a vertical line, normally in the center of the image input.</p>

<p>This information is no longer included with randomly selected pixels and has been almost destroyed by flattening all pixels into a single vector.</p>

<hr>

<p>If your use-case requires a certain accuracy and you are reaching that with your simple neural network (or otherwise), then of course it is perfectly valid and you can be happy  :-)</p>
","1","2","45264","13013"
"40643","<p>If you are saying your model produces a vector of length N for each pixel (holding the <em>probability</em> of each class for that pixel), then you are doing the right thing to select the maximum value, of course.</p>

<p>For the plotting, if you leave the image in <code>128x128</code>, your output will simply be gray-scale and if you are plotting the raw maximum values you took from each vector, I imagine the image will be very noisy.</p>

<p>You need to map the class of each pixel to a predefined colour mapping. For example, if you have the classes: <code>tree, car, building, sky</code>, you should decide on a colour for each of those. For example, specify a dictionary mapping each class to a colour:</p>

<pre><code>colour_mappings = {
    'tree': (255, 255, 255)   # white
    'car': (255, 0, 0)        # red
    'building': (0, 255, 0)   # green
    'sky': (0, 0, 255)        # blue
    }
</code></pre>

<p>No when you have the output matrix of size <code>128x128</code>, you can use filtering to replace each of your classified pixels to the colour you put in the mapping above.</p>

<h2>Example</h2>

<p>I use the colour mapping above along with a simulated matrix of pixel classification results (for simplicity just 5x5, not 128x128), which is randomly filled with values from <code>[0, 1, 2, 3]</code>:</p>

<pre><code>classes = array([[0, 3, 3, 1, 0],
                 [2, 2, 0, 3, 2],
                 [3, 1, 0, 0, 1],
                 [1, 3, 2, 2, 2],
                 [1, 3, 1, 2, 1]])
</code></pre>

<p>Now create an array with three channels, which we will fill based on our results. I just initialise it with zeros, which would be black in colour, which is ok because non of my classes in the mapping above are black (just to avoid confusion):</p>

<pre><code># initialise an empty (black) matrix to hold the image
segmented_image = np.ones((5, 5, 3))          # we require three channel dims for RGB colour
</code></pre>

<p>Let's say the classes above (in the same order) have the numbers <code>0, 1, 2, 3</code> - you could then fill the empty image matrix with the colours as follows.</p>

<pre><code># fill the segmented image based on the class, using your mappings
segmented_image[classes == 0] = colour_mappings['tree']
</code></pre>

<p>This will produce the following image, where we see all pixels from the class 'tree' now show the mapped colour, white:</p>

<p><a href=""https://i.stack.imgur.com/5qL1h.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5qL1h.png"" alt=""first class has been filled. Trees are mapped to white""></a></p>

<p>Now we can do the same for the remaining classes:</p>

<pre><code>segmented_image[classes == 1] = colour_mappings['car']                 
segmented_image[classes == 2] = colour_mappings['building']        
segmented_image[classes == 3] = colour_mappings['sky']
</code></pre>

<p>This produces the following final image:</p>

<p><a href=""https://i.stack.imgur.com/cYE2o.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cYE2o.png"" alt=""enter image description here""></a></p>

<p>Which can be checked against the <code>classes</code> matrix shown above.</p>
","2","2","45264","13013"
"40705","<p>The point of a <em>""Data Lake""</em> is to store unstructured data together, along with the related meta-data, all in one place. The data is usually stored in the format in which it was recorded i.e. the raw format. Data is then only extracted and processed for usage as required.</p>

<p><a href=""http://www.dataversity.net/pros-and-cons-warehouse-vs-data-lakes/"" rel=""nofollow noreferrer"">There is a nice article that compares data warehouses and data lakes</a> in an unbiased and practical manner.</p>

<hr>

<p>From your description of your data and the problems you want to solve, I don't personally think a data lake would be required.</p>

<p>Perhaps your colleague is planning to create a data lake for larger purposes, e.g. to house the data for many projects such as yours, which might also want to use your data? There are the issues of security, data ownership and data stewardship to think about in this case. Questions to answer would be:</p>

<ul>
<li>who actually owns the data?</li>
<li>who is allowed to use what data?</li>
<li>who is allowed to view what data?</li>
<li>who is responsible for ensuring data is available at all times?</li>
</ul>
","2","2","45264","13013"
"40891","<p>I made a small example, which basically does the same as your code, and I show that the expected results are obtained:</p>
<h3>Imports</h3>
<pre><code>In [1]: from keras.models import Sequential
In [2]: from keras.layers import Dense
</code></pre>
<h2>Build a model</h2>
<p>Note the <code>name</code> arguments:</p>
<pre><code>In [3]: model = Sequential()
In [4]: model.add(Dense(50, input_shape=(20, 20), name='dense1'))
In [5]: model.add(Dense(30, name='dense2'))
In [6]: model.add(Dense(1, name='dense3'))
In [7]: model.compile('rmsprop', 'mse')
In [8]: model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense1 (Dense)               (None, 20, 50)            1050      
_________________________________________________________________
dense2 (Dense)               (None, 20, 30)            1530      
_________________________________________________________________
dense3 (Dense)               (None, 20, 1)             31        
=================================================================
</code></pre>
<h2>Rename the layers</h2>
<p>EDIT: it seems newer versions of Keras and the <code>tf.keras</code> API now do not allow renaming layers via <code>layer.name = &quot;new_name&quot;</code>. Instead you must assign your new name to the <em>private</em> attribute, <code>layer._name</code>.</p>
<pre><code>In [9]: for i, layer in enumerate(model.layers):
    ...:     # layer.name = 'layer_' + str(i)    &lt;-- old way
    ...:     layer._name = 'layer_' + str(i)
In [10]: model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
layer_0 (Dense)              (None, 20, 50)            1050      
_________________________________________________________________
layer_1 (Dense)              (None, 20, 30)            1530      
_________________________________________________________________
layer_2 (Dense)              (None, 20, 1)             31        
=================================================================
</code></pre>
<p>So we see the layers now have the new names!</p>
<h2>Further inspection</h2>
<p>I can see that the internal identifiers of the layers themselves <strong>are not changed</strong>. I show the first layer's weights, as we see the original label is still present:</p>
<pre><code>In [11]: a = model.layers[0]
In [12]: a.weights
Out[12]: 
[&lt;tf.Variable 'dense1/kernel:0' shape=(20, 50) dtype=float32_ref&gt;,
 &lt;tf.Variable 'dense1/bias:0' shape=(50,) dtype=float32_ref&gt;]
</code></pre>
<p>We see the <code>dense1</code> name is still visible in the layer container, along with its unchanged shape and datatype.</p>
<p>I also initialised the model weights by running <code>model.get_weights()</code>, then renamed the layers as before to new names, compiled the model and saw that the layers' names were altered as before, and the weights themselves were not changed i.e. reinitialised. The exact same values were contained. This means that renaming your layers cannot be behind the drop in accuracy.</p>
","6","2","45264","13013"
"40893","<p>One of the values being returned by <code>touch.x</code> or <code>touch.y</code> are bigger than the dimensions of the <code>sand</code> array.</p>

<p>You could try adding a try/except block around it and print a useful message - just as an example, as I have no idea what this programme is doing (in the <code>on_touch_down</code> function):</p>

<pre><code>def on_touch_down(self, touch)
...
...
    try:
        sand[int(touch.x), int(touch.y)] = 1
    except IndexError:
        print(""Touchdown did not occur within the end-zone! No goal!"")
</code></pre>
","0","2","45264","13013"
"41187","<p>You begin by asking about image normalisation, but then refer to other techniques, which I believe all fall under ""image augmentation"". So I will answer the more general question:</p>

<blockquote>
  <p>how can I perform image augmentation to improve my model?</p>
</blockquote>

<p>I would generally say that the more augmentation you can apply, the better. A <a href=""https://www.merriam-webster.com/dictionary/caveat"" rel=""noreferrer"">caveat</a> to that statement is that <em>the augmentations must make sense for your target case</em>. I will explain at the end what I mean by that.</p>

<p>Let's list some ways to augment an image, along with some common values:</p>

<h1>Augmentations</h1>

<h3>1. Flipping</h3>

<p>Starting with perhaps the easiest to implement, you can (easily) flip images:</p>

<ul>
<li><strong>horizontally:</strong> makes an image of an arrow pointing left point in the opposite direction - to the right, and vice-versa</li>
<li><strong>vertically:</strong> makes an arrow pointing down point up, and vice-versa.</li>
</ul>

<p>It is more difficult, due the general rectangular shape of images, but you could flip an image diagonally, but I haven't seen anyone do that really. It is getting close to the idea of rotation, I suppose.</p>

<h3>2. Rotation</h3>

<p>In this case, we simply rotate the image about its center by a random number of degrees, usually within a range of <code>[-5, +5]</code> degrees. Even tiny small angles, which we can maybe hardly perceive, this is a simple trick that can add a lot of rubustness to a model.</p>

<h3>3. Crop</h3>

<p>We take ""chunks"" of an image, either at random or using a pre-defined pattern. This helps the model perhaps focus on certain areas of images and not be overwhelmed by perhaps unimportant features. It also simply creates more data.</p>

<p>You might also incorporate this augmentation method as a means to resize your input images to fit into a pre-trained model, which took input images of a different size to your data.</p>

<h3>4. Normalisation</h3>

<p>This is more of a numerical optimisation trick that one which can really be interpreted from a <em>visual</em> perspective. Many algorithms will be more stable whilst working with smaller numbers, as values are less likely to <em>explode</em>. Numbers closer together (in the sense of continuous space with a linear scale) are also more liekly to produce a smoother optimisation path.</p>

<p>One way of performing normalisation is:</p>

<ul>
<li>first computing a mean and standard deviation of the images, then</li>
<li>(from all images) we subtract the mean and divide by the standard deviation.</li>
</ul>

<p>This will produce images whose pixel values that lie over some range, but have a mean value of zero. <a href=""https://forums.fast.ai/t/images-normalization/4058"" rel=""noreferrer"">Have a look here for some other methods and more discussion</a>. You can normalise e.g. over whole images or just over the separate colour channels (RedGreenBlue).</p>

<p>One thing to keep in mind, is that the values used for optimisation must come from the training dataset itself, and must not be computed over the entire dataset including the validation/test datasets. This is because that information should not be passed to the model in any way - it is kind of cheating.</p>

<h3>5. Translations</h3>

<p>SImply move the image up, down, left or right by some amount, such as 10 pixels, or again within a range of <code>[-5, +5]</code> % of the image size. This will produce pixels along either one or two axes, which must be filled by another colour because the picture has moved out of the frame. It is common to use black or white for these ""empty"" parts, but you could also use the average pixel value or even crop the shifted image to remove them.</p>

<h3>6. Rescaling</h3>

<p>This is a less obvious augmentation step, but can add value because the model will be able to extract different features (more or less <em>general</em>) from larger and smaller images. A small image without much detail would only allow the model to learn higher level, or <em>hazier</em> features and not be able to focus on specific details that are available in high resolution images.</p>

<p>Methods such as <a href=""https://miguel-data-sc.github.io/2017-11-23-second/"" rel=""noreferrer""><strong>progressive resizing</strong></a>, commonly used for training GAN architectures, start with smaller images and slowly work up to larger images. This can be done to keep training as stable as possible (in GANs it can increase robustness against <a href=""http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/"" rel=""noreferrer""><strong>mode collapse</strong></a>), but also coincides with the notion that early layers in a network learn high-level features, and layers deeper into e.g. a convolutional network learn representations that are more detailed. Why do the early layers require high resolution images? Let's give them low resolution images, train quicker and hopefully generalise slightly better!</p>

<h3>7. Be creative!!</h3>

<p>I once made a model for a self-driving car. I knew that the track where the model would be tested had many trees and quite a few high walls, which oth created shadows over the road. These shadows looked a lot like straight lines representing the edges of the road! So I added shadows to my images, at random intensities and random angles with random sizes. The car learnt that a change in brightness over a straight line did not necessarily mean the edge of the road and hence stopped freaking out when it reached one!</p>

<p>Think about the artefacts of your training data and the situations your model will face in the validation data, and try to incorporate them. This is really like feature engineering, incorporated into a model via augmentation... and it can help a lot!</p>

<h3>8. Examples</h3>

<p>Just to add a nice picture, here is a great display of how interesting variations can be made from a single image, taken from the Keras tutorial, linked below. The original image is top left and the remaing 7 are the output of combinations of a set of random augmentations:</p>

<p><a href=""https://i.stack.imgur.com/dR6FR.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/dR6FR.png"" alt=""Image augmentation examples""></a></p>

<p>You can have a <a href=""https://machinelearningmastery.com/image-augmentation-deep-learning-keras/"" rel=""noreferrer"">look at an article like this</a>, which explains augmentation quite well, with examples.</p>

<p><a href=""https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced"" rel=""noreferrer"">Another great example here</a>, part of a series of articles, goes through augmentation possibilities in some detail.</p>

<h1>Implementations</h1>

<h3>1. Keras: <a href=""https://keras.io/preprocessing/image/"" rel=""noreferrer""><strong>ImageDataGenerator</strong></a></h3>

<p>This is a single class, where you can say which augmentation steps to apply and (if relevant) how much. It allows very easy integration of augmentation into a model.</p>

<p><a href=""https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"" rel=""noreferrer"">Check out the official Keras tutorial</a> for some great examples with images.</p>

<h3>2. PyTorch: <a href=""https://pytorch.org/docs/stable/torchvision/transforms.html"" rel=""noreferrer""><strong>Transforms</strong></a></h3>

<p>You can build a small <em>pipeline</em> of transformations, gathering them together into a <em>Compose</em> object, to have fine-grained control over each augmentation step, its parameters and with which probability it is applied.</p>

<p><a href=""https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"" rel=""noreferrer"">Have a look at this tutorial</a> for an example of combining <code>Rescale</code> and <code>RandomCrop</code>.</p>

<h3>3. Augmentor: <a href=""https://github.com/mdbloice/Augmentor"" rel=""noreferrer""><strong>Augmentation library</strong></a>:</h3>

<p>This standalone library allows you build up great augmentation pipelines, which then hand off the images into a model. There are a great number of possibilities implemented, such as random warping/distortion, which looks really funky:</p>

<p><a href=""https://i.stack.imgur.com/tLMYi.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/tLMYi.gif"" alt=""image distortion""></a></p>

<p>It also look as easy to use as the Keras solution shown above!</p>

<h1>Going back to the start</h1>

<p>Now going back to my opening statement, the reason we want to apply as many augmentations as possible, is because it is synthetically creating more data for our model to learn from. We are trying to teach the model a distribution of possible data inputs and make it learn how to produce the correct output. We usually do this on a training set and then ask for its predictions on a validation set, which contains unseen data. The more data that the model has seen from the distribution/function that generates the data, the less surprised and better it will perform on the ""unseen"" data. We want our model to be as robust as possible to make kinds of alterations to data, and exposing it to more of that helps.</p>

<p>The caveat, that we only use relevant augmentations, is necessary because as soon as we start using augmentations that skew the input distribution somehow (e.g. showing the model images that make no sense<sup>1</sup>).</p>

<p>So use as much augmentation as possible, within the constraints of reality for your problem; as well as the time it takes to train the model  ;-)</p>

<hr>

<p><sup>1</sup> - Imagine training a model to detect cars on a road. If we perform a vertical flip, images shown to the model will contain cars that are essentially driving upside down; ""on a grey the sky"". This really doesn't make sense and will never occur in a realistic validation set or in the real world.</p>
","6","2","45264","13013"
"41275","<p>From the <a href=""https://keras.io/models/sequential/"" rel=""nofollow noreferrer"">relevant Keras documnetation:</a></p>

<blockquote>
  <p><strong>steps_per_epoch</strong>: Integer. Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of samples of your dataset divided by the batch size. Optional for Sequence: if unspecified, will use the len(generator) as a number of steps.</p>
</blockquote>

<p>So your assumptions are correct.</p>

<p>If you want to use all your data in each epoch, you should chose a <code>batch_size</code> and <code>steps_per_epoch</code> that multiply together to give your total number of samples.</p>

<p>Usually it will be your resources that decide this for you. If memory is an issue, you have to reduce batch-size until you can fit a batch onto a GPU (as an example).</p>

<p>In your case, I would probably set <code>batch_size</code> to the desired amount, then let Keras work out <code>step_per_epoch</code> for you! Only change it if you really want the model to not use all data in each epoch (which actually bends the definition of the word ""epoch"").</p>
","1","2","45264","13013"
"41299","<p><a href=""https://en.wikipedia.org/wiki/Logistic_regression"" rel=""nofollow noreferrer"">Logistic regression</a> is normally used to perform binary classification, which answers a <strong>yes or no</strong> question, e.g.:</p>

<ol>
<li>Is this an 8 or not?</li>
<li>Will it rain today or not?</li>
</ol>

<p>Perhaps I have misunderstood your explanation, but it sounds like you are trying to perform <em>multi-class classification</em>, i.e. to classify an image as one of a certain number of options. So a single image must contain (be classified as) a number from the list: <code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</code>.</p>

<p>Usually, to add an extra class to a model that already does this, you would need to adjust the final output of the model to predict <a href=""https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f"" rel=""nofollow noreferrer"">a one-hot vector</a> that is simply one element longer!</p>

<p>In the specific case of the Scikit-Learn <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"" rel=""nofollow noreferrer""><strong>LogosticRegression</strong></a> class, it seems as though you don't need to specify anything - the class will automatically use a multinomial model and relevant optimiser as soon as it sees that data is not binary (i.e. a yes-no model as explained above).</p>

<p>Have a look at <a href=""https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-20newsgroups-py"" rel=""nofollow noreferrer"">this official tutorial</a>, which should a multinomial model trained on a dataset with a target variable that has a total of 20 classes. The number of features (<code>n_classes</code> is equal to <code>20</code>) is not passed to the model at all, it is inferred.</p>
","0","2","45264","13013"
"41462","<h1>Answer</h1>

<pre><code>[tuple(s[s.find(""-"") + 1:].split(""_"")) for s in strings]
</code></pre>

<h1>Explanation</h1>

<p>Each string has a nice regular format:</p>

<ol>
<li>a description</li>
<li>employee number</li>
<li>id number</li>
<li>'sc' number      (don't know what that could be)</li>
</ol>

<p>These attributes are all separated by an underscore: <code>_</code>.</p>

<p>You're result doesn't need to description, so find the place of the end of the description and remove it. I find the first hyphen (<code>-</code>) then only keep everything after that.</p>

<p>Then I split the remaing string into three strings, using <code>split(""_"")</code>.</p>

<p>This returns the three parts you want, which I then put into a tuple.</p>

<p>I perform this for each string in <code>strings</code>.</p>

<p>You can put it in a function like this:</p>

<pre><code>def extract_tags(strings):
    result = [tuple(s[s.find(""-"") + 1:].split(""_"")) for s in strings]
    return result
</code></pre>

<p>Here is the output on your test string:</p>

<pre><code>[('emp-001', 'id-01', 'sc-01'),
 ('emp-002', 'id-02', 'sc-12'),
 ('emp-003', 'id-03', 'sc-10')]
</code></pre>
","2","2","45264","13013"
"41875","<h2>To answer the title of your question:</h2>

<blockquote>
  <p>Is Loss not a good indication of performance?</p>
</blockquote>

<p>It is only a <strong>relative</strong> indicator of performance over the training session.</p>

<p>First, what is loss anyway? In general, the loss is some expression of the difference between the model's predicted output and the target output. Depending on the loss function used (e.g. if <code>log</code> is involved) or on the nominal values of the inputs themselves, the value of the loss can also be very small or very large.</p>

<p>People usually normalise data so the values are smaller, but the point is that you cannot always say that a validation loss of <code>0.0012345</code> is actually a <em>good</em> value. Or that <code>12345</code> is definitely bad!</p>

<h2>Other possible loss functions</h2>

<p>Other 3d segmentation models that I have seen it is common to use the <a href=""https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient"" rel=""nofollow noreferrer"">Dice Coefficient</a> for your cost/loss. Maybe give that a try.  <a href=""https://stats.stackexchange.com/questions/273537/f1-dice-score-vs-iou"">The Dice coefficient is essentially the same as the F1 score</a>; you are really finding a trade-off in how to penalise a model for its mistakes in classification e.g. of pixels or voxels. Do you want to strongly punich bad cases or rather a more averging approach. As that link points out, it is similar to the difference between the <span class=""math-container"">$L_1$</span> and <span class=""math-container"">$L_2$</span> losses.</p>

<p>There is also the Jaccard index, which is essentially the same as the Dice Coefficient.</p>

<p>The <a href=""https://en.wikipedia.org/wiki/Tversky_index"" rel=""nofollow noreferrer"">Tversky index</a> is a generalisation of the two - it is an asymmetric <em>similarity measure</em>. :</p>

<p><span class=""math-container"">$$
Tversky(A, B; α, β) = \frac{|TruePos|}{|TruePos| + \alpha |FalsePos| + \beta |FalseNeg|}
$$</span></p>

<ul>
<li>The <em>Dice coefficient</em> is this with <span class=""math-container"">$\alpha = \frac{1}{2}$</span> and <span class=""math-container"">$\beta = \frac{1}{2}$</span></li>
<li>The <em>Jaccard index</em> instead has <span class=""math-container"">$\alpha = 1$</span> and <span class=""math-container"">$\beta = 1$</span></li>
</ul>

<p>This might be a nice approach for your problem to tweak how the loss is computed.
There are plenty of source online to explain more about these well-defined measures and perhaps help you gain an intuition for their results.</p>

<h2>Your results</h2>

<p>With the losses you posted, it doesn't actually look as good as you explained. I am not sure exactly what your <code>idx</code> values mean, but the loss values are actually going up:</p>

<pre><code>In [1]: import numpy as np                                                         
In [2]: import pandas as pd
In [3]: import matplotlib.pyplot as plt
In [4]: val_loss = np.array([0.029650183394551277, 0.009899887256324291, 0.0504908040
   ...: 1659012, 0.02019292116165161, 0.04724293574690819, 0.02810296043753624, 0.026
   ...: 42594277858734, 0.029894422739744186, 0.04158024489879608, 0.0457481481134
   ...: 8913, 0.05406259000301361])    
In [5]: pd.Series(val_loss, index=range(0, 51, 5)).plot()                           
Out[5]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4405326390&gt;
In [6]: plt.show()  
</code></pre>

<p><a href=""https://i.stack.imgur.com/JIi8z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JIi8z.png"" alt=""your validation loss""></a></p>

<p>This suggests you might be overfitting. However, the differences between the predictions and ground truth in the images you show suggest there is a more fundamental problem.</p>
","2","2","45264","13013"
"41896","<p>Do you mean you mave multiple sensors? Ir one single sensor that record one measurement every day?</p>

<p>In short, you will need to simply match the dimensions of your data to the LSTM. To do that, you design your LSTM to consume <em>tensors</em> of the correct shape.</p>

<p>Let's make the following assumptions:</p>

<ol>
<li>You have 2 sensors</li>
<li>Each sensor records one measurement per hour (so 24 per day)</li>
<li>You want to consider blocks of 5 consecutive hours to predict the 6th hour</li>
</ol>

<p>Your LSTM shape would have the following <code>input_shape</code> shape:</p>

<pre><code>(5, 2)
</code></pre>

<p>You can pass any number of these sample blocks as a batch, using a <code>batch_size</code> parameter.</p>

<p>So it summarises to this format:</p>

<pre><code>(batch_size, num_time_steps, num_features)
</code></pre>

<p>This is a brief introduction that hopefully generalises the idea for you. I would suggest looking at more examples to start getting a feeling for the required dimensions and the terminology that people generally use.</p>

<ul>
<li><a href=""https://datascience.stackexchange.com/questions/27533/keras-lstm-with-1d-time-series/27535#27535""><strong>Have a look at this answer</strong></a>, where I explained relevant points for a similar question.</li>
<li>If you have multiple sensors, you need a <em>multivariate LSTM</em> - <a href=""https://datascience.stackexchange.com/questions/27563/multi-dimentional-and-multivariate-time-series-forecast-rnn-lstm-keras/27572#27572""><strong>check out this answer</strong></a></li>
<li>Another Great Source that explains LSTM use in TimeSeries <a href=""https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"" rel=""nofollow noreferrer""><strong>Jason Brownlee - LSTM TimeSeries Prediction in Keras</strong></a></li>
</ul>
","1","2","45264","13013"
"41958","<p>First some stupid sanity-check questions: do you have a GPU in your local machine? (you didn't mention that explicitly). I ask because it will not work e.g. on an integrated Intel graphics card found in some laptops.</p>

<p>Second, you installed Keras and Tensorflow, but did you install the GPU version of Tensorflow? <a href=""https://anaconda.org/anaconda/tensorflow-gpu"" rel=""nofollow noreferrer"">Using Anaconda</a>, this would be done with the command:</p>

<pre><code>conda install -c anaconda tensorflow-gpu 
</code></pre>

<p>Other useful things to know:</p>

<ul>
<li>what operating system are you using? (I assume Linux e.g. Ubuntu)</li>
<li>what GPU do you expect to be shown as available?</li>
<li>Can you run the command: <code>nvidia-smi</code> in a terminal and update your question with the output?</li>
</ul>

<p>If you have installed the correct package (the above method is one of a few possible ways of doing it), and if you have an Nvidia-GPU available, Tensorflow would usually by default reserve all available memory of the GPU as soon as it starts building the static graph.</p>

<hr>

<p>If you were not already, it is probably a good idea to use a conda environment, which keeps the requirements of your model separate from whatever your system might already have installed. <a href=""https://towardsdatascience.com/tensorflow-gpu-installation-made-easy-use-conda-instead-of-pip-52e5249374bc"" rel=""nofollow noreferrer""><strong>Have a look at this nice little walkthrough on getting started</strong></a> - this will likely be a good test to see if your system is able to run models on a GPU as it removes all other possible problems created components unrelated to your script. In short, create and activate a new envrinment that includes the GPU version of Tensowflow like this:</p>

<pre><code>conda create --name gpu_test tensorflow-gpu    # creates the env and installs tf
conda activate gpu_test                        # activate the env
python test_gpu_script.py                      # run the script given below
</code></pre>

<hr>

<h2>UPDATE</h2>

<p>I would suggest running a small script to execute a few operations in Tensorflow on a CPU and on a GPU. This will rule out the problem that you might not have enough memory for the RNN you're trying to train.</p>

<p>I made a script to do that, so it tests the same operations on a CPU and GPU and prints a summary. You should be able to just copy-paste the code and run it:</p>

<pre><code>import numpy as np
import tensorflow as tf
from datetime import datetime

# Choose which device you want to test on: either 'cpu' or 'gpu'
devices = ['cpu', 'gpu']

# Choose size of the matrix to be used.
# Make it bigger to see bigger benefits of parallel computation
shapes = [(50, 50), (100, 100), (500, 500), (1000, 1000)]


def compute_operations(device, shape):
    """"""Run a simple set of operations on a matrix of given shape on given device

    Parameters
    ----------
    device : the type of device to use, either 'cpu' or 'gpu' 
    shape : a tuple for the shape of a 2d tensor, e.g. (10, 10)

    Returns
    -------
    out : results of the operations as the time taken
    """"""

    # Define operations to be computed on selected device
    with tf.device(device):
        random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1)
        dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))
        sum_operation = tf.reduce_sum(dot_operation)

    # Time the actual runtime of the operations
    start_time = datetime.now()
    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:
            result = session.run(sum_operation)
    elapsed_time = datetime.now() - start_time

    return result, elapsed_time



if __name__ == '__main__':

    # Run the computations and print summary of each run
    for device in devices:
        print(""--"" * 20)

        for shape in shapes:
            _, time_taken = compute_operations(device, shape)

            # Print the result and also the time taken on the selected device
            print(""Input shape:"", shape, ""using Device:"", device, ""took: {:.2f}"".format(time_taken.seconds + time_taken.microseconds/1e6))
            #print(""Computation on shape:"", shape, ""using Device:"", device, ""took:"")

    print(""--"" * 20)
</code></pre>

<h3>Results from running on CPU:</h3>

<pre><code>Computation on shape: (50, 50),       using Device: 'cpu' took: 0.04s
Computation on shape: (500, 500),     using Device: 'cpu' took: 0.05s
Computation on shape: (1000, 1000),   using Device: 'cpu' took: 0.09s
Computation on shape: (10000, 10000), using Device: 'cpu' took: 32.81s
</code></pre>

<h3>Results from running on GPU:</h3>

<pre><code>Computation on shape: (50, 50),       using Device: 'gpu' took: 0.03s
Computation on shape: (500, 500),     using Device: 'gpu' took: 0.04s
Computation on shape: (1000, 1000),   using Device: 'gpu' took: 0.04s
Computation on shape: (10000, 10000), using Device: 'gpu' took: 0.05s
</code></pre>
","12","2","45264","13013"
"41961","<p>Yes it is possible. Once you actually return the results from the Tensorflow model, they will (by default) be returned as NumPy arrays. You can then use them as input e.g. to a SciKit Learn model.</p>

<p>Have a <a href=""https://stackoverflow.com/questions/34097281/how-can-i-convert-a-tensor-into-a-numpy-array-in-tensorflow""><strong>look at this thread</strong></a>, which shows some nice examples the types returned by TF models.</p>

<p>One last consideration: The model must return results in such a way that they are in the normal memory, reachable for the CPU - not somehow left in GPU memory.</p>
","2","2","45264","13013"
"41999","<p>If I understand your question<sup>1</sup>, you'd like an input tensor of shape <code>(2, 3)</code>, where each element is actually a vector. If that is the case, you are essentially talking about a 3d tensor (instead of 2d). </p>

<p>For example, if each vector is 5 elements long, your input would have the shape <code>(2, 3, 5)</code>.</p>

<p>To compute convolutions over this, there are layers that take the dimensions as parameters - have a look at the Convolutional layers <a href=""https://pytorch.org/docs/stable/nn.html#conv3d"" rel=""nofollow noreferrer""><strong>like Conv3d</strong></a>.</p>

<p>To compute a linear layer on this input, you still just need to <em>flatten</em> or <em>reshape</em> the tensor to be a single vector.</p>

<p>If you vector lengths can vary, you will have to consider making them all match by either truncating them (making some shorter), or padding (adding e.g. zeros to make shorter ones longer) - or a mixture of both techniques.</p>

<hr>

<p><sup>1</sup> If I haven't fully understood, please add more details to the example in your question</p>
","1","2","45264","13013"
"42001","<p>Non-differentiable (loss) functions can be labelled according to the stardard definitions of differentiation: that the function has a derivative at all points in its domain. For example, the ReLU loss function is technically non-differentiable, because its gradient is not defined at zero (or wherever the two lines meet, if you're using a variant of the standard ReLU).</p>

<p>Non-decomposable functions must be looked at slightly differently. A loss function that is not decomposable is usually one that is composed of several statistics across the training metrics. Take the <a href=""https://en.wikipedia.org/wiki/F1_score"" rel=""nofollow noreferrer""><strong>F1 score</strong></a>, for example:</p>

<p><span class=""math-container"">$$
F1 = 2 \cdot \frac{precision \cdot recall}{precision + recall}
$$</span></p>

<p>Using this as a loss function means you would have to expand the terms for <code>precision</code> and <code>recall</code> in terms of your predictions over a batch, then work out the gradients of each function, finally combining them. It can be done (<a href=""http://proceedings.mlr.press/v54/eban17a/eban17a.pdf"" rel=""nofollow noreferrer""><strong>but it gets complicated</strong></a>), but I would imagine there are trade-offs when optimising at the level of such a metric, which really summarises your model's performance as an average over samples. Using a traditional loss function focusses on improving the model at the more granular <em>sample level</em>. This means the gradients are concentrated at the purest level and I would imagine better optimisation is possible. It could be that optimising at the more abstract level of the F1 score leads to less overfitting or even better generalisation. This is all just an idea though; I haven't seen any work making such comparisons directly.</p>

<hr>

<p><a href=""https://www.reddit.com/r/MachineLearning/comments/6lf57v/d_can_we_optimize_for_f1_score_directly/"" rel=""nofollow noreferrer""><strong>Here is a short reddit thread</strong></a> with some interesting points and examples about when you might even consider trying to optimise such a non-decompasable function directly (i.e. using it as a loss function).</p>

<p><a href=""https://arxiv.org/pdf/1410.6776.pdf"" rel=""nofollow noreferrer""><strong>Have a look at this paper</strong></a>, which speak about non-decomposable loss functions and offers a way to deal with them. Reading through that and comparing the functions that the authors talk about should help you gain a better understanding of the key differences.</p>
","2","2","45264","13013"
"42471","<p>In Python, <code>True</code> and <code>False</code> are <em>cast</em> implicitly into integers:</p>

<pre><code>True == 1     # True!
False == 0    # True!
</code></pre>

<p>Although they are not the same objects - you can test this with <code>True is 1</code>, which returns <code>False</code>.</p>

<p>This means that an algorithm running in pure Python should work without conversion. Many libraries/algorithms have some part implemented in C/C++ in the background, in which case you might run into problems.</p>

<p>You could try the model on your Pandas DataFrame as boolean. If it crashes, you know you must convert to integers/floats.</p>

<p>Even if it doesn't crash, you could convert the values to integers or floats and run it for comparison. Here is a short example:</p>

<pre><code>In [1]: import pandas as pd                                                                               
In [2]: df = pd.DataFrame({'a':[0, 2, 3, 4, 5], 'b':[True, False, True, False, False]})
In [3]: df                                                                                                
Out[3]: 
   a      b
0  0   True
1  2  False
2  3   True
3  4  False
4  5  False
</code></pre>

<p>Convert everything to boolean</p>

<pre><code>In [4]: df.astype(bool)                                                                                   
Out[4]: 
      a      b
0  False   True
1  True   False
2  True    True
3  True   False
4  True   False

In [5]: df.astype(float)                                                                                  
Out[5]: 
     a    b
0  0.0  1.0
1  2.0  0.0
2  3.0  1.0
3  4.0  0.0
4  5.0  0.0
</code></pre>
","3","2","45264","13013"
"42479","<p>I have personally only heard the letters being spelled out:</p>

<blockquote>
  <p>""Here we see the R-O-C curve on this pronunciation model is far from optimal""</p>
</blockquote>
","4","2","45264","13013"
"43925","<blockquote>
  <p>this is pretty simple approach </p>
</blockquote>

<p>Firstly, can you evaluate your scripts on more images to get an idea of how well it performs? If you get an acceptable classification accuracy (or e.g. a good F1 score), then there is no need to try out a CNN!</p>

<blockquote>
  <p>I have no idea about the false positives</p>
</blockquote>

<p>Actually if you cannot evaluate your method like that, then a CNN is also not possible!</p>

<p>If you still want to try a deep learning method, bear in mind that you will generally need a lot of images, let's say at least 1000 in your ""master image"" training set - then hopefully a good percentage of images to test again (hold-out set / test set).</p>

<blockquote>
  <p>I don't know the approach for the problem is the best one</p>
</blockquote>

<p>Your current method seems reasonable to me. There probably is ""the best"" method in general, so don't be too worried.</p>

<p>Somebody has written <a href=""https://www.researchgate.net/publication/279181075_An_analysis_of_deep_neural_networks_for_texture_classification"" rel=""nofollow noreferrer""><strong>An Analysis of Deep Neural Networks for Texture classification</strong></a> - maybe that contains some ideas to get you started  :-)</p>

<hr>

<p>Here is a short video intro to one approch that introduced the <a href=""https://www.youtube.com/watch?v=xJsHo11RAXg"" rel=""nofollow noreferrer""><strong>Local Binary CNN</strong></a>, which was originally used for image classification, but perhaps could be adjusted to your problem.</p>

<p>Instead of aiming for target classes, you would simply need to map input to the labels, and could even relax the focus on colour/texture - the CNN would extract what it needs to learn the mapping.</p>

<p>NOTE: the downside of an end-to-end system like that is that you would no longer know <em>what</em> your model is using as its features to make its prediction! Colour? Texture? Raw RGB values in the top left corner?</p>

<p>This is actually where the LBCNN could shine, because its sparse (stochastically) binary kernels are much less likely to overfit to your training data, compared to a standard CNN.</p>

<p>While I couldn't find the code of the authors from the video above, here is <a href=""https://github.com/whoisraibolt/LBCNN"" rel=""nofollow noreferrer""><strong>an LBCNN implementation</strong></a> for face detection.</p>
","1","2","45264","13013"
"43926","<p>Starting with your last question: The final fully-connected layer shoud output the number of target classes you have. This output can then be passed to a softmax, which <em>normalises</em> the values between the range [0, 1] - allowing them to be interpreted as probabilities.</p>

<p>Remember, you train using the following dimensions:</p>

<pre><code>[batch_size, height, width, channels]        # channels = 1 (in your case) so is removed
</code></pre>

<p>You always need to maintain the <code>batch_size</code> through one loop forwards and backwards through your neuiral network. Normally it set to <code>None</code> which allows the model to accept any batch size you decide on.</p>

<p>What people mean by ""make a vector"" is that the output of your convolutional layers will be a 3D (including channels) and so 2D in your case. You need to <em>flatten</em> these into a single vector. The conv layer output, which then flows into the fully-connected layers will then be:</p>

<pre><code>[batch_size, height * width]         # height and width are those from the final conv layer
</code></pre>

<p>Depending on your framework, there will be a layer called <code>Flatten</code> or <code>view</code> etc., which gives you the vector for each image, keeping the <code>batch_size</code>. This is performed as part of the model structure and so is applied to all data as it passes through the model (training, validation and test data).</p>
","2","2","45264","13013"
"43936","<p>Even if input to a neural netwrk are scaled or normalised, the raw output values can still go outside of that range.</p>

<p>In your case, the output values are being interpreted as to make a binary YES/NO decision, but the raw values cannot necessarily be interpreted as raw probabilities! They are merely the final activations of the network.</p>

<p>To get what you expect, the final activations are usually passed through a <code>softmax</code> function, which essentially <em>squashes</em> the values you see in your table to sum to 1 on each row - this allows us to treat them as probabilities to make the final classification.</p>

<p>In practice, this means simply adding the softmax activation to your final <code>Dense</code> layer in Keras (<code>activation=""softmax""</code>) and then compile the model using:
 <code>loss=""categorical_crossentropy""</code></p>
","1","2","45264","13013"
"43939","<p>There are two ways I'll show you (there are probably a lot more using NumPy):</p>

<h2>First method: chaining operations</h2>

<p>You can use ""masking"" followed by the comparison and finally a sum operation:</p>

<p>We want all values in <code>a</code> from the indices where <code>b</code> is equal to 1:</p>

<pre><code>part1 = a[b == 1]
</code></pre>

<p>Now we want all places where <code>part1</code> is equal to 1</p>

<pre><code>part2 = part1[part1 == 1]
</code></pre>

<p>now we are left with all the places where <code>a</code> and <code>b</code> are equal to 1, so we can simply sum them up:</p>

<pre><code>result = part2.sum()
</code></pre>

<h2>Method 2: built in <code>numpy.where</code></h2>

<p>This is much shorted and probably faster to compute. NumPy has a nice function that returns the indices where your criteria are met in some arrays:</p>

<pre><code>condition_1 = (a == 1)
condition_2 = (b == 1)
</code></pre>

<p>Now we can combine the operation by saying ""and"" - the binary operator version: <code>&amp;</code>.</p>

<pre><code>part1 = numpy.where(condition_1 &amp; condition_2)
</code></pre>

<p>To get your desired output, we can take the length of the resulting set of indices:</p>

<pre><code>result = len(part1)
</code></pre>

<p>Read the documentation about <a href=""https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.where.html"" rel=""nofollow noreferrer""><strong>numpy.where</strong></a> to see the other things it can do for you!</p>
","2","2","45264","13013"
"44332","<p>I don't think there will be a huge difference... although it will depend on how <em>small</em> you resize. The resizing is doing some kind of reduction and/or necessary interpolation (depending on the implementation you use).</p>

<p>If you plan to use OpenCV, you can <a href=""https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#void%20resize(InputArray%20src,%20OutputArray%20dst,%20Size%20dsize,%20double%20fx,%20double%20fy,%20int%20interpolation)"" rel=""nofollow noreferrer"">check out the description here</a>.</p>

<p>The benefit of normalisation after resizing would be that fewer operations would be performed (dividing fewer numbers by 255.0) - meaning slightly faster, but that difference is totally negligible. We both already spent more time talking about this topic than you will probably ever get back from that saving  ;)</p>

<p>Finally, you might want to just experiment: pick 5 images, try out both methods on all of them and plot them next to each other to see if there are any differing artefacts. You can also compute things like the mean and variance of the final pixel values of the images transformed one way versus images transformed the other way.</p>
","0","2","45264","13013"
"44390","<p><a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.factorize.html"" rel=""nofollow noreferrer"">From the documnetation</a></p>

<blockquote>
  <p>Encode the object as an enumerated type or categorical variable.</p>
  
  <p>This method is useful for obtaining a numeric representation of an
  array when all that matters is identifying distinct values. factorize
  is available as both a top-level function pandas.factorize(), and as a
  method Series.factorize() and Index.factorize().</p>
</blockquote>

<p>The examples section goes on to show that the output of the <code>factorize</code> method actually returns two things:</p>

<ol>
<li><strong>labels</strong> - referring to the new values for each of your classes</li>
<li><strong>uniques</strong> - essentially the mapping back to your original labels</li>
</ol>

<p>In your line of code:</p>

<pre><code>df['product_name'] = df['product_name'].factorize()[0]
</code></pre>

<p>The part at the end: <code>[0]</code> means you are only taking the <code>labels</code>, throwing away the <code>uniques</code> that map back to your input.</p>

<p>If you keep both by making the same line:</p>

<pre><code>df['product_name'], mapping = df['product_name'].factorize()
</code></pre>

<p>You could now do the rest of your work and end up with a <code>results</code> column full with the factorised output, you can use this line to get the original values back from those factorized labels:</p>

<pre><code>mapped_back_to_product_name = mapping.take(results)
</code></pre>

<p>I suggest reading the documentation to get more information on how best to use the method  :-)</p>
","4","2","45264","13013"
"44479","<p>If you think it isn't a variable that explains your target variable (churn), you are of course allowed to leave it out - it is in a way then a <em>hand-crafted</em> feature, because you are deliberately telling the model to ignore it (by omitting it).</p>

<p>There are models that would be able to take it into consideration and then decide whether to use it or not, such as component-wise gradient boosting (CWGB) or neural networks. CWGB would simply end up assigning the variable a coefficient of zero if it decides that it doesn't help explain churn rate.</p>

<p>In general, which the percentage split that you gave for males and females, I personally think it could be a valuable feature - even if the model is able to deduce that roughly 14% of <em>all people</em> churn. There might also be correlation between the gender and other features in your dataset that, together, are able to give a stronger signal regarding churn probability.</p>
","2","2","45264","13013"
"44481","<p>There are a couple of points I can maybe give to help.</p>

<p>Firstly, Pandas is not great at merging multiple large dataframes in general because every time you merge a new dataframe to an old one, it makes a copy of both to make a third dataframe - this obviously starts taking a lot of time as your <em>master</em> dataframe grows in each step.</p>

<p>Secondly (untested), you could try to overcome any memory bottlenecks in the merging steps by trying something dumb, like merging two or three files, then writing it to disk. Repeat that until you have 30 larger CSV files, then do it again so you have, say 10 files, and so on. I am not sure at which point the python garbage collection would kick in with you single <code>reduce</code> function, so it could even be the case that you start using SWAP memory, which would make things really slow! This solution could at least prevent that from happening. You should perform each repeat in a new Python session.</p>

<p>One other idea, as you are reading the data from CSV files: you could use a terminal tool to simply paste all the CSV files into one file, then read that into pandas <em>once</em>. Then perform required clean up actions. The cleaning might be a pain (given your <code>merge on</code> requirements, but you would likely overcome the long waiting times for pandas to make the copies. If you files are all in one folder, you can run the following in terminal to put them all in one file</p>

<pre><code>cat *.csv &gt; merged.csv
</code></pre>

<p>There are <a href=""https://superuser.com/questions/62970/unix-cat-starting-from-line""><strong>other methods here</strong></a>, in case you need to leave out the header line etc.</p>

<p>Lastly I would recommend having a look at the <strong>data.table</strong> package, which is now functional in Python - it started in the R world. It has a much more robust memory management system (built on ideas from databases) and is able to store huge amounts of data in a single ""DataTable"", in cases where Pandas would crash. It is a great package to fill the gap between Pandas and Dask (both awesome packages!).</p>

<p><a href=""https://www.youtube.com/watch?v=Ddr8N9STSuI"" rel=""nofollow noreferrer""><strong>Here is a video</strong></a>, showing how Python data.table works and also shows some benchmarks.</p>
","1","2","45264","13013"
"44585","<p>Changing the batch size might change things, but I would doubt an increase in acciracy of 16% by change to batch size alone, given you have 30,000 images.</p>

<p>I would recommend playing with the learning rate in conjunction with batch size or at least allowing your model to train for more epochs. You can use the following points to guide your trials.</p>

<p>Changing the batch size to something smaller will generally:</p>

<ul>
<li>Make the learning curve more volatile, because each loss update is the average of a smaller set of samples, meaning the variation between batches will likely increase</li>
<li>Make learning faster because the model will get bigger (but maybe more biased) updates for weights in each step</li>
<li>Decrease overall model accuracy - assuming the model with larger batches were to be allowed to train for much longer, given the first two points above</li>
</ul>

<p>There has been a lot of research published on the effects of batch size, so maybe have a quick look through some papers (<a href=""https://arxiv.org/abs/1606.02228"" rel=""nofollow noreferrer""><strong>like this one</strong></a>) for the general outcomes.</p>

<p>Also have a look at the answer to <a href=""https://stats.stackexchange.com/questions/316464/how-does-batch-size-affect-convergence-of-sgd-and-why"">this question about the effects of batch</a> size on DNNs.</p>
","1","2","45264","13013"
"45105","<p>If you are working locally (running e.g. starting up a Jupyter notebook from a terminal on localhost) code completiong should work just fine - I have never had any problems.</p>

<p>If you are running a Jupyter notebook attached to a Python process in a Docker container, there can be issues with code completion (the two-way communication may need to be activated).</p>

<p>You can try running the following <code>ipython</code> ""magic command"" that makes the interpreter <em>greedy</em> - this has worked for me in the past:</p>

<pre><code>%config IPCompleter.greedy = True
</code></pre>

<p>Simply run that in the first Jupyter cell.</p>

<p>Here is a list of all <a href=""https://ipython.readthedocs.io/en/stable/interactive/magics.html"" rel=""nofollow noreferrer"">built-in magic commands</a>.</p>

<p>I am afraid I don't know of any differences in these cases between jupyter notebooks and Spyder.</p>
","2","2","45264","13013"
"45106","<p>I cannot say how to fix this is Pandas, without having to likely restructure your approach to the problem (due to the size of your data).</p>

<p>Have a look at the <a href=""https://github.com/h2oai/datatable"" rel=""nofollow noreferrer"">python datatable</a> package, which is like a database running in memory and is much more performant for larger dataframes, where Pandas might start to crash. It consumes less memory and internally works like a database.</p>

<p>I should additionally mention <a href=""http://docs.dask.org/en/latest/"" rel=""nofollow noreferrer"">Dask</a>, which is a distributed version of Pandas that can perhaps cope better than Pandas itself with larger amounts of data.</p>
","1","2","45264","13013"
"45122","<p>You don't really need to implement an algorithm to achieve this. There are a few tools that will do this for you.</p>

<p>You can get the data assigned to buckets for further processing using Pandas, or simply count how many values fall into each bucket using NumPy.</p>

<h2>Assign to buckets</h2>

<p>You just need to create a Pandas DataFrame with your data and then call the handy <a href=""https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.cut.html"" rel=""noreferrer""><strong><code>cut</code> function</strong></a>, which will put each value into a bucket/bin of your definition. From the documentation:</p>

<blockquote>
  <p>Use <code>cut</code> when you need to segment and sort data values into bins.</p>
</blockquote>

<pre><code>In [1]: import pandas as pd
In [2]: import numpy as np    # to create dummy data
</code></pre>

<p>Create some dummy data, put it in a dataframe and define the bins:</p>

<pre><code>In [3]: data = np.random.randint(low=1, high=10001, size=1000)                 
In [4]: df = pd.DataFrame(data=data, columns=[""data""])
In [5]: bins = np.array([1,5,25,50,150,250,1000,5000,10000])
</code></pre>

<p>Pass the data, along with the bin definitions to the <code>cut</code> function and assign it back as a new column in the dataframe:</p>

<pre><code>In [6]: df[""bucket""] = pd.cut(df.data, bins)
</code></pre>

<p>You can then inspect the first few rows to see that the values have now been labelled with the relevant bucket:</p>

<pre><code>In [7]: df.head()                                                              
Out[7]: 
   data         bucket
0  8754  (5000, 10000]
1  2970   (1000, 5000]
2  6778  (5000, 10000]
3  2550   (1000, 5000]
4  5226  (5000, 10000]
</code></pre>

<h2>Counting how many in each bucket</h2>

<p>Here is an example using NumPy, to get an idea of the distribution, as a <a href=""https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.histogram.html"" rel=""noreferrer""><strong>histogram</strong></a>.</p>

<p>Using the <code>data</code> and <code>bins</code> as defined above, we pass them to the numpy <code>histogram</code> function, which will count how many data points fall into each bin:</p>

<pre><code>In [8]: np.histogram(data, bins)
Out[8]: 
(array([  0,   2,   1,   8,   6,  61, 417, 505]),
 array([    1,     5,    25,    50,   150,   250,  1000,  5000, 10000]))
</code></pre>

<p>Where the first row tells you how many values fell into each bin, and the second row confirms the bins used.</p>

<hr>

<p>You can get your dictionary of data into the same form as my dummy data above (into a numpy array) by doing this:</p>

<pre><code>data = np.array([v for v in your_dict.values()])
</code></pre>
","8","2","45264","13013"
"45295","<p>It looks like you have lost a bit of information in that dataset. You shouldn't have 4  measurement for one timestep for one variable - how do you know which of the first four rows to use for <code>2010-05-02</code>?</p>

<p>I would suggest checking your data source, or then working out a way to explain the meaning of the four values... are they different somehow (using other information)?</p>

<p>How are you even creating lags on that Date index? Take the average over each day?
Depending on the package you use for your Dickey-Fuller test (and other methods), they might not be made to deal with identical timesteps as input... so could explain why the session crashes.</p>
","0","2","45264","13013"
"45501","<p>Yes, every layer that contains <em>weights</em> needs to be initialised.</p>

<p>Every <em>weight</em> is just a number (usually a decimal, floating point number). It has to be initialised with some value so that the algorithm (e.g. backpropagation) has some values to work with.</p>

<p>Any learned parameters need to be initialised, as they will be adapted during training. This means you can inialised e.g. the parameters of momentum, and it is changed over time.</p>

<hr>

<p>There are ""layers"" that don't need initialisation. I mean layers that perform some operations, but don't have dynamic weights; weights that are altered during training.</p>

<p>An example would be a standard <em>dropout</em> layer (with no learned parameters). This is called a layer, but doesn't require initialisation as it doesn't have any weights that are changed during training. </p>
","1","2","45264","13013"
"45637","<p>Another approach could be to train several models and then simply take the average of their predictions - essentially using an <em>ensemble</em>. It will be necessary to initialise the models differently and ensure there is some randomness/stochasticity during training such that the models are indeed different.</p>

<p>This approach should allow your grupo of models to tolerate one or two models giving odd predictions in each new prediction, while still giving the most sensible result.</p>

<p>Studies show that this can dramatically smooth predictions. Have a look at this <a href=""https://repository.upenn.edu/cgi/viewcontent.cgi?article=1005&amp;context=marketing_papers"" rel=""nofollow noreferrer""><strong>comparison of use cases and various ensemble technique from Armstrong</strong></a>. There are some guiding principles suggested, depending on saveral factors such as domain knowledge and variance of predictions.</p>
","2","2","45264","13013"
"45700","<p>I don't think Jupyter notebooks (even via extensions) currently offer pausing/restarting cell blocks. I would suggest putting the code of both cells into a single cell and using Python logic to determine the order of execution.</p>

<p>In general, however,  you cannot strictly <em>pause</em> the execution of something and come back to it later. If you abort the execution of the function, the intermediate results are lost (because they are stored within the namespace of the function you effectively just killed). This is because Python only runs one single process at a time (key term: Global Interpreter Lock).</p>

<p>The only thing you might be able to do is create a <em>cell 0</em>, which performs some kind of check, testing how long cell 1 might take to run, then just put cell 1 and cell 2 in and if/else construction that gives the correct order to use.</p>

<hr>

<h2>Other approaches</h2>

<p>You might want to look into something like the <a href=""https://docs.python.org/3.6/library/multiprocessing.html"" rel=""nofollow noreferrer""><strong><em>multiprocessing</em></strong></a> library. There you can create a group of <em>worker</em> processors, to which you can send the contents of cell 1 and <strong>at the same time</strong> the contents of cell 2. They will be computed at the same time, using two different processes i.e. two different instances of the Python interpreter. This can be easy to achieve if there is no direct dependency between (in your case) cell 1 and cell 2. Multiprocessing is particularly useful in the case that your long-running cell is <em>compute bound</em>, meaning is has to perform a large computation.</p>

<p>Yet another option would be to investigate the <a href=""https://docs.python.org/3/library/threading.html"" rel=""nofollow noreferrer""><strong><em>threading module</em></strong></a> and <a href=""https://realpython.com/python-concurrency/"" rel=""nofollow noreferrer""><strong><em>concurrent programming</em></strong></a> in general, but this get a little more complicated and is probably beyond what you want in your situation (it also requires more effort to get working that multiprocessing). This allows Python to kind of do two things at once, but with shared state - so each running thread can change variables that the other thread might also be changing, which can require a lot of work to make safe. This approach is however beneficial when you tasks are IO bound, i.e. there is no big computation, but rather you send data e.g. to a website and wait for a return - most your time is spent waiting.</p>
","1","2","45264","13013"
"45715","<blockquote>
  <p>I want to view a specific image or a dataset's distribution, and see
  if they are different. Does this do the trick?</p>
</blockquote>

<p>It depends what you want to understand or learn about your data.</p>

<blockquote>
  <p>what does each axis mean then?</p>
</blockquote>

<p>In all of your plots, the x-axis ranges from <code>0-255</code>, which is because in all your plots, you are creating histograms of the individual pixel values of your images. A single pixel consists of a vector (tuple) of three values: <code>(red, green, blue)</code>. Each of those three colours can take a value <code>0</code> to <code>255</code> (usually an <a href=""https://en.wikipedia.org/wiki/8-bit"" rel=""nofollow noreferrer""><em>unsigned 8-bit integer</em></a>)So by creating a histogram over your images, you are essentially counting how many time each of the possible values appear.</p>

<p>These plots can therefore tell you something about the average colour distribution. The peaks seem to be around 100 for each of your plots, so I guess there are a lot of <em>mixed</em> colours - colour where the RGB values are in the range 100 - 150. So less pixels are purely reg, green or blue e.g. (0, 0, 255) would be purely blue.</p>

<p>You can also <a href=""https://docs.opencv.org/3.1.0/d1/db7/tutorial_py_histogram_begins.html"" rel=""nofollow noreferrer"">compute/visualise the histogram of colours using the OpenCV library</a>, which has great functions for doing exactly this kind of thing (and will run way faster than <code>matplotlib</code>'s histogram method because OpenCV uses the C++ backend library). Have a look at <a href=""https://lmcaraig.com/image-histograms-histograms-equalization-and-histograms-comparison/"" rel=""nofollow noreferrer"">this great walkthrough</a>.</p>

<hr>

<blockquote>
  <p>Why do I get ten bins for the single image as well?</p>
</blockquote>

<p>The reason all your histograms have 10 bins, is because you are not specifying a value for the <a href=""https://matplotlib.org/api/_as_gen/matplotlib.pyplot.hist.html"" rel=""nofollow noreferrer""><strong><code>bins</code> argument to <code>plt.hist</code></strong></a>, so the default value is taken from the basic configurations of <code>matplotlib</code>, which you can see by running this:</p>

<pre><code>print(plt.rcParams[""hist.bins""])    # will print 10 by default
</code></pre>

<p>The y-axes on those two plots do make sense; the dataset with 50k images has higher counts for each pixel value compared to the 10k dataset.</p>

<hr>

<p>What should I be looking for when it comes to image/dataset distribution?</p>

<blockquote>
  <p>Is it the raw values for the whole dataset only? or Is it the raw values for each class? or even each image?</p>
</blockquote>

<p>Each plot is showing the distribution of raw values only, for whichever set of data you use. You used <code>mtdataset</code> and <code>mytestset</code> as input, so in each case you are only seeing the distribution for those images of course. There is no inclusion of the actual labels anywhere, so you are not breaking down the distributions into the target classes, for example. Just raw pixel values are considered.</p>

<hr>

<blockquote>
  <p>What should I be looking for when it comes to image/dataset
  distribution?</p>
</blockquote>

<p>This is problem specific.</p>

<p>You could be looking to see that there is indeed a distribution of RGB values, as a sanity check that you don't have some really skewed set of colour images that e.g. are mostly black or white. You might compare the distributions of the training and test sets to one another, to ensure that they are similar - meaning the training set is indeed representative of the test set. If this were not the case, any model you might train on a specific task could be biased towards the training set and perform badly on the test set (it will not have seen images similar to the test set if the distributions are very different!)</p>

<p>For training neural networks with images, it is common to normalise the distribution of pixel values to the range <code>[-1, +1]</code>, which helps smoother learning via smoother gradient updates.</p>
","3","2","45264","13013"
"46021","<p>The confusion matrix is used to tell you how many predictions were <strong>classified</strong> correctly or incorrectly. You are looking at a regression model, which gives you a continous output (not classification).</p>

<p>So when you run <code>confusion_matrix(y_test, y_pred)</code> it will throw the <code>ValueError</code> because it expected class predictions, not floating point numbers.</p>

<p>Are you trying to predict classes, or really just a number output? If not, then you should not be using the confusion matrix.</p>

<hr>

<p>If you want to predict e.g. <strong>1</strong> or <strong>0</strong> for your <code>y</code> values, then you would have to convert your linear regression predictions to either of these classes. You could say any value in <code>y_pred</code> above <code>0.7</code> is a <code>1</code> and anything below is <code>0</code>.</p>

<pre><code>cutoff = 0.7                              # decide on a cutoff limit
y_pred_classes = np.zeros_like(y_pred)    # initialise a matrix full with zeros
y_pred_classes[y_pred &gt; cutoff] = 1       # add a 1 if the cutoff was breached
</code></pre>

<p>you have to do the same for the actual values too:</p>

<pre><code>y_test_classes = np.zeros_like(y_pred)
y_test_classes[y_test &gt; cutoff] = 1
</code></pre>

<p>Now run the confusion matrix as before:</p>

<pre><code>confusion_matrix(y_test_classes, y_pred_classes)
</code></pre>

<p>which gives output:</p>

<pre><code>array([[2, 3],
       [0, 0]])
</code></pre>
","4","2","45264","13013"
"46101","<p>The output from <code>db_scan.labels_</code> is the assigned cluster value for each of the points that you provide as input to the algorithm.</p>

<p>You provided 20 points, so there are 20 labels. As explained in <a href=""http://labels_%20:%20array,%20shape%20=%20[n_samples]%20Cluster%20labels%20for%20each%20point%20in%20the%20dataset%20given%20to%20fit().%20Noisy%20samples%20are%20given%20the%20label%20-1."" rel=""nofollow noreferrer"">the relevant documentation</a>, you will see:</p>

<blockquote>
  <p>labels_ : array, shape = [n_samples]
  Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1.</p>
</blockquote>

<p>So each of your points was assigned to one of two clusters, and there were no points marked as <em>noise</em> (receiving a value of <code>-1</code>).</p>

<hr>

<h2>Interpreting the clusters</h2>

<p>Your plot shows the original data, plotted on the axes using the original values; only the colour encodes the cluster to which each point was assigned.</p>

<p>As with many clustering algorithms, DBSCAN measures the distance between points in some n-dimensional space. It uses parameters you provide to decide if points belong to the same group, based on their distance from one another.</p>

<blockquote>
  <p>Why do I have the negative scales both on the x-axis and y-axis?</p>
</blockquote>

<p>The distance metrics used do not really care about the <em>sign</em> of the points (where they lie on an axis), simply the distances. In the case of DBSCAN, it is the Euclidean distance. There are other distances that you can pass to the <code>metric</code> parameter of the DBSCAN algorithm; <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances"" rel=""nofollow noreferrer"">here is the valid list</a>.</p>
","1","2","45264","13013"
"46181","<h2>Graph Embeddings</h2>

<p>You try to use <a href=""https://datascience.stackexchange.com/questions/24081/what-are-graph-embedding""><strong>graph embeddings</strong></a>. These would offer the ability to map your graph-based data into a single latent space, at based on the vertices, the edges or the graph schema as a whole. This has become somewhat of an exciting research topic within research over the last few years; combining graph analysis with modern embedding techniques.</p>

<p>Here is one <a href=""https://arxiv.org/abs/1705.02801"" rel=""nofollow noreferrer"">survey paper</a> that list approaches to these embeddings, along with the authors' own <a href=""https://github.com/palash1992/GEM"" rel=""nofollow noreferrer""><strong>libraries for both static graphs</strong></a> and <a href=""https://github.com/palash1992/DynamicGEM"" rel=""nofollow noreferrer""><strong>dynamic graphs</strong></a>. Looking through the results of that survey paper (they perform some becnhmarks towards the end), it would seem that the most promising model would be:</p>

<ul>
<li><a href=""https://arxiv.org/abs/1607.00653"" rel=""nofollow noreferrer""><strong>node2vec</strong></a> - if you want to classify the nodes as cleanly as possible (<a href=""https://github.com/aditya-grover/node2vec"" rel=""nofollow noreferrer""><em>reference implementation</em></a>)</li>
<li><a href=""https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf"" rel=""nofollow noreferrer""><strong>Structural Deep Network Embedding</strong></a> - if you want to re-construct the graph (<a href=""https://github.com/suanrong/SDNE"" rel=""nofollow noreferrer""><em>reference implementation</em></a>)</li>
</ul>

<hr>

<h2>Anomaly Detection</h2>

<p>As to the end goal of anomaly detection, I am not sure which of the embedding methods would be best suited. It might be a case as to whether you want to preserve more <a href=""https://datascience.stackexchange.com/questions/36889/what-does-it-mean-by-t-sne-retains-the-structure-of-the-data""><strong>local or global structure</strong></a> (as with the trade-off between methods such as t-SNE and PCA).</p>

<p>I think you can imagine what to try out with the embeddings in order to detect anaomalies. There might be more clues in the papers linked above as to which way researchers tend to go for certain problems.</p>

<p>Given the hierarchical nature of your graph data, I would be tempted to start by looking into using <a href=""https://bdpi.usp.br/bitstream/handle/BDPI/51005/2709770.pdf?sequence=1&amp;isAllowed=y"" rel=""nofollow noreferrer""><strong>Hierarchical Density Based Spatial Clustering with Applications of Noise</strong></a> (i.e. HDBSCAN) - the main library specifically includes a few few methods of <a href=""https://hdbscan.readthedocs.io/en/latest/outlier_detection.html"" rel=""nofollow noreferrer""><strong>outlier detection</strong></a>! Here a citation from the linked paper:</p>

<blockquote>
  <p>Based on these definitions, we can devise an algorithm DBSCAN* (similar to DBSCAN) that conceptually finds clusters as the connected components of a graph in
  which the objects of X are vertices and every pair of vertices is adjacent if and only
  if the corresponding objects are ε-reachable w.r.t. user-defined parameters ε and mpts.
  Noncore objects are labeled as noise.</p>
</blockquote>
","1","2","45264","13013"
"46524","<p>If your model is still improving (according to the <strong>validation loss</strong>), then more epochs are better. You can confirm this by using a hold-out test set to compare model checkpoints e.g. at epoch 100, 200, 400, 500.</p>

<p>Normally the amount of improvement reduces with time (""diminishing returns""), so it is common to stop once the curves is <em>pretty-much</em> flat, for example using <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping"" rel=""nofollow noreferrer""><strong>EarlyStopping</strong></a> callback.</p>

<hr>

<p>Different model requires different times to trains, depending on their size/architecture, and the dateset. Some examples of large models being trained on the <a href=""http://www.image-net.org/about-stats"" rel=""nofollow noreferrer"">ImageNet dataset</a> (~1,000,000 labelled images of ~1000 classes):</p>

<ul>
<li>the original YOLO model <a href=""https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088"" rel=""nofollow noreferrer"">trained in 160 epochs</a></li>
<li>the ResNet model <a href=""https://arxiv.org/abs/1811.12019"" rel=""nofollow noreferrer"">can be trained in 35 epoch</a></li>
<li>fully-conneted DenseNet model <a href=""https://github.com/GeorgeSeif/Semantic-Segmentation-Suite"" rel=""nofollow noreferrer"">trained in 300 epochs</a></li>
</ul>

<hr>

<p>The number of epochs you require will depend on the size of your model and the variation in your dataset.</p>

<p>The size of your model can be a rough proxy for the complexity that it is able to express (or learn). So a huge model can represent produce more nuanced models for datasets with higher diversity in the data, however would probably take longer to train i.e. more epochs.</p>

<p>Whilst training, I would recommend plotting the training and validation loss and keeping an eye on how they progress over epochs and also in relation to one another. You should of course expect both values to decrease, but you need to stop training once the lines start diverging - meaning that you are <a href=""https://stats.stackexchange.com/questions/292700/overfitting-in-neural-network"">over-fitting to your specific dataset</a>.</p>

<p>That is likely to happen if you train a large CNN for many epochs, and the graph could look something like this:</p>

<p><a href=""https://i.stack.imgur.com/XZ9DC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XZ9DC.png"" alt=""overfitting""></a></p>

<p><a href=""http://cs231n.github.io/neural-networks-3/#baby"" rel=""nofollow noreferrer"">Image source</a></p>
","4","2","45264","13013"
"46543","<p>It is true that a set is not <a href=""https://stackoverflow.com/questions/14535730/what-do-you-mean-by-hashable-in-python""><em>hashable</em></a> (it cannot be used as a key in a hashmap a.k.a a dictionary). So what you can do is to just convert the column to a type that is hashable - I would go for a <code>tuple</code>.</p>

<p>I made a new column that is just the <code>""z""</code> column you had, converted to tuples. Then you can use the same method you tried to, on the new column:</p>

<pre><code>In [1] : import pandas as pd 
    ...:  
    ...: lnks = [ ( 'a' , 'b' , { 'a' , 'b' } ) , ( 'b' , 'c' , { 'b' , 'c' } ) 
    ...: , ( 'b' , 'a' , { 'a' , 'b' } ) ] 
    ...: lbls = [ 'x' , 'y' , 'z' ] 
    ...: df = pd.DataFrame.from_records( lnks , columns = lbls)                 

In [2]: df[""z_tuple""] = df.z.apply(lambda x: tuple(x))                         

In [3]: df.drop_duplicates(subset=""z_tuple"", keep=""first"")                     
Out[3]: 
   x  y       z z_tuple
0  a  b  {b, a}  (b, a)
1  b  c  {c, b}  (c, b)
</code></pre>

<p>The <code>apply</code> method lets you apply a function to each item in a column, and then returns the values as a new column (a Pandas Series object). This lets you assign it back to the original DataFrame as a new column, as I did.</p>

<p>You can also remove the <code>""z_tuple""</code> column then if you no longer want it:</p>

<pre><code>In [4] : df.drop(""z_tuple"", axis=1, inplace=True)                               

In [5] : df                                                                     
Out[5] : 
   x  y       z
0  a  b  {b, a}
1  b  c  {c, b}
2  b  a  {b, a}
</code></pre>
","3","2","45264","13013"
"46577","<p>There are a couple of parts that I think changing will help.</p>

<p>First, a general one for all model building: I would suggest you scale your data before putting it into the model. </p>

<p>It might not directly solve the problem of receiving the same predicted value in each step, but you might notice that you predictions lie somewhere in the ranges of your input values - as you are using <strong>unscaled volume</strong>, that is making things difficult for the model. It is essentially have to work on two different scales at the same time, which is cannot do very well.</p>

<p>Have a look at the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"" rel=""nofollow noreferrer"">StandardScaler in sklean</a> for a way how to do that.</p>

<hr>

<p>Next a few suggestions of things to change, specifically because you are working with stock prices:</p>

<p>I would normally predict the value of the stock market <em>tomorrow</em>, and not the closing prices on the same data, where you are using <em>open/high/low/volume</em>. For me that only make sense if you were to have high-frequency (intraday) data.
Given this, you would need to shift your <code>y</code> value by one step. There is a <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shift.html"" rel=""nofollow noreferrer"">method on Pandas DataFrames to help with that</a>, but as you dont have a <code>date</code> column and you only need to shift by one timestep anyway, you can just do this:</p>

<pre><code>features = df.loc[:-1, df.columns != 'Closing'].values    # leave out last step
targets = df.loc[1:, df.columns  == 'Closing'].values     # start one step later
</code></pre>

<p>You could then even then predict the opening price of the following day, or keep <code>closing</code> data in the <code>features</code> data, as that would not introduce temporal bias.</p>

<hr>

<p>Something that would require more setup, would be to look at shuffling your data. Again, because you want to use historical values to predict future ones, you need to keep the relevant hsitory together. Have a look at <a href=""https://datascience.stackexchange.com/questions/30762/how-to-predict-the-future-values-of-time-horizon-with-keras/30908#30908"">my other answer to this question and the diagram</a>, which explains more about this idea. </p>

<h2>EDIT</h2>

<p>You should also scale <code>y_train</code> and <code>y_test</code>, so that the model knows to predict within that range. Do this using the same <code>StandardScaler</code> instance, as not to introduce bias. Have a <a href=""https://chrisalbon.com/machine_learning/model_evaluation/split_data_into_training_and_test_sets/"" rel=""nofollow noreferrer""><strong>look at this short tutorial</strong></a>. Your predictions will then be within the same range (e.g. <code>[-1, +1]</code>). You can compute errors on that range too. If you really want, you can then scale your predictions back to the original range so they look more realistic, but that isn't really necessary to validate the model. You can simply plot the predictions against ground truth in the scaled space.</p>

<p><a href=""https://stats.stackexchange.com/questions/267012/difference-between-preprocessing-train-and-test-set-before-and-after-splitting""><strong>Check out this thread</strong></a>, which explains a few reasons as to why you should use the same instance of <code>StandardScaler</code> on the test data.</p>
","1","2","45264","13013"
"46604","<p>Within the documentation for HDBSCAN (Hierarchical <a href=""https://en.wikipedia.org/wiki/DBSCAN"" rel=""nofollow noreferrer"">DBSCAN</a>), there is <a href=""https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html"" rel=""nofollow noreferrer""><strong>a really nice comparison of clustering algorithms</strong></a>. It is a bit biased, highlighting its own strengths (of course), but will still give you the examples and some boilerplate code to get up and running quickly. DBSCAN and HDBSCAN are generally known not to be so good at handling high variance in the clusters. If that ends up being important for your use case, you might want to look at using OPTICS, which is better at handling that.</p>

<p>Alternatively, taking a step back, you could try computing distances between </p>

<p>There is also another library called <a href=""https://codedocs.xyz/annoviko/pyclustering/"" rel=""nofollow noreferrer""><strong><code>pyclustering</code></strong></a>, which contains many algorithms, as well as <a href=""https://codedocs.xyz/annoviko/pyclustering/index.html#example_sec"" rel=""nofollow noreferrer"">a set of examples</a>. These algorithms are mainly implemented in C++ under the hood, so generally are much faster than the versions in more standard python libraries.</p>
","3","2","45264","13013"
"46611","<p>Yes, there are easy ways to do this in Python. My favourite would be to put the data into a Pandas DataFrame, which has a convenient method called <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html"" rel=""nofollow noreferrer""><strong><code>rolling</code></strong></a> that will cycle over your data in a given frame-size and compute whatever you like on that block.</p>

<p>Let me show you an example - say we start with the following column of data:</p>

<pre><code>In [1]: import pandas as pd                                                     
In [2]: import numpy as np
In [3]: df = pd.DataFrame({""A"": np.random.randint(0, 100, (20,)),
                           ""B"": np.random.randn(20)})
</code></pre>

<p>Look at the first 10 rows:</p>

<pre><code>In [4]: df.head(10)
Out[4]: 
     A         B
0   63 -0.003947
1   55  0.442597
2    6  0.684125
3   17  0.968987
4   33 -0.018640
5   50 -0.579558
6   71  0.563125
7   31  1.417384
8    8  0.607813
9   36  0.186146
</code></pre>

<p>We can compute the rolling average over each column and save it back to the dataframe like this:</p>

<pre><code>In [6]: df[[""rolling_a"", ""rolling_b""]] = df.rolling(5).mean()
In [7]: df.head(10)
In [9]: df
Out[9]: 
     A         B  rolling_a  rolling_b
0   63 -0.003947        NaN        NaN
1   55  0.442597        NaN        NaN
2    6  0.684125        NaN        NaN
3   17  0.968987        NaN        NaN
4   33 -0.018640       34.8   0.414624
5   50 -0.579558       32.2   0.299502
6   71  0.563125       35.4   0.323608
7   31  1.417384       40.4   0.470260
8    8  0.607813       38.6   0.398025
9   36  0.186146       39.2   0.438982
</code></pre>

<p>You might notice that the first 4 rows contain <code>NaN</code> values (Not a Number). This is because the <code>rolling()</code> method will let the <code>mean()</code> method work an a window-size smaller than 5 (in our example). There are a lot of options in the <code>rolling()</code> method that you can experiment with.</p>

<p>You can do the same above for single column of a large dataframe like this:</p>

<pre><code>&gt;&gt;&gt; df[""rolling_some_column_name""] = df.some_column_name.rolling(5).mean()
</code></pre>

<p>You can also apply just about any function to the rolling frame - not just <code>mean()</code>.</p>
","3","2","45264","13013"
"46613","<p>In short, no: those operations are not equivalent, even if they seem to have the same effect. Just have a look at the <a href=""https://en.wikipedia.org/wiki/Logarithm"" rel=""nofollow noreferrer""><strong>Wikipedia page for Logarithm</strong></a>, where there is a nice short comparison of the basic mathematical operations:</p>

<p><a href=""https://i.stack.imgur.com/ubVRK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ubVRK.png"" alt=""enter image description here""></a></p>
","0","2","45264","13013"
"46789","<p>Having a look at the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html"" rel=""nofollow noreferrer""><strong>Pandas plot method</strong></a> (on the DataFrame object), we can see that it returns a matplotlib Axes object.</p>

<p>Try something like this:</p>

<pre><code>ax = df.groupby('owner_team').inc_subj.count().plot.bar(ylim=0)

ax.set_xticklabels(df.owner_team)     # if they are still present as strings
</code></pre>

<p>If you removed that column, go back to your original processing and keep a copy of it somewhere then use that column above, instead.</p>

<hr>

<p>Matplotlib will also generally be able to link to the current/latest plot (figure) that has been created. So using the Pandas plot method, you would need to intercept that.You can then try using standard matplotlib methods (e.g. <code>plt.xlabels</code> and so on).</p>

<p>Ther emight be a nice way using the pandas API directly, but I haven't come across that.</p>
","2","2","45264","13013"
"46960","<p>You can set the environment variable via python's built in <code>os</code> module.</p>

<pre><code>import os

os.environ[""KERAS_BACKEND""] = ""tf""   # or theano
</code></pre>

<p>Making changes in one notebook does not seem to carry over to other notebooks that are started in the same Jupyter session (run from a single terminal).</p>

<p>Here I try to get the environment variable in <code>notebook 2</code>, and we see it doesn't exist:
<a href=""https://i.stack.imgur.com/dFKSA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dFKSA.png"" alt=""Notebook 2""></a></p>

<h1>Create environment variable in Notebook 1:</h1>

<p>Now make the changes in <code>notebook 1</code> (after also confirming that the environment variable didn't exist - we catch the exception)</p>

<p><a href=""https://i.stack.imgur.com/ssNyQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ssNyQ.png"" alt=""Notebook 1""></a></p>

<p>And now go back to <code>notebook 2</code> to see if it picks up on the changed value:</p>

<p><a href=""https://i.stack.imgur.com/AdG4K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AdG4K.png"" alt=""Notebook 2 again""></a></p>

<hr>

<p>There are also settings that should be read by Keras on import. <a href=""https://keras.io/backend/"" rel=""nofollow noreferrer""><strong>Have a look at the documentation</strong></a>. If you are on windows, there are some separate steps you should check out there.</p>
","1","2","45264","13013"
"47003","<p><em>This answers assumes I have correctly understood the question... I can alter my answer if OP updates the question with more details</em></p>

<p>Using your example data, you can use Pandas easily drop all duplicates.</p>

<h2>Setup</h2>

<p>First dump your data above into a Dataframe with three columns (one for each of the items in each row.</p>

<p>Import pandas:</p>

<pre><code>import pandas as pd
</code></pre>

<p>import your data - assuming it is a list of lists - each of your rows is a list  of three items, so we have three columns:</p>

<pre><code>df = pd.DataFrame.from_records(your_list_of_lists, columns=[""col1"", ""col2"", ""col3""])
</code></pre>

<p>Have a look at the first 5 rows:</p>

<pre><code>df.head()
   col1  col2     col3
0   600   900  3418309  
1   600   900  3418309  
2   600   900  3418314  
3   600   900  3418314  
4   600   900  3418319  
</code></pre>

<p>The values will be integers by default, not strings (if they all were). But the solutions below should work the same in either case.</p>

<h3>Solutions</h3>

<p>If you want to get all unique values of <code>col3</code>, you can do one of the following:</p>

<pre><code>uniques1 = set(df.col3)              # returns a Python set
uniques2 = df.col3.unique()          # returns a NumPy ndarray
uniques3 = df.col3.drop_duplicates() # returns a pandas Series object
</code></pre>

<p>If you want to remove only those rows where <code>col3</code> repeats itself consecutively (which in some cases could produce the same result as the methods above, depending on the data), then you can <a href=""https://stackoverflow.com/questions/19463985/pandas-drop-consecutive-duplicates"">look at the methods</a> here. [<strong>An example</strong>] adapted for your columnar situation<a href=""https://stackoverflow.com/a/53609021/3126298"">2</a>:</p>

<pre><code>def drop_consecutive_duplicates(a, col_name):      # returns the dataframe
    ...:         ar = a[col_name].values 
    ...:         return a[np.concatenate(([True],ar[:-1] != ar[1:]))]
</code></pre>

<h2>This will return the entire dataframe, which those rows of consecutive values removed.</h2>

<h2>Performance</h2>

<p>There are many other ways to achieve the same result. Of the above, the first method is the fastest (on your small dataset example). Here are the benchmarks:</p>

<pre><code>In [23]: %timeit df.col3.drop_duplicates()                                      
263 µs ± 883 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)

In [24]: %timeit df.col3.unique()                                               
37.2 µs ± 3.19 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)

In [25]: %timeit set(df.col3)                                                   
10.5 µs ± 45.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
</code></pre>

<p>The consective version:</p>

<pre><code>In [26]: %timeit drop_consecutive_duplicates(df, ""col3"")                                                                                                                                                                                          
266 µs ± 3.61 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</code></pre>

<p>I imagine the Pandas methods might be able to scale better to a DataFrame with many rows, so this example might be rather biased on the dummy dataset with only ~20 rows.</p>

<p>The final method clearly has a little bit of overhead as it has to perform some extra operations</p>
","1","2","45264","13013"
"47091","<p>It could be the case that your GPU cannot manage the full model (Mask RCNN) with batch sizes like 8 or 16.</p>

<p>I would suggest trying with batch size 1 to see if the model can run, then slowly increase to find the point where it breaks.</p>

<p>You can also use the configuration in Tensorflow, but it will essentially do the same thing - it will just not immediately block all memory when you run a Tensorflow session. It will only take what it needs, which (given a fixed model) will be defined by batch size.</p>

<hr>

<p>You should alter your code example to be:</p>

<pre><code>config = tf.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.Session(config=config)
</code></pre>
","0","2","45264","13013"
"47102","<p>Activate your conda environment, then use the <code>pip</code> that will also be in your environment (just like the Python interpreter is that of you activated environment).</p>

<p>For example:</p>

<pre><code>source activate your_tf_env          # now we are in the conda env
which -a pip                         # should list all pip executables
</code></pre>

<p>The first <em>pip</em> one should be in <code>your_tf_env</code> somewhere.</p>

<p>Now you can simply install your package, e.g. using <code>pip install fastai</code>, which will use the pip of your environment and the package will also be installed in your environment. You do not need to manually copy the files anywhere.</p>

<p>With your conda env still activated, you can then confirm the package is installed by running:</p>

<pre><code>conda list                     # list all installed packages
conda list | grep fastai       # grep filters results, e.g. for  ""fastai""
</code></pre>
","2","2","45264","13013"
"47254","<p>Assuming you have a string to pass into <code>glob</code> that does wildcard matching, <code>glob</code> will return a <strong>list</strong> of matches. So you don't need to make a string out of that list and replace the square-brackets and so on. You can just iterate over that list and do something with each of the values, which are already strings.</p>

<pre><code>results = glob.glob(your_pattern)
</code></pre>

<p>Based on your code, the results is actually a list of folder names for which you want to count the number of files in each. We can loop over <code>results</code> and print the counts:</p>

<pre><code>for folder in results:
    n = len(os.listdir(folder))
    print(""Folder: {0} --&gt; \t {1} files"".format(folder, n))
</code></pre>
","1","2","45264","13013"
"48320","<p>I think I have understood your problem (mostly from the comments added in your function).</p>

<p>I'll show step by step what the logic is, building upon each previous step to get the final solution.</p>

<p>First we want to find all position where the matrix is larger than 5:</p>

<pre><code>a &gt; 5    # returns a boolean array with true/false in each position
</code></pre>

<p>Now we want to check each row to count if the proportion of of matches (> 5) has reached a certain <strong>threshold</strong>; <span class=""math-container"">$N * 0.95$</span>. We can divide by the number of simulations (number of columns) to essentially normalise by the number of simulations:</p>

<pre><code>(a &gt; 5) / SIMULATION    # returns the value of one match
</code></pre>

<p>These values are required to sum to your threshold for an experiment to be valid.</p>

<p>Now we cumulatively sum across each row. As the True/False array is ones and zeros, we now have a running total of the numbers of matches for each experiment (each row).</p>

<pre><code>np.cumsum((a &gt; 5) / SIMULATION, axis=1)     # still same shape as b
</code></pre>

<p>Now we just need to find out where (in each row) the sum of matches reaches your threshold. We can use <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html"" rel=""nofollow noreferrer"">np.where</a>:</p>

<pre><code>## EDIT: we only need to check the cumsum is greater than 0.95 and not (0.95 * SUMLATION)
## because we already ""normalised"" the values within the cumsum.
condition = np.cumsum((a &gt; 5) / SIMULATION, axis=0) &gt; 0.95
mask = np.where(condition)
</code></pre>

<p><em>I broke it down now as the expressions are getting long.</em></p>

<p>That gave us the <code>i</code> and <code>j</code> coordinates of places where the condition was True. We just want to find the place where we first breached the threshold, so we want to find the <em>indices</em> for the first time in each row:</p>

<pre><code>valid_rows = np.unique(mask[0], return_index=True)[1]    # [1] gets the indices themselves
</code></pre>

<p>Now we can simply use these indices to get the first index in each valid row, where the threshold was breached:</p>

<pre><code>valid_cols = mask[1][valid_rows]
</code></pre>

<p>So now you can get the corresponding values from the parameter matrix using these valid rows/columns:</p>

<pre><code>params = b[valid_rows, valid_cols]
</code></pre>

<hr>

<p>If this is correct, it should be significantly faster than your solution because it avoids looping over the 2d array and instead utilises NumPy's vectorised method and <a href=""https://docs.scipy.org/doc/numpy/reference/ufuncs.html"" rel=""nofollow noreferrer"">ufuncs</a>.</p>
","1","2","45264","13013"
"48720","<p>You seem to be looking at the latest Quatro 4000, which has the following compute rating:</p>

<p><a href=""https://i.stack.imgur.com/dhFNx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dhFNx.png"" alt=""enter image description here""></a></p>

<p>You can find <a href=""https://developer.nvidia.com/cuda-gpus"" rel=""nofollow noreferrer"">the complete list</a> here for all Nvidia GPUs.</p>

<p>While it seems to have an impressive score of 7.5 (the same as the RTX 20180ti), the main draw back the memory of 8Gb. This is definitely enough to get started with ML/DL and will allow you to do many things. However, memory is often the thing that will slow you down and limit your models.</p>

<p>The reason is that a large model will require large number of parameters. Take a look at the following table (<a href=""https://keras.io/applications/#available-models"" rel=""nofollow noreferrer"">models included in Keras</a>), where you can see the number of parameters each model requires:</p>

<p><a href=""https://i.stack.imgur.com/i4Vtv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i4Vtv.png"" alt=""enter image description here""></a></p>

<p>The issue is that the more parameters you have, the more memory you need and so the smaller the <em>batch size</em> you are able to use during training. There are many arguments for larger vs. smaller batch sizes - but having less memory will force you to still to smaller batch sizes when using large models.</p>

<p>It seems from Nvidia's marketing, that the Quadro product line is more aimed towards creative developers (films/image editing etc.), whereas the Geforce collection is for gaming an AI. This highlights that Quadro is not necessarily optimised for fast computation.</p>
","3","2","45264","13013"
"49555","<p>Because you know the names of the columns that you want, it is simple to just pull them to the front and put all other columns just as they were, <em>after</em> your target columns.</p>
<p>Get column names you care about:</p>
<pre><code>desired_cols = ['medcine_preg_oth1' 'medcine_preg_oth2' 'medcine_preg_oth3']
</code></pre>
<p>Now get all column names and remove the ones you care about, so we only have <em>the remainder</em> left. There are a couple of ways to do this, but I like sets...</p>
<pre><code>all_cols = set(df.columns)    # get the column names
other_cols = all_cols.difference(desired_cols)    # ones you don't care about
</code></pre>
<p>Now we just need to re-arrange all the columns so your desired columns are at the beginning:</p>
<pre><code>tidy_cols = list(desired_cols) + list(other_cols)
tidy_df = df[tidy_cols]
</code></pre>
<hr />
<p>I would also say this is generally not a significant action, because it only helps when printing a dataframe, for example. Otherwise all columns as still available by name as you require them.</p>
","0","2","45264","13013"
"49590","<p>In order to be able to create a dictionary from your dataframe, such that the keys are tuples of combinations (according to your example output), my idea would be to use a <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html"" rel=""nofollow noreferrer"">Pandas MultiIndex</a>. This will then generate a dictionary of the form you want.</p>

<p>First I just recreate your example dataframe (would be nice if you provide this code in the future!):</p>

<pre><code>import pandas as pd

# Create the example dataframe
df = pd.DataFrame([""4-Grain Flakes"", ""4-Grain Flakes, Gluten Free"", ""4-Grain Flakes, Riihikosken Vehnämylly""])
df[""id""] = [11005, 35146, 32570]
df[""energy""] = [1404, 1569, 1443]
df[""fibre""] = [11.5, 6.1, 11.2]
df.columns = [""name""] + list(df.columns[1:])

print(df)
                                     name     id  energy  fibre
0                          4-Grain Flakes  11005    1404   11.5
1             4-Grain Flakes, Gluten Free  35146    1569    6.1
2  4-Grain Flakes, Riihikosken Vehnämylly  32570    1443   11.2
</code></pre>

<p>Now we can create the combinations of each value in ""name"" with each of the other column names. I will use lists, within a list comprehension, where I bundle up the values together into tuples. We end with a list of tuples:</p>

<pre><code>names = df.name.tolist()
others = list(df.columns)
others.remove(""name"")         # We don't want ""name"" to be included

index_tuples = [(name, other) for name in names for other in others]
</code></pre>

<p>We can create the MultiIndex from this list of tuples as follows:</p>

<pre><code>multi_ix = pd.MultiIndex.from_tuples(index_tuples)
</code></pre>

<p>Now we can create a new dataframe using out <code>multi_ix</code>. To populate this dataframe, notice that we simple need to <strong>row-wise</strong> values from columns <code>[""id"", ""energy"", ""fibre""]</code>. We can do this easily by extracting as an <code>n * 3</code> NumPy array (using the <code>values</code> attribute of the dataframe) and then flattening the matrix, using <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html"" rel=""nofollow noreferrer"">NumPy's ravel method</a>:</p>

<pre><code>df1 = pd.DataFrame(df[others].values.ravel(), index=multi_ix, columns=[""data""])

print(df1)

                                                  data
4-Grain Flakes                         id      11005.0
                                       energy   1404.0
                                       fibre      11.5
4-Grain Flakes, Gluten Free            id      35146.0
                                       energy   1569.0
                                       fibre       6.1
4-Grain Flakes, Riihikosken Vehnämylly id      32570.0
                                       energy   1443.0
                                       fibre      11.2
</code></pre>

<p>Now we can simply use to <code>to_dict()</code> method of the datframe to create the dictionary you are looking for:</p>

<pre><code>nutritionValues = df1.to_dict()[""data""]

print(nutritionValues)

{('4-Grain Flakes', 'energy'): 1404.0,
 ('4-Grain Flakes', 'fibre'): 11.5,
 ('4-Grain Flakes', 'id'): 11005.0,
 ('4-Grain Flakes, Gluten Free', 'energy'): 1569.0,
 ('4-Grain Flakes, Gluten Free', 'fibre'): 6.1,
 ('4-Grain Flakes, Gluten Free', 'id'): 35146.0,
 ('4-Grain Flakes, Riihikosken Vehnämylly', 'energy'): 1443.0,
 ('4-Grain Flakes, Riihikosken Vehnämylly', 'fibre'): 11.2,
 ('4-Grain Flakes, Riihikosken Vehnämylly', 'id'): 32570.0}
</code></pre>

<p>It is also possible to get your final example of a <em>multidict</em>, directly from the multi-indexed dataframe. You need to just use <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#advanced-indexing-with-hierarchical-index"" rel=""nofollow noreferrer"">multi-index slicing</a>:</p>

<pre><code>fibre_df = final_df.loc[(slice(None), [""fibre""]), :]
print(fibre_df)

                                                 0
4-Grain Flakes                         fibre  11.5
4-Grain Flakes, Gluten Free            fibre   6.1
4-Grain Flakes, Riihikosken Vehnämylly fibre  11.2
</code></pre>

<p>You can then generate a dictionary as before:</p>

<pre><code>d = final_df.loc[(slice(None), [""fibre""]), :].to_dict()[0]
print(d)

{('4-Grain Flakes', 'fibre'): 11.5,
 ('4-Grain Flakes, Gluten Free', 'fibre'): 6.1,
 ('4-Grain Flakes, Riihikosken Vehnämylly', 'fibre'): 11.2}
</code></pre>

<p>And you can drop the <code>""fibre""</code> value from the tuple-keys with a simple dictionary comprehension:</p>

<pre><code>final_dict = {k[0]: v for k, v in d.items()}
print(final_dict)

{'4-Grain Flakes': 11.5,
 '4-Grain Flakes, Gluten Free': 6.1,
 '4-Grain Flakes, Riihikosken Vehnämylly': 11.2}
</code></pre>
","1","2","45264","13013"
"49642","<p>You can actually do the string-spitting and indexing on the columns themselves - no need to extract the column and do list comprehensions.</p>

<p>Below I take whatever is before the first comma and put it in a column called <code>food_group</code> and then the first field after the same column and put it in a new column called <code>sub_cat</code>-egory:</p>

<pre><code>df[""food_group""] = df.name.str.split("","").str[0]
df[""sub_cat""] = df.name.str.split("","").str[1]
</code></pre>

<p>Here is example output for some Yogurt data:</p>

<pre><code>    id                                               name      food_group     sub_cat

44  4082                    Yoghurt, Plain, Organic, 3% Fat    Yoghurt        Plain
45  4083  Yoghurt, Plain, Pirkka Reducol, 2.5% Fat, Low-...    Yoghurt        Plain
46  4084                    Yoghurt, Turkish/Greek, 10% Fat    Yoghurt        Turkish/Greek
47  4085      Yoghurt, Turkish/Greek, 10% Fat, Lactose-Free    Yoghurt        Turkish/Greek
48  4086                                      Yoghurt Sauce    Yoghurt Sauce  NaN
</code></pre>

<p>Notice that any fields that are empty are filled with <code>NaN</code>. This will happen, when your <code>name</code> column only contains a single field (i.e. no commas).</p>

<h3>EDIT</h3>

<p>Here is the top of my dataframe, after the operation above:</p>

<pre><code>In [13]: df.head(10)                                                                                                                                                   
Out[13]: 
   id                                    name       food_group                  sub_cat
0   0                          4-Grain Flakes   4-Grain Flakes                      NaN
1   1             4-Grain Flakes, Gluten Free   4-Grain Flakes              Gluten Free
2   2  4-Grain Flakes, Riihikosken Vehnämylly   4-Grain Flakes   Riihikosken Vehnämylly
3   3                                  Almond           Almond                      NaN
4   4          Almond Drink, Sweetened, Alrpo     Almond Drink                Sweetened
5   5        Almond Drink, Unsweetened, Alrpo     Almond Drink              Unsweetened
6   6                         Amaranth Flakes  Amaranth Flakes                      NaN
7   7                                 Anchovy          Anchovy                      NaN
8   8               Apple, Average, With Skin            Apple                  Average
9   9           Apple, Domestic, Without Skin            Apple                 Domestic
</code></pre>

<h3>EDIT</h3>

<p>In order to replace a row with another string, given a desired string is in that row, you can perform the following:</p>

<pre><code>for keyword in keywords:
    df[""new_col""] = df.name.apply(lambda x: keyword if keyword in x else x)
</code></pre>

<p>where <code>keywords</code> could be a list like this:</p>

<pre><code>keywords = [""Yogurt"", ""chicken"", ""Drink""]
</code></pre>

<p>It still requires manually defining a list of keywords andlooping over them. You could also make this insensitive to the case of the word, but doing everything in e.g. lower-case:</p>

<pre><code>lower_keywords = [""yogurt"", ""chicken"", ""drink""]

for keyword in lower_keywords:
    df[""new_col""] = df.name.apply(lambda x: keyword if keyword in x.tolower() else x)
</code></pre>

<hr>

<p>You could continue to make a <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html"" rel=""nofollow noreferrer"">multi-index</a> from these two new columns, but is might not be necessary - it depends on what you want to do afterwards with the data.</p>
","3","2","45264","13013"
"49647","<p>Does the layer contain two matrices, one for the actual weights and one for the <strong>biases</strong>?</p>

<p>There could be one bias value for each of the columns in your weight matrix, depending on how you built your model.</p>
","1","2","45264","13013"
"50937","<p>There are a couple of problems and things you might want to add to your existing script.</p>

<p>Below I separate your example data into two NumPy arrays:</p>

<ul>
<li>input values <code>x</code></li>
<li>labels <code>y</code></li>
</ul>

<p>It is also important to make sure they are of type <code>float32</code>, because Tensorflow will complain if you pass it integers (as they otherwise would be interpreted).</p>

<p>The following works for me, the model trains to completion:</p>

<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras import layers

syslog_data = [
    [302014, 0, 0, 63878, 30, 3, 1],
    [302014, 0, 0, 3891, 0, 0, 0],
    [302014, 0, 0, 15928, 0, 0, 2],
    [305013, 5, 0, 123, 99999, 0, 3],
    [302014, 0, 0, 5185, 0, 0, 0],
    [305013, 5, 0, 123, 99999, 0, 3],
    [302014, 0, 0, 56085, 0, 0, 0],
    [110002, 4, 2, 50074, 99999, 0, 4],
]

print(tf.VERSION)
print(tf.keras.__version__)

x = np.array([arr[:-1] for arr in syslog_data], dtype=np.float32)
y = np.array([arr[-1:] for arr in syslog_data], dtype=np.float32)

model = tf.keras.Sequential()
# Adds a densely-connected layer with 64 units to the model:
model.add(layers.Dense(64, activation=""relu""))
# Add another:
model.add(layers.Dense(64, activation=""relu""))
# Add a softmax layer with 10 output units:
model.add(layers.Dense(10, activation=""softmax""))

model.compile(optimizer=tf.train.AdamOptimizer(0.001), loss=""categorical_crossentropy"", metrics=[""accuracy""])

model.fit(x, y, epochs=10, steps_per_epoch=30)
</code></pre>
","2","2","45264","13013"
"50951","<p>Theoretically, it is possible to find a global minimum using gradient descent.</p>

<p>In reality, however, it rarely happens - it is also pretty much impossible to prove you have the global minimum!</p>

<p>Imagine we have a 2d loss surface; a loss curve as in the figures below. In order to reach the global minimum (the lowest point on the curve), you would need to make steady progress towards the minimum and also reduce the size of the steps that you take on the way, as to not overshoot the minimum (left figure). If you slow down too fast, you never quite reach the minimum (middle figure). If you get it just right, you make it to the global minimum (right figure). The size of those jumps is controlled by the <em>learning rate</em>, which is multiplied with the loss in each iteration of back-propagation.</p>

<p><a href=""https://i.stack.imgur.com/VxQgz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VxQgz.png"" alt=""gradient descent variants""></a></p>

<p>For this theoretical achievement, the theory requires the loss curve to be <em>convex</em> - meaning it has a shape similar to those in the figures. Here is a recent paper that explores this in more detail. The title says it all: <a href=""https://arxiv.org/abs/1811.03804"" rel=""nofollow noreferrer""><strong>Gradient Descent Finds Global Minima of
Deep Neural Networks</strong></a></p>

<p>There are methods to increase your chances of success, usually involving using a dynamic learning rate, which is adjusted to the rate at which the model learns i.e. how well it improves on a validation data set whilst learning from the training set. As the figure on the right above shows, it is desirable to reduce the learning rate as the loss curve flattens out.</p>

<p>In reality, the <em>""loss surface""</em> on which we are searching for are not 2-dimensional, but have much higher orders e.g. 200. Things such as random initialisation of your model could make the difference between which one you finally land in.</p>

<p>This is perhaps why using <a href=""http://cs231n.github.io/neural-networks-3/#ensemble"" rel=""nofollow noreferrer""><strong>ensemble methods</strong></a> (groups of models) generally performs better than a single model. It explores the loss landscape more thoroughly and increases the odds of finding better minima.</p>

<hr>

<h3>Local Minima</h3>

<p>If a model seems to covnerge (verified by e.g. loss on the validation set stops improving), but the results on the test set appear to be far from optimal or what is should be possible, then it could be the case the the algorithm has ended up in a <em>local minumum</em>, unable to escape.</p>

<p>You can imagine that the updates to the parameters became so small that it is no longer ""active"" or energetic enough to jump out of the small minimum. Common approaches to get around this situation (or to prevent it) include:</p>

<ul>
<li>using an update rule for weights that carries some momentum (for example <a href=""https://arxiv.org/abs/1412.6980"" rel=""nofollow noreferrer"">the Adam optimizer</a>)</li>
<li>using <a href=""https://en.wikipedia.org/wiki/Simulated_annealing"" rel=""nofollow noreferrer""><strong>simulated annealing</strong></a>, which is a fancy way of saying that we perform larger jumps to different areas on the loss curve, exploring more of it and not limiting ourselves to local minima. Here is a small graphic (from Wikipedia) that shows it in action:</li>
</ul>

<p><a href=""https://i.stack.imgur.com/ah3z1.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ah3z1.gif"" alt=""Simulated annealing""></a></p>

<p>Here is <a href=""https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/"" rel=""nofollow noreferrer""><strong>a superb blog post</strong></a> about these optimisers and simulated annealing as another way to ""jump"" out of local minima.</p>

<hr>
","5","2","45264","13013"
"50983","<p>If I understand correctly, you are trying to understand how many of the previous pairs affect the next pair (<span class=""math-container"">$x_{25}, v_{25}$</span>). Given that you are tlaking about physical properties (position and momentum), I think it will always be the case that the most recent timesteps are most relevant, but you can verify this statistically.</p>

<h3>Terminology:</h3>

<p>You have a time-series dataset, with 24 ""lags"" and want to predict ahead with a horizon of 1. This equates to an <em>edogenous auto-regressive model</em> - a model that uses past values of itself to predict the future. (exogenous models could in your model include other variables, such as air humidity, viscosity surrounding medium, wind etc.)</p>

<h2><a href=""https://en.wikipedia.org/wiki/Partial_autocorrelation_function"" rel=""nofollow noreferrer""><strong>Partial Autocorrelation</strong></a></h2>

<p>Given this context, you can try running a Partial Autocorrelation Function on your data. It will tell you how well correlated each points it, pair-wise, with each of the preceding 24 lags. Doing so in Python, using the <a href=""https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html"" rel=""nofollow noreferrer"">statsmodels</a> package will give you a nice plot like this:</p>

<p><a href=""https://i.stack.imgur.com/8Am7V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Am7V.png"" alt=""PACF output""></a></p>

<p>Each vertical line tells you how correlated that particular lag is with the next value in the series (<span class=""math-container"">$x_{25}, v_{25}$</span>).</p>

<p>Have a look at <a href=""https://stats.stackexchange.com/questions/134487/analyse-acf-and-pacf-plots""><strong>this thread on Cross Validated</strong></a> for more detail and some code snippets.</p>

<hr>

<h2><strong>Models</strong></h2>

<p>There are other iterative model fitting techniques that can give you a final model, which can be interpreted and allow you to see which input variables (i.e. which of your 24 past data points were most important in fitting the model). you can have a look at component-wise gradient boosting. <a href=""https://epub.ub.uni-muenchen.de/12754/1/tr120.pdf"" rel=""nofollow noreferrer"">Here is a tutorial with some theory and examples.</a>. It is only available in R as far as I know, but it isn't very difficult to get working, using the example code in that tutorial.
It essentially only updates the coefficient (seen as ""importance"") of a single lags at a time. So after fully training the model, each coefficient's magnitude encodes it's relevance in predicting the next value - that is exactly what you are searching for.</p>
","0","2","45264","13013"
"50990","<p>I suppose it is possible that not all samples are selected during training, depending on the parameters you specify (or that are available in the implementation).</p>

<p>Looking at <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""nofollow noreferrer""><strong>Scikit-Learn's RandomForestClassifier documentation</strong></a>, we can see that there is a <code>bootstrap</code> argument that can be set to <code>False</code> to ensure all data points are used to fit each of the trees. Otherwise, say you pick some arguments to all be 1 (<code>num_estimators</code>, <code>max_depth</code>, <code>min_samples</code>), then not much data would be used at all! Looking through the <a href=""https://github.com/scikit-learn/scikit-learn/blob/7b136e9/sklearn/ensemble/forest.py#L753"" rel=""nofollow noreferrer"">source code</a>, there doens't seem to be a check that all data was used.</p>

<p>Another classifier, <a href=""https://en.wikipedia.org/wiki/Random_forest#ExtraTrees"" rel=""nofollow noreferrer"">ExtraTrees</a> (<em>Extremely Randomised Trees</em>) is generally designed to used all samples to train each estimator. However the SciKit learn implementation does allow you to disable that and use random bootstrapping, as is default with the other random forest algorithms.</p>

<p><strong>So to answer your question; it seems the unused samples are simply left out!</strong></p>
","3","2","45264","13013"
"51060","<p>Have a look into <a href=""https://en.wikipedia.org/wiki/K-d_tree"" rel=""nofollow noreferrer""><strong>KD-Trees</strong></a>. They work by partitioning your space into discrete blocks, via a (wait for it...) tree. Here is the wikipedia example for a 3d space:</p>

<p><a href=""https://i.stack.imgur.com/9FvDB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9FvDB.png"" alt=""3d kdtree""></a></p>

<p>Each of the cuboids is represented by a leaf or node in the tree. It works via binary splits (splitting the space into two via a criterion that you could choose yourself). Here is a <a href=""https://www.youtube.com/watch?v=TLxWtXEbtFE"" rel=""nofollow noreferrer""><strong>short intro to the algorithm</strong></a>.</p>

<p>There is an <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html"" rel=""nofollow noreferrer""><strong>implementation in Scikit-Learn</strong></a>. You build a tree and then you can query the data for a given point and radius, returning all points within that radius/distance.</p>

<p>This has the benefit that you could also test your algorithms over a group of subsets by querying a selection of coordinates - and you only need to construct the tree once.</p>

<hr>

<h2>Small demo</h2>

<p>I will generate some random points random points on a 2d integer grid, so values between 0 and 10. Some points on the grid could therefore be empty.</p>

<p>I put them into a Pandas Dataframe just to make things a little easier to manage and plot:</p>

<pre><code>In [1]: import numpy as np                                                      

In [2]: import pandas as pd                                                     

In [3]: from sklearn.neighbors import KDTree         # could use cKDTree                           

In [4]: import matplotlib.pyplot as plt                                         

In [5]: data = {""x"": np.random.randint(0, 11, 200), ""y"": np.random.randint(0, 10, 200)}    # 200 random coords                                    

In [6]: df = pd.DataFrame(data)                                                 

In [7]: df.head()                                                               
Out[7]: 
   x  y
0  1  7
1  9  8
2  6  1
3  3  2
4  4  3
</code></pre>

<p>Now I create the KDTree and query the coordinate <code>(5, 5)</code> in the centre - I want all points back that lie within a radius distance of 3:</p>

<pre><code>In [8]: tree = KDTree(df.values)

In [9]: ix = tree.query_radius([(5, 5)], r=3)[0] 
</code></pre>

<p>Now I use the returned indices (<code>ix</code>) to filter out those points I want - and just plot them here in red, along with the original complete dataset in blue:</p>

<pre><code>In [10]: ax = df.plot.scatter(""x"", ""y"", c=""b"", alpha=0.5);   # blue base points

In [11]: df.iloc[ix].plot.scatter(""x"", ""y"", c=""r"", ax=ax);   # query results

In [12]: plt.scatter(5, 5, c=""g"");    # query point

In [11]: plt.show()
</code></pre>

<ul>
<li>The queried point is green (5, 5)</li>
<li>Original points are blue</li>
<li>Query results are red</li>
</ul>

<p><a href=""https://i.stack.imgur.com/58wBt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/58wBt.png"" alt=""KDTree query radius results""></a></p>

<h2>Faster please!</h2>

<p>There is also the <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.cKDTree.query.html"" rel=""nofollow noreferrer""><strong>cKDTree class</strong></a>, which works very similarly, but is implemented in C and so should be faster in many cases. Have a <a href=""https://stackoverflow.com/questions/6931209/difference-between-scipy-spatial-kdtree-and-scipy-spatial-ckdtree""><strong>look here</strong></a> at some differences. the cKDTree is the best choice if you want to do simple querying like in my example!</p>
","1","2","45264","13013"
"51267","<p>How to use stateful LSTMs is quite well documented in the <a href=""https://keras.io/examples/lstm_stateful/"" rel=""nofollow noreferrer""><strong>official Keras documentation</strong></a>. There is als a nice <a href=""http://philipperemy.github.io/keras-stateful-lstm/"" rel=""nofollow noreferrer""><strong>blog post here</strong></a>. </p>

<p>There is a note in the Recurrent Layers section:</p>

<blockquote>
  <p>Note on using statefulness in RNNs</p>
  
  <p>You can set RNN layers to be 'stateful', which means that the states
  computed for the samples in one batch will be reused as initial states
  for the samples in the next batch. This assumes a one-to-one mapping
  between samples in different successive batches.</p>
  
  <p>To enable statefulness: - specify <code>stateful=True</code> in the layer
  constructor. - specify a fixed batch size for your model, by passing
  if sequential model: <code>batch_input_shape=(...)</code> to the first layer in
  your model. else for functional model with 1 or more Input layers:
  <code>batch_shape=(...)</code> to all the first layers in your model. This is the
  expected shape of your inputs including the batch size. It should be a
  tuple of integers, e.g. <code>(32, 10, 100)</code>. - specify <code>shuffle=False</code> when
  calling fit().</p>
  
  <p>To reset the states of your model, call <code>.reset_states()</code> on either a
  specific layer, or on your entire model.</p>
</blockquote>

<h3>Small <em>gotcha</em></h3>

<p>The small snag that you have to fix yourself is to make sure your number of samples is an integer multiple of yur batch size: <code>num_samples % batch_size == 0</code>. You need to ensure that maanually because Keras will just take remaining samples for the final batch and this is usually smaller than you desired batch size.</p>
","0","2","45264","13013"
"51384","<p>In <a href=""https://rads.stackoverflow.com/amzn/click/com/1491962291"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">Auriel Geron's book</a>, there is a short description of the approach:</p>
<blockquote>
<p>you   could compute a hash    of  each    instance’s  identifier, keep    only    the last    byte    of  the hash,   and put
the   instance    in  the test    set if  this    value   is  lower   or  equal   to  51  (~20%   of  256).   This    ensures
that  the test    set will    remain  consistent  across  multiple    runs,   even    if  you refresh the dataset.
The   new test    set will    contain 20% of  the new instances,  but it  will    not contain any instance
that  was previously  in  the training    set.</p>
</blockquote>
<p>While a full explanation of what exactly happens and why is probably best placed on StackOverflow, I can try to answer your questions, first with some background info.</p>
<p>The method uses a <a href=""https://en.wikipedia.org/wiki/Cyclic_redundancy_check"" rel=""nofollow noreferrer""><strong>cyclic redundancy check</strong></a>, which is a method of checking that the raw blocks of memory have not been damaged/changed. It is a way to ensure data integrity, e.g. in network traffic - checking if a message way altered between being sent and received.</p>
<p>For train/test splits, it is checking the unique identifier of each sample. We have a column that gives each sample an ID - this should <strong>never</strong> be changed! Don't delete rows, only append to the end with new unique IDs.</p>
<p>In this part: <code>test_ratio * 2**32</code>, the part <span class=""math-container"">$2^{32}$</span> represents the largest integer of a 32-bit system.</p>
<blockquote>
<p>0xFFFFFFFF is a large number; it's the hexadecimal representation of <span class=""math-container"">$2^{32}-1$</span></p>
</blockquote>
<p>To answer your questions:</p>
<blockquote>
<ol>
<li>What is the 3rd line doing:</li>
</ol>
<p><code>crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32</code></p>
</blockquote>
<p>Based on the information I gave above, we see the <a href=""https://docs.python.org/3/library/binascii.html#binascii.crc32"" rel=""nofollow noreferrer""><code>crc32</code> function</a> finds the checksum value in memory (of the unique identifier). If we know the unique ID has never changed, then We ensure that <code>crc32(np.int64(identifier)) &amp; 0xffffffff</code> will always return exactly the same numeric value, across all Python versions and platforms.</p>
<p>Imagine we give IDs in the range 0-80 for train, and 81-100 for test. No we want to make sure a sample'd s ID falls in the first bucket. We check its ID is simple less that 81, right? Well the numeric value we made above is checked to be less than our <code>test_ratio * 2**32</code>, where <code>2**32</code> is the largest 32-bit number. It checks that the sample's ID is within the range of train data, not in the test bucket:: <code>&gt; test_ratio * 2**32</code>.</p>
<blockquote>
<ol start=""2"">
<li>What is the anonymous function doing in the 2nd to last line?</li>
</ol>
<p><code>lambda id_: test_set_check(id_, test_ratio)</code></p>
</blockquote>
<p>This simply applies our <code>test_set_check</code> function to each sample's unique identifiers. Using the <code>apply</code> methd on a Pandas Series object (here it is one column of a Pandas DataFrame).</p>
<blockquote>
<ol start=""3"">
<li>In practice, do you commonly split datasets by ID in this manner?</li>
</ol>
</blockquote>
<p>Not really... Scikit-Learn's <code>train_test_split</code> is often good enough. I think there are many other ways to remove bias and errors from your models before worrying too much about the impact of random splits.</p>
<p>For example, the <strong>snoop bias</strong>, whereby you analyse the entire dataset yourself before deciding on a model architecture/pipeline, thereby incorporating knowledge of the entuire distribution, which is inherently biasing our model.</p>
<p>There is also bias in overfitting e.g. in sequential imaging data (think frames of <em>videos</em>) such that the background is consistent, even though the objects you might want to detect are not. Your model will learn what to expect based on the background, which is not robust! Here you might look into using a geographical split (not random at all).</p>
<hr />
<p>On a side note, there is also a slightly robuster way of setting random seeds in Python (instead of using NumPy's random seed generator). Have a <a href=""https://stackoverflow.com/questions/7029993/differences-between-numpy-random-and-random-random-in-python""><strong>look here for some differences</strong></a>.</p>
<hr />
Helpful resources:
<ol>
<li><a href=""https://stackoverflow.com/questions/36819849/detect-int32-overflow-using-0xffffffff-masking-in-python"">https://stackoverflow.com/questions/36819849/detect-int32-overflow-using-0xffffffff-masking-in-python</a></li>
<li><a href=""https://pynative.com/python-random-module/"" rel=""nofollow noreferrer"">https://pynative.com/python-random-module/</a></li>
<li><a href=""https://stackoverflow.com/questions/30092226/how-to-calculate-crc32-with-python-to-match-online-results"">https://stackoverflow.com/questions/30092226/how-to-calculate-crc32-with-python-to-match-online-results</a></li>
<li><a href=""https://stackoverflow.com/questions/49331030/bitwise-xor-0xffffffff/49332291#49332291"">https://stackoverflow.com/questions/49331030/bitwise-xor-0xffffffff/49332291#49332291</a></li>
</ol>
","7","2","45264","13013"
"51390","<p>Given you have images stretched out as columns in a table with ~48,500 rows, I am assuming you have the raw images that are 220x220 in dimension. </p>

<p>You can use a function available via OpenCV called <code>inpaint</code>, which will restore missing pixel values (for example black pixels of degraded photos).</p>

<p>Here is an image example. Top-left shows the image with missing values (in black). Top-right shows just the missing values (the mask). Bottom-left and bottom-right are the final output, comparing two different algorithms for filling the images.</p>

<p><a href=""https://i.stack.imgur.com/ZmNrG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZmNrG.png"" alt=""restored image""></a></p>

<p>I would suggest trying both methods on your images to see what looks best.</p>

<p>Have a <a href=""https://docs.opencv.org/trunk/df/d3d/tutorial_py_inpainting.html"" rel=""nofollow noreferrer""><strong>look at the Documentation for more details</strong></a> on the algorithms themselves. Here is the <a href=""https://docs.opencv.org/2.4/modules/photo/doc/inpainting.html"" rel=""nofollow noreferrer""><strong>documentation of the actual function</strong></a>.</p>

<p>As for code, it will look something like this:</p>

<pre><code>import opencv as cv    # you will need to install OpenCV

dst = cv.inpaint(img, mask, 3, cv.INPAINT_TELEA)
</code></pre>

<ul>
<li>the first argument is your image with missing values</li>
<li>the second is the mask, with locations of where missing pixels are, i.e. which pixels should be filled/interpolated.</li>
<li>third is the radius around missing pixels to fill</li>
<li>fourth is the flag for the algorithm to use (see link above for two alternatives)</li>
</ul>

<p>For each image, you can generate the mask with something like this:</p>

<pre><code>mask = image[image == np.nan]
</code></pre>
","9","2","45264","13013"
"51398","<p>I personally think that the general idea of optimising your model with different random seeds is not a good idea. There are many other, more important, aspects of the modelling process that you can worry about, tweak and compare before spending time on the effects of random initialisation.</p>

<p>That being said, if you just want to test the effect of <em>random initialisation</em> of model weights on a final validation metric, this could be an approach to do so. Kind of the reverse argument to my point above. If you can show for different random seeds (<em>ceteris paribus</em>: with all other parameters equal) that the final model performs differently, it shows maybe that their is either inconsistency in the model, or a bug in the code even. I would not expect a well-validated model to give hugely differing results if being run with a different random seed, so if it does, it tells me something weird is going on!</p>
","9","2","45264","13013"
"51405","<p>I'm not sure it is a good problem setting for input to a feed-forward network (especially such a small network with only one activation over 6 neurons).</p>

<p>What is there even to predict here if you are just using the day of the week and the month in the year? You could simply take the average for each of the pairwise combinations and use that.</p>

<hr>

<p>If you want to adjust for local trends over your dataset's timespan, then perhaps look int temporal decomposition. For example, you can use <a href=""http://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html"" rel=""nofollow noreferrer""><strong>the <code>seasonal_decompose</code> method available in the statsmodels</strong></a> package for Python.</p>

<p>What this does is to find:</p>

<ol>
<li>a global trend over the range of your data</li>
<li>parts of your data that vary on a repeating pattern (<em>seasonally</em>), </li>
<li>any remaining noise, or residuals not fitting parts 1. and 2.</li>
</ol>

<p>It works by applying convolutions over the data to find a repeating frequency. You can think of it like finding correlations in time, compared to the ConvNets, which look for correlations over pixels in an image, for example.</p>

<p>Here is an example, using a dataset taken of total number of international airline passengers over time. I am using the statsmodels package to load this well-known R dataset as a <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html"" rel=""nofollow noreferrer""><strong>Pandas DataFrame</strong></a>:</p>

<pre><code>In [1]: import statsmodels.api as sm                                                 

In [2]: import matplotlib.pyplot as plt                                              

In [3]: ap = sm.datasets.get_rdataset(""AirPassengers"")       # ap = AirPassengers
</code></pre>

<p>The actual dataframe is stored on the <code>data</code> attribute of <code>ap</code> and has two columns:</p>

<pre><code>In [4]: ap.data.columns                                                              
Out[4]: Index(['time', 'value'], dtype='object')
</code></pre>

<p>We can see how the data looks, where there is clearly an upward trend as well as a repeating seasonality (more people fly over the summer time):                                       </p>

<pre><code>In [5]: ap.data.plot(""time"", ""value"");                                            

In [6]: plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/a8F2i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a8F2i.png"" alt=""enter image description here""></a></p>

<hr>

<p>Now we can perform the seasonal decomposition of the data (<code>freq=12</code>, because the data is monthly):</p>

<p>In [7]: decomp = sm.tsa.seasonal_decompose(ap.data.value.values, freq=12)           </p>

<p>This <code>decomp</code> result has a few attributes, but most helpful is the plot... We can see the nicely separated seasonlity and the trend, along with some noise</p>

<p>In [8]: decomp.plot();</p>

<p>In [9]: plt.show()   </p>

<p><a href=""https://i.stack.imgur.com/De98S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/De98S.png"" alt=""data decomposition""></a></p>

<p>We see it isn't perfect, as the seasonality is best fitted to the middle section, not the start or end phases - the residuals are largest at the beginning and end.</p>

<p>You can access each of the values: <code>observed</code>, <code>trend</code>, <code>seasonal</code>, <code>resid</code> and <code>nobs</code> on the <code>decmp</code> object.</p>

<p>Now if this analysis helps you discover seasonality, you might want to look into Seasnal ARIMA models (also available in the statsmodels package).</p>

<p>Here is <a href=""https://www.machinelearningplus.com/time-series/time-series-analysis-python/"" rel=""nofollow noreferrer""><strong>a blog post</strong></a> I just found that looks pretty detailed and would help you on this path.</p>

<hr>

<p>If you are doing this excercise to learn about Keras and deep learning, it might be worth looking into LSTM models that are better for time-series analysis. You will also need to re-think how you feed data into the model for training, not to mix up the temporal information or create biases (letting the model cheat).</p>
","1","2","45264","13013"
"51433","<p>You can actually compute how much memory it will take to hold 57,000 images in memory (it is a lot!). You are also holding them twice: once in the <code>features</code> list and then trying again in <code>npfeatures</code>. This second part will make a full copy of the entire <code>features</code> list. Hence why it runs out of memory there.</p>

<p>Here are some starting steps that should help you understand the limitations of your approach and perhaps get a working method:</p>

<h3>1. You don't need this line: <code>x = np.expand_dims(x, axis=0)</code>:</h3>

<p>They do that in the Keras documentation, because you need a single image to have an additional dimension for the batch size. You do not need that because you append many images to a list (<code>features</code>), and the length of that list is the same thing, the batch size (the number of images).</p>

<h3>2. Do your own scaling</h3>

<p>In the case f VGG16 (and 19 I believe), the <code>preprocess_input</code> method simply scales pixel values between -1 and +1. You can probably do this a little more light-weight yourself. You can do the scaling on a numpy array like this:</p>

<pre><code>x = (x / 127.5 - 1)
</code></pre>

<p>... and now remove the line with <code>preprocess_input</code></p>

<h3>3. Break it down into smaller pieces:</h3>

<p>You can then either feed them directly into you models (if that is possible), or save the <code>npfeatures</code> to disk, one chunk at a time.</p>

<h3>4. Try seeing how much memory your machine has while running this script:</h3>

<p>Add a simple line after <code>features.append(x)</code>, like this:</p>

<pre><code>print(""Loaded {} images""format(len(features)))
</code></pre>

<p>If you are running this on a Linux machine or Mac OSX, try using a tool like <a href=""https://www.tecmint.com/install-htop-linux-process-monitoring-for-rhel-centos-fedora/"" rel=""nofollow noreferrer"">htop</a> in terminal. There is also system monitor... same for Windows. You should be able to see the memory consumption grow until the point your script crashes. Now you know how many images you can do in one cycle - the last printed number before the crash.</p>
","0","2","45264","13013"
"51510","<p>When you use matplotlib's plot function, it holds an object behind the scenes for you. You can change this object with more calls to <code>plt</code> and then only once everything has been done should you <code>plt.show()</code> the graph.</p>

<p>Here is a simply example that does what you want:</p>

<pre><code>In [1]: import matplotlib.pyplot as plt
In [2]: import numpy as np                                                           

In [3]: x = np.linspace(0, 10, 100)                                                  
In [4]: y = x ** 2                                   
</code></pre>

<p>The following lines change the plot object, adding the axes labels - but we don't show it until all are complete...</p>

<pre><code>In [5]: plt.plot(x, y)                                                               
Out[5]: [&lt;matplotlib.lines.Line2D at 0x7f3c896bb9b0&gt;]

In [6]: plt.xlabel(""Time"")                                                           
Out[6]: Text(0.5, 0, 'Time')

In [7]: plt.ylabel(""Speed"")  
Out[7]: Text(0, 0.5, 'Speed')
</code></pre>

<p>Now we are done, so show it:</p>

<pre><code>In [8]: plt.show()                                                                  
</code></pre>

<p><a href=""https://i.stack.imgur.com/7InL6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7InL6.png"" alt=""enter image description here""></a></p>

<p><a href=""https://matplotlib.org/gallery/subplots_axes_and_figures/axes_demo.html#sphx-glr-gallery-subplots-axes-and-figures-axes-demo-py"" rel=""nofollow noreferrer"">Have a look here for a more thorough demo</a>, which also shows the object explicitly and lets you better understand what is going on.</p>
","1","2","45264","13013"
"51513","<p>Under- and over-sampling as a technique have already been mentioned, but I thought I would point to a commonly used variant: </p>

<h3><strong>SMOTE</strong>: Synthetic Minority Over-sampling Technique</h3>

<p>It was presented in <a href=""http://This%20paper%20shows%20that%20a%20combination%20of%20our%20method%20of%20over-sampling%20the%20minority%20(abnormal)%20class%20and%20under-sampling%20the%20majority%20(normal)%20class%20can%20achieve%20better%20classifier%20performance%20(in%20ROC%20space)%20than%20only%20under-sampling%20the%20majority%20class."" rel=""nofollow noreferrer"">this paper in 2002</a>. Here is a snippet from the abstract:</p>

<blockquote>
  <p>This paper shows that a combination of our method of over-sampling the
  minority (abnormal) class and under-sampling the majority (normal)
  class can achieve better classifier performance (in ROC space) than
  only under-sampling the majority class.</p>
</blockquote>

<hr>

<p>You can use it easily in Python, using <a href=""https://github.com/scikit-learn-contrib/imbalanced-learn"" rel=""nofollow noreferrer""><strong>the <code>imbalanced-learn</code> package</strong></a>, which is contained in the <em>contrib</em> module of Scikit-Learn and must be installed separately.</p>

<blockquote>
  <p>imbalanced-learn is a python package offering a number of re-sampling
  techniques commonly used in datasets showing strong between-class
  imbalance.</p>
</blockquote>

<p>That package includes methods for combining over-/under-sampling as well as a set of utilities to generate batches of data that can flow into Keras/Tensorflow.</p>
","0","2","45264","13013"
"51561","<p>I thought it might be because of the different types being used in the columns, but I created an example below, which works fine over mixed column types. The only real different is the size - that is why I think you are probably running out of memory.</p>

<blockquote>
  <h1>Working example</h1>
</blockquote>

<p>I use <code>int</code>, <code>str</code> and <code>datetime</code> objects:</p>

<pre><code>In [1]: import pandas as pd                                                                                                                                                                                                                          

In [2]: import datetime                                                                                                                                                                                                                              

In [3]: df = pd.DataFrame({'Branch': 'A A A A A A A B'.split(),
                           'Buyer': 'Carl Mark Carl Carl Joe Joe Joe Carl'.split(),
                           'Quantity': [1, 3, 5, 1, 8, 1, 9, 3],
                           'Date':[datetime.datetime(2013, 1, 1, 13, 0),
                                   datetime.datetime(2013, 1, 1, 13, 5),
                                   datetime.datetime(2013, 10, 1, 20, 0),
                                   datetime.datetime(2013, 10, 2, 10, 0), 
                                   datetime.datetime(2013, 10, 1, 20, 0),
                                   datetime.datetime(2013, 10, 2, 10, 0),
                                   datetime.datetime(2013, 12, 2, 12, 0),
                                   datetime.datetime(2013, 12, 2, 14, 0)]})                                                                                                                                                                                                                                          

In [4]: df                                                                                                                                                                                                                                           
Out[4]: 
  Branch Buyer  Quantity                Date
0      A  Carl         1 2013-01-01 13:00:00
1      A  Mark         3 2013-01-01 13:05:00
2      A  Carl         5 2013-10-01 20:00:00
3      A  Carl         1 2013-10-02 10:00:00
4      A   Joe         8 2013-10-01 20:00:00
5      A   Joe         1 2013-10-02 10:00:00
6      A   Joe         9 2013-12-02 12:00:00
7      B  Carl         3 2013-12-02 14:00:00


In [5]: df.shape                                                                                                                                                                                                                                    
Out[5]: (8, 4)
</code></pre>

<p>Now I just repeat the dataframe again, but add one hour to each of the datetime values, just to increase the number of groupby combinations to expect:</p>

<pre><code>In [14]: df.iloc[0:8, 3] += datetime.timedelta(hours=1)                                                                                                                                                                                              
</code></pre>

<p>Now perform a groupby over all columns, and sum only on <code>Quantity</code> (it is my only numeric column).</p>

<p>The reuslts are as expected:</p>

<pre><code>In [16]: df.groupby([""Branch"", ""Buyer"", ""Quantity"", ""Date""])[""Quantity""].sum()                                                                                                                                                                       
Out[16]: 
Branch  Buyer  Quantity  Date               
A       Carl   1         2013-01-01 13:00:00    1
                         2013-01-01 14:00:00    1
                         2013-10-02 10:00:00    1
                         2013-10-02 11:00:00    1
               5         2013-10-01 20:00:00    5
                         2013-10-01 21:00:00    5
        Joe    1         2013-10-02 10:00:00    1
                         2013-10-02 11:00:00    1
               8         2013-10-01 20:00:00    8
                         2013-10-01 21:00:00    8
               9         2013-12-02 12:00:00    9
                         2013-12-02 13:00:00    9
        Mark   3         2013-01-01 13:05:00    3
                         2013-01-01 14:05:00    3
B       Carl   3         2013-12-02 14:00:00    3
                         2013-12-02 15:00:00    3
Name: Quantity, dtype: int64
</code></pre>

<blockquote>
  <h1>Break your problem down</h1>
</blockquote>

<p>It might be difficult to break down your problem, because you need to whole data for the groupby operation. You could however save each of the groups to disk, perform the <code>mean()</code> computation on them separately and merge the results yourself. The name of each group is actually the combination of the <code>groupby</code> columns selected. This can be used to build the index of the reuslting dataframe.</p>

<p>It could look something like this:</p>

<pre><code>for name, group in df1.groupby(['date', 'unit', 'company', 'city']):
    print(""Processing groupby combination: "", name)    # This is the current groupby combination
    result = group.mean()
    _df = pd.DataFrame(index=[name], data=[result])
    _df.to_csv(""path/somewhere/"" + name + "".csv
</code></pre>

<p>You will then have a folder full of the results for each group and will have to just read them back in and combine them.</p>

<blockquote>
  <h1>Other methods</h1>
</blockquote>

<p>It is known that Pandas does not handle many operations on huge datasets very efficienty (compared to e.g. the <a href=""https://github.com/h2oai/datatable"" rel=""nofollow noreferrer""><strong><code>data.table</code></strong></a> package). There is the <a href=""https://dask.org/"" rel=""nofollow noreferrer""><strong>Dask</strong></a> package, which essentially does Pandas things in a distributed manner, but that might be overkill (and you'll of course need more resources!)</p>
","1","2","45264","13013"
"51573","<p>Some parts of the code are missing that might contain clues (e.g. you plot method would only plot random noise, as it stands). What might be happening is that you are showing the image in matplotlib in a figure that is much smaller than the image, so matplotlib will automatically scale the image to the pixel space.</p>

<p>The <code>plt.imshow()</code> method can take an argument <code>interpolation</code>, which can produce dramatically different effects. Here are them all compared side-by-side:</p>

<p><a href=""https://i.stack.imgur.com/AKpmk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AKpmk.png"" alt=""interpolation methods""></a></p>

<p>Have a look at <a href=""https://matplotlib.org/gallery/images_contours_and_fields/interpolation_methods.html"" rel=""nofollow noreferrer""><strong>the relevant documentation</strong></a></p>

<hr>

<p>I personally stick to using OpenCV and <code>cv2.imread()</code> to load images. You just have to be aware (in the context of using Tensorflow/Keras) that the channel are the first dimension. i.e. <code>(Channels, Height, Width)</code> - whereas e.g. <code>plt.imread()</code> will return <code>(Height, Width, Channels)</code>.</p>
","1","2","45264","13013"
"51614","<p>It looks fine to me :)  the only problem is that your plot (resulting from <code>In [18]</code>) is being displayed on your computer in a separate window somewhere - maybe you have to find it. Once you close that window, your iPython prompt woill return to <code>In [19]</code>. You could alternatively press <code>Ctrl-C</code> in the iPython session, but this will end the session.</p>

<p>If the problem persists (and you cannot even find/close the plot from <code>plt.show()</code>) - maybe have a <a href=""https://stackoverflow.com/questions/34108696/cannot-manually-close-matplotlib-plot-window"">look at this post</a>, which discusses the various backends used for matplotlib.</p>

<p>I assume you do this to begin with:</p>

<pre><code>import matplotlib.pyplot as plt
</code></pre>

<p><code>plt</code> is a module (as the error message says), so you cannot use it directly like that, but rather the functions that are contained in that module, like <code>scatter()</code> (which worked for you) and then others like <code>plot()</code>, <code>hist()</code> and so on.</p>

<p>Have a look at <a href=""https://matplotlib.org/tutorials/introductory/pyplot.html"" rel=""nofollow noreferrer""><strong>the relevant documentation</strong></a> with lots of exmaples.</p>

<p>Here is a full example:</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np

# evenly sampled time at 200ms intervals
t = np.arange(0., 5., 0.2)

# red dashes, blue squares and green triangles
plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/7of06.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7of06.png"" alt=""enter image description here""></a></p>
","3","2","45264","13013"
"51727","<p>I think the main time-consumer will be the fact that you are iterating over 700,000 rows, one-by-one.</p>

<p>You could perhaps do a few of your checks/comparisons (used in the line with bit-wise stuff: <code>&amp;</code>, <code>|</code> etc) to begin with, <strong>for all</strong> rows. So for that row, it looks like this now:</p>

<pre><code>data.loc[
    (data['Category'] != category) &amp; 
    (pd.Series(list1).isin(data[""List1""]).any()) &amp; 
    (pd.Series(list2).isin(data[""List2""]).any()) |
    (data[""List2""][0] == 'NULL') |
    (list2[0] == 'NULL'))]
</code></pre>

<p>Before the loop, you could pre-compute each of those rows, adding the results for each comparison as a new columns:</p>

<pre><code>data[""check1""] = data['Category'] != category
data[""check2""] = ...
data[""check3""] = ...
data[""check4""] = ...
data[""check5""] = data.List2 == ""NULL""
</code></pre>

<p>Now use those columns' values in your line with all comparisons.</p>

<hr>

<p>To gain a better understanding of what it taking up all the time, you could <a href=""https://docs.python.org/3/library/profile.html"" rel=""nofollow noreferrer""><em>profile</em> the code</a>, e.g. using the <a href=""https://docs.python.org/3/library/profile.html#module-cProfile"" rel=""nofollow noreferrer"">CProfile</a> module. You could just run over the first 100 rows of your data for that purpose - the profiling tool will give summary statistics.</p>

<hr>

<p>If you really want to get stuck into it, check out <a href=""http://pandas-docs.github.io/pandas-docs-travis/user_guide/enhancingperf.html#enhancing-performance"" rel=""nofollow noreferrer""><strong>tha Pandas performance improvement methods</strong></a>, which get into using Numba and Cython. There can be huge gains here, but you need to maybe invest a little more effort. <a href=""https://numba.pydata.org/index.html"" rel=""nofollow noreferrer""><strong>Numba</strong></a> would be the easiest to begin with; unless you are experienced with C-languages.</p>
","1","2","45264","13013"
"52421","<p>If you know what you will decide based on the length of the tuple, then you can probably just hard-code your rules! Did the SQL version not work for you? Or are you just experimenting with decision trees?</p>

<p>You can of course use a decision tree. You data can be split based on the features you provide, so you can look at more than the length of the tuple e.g. look at the values of the <code>DestinationIP</code> and your other variables in the tuple.</p>

<p>It could look something like this:</p>

<pre><code>from sklearn import tree
from sklearn.model_selection import train_test_split
</code></pre>

<p>Create a train-test split from your data, assuming you have some input tuples and the response that you want to predict:</p>

<pre><code>X = your_tuple_data
y = corresponding_responses_to_predict

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test
</code></pre>

<p>Create Decision Tree classifer object</p>

<pre><code>clf = DecisionTreeClassifier()    
# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)
</code></pre>

<p>Predict the response for test dataset</p>

<pre><code>y_pred = clf.predict(X_test)
</code></pre>

<p>Have a look at <a href=""https://scikit-learn.org/stable/modules/tree.html"" rel=""nofollow noreferrer"">the SciKit-Learn documentation</a> for more information. Here is <a href=""https://www.datacamp.com/community/tutorials/decision-tree-classification-python"" rel=""nofollow noreferrer"">a tutorial</a> with more ideas behind why this could work.</p>
","0","2","45264","13013"
"52773","<p>You can simply generate them yourself. They are simply representations of all integers up to <span class=""math-container"">$2^{32}$</span></p>

<p>Which format do you want the address? <a href=""https://en.wikipedia.org/wiki/IPv4#Address_space_exhaustion"" rel=""nofollow noreferrer"">There are many ways to represent them</a> according to your requirements, due to the fact that they are simply representation of a set of normal numbers:</p>

<ul>
<li>standard dot-separated e.g. 192.168.0.1</li>
<li>unsigned int32 : e.g 3548551264</li>
<li>hex, dotted hex, octal bytes</li>
</ul>

<hr>

<p>You could generate them in Python like this:</p>

<pre><code>def generate_ips(): 
    """"""Generate all possible IPv4 addresses, on by one"""""" 
    for i in range(255): 
        for j in range(255):
            for k in range(255):
                for l in range(255):
                    yield ""{0}.{1}.{2}.{3}"".format(i, j, k, l)
</code></pre>

<p>The you must loop on the generator function like this to actuall use the values:</p>

<pre><code>for ip in generate_ips(): 
    print(ip) 

# output
0.0.0.0
0.0.0.1
0.0.0.2
0.0.0.3
0.0.0.4
0.0.0.5
0.0.0.6
0.0.0.7
</code></pre>

<p>I used a generator (with the <code>yield</code> keyword instead of saving all values in a list) because it will just give you one at a time and not consume all those Gb of memory that you mentioned!</p>

<p>If you want to store all values in a list for some reason, you can alter the function to append each IP address to a list, then return that:</p>

<pre><code>def generate_ips():
    """"""Return a list of all possible IPv4 addresses""""""
    all_ips = []
    for i in range(255):
        for j in range(255):
            for k in range(255):
                for l in range(255):
                    ip = ""{0}.{1}.{2}.{3}"".format(i, j, k, l)
                    all_ips.append(ip)
    return all_ips
</code></pre>
","1","2","45264","13013"
"52799","<p>You can see from <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html"" rel=""nofollow noreferrer""><strong>the documentation</strong></a> of the method that you can change the <code>keep</code> argument to be <code>""last""</code>.</p>

<p>In your case, as you only want to consider the values in one of your columns (<code>datestamp</code>), you must specify this in the <code>subset</code> argument. You had tried passing all column names, which is actually the default behaviour. Now we can use this (along with the correct value for the <code>keep</code> argument) to get this:</p>

<p>For example, a dataframe with duplicates:</p>

<pre><code>In [1]: import pandas as pd

In [2]: df = pd.DataFrame({'datestamp': ['A0', 'A0', 'A2', 'A2'],
                           'B': ['B0', 'B1', 'B2', 'B3'],
                           'C': ['B0', 'B1', 'B2', 'B3'],
                           'D': ['D0', 'D1', 'D2', 'D3']}, 
                           index=[0, 1, 2, 3]).T

In [3]: df                                                                      
Out[3]: 
  datestamp   B   C   D
0        A0  B0  B0  D0
1        A0  B1  B1  D1
2        A2  B2  B2  D2
3        A2  B3  B3  D3
</code></pre>

<p>Now we drop duplicates, passing the correct arguments:</p>

<pre><code>In [4]: df.drop_duplicates(subset=""datestamp"", keep=""last"")                     
Out[4]: 
  datestamp   B   C   D
1        A0  B1  B1  D1
3        A2  B3  B3  D3
</code></pre>

<p>By comparing the values across rows 0-to-1 as well as 2-to-3, you can see that only the last values within the <code>datestamp</code> column were kept.</p>
","1","2","45264","13013"
"52836","<p>The first major point here is that you do not have historical water level values. These are the labels that you'd use to teach/train any <em>supervised</em> model. Without labels, you are limited to using <em>unsupervised</em> methods. Have a look at<a href=""https://en.m.wikipedia.org/wiki/Unsupervised_learning?wprov=sfla1"" rel=""nofollow noreferrer""> the Wikipedia overview</a></p>

<p>You could try looking into something like clustering, with k-Means for example, which would be quite fast. Other clustering algorithms like DBSCAN are also very good, but your variables (temperature and pressure) might not necessarily cluster well on their density in the input space (the D in DBSCAN).</p>

<p>There are unsupervised neural network approaches, but these are a bit more involved. Autoencoders might be a good place to start. </p>

<p>Have a <a href=""https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://www.quora.com/Are-there-any-unsupervised-learning-algorithms-for-time-series-data&amp;ved=2ahUKEwjU49622sDiAhUPElAKHW-5AYYQjjgwDHoECAoQAQ&amp;usg=AOvVaw2-vzZ-7pck7aPs7ddIVGR5"" rel=""nofollow noreferrer"">look here for some more ideas</a> on specific models to try out, such as Denoising AutoEncoders, which can use recurrent components (such as LSTMs), which can perform well on time series data. </p>
","0","2","45264","13013"
"53583","<p>I assume you know which distributions are possible once you are inside your function? I would probably just pass a parameter for the distribution name and then a set of parameters in a Python dictionary.</p>

<p>This method also allows for pretty easy testing, because you know that distributions expects only certain parameters, so you can also add small checks to confirm everything is available.</p>

<pre><code>def my_distribution(dist_name, dist_params):
    if dist_name == ""normal"":
        assert (""mean"" in params.keys()) and (""variance"" in params.keys()), ""Missing expected parameters for {} distribution"".format(dist_name)
    # Perform some checks for other distributions as necessary...

    # perform your own steps...
</code></pre>

<p>Now you use it like this:</p>

<pre><code># assume these are provided by your earlier code
distr = ""normal""
params = {""mean"": 5.0, ""variance"": 2.0}

result = my_distribution(dist_name=distr, dist_params=params)
</code></pre>

<h2>Edit:</h2>

<p>Here is an example of a single function that can handle multiple distributions in a single call:</p>

<pre><code>def my_distributions(dist_collection):
    # Perform some checks for other distributions as necessary...
    allowed_dists = [""normal"", ""uniform"", ""dirichlet"", ""rayleigh""]
    assert all(
        dist_name in allowed_dists for dist_name in dist_collection
    ), ""Input contains disallowed distribution""

    # Do something for each distribution name with its parameters:
    for dist_name, dist_params in params.items():
        print(dist_name, dist_params)
</code></pre>

<p>Now the input needs to be specified a little differently:</p>

<pre><code>all_distributions = {
    ""normal"": {""loc"": 1, ""scale"": 2, ""size"": 100},
    ""uniform"": {""low"": 0, ""high"": 1, ""size"": 100},
}
</code></pre>

<p>A single call can now work over many distributions:</p>

<pre><code>my_distributions(all_distributions)
</code></pre>

<hr>

<p>You could of course make classes that hold all configurations and checks, but I would argue you are starting to make things unnecessarily complicated, as Numpy does so much for you already with it's built-in distribution capabilities.</p>
","1","2","45264","13013"
"53929","<p><a href=""https://keras.io/models/sequential/#fit"" rel=""nofollow noreferrer""><strong>The relevant documentation</strong></a> doesn't mention random sampling per se.</p>

<p>NOTE: this all has nothing to do with the <code>Sequential</code> model type versus the <code>Model</code> type. OP was specifically talking about <code>Sequential</code> models.</p>

<p>You can specify the <code>shuffle</code> parameter to get random samples across the training dataset, but there is not a strict/parameterised sampling methodology. Using <code>shuffle=True</code> is however equiavalent to random selection without replacement (smaples can only be sampled once per epoch).</p>

<p>You can look through <a href=""https://github.com/keras-team/keras/blob/master/keras/engine/training.py"" rel=""nofollow noreferrer""><strong>the source code</strong></a> to see how Keras builds up the train function, but it doesn't include any random sampling. This is taken care of deeper in the internals, via e.g. the <code>fit_loop</code> function, which simply <a href=""https://github.com/keras-team/keras/blob/master/keras/engine/training_arrays.py#L180"" rel=""nofollow noreferrer""><strong>shuffles the indices</strong></a> of the training samples:</p>

<pre><code>if shuffle == 'batch':
    index_array = batch_shuffle(index_array, batch_size)
elif shuffle:
    np.random.shuffle(index_array)
</code></pre>

<hr>

<p>You could pass <code>class_weight</code> argument to tell the Keras that some samples should be considered more important when computing the loss (although it doesn't affect the sampling method itself):</p>

<blockquote>
  <p><strong>class_weight</strong>: Optional dictionary mapping class indices (integers) to
  a weight (float) value, used for weighting the loss function (during
  training only). This can be useful to tell the model to ""pay more
  attention"" to samples from an under-represented class.</p>
</blockquote>
","2","2","45264","13013"
"54025","<p>Your input matrices (with <strong>3 rows</strong> and <strong>multiple columns</strong>) are saying that there are <strong>3 samples</strong>, with <strong>multiple attributes</strong>. So the output you will get will be a 3x3 matrix, where each value is the similarity to one other sample (there are <code>3 x 3 = 9</code> such combinations)</p>

<p>If you were to print out the pairwise similarities in sparse format, then it might look closer to what you are after.</p>

<p>I have created two example matrices of random numbers that fits your description:</p>

<pre><code>from sklearn.metrics.pairwise import cosine_similarity
from scipy import sparse

a = np.random.random((3, 10))
b = np.random.random((3, 10))

# Create sparse matrices, which compute faster and give more understandable output
a_sparse, b_sparse = sparse.csr_matrix(a), sparse.csr_matrix(b)

sim_sparse = cosine_similarity(a_sparse, b_sparse, dense_output=False)
print(sim_sparse)
</code></pre>

<p>Output:</p>

<pre><code>  (0, 2)    0.7938732813430508
  (0, 1)    0.7575978172453429
  (0, 0)    0.7897664361147338
  (1, 2)    0.740418315571796
  (1, 1)    0.833981672896221
  (1, 0)    0.7184526671218405
  (2, 2)    0.8746293481677073
  (2, 1)    0.6456666045233884
  (2, 0)    0.7925289217609924
</code></pre>

<p>Hopefully this output makes it clearer, what you are actually getting as output.</p>

<p>Have a look here for a <a href=""https://stackoverflow.com/questions/17627219/whats-the-fastest-way-in-python-to-calculate-cosine-similarity-given-sparse-mat""><strong>few more details</strong></a> on performance aspects, and the <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"" rel=""noreferrer""><strong>documentation on sparse matrices is here.</strong></a></p>
","5","2","45264","13013"
"54455","<p>Do I understand correctly from your test RMSE, that the error is lower as you increase the size of your fraction used in training?</p>

<p>If you do 5-fold cross validation using a fraction of 0.01, that implies here that you are only a total fraction of only 0.05 of your data? So 0.04 for training and 0.01 for testing.</p>

<p>It should not come as a surprise then, that the performance improves with increased amounts of data used for training. Your RMSE values increase with fraction size - I would normally expect such a correlation (assuming your data is fairly coherent).</p>

<h2>EDIT:</h2>

<p>To answer your questions,</p>

<ol>
<li><p>I would argue it is a well-known relationship, even in the simplest of regression problems. <a href=""https://stats.stackexchange.com/questions/37833/minimal-number-of-points-for-a-linear-regression""><strong>Have a read here</strong></a> (including comments on accepted answer) for more discussion on minimum required data points for valid models. One reason highlighted is that very few data points makes any estimate of variance less meaningful</p></li>
<li><p>I don't think there is a name given to this relationship - at least I don't know of one (and neither do people in the link above). I would just describe it as being inherent to problem that the dataset is trying to represent. For example, if we have a high-dimensional complex parameter space, and then only a handful of data-points sampled from that space, it is very unlikely that we can fully describe the original space. The more samples we have, the closer we can approximate it. This is extremely evident in data-intensive modelling techniques, such as image-based tasks (i.e. convolutional neural networks, with millions of parameters.) Other methodologies, such as <a href=""https://github.com/fmfn/BayesianOptimization"" rel=""nofollow noreferrer""><strong>Bayesian optimisation</strong></a> using Gaussian processes. These are computationally more intense, but perhaps offer higher data efficiency.</p></li>
<li><p>There might be some convergence theories, stating how approximations to ground truth reach a threshold given a certain amount of data (that kind of framework would make sense to me) - but alas, I have not seen any formal analysis in this direction. Regarding number of samples, in the meantime, I just follow the mantra: the more the merrier!</p></li>
</ol>
","3","2","45264","13013"
"54572","<p>You are right - the difference is minimal. The base LSTMCell class implements the main functionality required, such as the <a href=""https://github.com/tensorflow/tensorflow/blob/v2.0.0-beta1/tensorflow/python/keras/layers/recurrent.py#L2136"" rel=""nofollow noreferrer""><code>build</code></a> method, whereas the LSTM class only container an entry point: the <a href=""https://github.com/tensorflow/tensorflow/blob/v2.0.0-beta1/tensorflow/python/keras/layers/recurrent.py#L2529"" rel=""nofollow noreferrer""><code>call</code></a> method, as well as a bunch of <em>getters</em> to retrieve attribute values. <a href=""https://github.com/tensorflow/tensorflow/blob/v2.0.0-beta1/tensorflow/python/keras/layers/recurrent.py#L2028"" rel=""nofollow noreferrer""><strong>LSTMCell</strong></a> is the base class, which is used as a <a href=""https://github.com/tensorflow/tensorflow/blob/v2.0.0-beta1/tensorflow/python/keras/layers/recurrent.py#L2500"" rel=""nofollow noreferrer""><strong>cell</strong></a> that is used inside the <a href=""https://github.com/tensorflow/tensorflow/blob/v2.0.0-beta1/tensorflow/python/keras/layers/recurrent.py#L2381"" rel=""nofollow noreferrer""><strong>LSTM</strong></a> class.</p>

<p>All links point to relevant parts of <code>tensorflow.keras</code> source code.</p>

<p>My advice would be to use the standard LSTM class in your model as a normal layer. If you have a GPU at your disposal, you might want to use the version of the layer that is optimised with CUDA for execution on a GPU. As per the documentation:</p>

<blockquote>
  <p>Note that this cell is not optimized for performance on GPU. Please use
    <code>tf.keras.layers.CuDNNLSTM</code> for better performance on GPU.</p>
</blockquote>

<p>There is also a <code>GRU</code> layer as well as a <code>CuDNNGRU</code> layer.</p>

<hr>

<p>If you want to tweak how things work under the hood, you might create a class and inherit from the LSTMCell, or even the base class:</p>

<pre><code>from tensorflow.python.keras.engine.base_layer import Layer

class MyLSTM(Layer):
    pass
</code></pre>

<p>But you would have to implement many things for yourself.</p>
","4","2","45264","13013"
"54604","<p>If you want to shuffle the data in a deterministic way, how about shuffling the dataset beforehand e.g. in a simple list of filenames, then simply reading that list deterministically in a single-processed loop, with <code>shuffle = False</code> in the <code>DataLoader?</code>?</p>

<p>Another things that may cause non-deterministic behaviour is using multiple processes - then there are operations that are passed out and managed by the operating system, which doesn't pay attention to any of the random seeds you set. Performance is dependent on available resources i.e. affected by other activities running on your host machine.</p>

<p>In addition to that, any interaction between CPU and GPU could be causing non-deterministic behaviour, as data transfer is non-deterministic (<a href=""https://devtalk.nvidia.com/default/topic/1043691/general/real-time-data-transfer-between-cpu-and-gpu/"" rel=""nofollow noreferrer""><strong>related Nvidia thread</strong></a>). Data packets can be split differently every time, but there are apparent CUDA-level solutions in the pipeline.</p>
","3","2","45264","13013"
"54609","<p>The principal components describe the amount of the total variance that can be explained by a single dimension of the data.</p>

<p>This is equivalent to the spread of the datapoints in a given dimension. The dimensions are (of course) direction that are <a href=""https://en.wikipedia.org/wiki/Orthogonality"" rel=""nofollow noreferrer""><strong>orthogonal</strong></a> i.e. at 90 degrees to one another.</p>

<p>Have a look at this example of data points, where the red lines show the breasth of the data in two dimensions:</p>

<p><a href=""https://i.stack.imgur.com/e8x1z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e8x1z.png"" alt=""simple example of PCA""></a></p>

<p>The dimensions don't have to be in X and Y - they could be pointing in any direction, but must be orthogonal. (more detail in <a href=""https://datascience.stackexchange.com/questions/14300/how-to-interpret-the-loading-values-of-a-pca"">this answer</a>)</p>

<p>We can clearly see a bigger spread in the horizontal X-dimension, so I might expect it to account for 80% of the variance in the dataset. The vertical Y-dimension has less variance, less spread, so explains a smaller amount of the total variance. In this simple 2d example, it would explain the remaining 20% of the variance (it must sum to 100%).</p>

<hr>

<p>In practical terms, if principal components have all very similar values, you might expect the data to form a circle (in 2d), and this means there is little directionality in the feature-space. You might like to think in terms of correlation between the features; a movement in one direction of the space does not guarantee a movement of a certain direction in the second feature. The opposite would be true if e.g. the first component had a normalised value of ~1 i.e. explained approximately 100% of the variance.</p>

<p>I say <em>normalised</em>, because the raw values that come out of PCA do not necessarily between 0 and 1 - so you can normalise them to help interpretation.</p>

<hr>

<p>In a higher dimensional space, say with 10 variables (so 10d feature space), PCA computes eigenvectors and eigenvalues, which look for orthogonal dimensions that explain the variance of the data points, but these are not all all constricted to the dimensions of your features themselves! This means that you cannot just say that the first component (e.g. with a value of 0.6) is there because of a <code>feature X</code>, i.e. not due to a single feature, but a mixture of the features.</p>

<hr>

<h3>About the dataset</h3>

<p>If the first PC explains nearly all of the data's variance, it is likely that you can express you data a lot more succinctly. PCA is often used as a dimensionality reduction method - so in this extreme case, you can reduce a high-dimensional space and convert it to a lower dimensional space without (theoretically) losing much of the explanatory power i.e. a model should be able to learn as much about the feature space from the one predominant PC, compared to the rich high-dimensional space. This of course saves compute time for any model.</p>

<p>I said *theoretically) above, because using PCs to model will make the interpretation of the resulting model itself - in terms of the original input features - more difficult.</p>

<p>For a more thorough introduction, read through a blog post <a href=""https://towardsdatascience.com/dive-into-pca-principal-component-analysis-with-python-43ded13ead21"" rel=""nofollow noreferrer"">like this one</a>.
For some practical examples, check out the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"" rel=""nofollow noreferrer"">Sci-kit learn documentation</a>.</p>
","1","2","45264","13013"
"54622","<p>You can see from the PyTorch documentation that <a href=""https://pytorch.org/docs/stable/nn.html?highlight=eval#torch.nn.Module.eval"" rel=""nofollow noreferrer""><strong>the eval</strong></a> and <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.Module.train"" rel=""nofollow noreferrer""><strong>the train</strong></a> do the same thing. Although they don't explicitly mention it, the documentation is identical:</p>

<blockquote>
  <p>Sets the module in evaluation mode.</p>
  
  <p>This has any effect only on certain modules. See documentations of
  particular modules for details of their behaviors in
  training/evaluation mode, if they are affected, e.g. Dropout,
  BatchNorm, etc.</p>
</blockquote>

<hr>

<p>Additional ideas from this <a href=""https://discuss.pytorch.org/t/model-train-and-model-eval-vs-model-and-model-eval/5744"" rel=""nofollow noreferrer"">PyTorch forum</a>:</p>

<blockquote>
  <p>Yes, they are the same. By default all the modules are initialized to train mode (self.training = True). Also be aware that some layers have different behavior during train/and evaluation (like BatchNorm, Dropout) so setting it matters.  Also as a rule of thumb for programming in general, try to explicitly state your intent and set model.train() and model.eval() when necessary.</p>
</blockquote>

<hr>

<p>Regarding the situations of self-play and then evaluation, I personally would start by doing self-play in <code>.eval()</code> mode in order to have the highest fidelity between self-play and the final execution mode (i.e. a real game). The discrepancy that you speak of should converge as the model does, but I agree it might be problematically large to begin with.</p>

<p>One idea would be to use a <em>warm start</em>... run in <code>.train()</code> mode for a given number of iterations (practice games, or just rounds of self-play), then phase this out as the model hopefully begins to converge. </p>
","0","2","45264","13013"
"54623","<p>In that section, within the for-loop, <code>n</code> is an integer (you loop over a <code>range()</code>.</p>

<p>If you use the square brackets to access the Pandas DataFrame (or a Series, in this case), you are trying to access the <em>index</em> of the table. In this case, the index is a timestamp - have a look at the <code>df</code> in section <code>Out [7]</code>.</p>

<p>So you need to get values by timestamp, or to use the row number itself, you must do as Vaalizaadeh said in a comment and use the <code>.iloc</code> accessor to the pandas object.</p>

<p>So just change <code>shift = dominantTheta[n]</code> to this: <code>shift = dominantTheta.iloc[n]</code>.</p>
","1","2","45264","13013"
"54677","<p>You can generate a config file in your home directory by running:</p>

<pre><code>jupyter notebook --generate-config
</code></pre>

<p>which should leave you with a file <code>~/.jupyter</code>, called <code>jupyter_notebook_config.py</code>.</p>

<hr>

<p>Have a look at all the possible configurations in <a href=""https://jupyter-notebook.readthedocs.io/en/stable/config.html"" rel=""nofollow noreferrer""><strong>the Jupyter documentation</strong></a>.</p>

<p>In addition to many server options, there is specifically an options to override Tornado configuration.</p>

<blockquote>
  <p>NotebookApp.tornado_settings : Dict Default: {}</p>
  
  <p>Supply overrides for the tornado.web.Application that the Jupyter
  notebook uses.</p>
</blockquote>
","2","2","45264","13013"
"54755","<p>It seems you are passing the same parameters to both Python classes: ARIMAX and SARIMAX... and this should produce the same answer, I agree.</p>

<p>The implementations are however separate, it seems - ARIMA inherits from ARMA models and SARIMAX inherits from MLEModel. Have a look at the source code for <a href=""https://www.statsmodels.org/dev/_modules/statsmodels/tsa/arima_model.html#ARIMA"" rel=""nofollow noreferrer""><strong>ARIMA</strong></a> (search fro ""<em>class ARIMA</em>"") and <a href=""https://www.statsmodels.org/dev/_modules/statsmodels/tsa/statespace/sarimax.html#SARIMAX"" rel=""nofollow noreferrer""><strong>SARIMAX</strong></a> (search for ""<em>class SARIMAX</em>"").</p>

<p>There are probably some different ordering of the operations, or different limits on the tolerances for MLE.</p>
","1","2","45264","13013"
"54988","<p>In the original training session, there will have been the mapping between class name and numerical class ID (e.g. Cat = 0, Dog = 1, ...). The model you use will output just the IDs and you need to find the mapping that was used.</p>

<p>If your new data has been added in addition to the old classes, they should each have a new unique ID. If you used VGG architecture to train on your new classes <strong>from scratch</strong>, then the model will not be able to make predictions for the original VGG classes.</p>
","0","2","45264","13013"
"55055","<p>I think your <code>train_images</code> array should have shape <code>(79, 698, 608, 3)</code>. The generator works through each of the first dimensions of those arrays, so is passing a batch of 4d numpy arrays, instead of a batch of 3d numpy arrays.</p>

<p>You can try seeing if that helps, using <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.squeeze.html"" rel=""nofollow noreferrer"">numpy.squeeze()</a>, like this:</p>

<pre><code>In [1]: import numpy as np                                                      

In [2]: a = np.random.randint(0, 10, (2, 1, 10, 10, 3))                         

In [3]: np.squeeze(a, axis=1).shape                                             
Out[3]: (2, 10, 10, 3)
</code></pre>

<p>So be sure to set the <code>axis=1</code> argument. Then just pass the updated array as you were doing.</p>
","1","2","45264","13013"
"55216","<p>I was able to produce the following with a marker for each point, based on some criteria:</p>

<p><a href=""https://i.stack.imgur.com/hhfGj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hhfGj.png"" alt=""markers based on sine being higher than cosine""></a></p>

<p>I made a dummy Pandas Dataframe, using the sine and cosine to get lines that cross each other nicely; also making the index itself a column (with <code>reset_index</code>) just to make plotting easier later on.</p>

<pre><code>df = pd.DataFrame.from_dict(dict(sin=np.sin(np.linspace(0, 2*np.pi, 50)),
                             cos=np.cos(np.linspace(0, 2*np.pi, 50))))
df.index.name = ""index""
df.reset_index(inplace=True)
df.head(5)

  index sin        cos
0   0   0.000000    1.000000
1   1   0.063424    0.997987
2   2   0.126592    0.991955
3   3   0.189251    0.981929
4   4   0.251148    0.967949
</code></pre>

<p>I make a new column which contains the criteria you want, so if one line is above or below another line:</p>

<pre><code>df[""criteria""] = df.sin &gt; df.cos    # where sin values are higher than cos values
</code></pre>

<p>This will allow us to select which points to plot with which style (e.g. which marker).</p>

<p>The final plot itself is made up of two parts (note that we re-use the same <code>ax</code> object, so all plots appear on the same graph)</p>

<ol>
<li><p>The underlying data itself can be added to create the simple lines (these are your moving average columns).</p>

<pre><code>ax = df.sin.plot(c=""gray"", figsize=(10, 6))    # plots the sine curve in gray
df.cos.plot(c=""cyan"", ax=ax)                   # re-uses the ax object - important!
</code></pre></li>
<li><p>The markers, based on our <code>criteria</code> column. We filter the points that satisfy our criteria and plot them with a certain marker, then filter the points that <strong>do not</strong> have the criteria (using the negation selector: <code>~</code>). Whereas we used the basic line <code>plot</code> above, we now need to use the <code>scatter</code> kind of plot:</p>

<pre><code>df[df.criteria].plot(kind=""scatter"", x=""index"", y=""sin"",
                     c=""green"", marker=""^"", ax=ax, label=""higher"")

df[~df.criteria].plot(kind=""scatter"", x=""index"", y=""sin"",
                     c=""red"", marker=""v"", ax=ax, label=""lower"");
</code></pre>

<p>Note that we continue to use the same <code>ax</code> object.</p></li>
</ol>

<p>We can also add the legend, which uses either the Pandas column names by default or the <code>label</code> that was passed in each plot method:</p>

<pre><code>plt.legend()=""lower"");    # adds legend for all plots
</code></pre>

<hr>

<p>You can see in the <a href=""https://matplotlib.org/3.1.0/api/markers_api.html#module-matplotlib.markers"" rel=""nofollow noreferrer"">matplotlib documentation</a> that there is a <code>marker</code> argument, but it seems you cannot simply specify a column of a Pandas dataframe to give the marker for each individual point.</p>
","1","2","45264","13013"
"55287","<p>It is hard to see where the problem is with your code; given your comments, my guess is a problem in the timezone conversion.</p>

<p>I cannot see a problem exactly, buy my suggestion to help debug your situation would be to convert all time to <em>timestamps</em> - by default that will be seconds since the epoch (January 1st 1970). These are then simply normal <code>float64</code> values (I convert to integer microseconds below). If your differences are still returning zero, subtracting simple number, then the problem cannot be in the differences.</p>

<p>Here is a minimal working example. A simple dateframe with timestamps with millisecond frequency:</p>

<pre><code>import pandas as pd
from datetime import datetime

In [1]: df = pd.DataFrame(pd.date_range(start=datetime(2016,1,1,0,0,1), 
    ...:     end=datetime(2016,1,1,0,0,2), freq='ms'), columns=[""dates""]).head(10)    # just take first 10 rows for simplicity
</code></pre>

<p>Make a new column, converting the dates into microseconds since the epoch, as an integer:</p>

<pre><code>In [2]: df[""timestamps""] = df.dates.apply(lambda x: int(datetime.timestamp(x) * 1e6))                              

In [3]: df                                                                                                         
Out[3]: 
                    dates        timestamps
0 2016-01-01 00:00:01.000  1451602801000000
1 2016-01-01 00:00:01.001  1451602801001000
2 2016-01-01 00:00:01.002  1451602801002000
3 2016-01-01 00:00:01.003  1451602801003000
4 2016-01-01 00:00:01.004  1451602801004000
5 2016-01-01 00:00:01.005  1451602801005000
6 2016-01-01 00:00:01.006  1451602801006000
7 2016-01-01 00:00:01.007  1451602801007000
8 2016-01-01 00:00:01.008  1451602801008000
9 2016-01-01 00:00:01.009  1451602801009000
</code></pre>

<p>You could at this point check that there are no duplicates in any of your columns, using:</p>

<pre><code>In [4]: df.T.duplicated()                                                                                          
Out[4]: 
dates         False
timestamps    False
dtype: bool
</code></pre>

<p>If there are duplicates, that could be the cause of differences equal to zero.</p>

<p>Now compute the differences, in my case all 1-millisecond differences (1000 microseconds):</p>

<pre><code>In [5]: df[[""date_diffs"", ""timestamp_diffs""]] = df.diff(1)                                                        

In [6]: df                                                                                                        
Out[6]: 
                    dates        timestamps      date_diffs  timestamp_diffs
0 2016-01-01 00:00:01.000  1451602801000000             NaT              NaN
1 2016-01-01 00:00:01.001  1451602801001000 00:00:00.001000           1000.0
2 2016-01-01 00:00:01.002  1451602801002000 00:00:00.001000           1000.0
3 2016-01-01 00:00:01.003  1451602801003000 00:00:00.001000           1000.0
4 2016-01-01 00:00:01.004  1451602801004000 00:00:00.001000           1000.0
5 2016-01-01 00:00:01.005  1451602801005000 00:00:00.001000           1000.0
6 2016-01-01 00:00:01.006  1451602801006000 00:00:00.001000           1000.0
7 2016-01-01 00:00:01.007  1451602801007000 00:00:00.001000           1000.0
8 2016-01-01 00:00:01.008  1451602801008000 00:00:00.001000           1000.0
9 2016-01-01 00:00:01.009  1451602801009000 00:00:00.001000           1000.0
</code></pre>

<p>Add a sample zero difference and retrieve the index of zero differences:</p>

<pre><code>In [7]: df.iloc[3, 3] = 0.0                                                                                       

In [8]: np.where(df == 0)                                                                                         
Out[8]: (array([3]), array([3]))
</code></pre>

<p>Hopefully that will be enough to find where there could actually be zero difference. If all of that works out without zero differences, I would either look into your timezone conversion (maybe they do some rounding there?) or report a bug to <a href=""https://github.com/pandas-dev/pandas/issues"" rel=""nofollow noreferrer""><strong>the pandas issues</strong></a></p>

<hr>

<h2>EDIT</h2>

<p>After your update trying my debugging method, and seeing this:</p>

<p><a href=""https://i.stack.imgur.com/Auk33.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Auk33.png"" alt=""difference between non-unique integers is never zero""></a></p>

<p>I believe there must be either a bug or an inherent limitation to your system.</p>

<h3>Pandas Bug</h3>

<p>A bug might be in your specific version of Pandas and its <code>df.diff()</code> method. Check your version of Pandas with <code>pd.__version__</code> and look on the issues page I linked above for any clues ... maybe just try the latest stable version anyway.</p>

<h3>32-bit system</h3>

<p>Another possible solution would be that you are running on a 32-bit system and so could actually lose the precision required for my example above.
A 32-bit integer can only retain precision for 10 digits. My timestamps above require 12 digits. You can find out on like this, on <a href=""https://www.cyberciti.biz/faq/linux-how-to-find-if-processor-is-64-bit-or-not/"" rel=""nofollow noreferrer"">Linux</a> or <a href=""https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64"" rel=""nofollow noreferrer"">Windows</a> or <a href=""https://apple.stackexchange.com/questions/12666/how-to-check-whether-my-intel-based-mac-is-32-bit-or-64-bit"">Mac</a>.</p>

<p>Additionally, you could retry my example, but only look at <em>seconds</em> instead of <em>microseconds</em>, just to be sure.</p>
","0","2","45264","13013"
"55533","<p>Why not do both? Like you mention, it might be worth first computing the percentage of all values that are values. Generally you might also have a percentage in mind that is acceptable, like up to 10% missing values, if they are scattered at random throughout your dataset. </p>

<p>There are libraries built specifically for visualising missing data, such as <a href=""https://github.com/ResidentMario/missingno"" rel=""nofollow noreferrer""><strong>missingno</strong></a>, which offers quite a few ideas. Here is an example heatmap of missing variables across features:</p>

<p><img src=""https://camo.githubusercontent.com/196fbc6986234a1d6289ee2bcd7e72c82531433e/68747470733a2f2f692e696d6775722e636f6d2f4a616c534b79452e706e67"" alt=""example""></p>

<p>""missing"" normally implies you have a sequential dataset, for example time-series data. If you had discrete observations e.g. of people's height versus shoe size, there is no sequential causality (autocorrelation: dependency on previous values). In this case, imputing makes little sense.</p>

<p>So assuming you do have sequential data, whether or not to impute or drop time steps with missing values will really depend on your use case. Also perhaps the frequency of the data. If all the missing vaiues appear in one chunk at either end of the time series, it is cokmmon to simply leave out that chunk. </p>

<p>For example, if you have minute frequency data and you wish to predict a value once per day, then missing a few minutes here and there might be tolerable, and imputation of some kind (e.g. fill-forwad) wouldn't have a huge impact overall, but could help the model optimisation work more effectively. Some models cannot handle missing values, so imputation is necessary.</p>

<p>In any case, it would always visualise the data before <strong>and</strong> after imputation. You can usually run the same visualisation anyway. Sure it costs a few extra minutes, but you might catch important issues. This can save a lot of time compared to only finding the issues later on while debugging your trained model.</p>
","3","2","45264","13013"
"55564","<p>I will assume your text is in two strings like this:</p>

<pre><code>In [1]: import pandas as pd                                                                            

In [2]: text1 = ""9 10 13 110 14 16""                                                                    

In [3]: text2 = ""12 1 6 1 1 2"" 
</code></pre>

<p>A one-liner solution would be:</p>

<pre><code>In [4] df = pd.DataFrame.from_records(zip(text1.split("" ""), text2.split("" "")))
</code></pre>

<p>A Pandas Dataframe can be created by passing it one list (or tuple) for each row that you want in the table. This is done by using the <code>from_records()</code> method you see above.</p>

<p>So the steps that make the above line work:</p>

<ol>
<li><code>split()</code> each string on the spaces, to get a list of strings - one per value.</li>
<li>create each row that we want in the dataframe, which is each matched pair from the two lists of values. <code>zip</code> does exactly that for us.</li>
<li>Put the result into the <code>from_records()</code> method.</li>
</ol>

<p>The final result:</p>

<pre><code>In [7]: df
Out[7]:

0   1
0    9  12
1   10   1
2   13   6
3  110   1
4   14   1
5   16   2
</code></pre>

<p>Because we just gave the dataframe lists of strings, the values are still strings in teh dataframe. If you want to actually use them as number, you can use the <code>astype()</code> method, like this</p>

<pre><code>df_integers = df.astype(int)    # now contains integers
df_floats = df.astype(float)    # now contains floats, i.e. decimal values
</code></pre>
","1","2","45264","13013"
"55822","<p>In your specific case, when you want to find something between two different <em>markers</em>, you can use the <code>.split(marker)</code> method as follows:</p>

<pre><code>In [1]: s = ""{:fbbbhbhbh{,:50k: ghgjlj45llj,ljhjlhlhj,clause :59:jhjhhjxjhj65@,j
   ...: jjhjhd :70""

In [2]: s.split(""50k:"")[1].split(""59:"")[0]                                        
Out[2]: ' ghgjlj45llj,ljhjlhlhj,clause '
</code></pre>

<p>If you want to get rid of the spaces at the end, just throw in another method at the end: <code>strip()</code>:</p>

<pre><code>In [3]: s.split(""50k:"")[1].split(""59:"")[0].strip()                                
Out[3]: 'ghgjlj45llj,ljhjlhlhj,clause'
</code></pre>

<p>You can add that to a function as you did with your own code, and put the results into a Pandas Dataframe.</p>

<pre><code>def my_parser(s, marker1, marker2):
    """"""Extract strings between markers""""""
    base = s.split(marker1)[1].split(marker2)
    part1 = base[0].strip()
    part2 = base[1].strip()
    return part1, part2
</code></pre>

<p>Now use the function to iterate over all thew string you have</p>

<pre><code>results = []    # to collect results of each example
m1 = ...        # put whatever markers you need here
m2 = ...
for line in lines:
    results.append(my_parser(line, m1, m2))

# Create a dataframe from a list of lists (i.e. from_records). Each inner list becomes a row
df = pd.DataFrame.from_records(results)
</code></pre>

<h3>Edit:</h3>

<p>Given you have your strings in a DataFrame already and just want to iterate over them, you could do the following, assuming you have <code>my_col</code>, containing the strings:</p>

<pre><code>for line in df.my_col:
    results.append(my_parser(line, m1, m2))    # results is a list as above
</code></pre>

<hr>

<p>So that is what you said you wanted to extract, but it will maybe not generalise well. For example, if there are multiple of those <em>markers</em> in your sentence, you might get unexpected results, or at least only the first occurrence of what you want to extract (if there can be many in your other text examples).</p>
","1","2","45264","13013"
"56023","<p>If you are using Linux and Nvidia GPUs, you can do the following in a terminal</p>

<pre><code>nvidia-smi
</code></pre>

<p>Which will show you some stats about the GPUs available on your system.</p>

<p>You can run it automatically every 2 seconds like this to see how power/memory usage changes during training:</p>

<pre><code>watch -n 2 nvidia-smi
</code></pre>

<p>If you see nothing or the <code>nvidia-smi</code> command fails, you likely don't have the correct drivers installed.</p>

<p>If theya re showing, but Keras/Tensorflow is not finding them, have <a href=""https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow"">a look at this thread</a> for more checks for Tensorflow backend.</p>
","1","2","45264","13013"
"56075","<p>If you have many features, likely meaning many columns in your table of data, then you could try clustering. Something as simple as <a href=""https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"" rel=""nofollow noreferrer"">k-Nearest Neighbours</a> could work nicely.</p>

<p>You would first fit a model using your available data, then look at the resulting clusters. Each cluster will represent combinations of your features.</p>

<p>Next you can put new data points into the model and it will tell you to which cluster it would best fit.</p>

<p>This would actually be a fully fledged predictive model! It is <a href=""https://en.wikipedia.org/wiki/Unsupervised_learning"" rel=""nofollow noreferrer"">unsupervised learning</a>, because you are not using and pre-existing labels (for each cluster in this case).</p>

<p>If you are working with Python, the <a href=""https://scikit-learn.org/stable/modules/neighbors.html"" rel=""nofollow noreferrer"">Sci-kit Learn documentation</a> is a good place to start. There are many other clustering algorithms, which might work better for you, depending on your type of data (if it is spatial, <a href=""https://en.wikipedia.org/wiki/DBSCAN"" rel=""nofollow noreferrer"">DBSCAN</a> is good, for example).</p>
","1","2","45264","13013"
"56092","<p>I would suggest using a Pandas DataFrame (you didn't mention it in your question).</p>

<p>Then it would be as simple as the following</p>

<ol>
<li><p>Create the dataframe from you list <code>x</code>, calling the single column <code>x</code>:</p>

<pre><code>In [1]: import pandas as pd
In [2]: df = pd.DataFrame(x, columns=[""x""])   # x is defined in your question
</code></pre></li>
<li><p>Add a new column (I call it <code>action</code>), which holds your result. As it will be either <code>-1</code> or <code>+1</code>, I fill it all with <code>+1</code> to begin with, then only change the values to <code>-1</code> where your criteria is met:</p>

<pre><code>In [3]: df[""action""] = 1

In [4]: df.head()                                                               
Out[4]: 
         x  action
0  2352.60     1
1  2410.26     1
2  2443.31     1
3  2525.78     1
4  2506.58     1
</code></pre></li>
<li><p>Use the <code>diff()</code> method to find the difference between each row of <code>x</code>. If the difference is less that zero, we can make <code>action</code> equal to <code>-1</code>.</p>

<pre><code>In [5]: df.action[df.x.diff() &lt; 0] = -1

In [6]: df                                                                                                 
Out[6]: 
          x  action
0   2352.60       1
1   2410.26       1
2   2443.31       1
3   2525.78       1
4   2506.58      -1
5   2530.69       1
6   2530.49      -1
</code></pre></li>
</ol>

<p>If you want to shift the difference <em>up</em> one row, because of your rule:</p>

<blockquote>
  <p>Action [ i ] = (-1) if x[ i+1 ] &lt; x[ i ]</p>
</blockquote>

<p>Then you simply include that in the assignment part, filling the <code>action</code> column from the start index to the penultimate element:</p>

<pre><code>In [7]: df.action[:-1] = df.action[1:]

In [8]: df.head()
Out[8]:
          x  action
0   2352.60     1.0
1   2410.26     1.0
2   2443.31     1.0
3   2525.78    -1.0
4   2506.58     1.0
5   2530.69    -1.0
</code></pre>

<p>Note that the last row will contain a value that could be wrong (you cannot know, without knowing the next value of <code>x</code>).</p>

<hr>

<h3>After edit from OP</h3>

<p>If you want to do it in normal lists, you can do this:</p>

<pre><code>In [9]: action = []

In [10]: for i, val in enumerate(x): 
     ...:    if i == (len(x) - 1): break 
     ...:    if x[i+1] &lt; val: 
     ...:        action.append(-1) 
     ...:    else: 
     ...:        action.append(1) 

In [11]: action[:10]         # Look at the first ten elements                                                                                  
Out[11]: [1, 1, 1, -1, 1, -1, 1, 1, -1, -1]
</code></pre>

<p><code>enumerate</code> iterates through the <em>iterable</em> you give it (here, <code>x</code>) and also returns the index of the current element.  We need the check that we do not exceed the length of <code>x</code>, because we check <code>i + 1</code> each time, so could try to check a value one element too far in the list <code>x</code>.</p>
","1","2","45264","13013"
"56095","<p>This is talking about RAM. There are a few factors that will decide how many rows/columns you can use. Instead of rows/columns, it is maybe easier to just think in total number of elements: <code>num_rows * num_cols</code>. The memory you will require is going to have a relationship to this number.</p>

<p>There are ways that might take less working memory to solve the problem - usually memory and speed are part of the trade-off. If you use a lot less memory while computing the result, you can do fewer at a time and so it takes longer.</p>

<p>If you have floating point numbers (those with decimals), then Pandas usually uses the data type <code>float64</code> by default. You could try using <code>float32</code> instead. It offers about half of the accuracy, but also only uses half of the memory. You can do this by simply adding this line before you compute the <code>cosine_similarity</code>:</p>

<pre><code>import numpy as np

normalized_df = normalized_df.astype(np.float32)
cosine_sim = cosine_similarity(normalized_df, normalized_df)
</code></pre>

<p>Here is <a href=""https://stackoverflow.com/questions/51003027/computing-cosine-similarity-between-two-tensors-in-keras"">a thread about using Keras to compute cosine similarity</a>, which can then be done on the GPU. I would point out, that (single) GPUs will generally have less working memory available than your computer itself.</p>

<p>Here is <a href=""https://medium.com/@rantav/large-scale-matrix-multiplication-with-pyspark-or-how-to-match-two-large-datasets-of-company-1be4b1b2871e"" rel=""nofollow noreferrer"">a blog that speaks about how to scale up your computation</a> to use tools like Spark, for distributed computing. That would allow you to deal with much larger matrices, provided you have several computers available.</p>
","1","2","45264","13013"
"56431","<p>Looking at <a href=""https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.bar.html"" rel=""nofollow noreferrer""><strong>the documentation</strong></a>, it seems there is no obvious way.</p>

<p>Looking at <a href=""https://github.com/matplotlib/matplotlib/blob/6aec225620563af2a86fd064add2fa9eeb9e2045/lib/matplotlib/axes/_axes.py#L2166"" rel=""nofollow noreferrer""><strong>the source code of the <code>plt.bar</code> method</strong></a> (held on Axes objects), and searching for uses of the <code>width</code> parameter, it starts getting quite complicated and I don't think it is meant to be used for this purpose explicitly! Lots of internal methods, starting with a <code>_</code>, like <a href=""https://github.com/matplotlib/matplotlib/blob/6aec225620563af2a86fd064add2fa9eeb9e2045/lib/matplotlib/axes/_axes.py#L2337"" rel=""nofollow noreferrer"">self._convert_dx</a> is a clue.</p>

<p>I would just offer a nasty hack for your toy example, hoping it is applicable to your actual data:</p>

<pre><code>In [1] : import pandas as pd 
    ...: import matplotlib.pyplot as plt  
    ...:  
    ...: fig, (ax1, ax2) = plt.subplots(2) 
    ...:  
    ...: pd.Series([0, 1, 0]).plot(kind='bar', ax=ax1) 
    ...: pd.Series([1, 2, 3]).plot(kind='bar', ax=ax2)
</code></pre>

<p>Gives</p>

<p><a href=""https://i.stack.imgur.com/BzV1G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BzV1G.png"" alt=""Aligned bars""></a></p>

<p>So the hack is to make the length of your datasets the same, adding zeros where needed.</p>
","1","2","45264","13013"
"56803","<p>I am afraid I do not know the software you mention, but I can show you the principles and suggest why maybe you are getting negative numbers.</p>

<p>I will do this in Python, making use of the numerical library, <a href=""https://numpy.org/"" rel=""nofollow noreferrer"">numpy</a>.</p>

<p>I import numpy and generate 12 random integers between 0 and 9 (10 is excluded as upper limit):</p>

<pre><code>In [1]: import numpy as np; samples = np.random.randint(0, 10, 12)              

In [2]: samples                                                                 
Out[2]: array([8, 5, 8, 4, 4, 7, 2, 2, 0, 5, 9, 1])
</code></pre>

<p>To scale the values to a range that makes their sum equal to 1, we can do the following. First sum up all values:</p>

<pre><code>In [3]: total = np.sum(samples)                                                 
</code></pre>

<p>Now simply divide each value by the sum (the division happens here individually for each element of <code>samples</code>:</p>

<pre><code>In [4]: normalised = samples/total                                              

In [5]: normalised                                                              
Out[5]: 
array([0.14545455, 0.09090909, 0.14545455, 0.07272727, 0.07272727,
       0.12727273, 0.03636364, 0.03636364, 0.        , 0.09090909,
       0.16363636, 0.01818182])
</code></pre>

<p>We can see that the result does indeed sum to 1:</p>

<pre><code>In [6]: np.sum(normalised)                                                      
Out[6]: 1.0
</code></pre>

<p>What you may have is a set of samples that contains some negative numbers, like the following in <code>samples_neg</code>, with ten integers ranging from -5 to +9:</p>

<pre><code>In [7]: samples_neg = np.random.randint(-5, 10, 10)                             

In [8]: samples_neg                                                             
Out[8]: array([ 4,  7,  0,  4, -3,  0,  9, -3,  9,  3])
</code></pre>

<p>We can follow the same recipe as before, summing the values and dividing each value by the sum:</p>

<pre><code>In [9]: total_neg = np.sum(samples_neg)                                        

In [10]: normalised_neg = samples_neg / total_neg                               
</code></pre>

<p>We see that the result this time includes negative values, as you mentioned:</p>

<pre><code>In [11]: normalised_neg                                                         
Out[11]: 
array([ 0.13333333,  0.23333333,  0.        ,  0.13333333, -0.1       ,
        0.        ,  0.3       , -0.1       ,  0.3       ,  0.1       ])
</code></pre>

<p>However, this does still satisfy the constraint you originally had, which was that they sum to 1:</p>

<pre><code>In [12]: np.sum(normalised_neg)                                                 
Out[12]: 0.9999999999999999        # this is 1, within rounding errors of floating point values
</code></pre>

<hr>

<p>A suggestion would be to first normalise the values in a range of [0, 1] and afterwards, re-weight the values such that their sum is 1.</p>
","1","2","45264","13013"
"56987","<p>Assuming you already have a dataframe <code>df</code>, created as below:</p>

<pre><code>In [1] : df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), 
                           columns=['a', 'b', 'c'])
In [2] : df                                                                                                                                                   
Out[2] : 
   a  b  c
0  1  2  3
1  4  5  6
2  7  8  9
</code></pre>

<p>you could try doing this:</p>

<pre><code>In [3] : df.melt(""a"").sort_values(""a"")                                                                                                                        
Out[3] : 
   a variable  value
0  1        b      2
3  1        c      3
1  4        b      5
4  4        c      6
2  7        b      8
5  7        c      9
</code></pre>

<p>where the sorting might be optional in your case. Also, you should swap <code>""a""</code> for the name of your column containing the <code>UID</code> values.</p>

<p>You can additionally drop the autogenerated <code>variable</code> column too, so the final solution would be:</p>

<pre><code>In [4] : df.melt(""a"").sort_values(""a"").drop(""variable"", axis=1)                                                                                               
Out[4] : 
   a  value
0  1      2
3  1      3
1  4      5
4  4      6
2  7      8
5  7      9
</code></pre>

<hr>

<p>From what I remember, the <code>melt</code> functionality is inspired on the <em>melt</em> function in the <code>R</code> programming language, where DataFrames are one of the core built-in data-types. It is really useful for plotting using the ggplot library (in R).</p>
","1","2","45264","13013"
"57158","<p>I'm pretty sure that you will need CUDA to use the GPU, given you have included the tag tensorflow. All of the <code>ops</code> in tensorflow are written in C++, which the uses the CUDA API to speak to the GPU. Perhaps there are libraries out there for performing matrix multiplication on the GPU without CUDA, but I haven't heard of a deep learning framework that doesn't use CUDA, when executing on the GPU.</p>

<p><strong>What you can perhaps do it find a solution that works using Docker</strong>. The latest version doesn't require any additional Nvidia software to be installed. It would mean your customer doesn't need to mess around with CUDA themselves, they just need docker installed, which is generally less than 5 minutes work. Here is <a href=""https://www.tensorflow.org/install/docker"" rel=""nofollow noreferrer""><strong>the Tensorflow documentation for a starter</strong></a>.</p>

<p>In addition, using Docker is perhaps the most professional and isolated way to deliver code to somebody else's machine. There are many benefits and it works on Windows, Mac, Linux. Any changes you make can simply be pushed to their machine, without them having to change anything. It can also scale arbitrarily to multiple machines, running locally or in the cloud, the list goes on and on...</p>
","0","2","45264","13013"
"57397","<p>You are simply defining your array so that it is made of python <code>set</code>s. That is a different data structure which is not able to be  multiplied, unlike an array.</p>

<p>Just change your code to this:</p>

<pre><code>array1 = np.array([[1,2,3,4],[5,6,7,8]])
</code></pre>

<p>The only difference is using square brackets instead of curly ones. These are python <code>list</code> objects (or standard arrays).</p>

<pre><code>array2=array1*array1  
print(array2)

[[ 1  4  9 16]
 [25 36 49 64]]
</code></pre>
","2","2","45264","13013"
"58125","<p>Trying to find certain things in text can be done quite easily if you have the real text, and not a scanned document that is a PDF or even an image. This is actually quite a big topic and can become quite difficult.</p>

<h2>Pure text</h2>

<p>If you have pure text, you can parse out parts you need using custom regular expressions, e.g. to find a date, you might use this:</p>

<pre><code>^(19|20)\d\d[- /.](0[1-9]|1[012])[- /.](0[1-9]|[12][0-9]|3[01])$
</code></pre>

<blockquote>
  <p>matches a date in yyyy-mm-dd format from 1900-01-01 through 2099-12-31, with a choice of four separators (<a href=""https://www.regular-expressions.info/dates.html"" rel=""nofollow noreferrer"">source</a>).</p>
</blockquote>

<p>I believe there are even a few libraries that specifically find dates for you within text.</p>

<h2>PDF</h2>

<p>There are actually many types of PDF, i.e. there are many ways a pdf can be encoded behind the scenes. Some types are easier to parse that others, but luckily there are libraries that can help with that. For example, check out <a href=""https://github.com/euske/pdfminer"" rel=""nofollow noreferrer"">PDFMiner</a>.</p>

<p>After using such a library, you will hopefully be left with the pure text, and can maybe go back to using methods from that section.</p>

<h2>Images</h2>

<p>If you are unlucky enough to have an image as a starting point, then you are now in the realms of OCR - <strong>Optical Character Recognition</strong>. I would recommend reading <a href=""https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa"" rel=""nofollow noreferrer""><strong>this blog post</strong></a> for a more complete description of possible methods,but in a nutshell, you can try using either:</p>

<ol>
<li>a traditional algorithm from compute vision (applying filters and looking for edges etc.)</li>
<li>a trained model specialised for text (e.g. <a href=""https://arxiv.org/abs/1704.03155"" rel=""nofollow noreferrer""><strong>EAST: an Efficient and Accurate Scene Text Detector</strong></a>)</li>
<li>a general model</li>
</ol>

<p>A nice model to help out with OCR is <a href=""https://stackabuse.com/pytesseract-simple-python-optical-character-recognition/"" rel=""nofollow noreferrer""><strong>the Tesseract library</strong></a>.</p>

<hr>

<p>You said you are learning NLP, so actually extracting tokens from a PDF might not be the best example with which to start. I would recommend first deciding exactly what you really want to learn and follow a course or a tutorial on that topic.area.</p>
","1","2","45264","13013"
"58663","<p>Just to point out a minor confusion that there seems to be in the wording: there is mixed use of the the words temporarily and temporally. <em>[OP has since corrected this]</em></p>

<p>We really only care about the final word, <strong>temporally</strong> i.e. relating to time, or the timestamps of the points in a cluster. Temporarily means only for a short period of time, which is a slightly different meaning to temporally' being related to time.</p>

<p>The way I understand the definition, a cluster is defined as a <em>stop cluster</em> if the minimum and maximum timestamps included that cluster (<span class=""math-container"">$mint$</span> and <span class=""math-container"">$maxt$</span>)</p>

<p>Also, it seems <em>definition 9</em> implies the usage of <em>definition 7</em>, so I would assume you need to implement it. If would be used a criterion as to whether a candidate cluster <span class=""math-container"">$C_i$</span> is defined as a <em>stop</em> cluster. So the final pseudo code would be something like:</p>

<pre><code>stop_clusters = []    # empty list
for candidate in candidate_cluster:    # iterate over each stop cluster candidate 
    TC = cluster_is_tc(cluster)        # check cluster is Temporally Continuous
    TO = cluster_is_not_to(cluster)    # check cluster is Temporally Overlapping

    if (TC &amp; TO):                      # if both criteria are met...
        stop_clusters.append(cluster)  # assign to stop clusters
</code></pre>

<p>How you implement those two check functions would be quite straight forward, according to the definition, assuming you have the timestamps of each point, you are trivially checking that all points are contained in the time bounds. Just skimming page 3, I am afraid the paper doesn't make these time bounds completely clear to me, but I hope it is clear to you as you are implementing it.</p>

<p>The same is true for the temporally overlapping criterion: you check that there is no overlap of timestamps in two clusters - if there is overlap (i.e. the intersection is <strong>not</strong> an empty set), then you cannot call the current candidate cluster a stop cluster.</p>

<h3>Edit:</h3>

<p>Looking at that implementation you linked, it could be that that are performing the check Temporally Overlapping check at <a href=""https://github.com/jayachithra/T-DBSCAN/blob/master/tdbscan.py#L105"" rel=""nofollow noreferrer"">this <code>if</code> line</a> for the case of being non-overlapping and <a href=""https://github.com/jayachithra/T-DBSCAN/blob/master/tdbscan.py#L107"" rel=""nofollow noreferrer"">this <code>else</code> line</a> for the other case that they do overlap in time (assuming there index values <code>idx</code> are the timestamps); they are checking that the largest timestamp is less than the minimum timestamp of the previous cluster.</p>

<p>I can't see evidence of the check for temporal continuity of a cluster.</p>
","1","2","45264","13013"
"58721","<p>As a long shot, if you have a Python version (or implement it yourself), running it with <a href=""https://pypy.org/"" rel=""nofollow noreferrer"">PyPy</a> instead of Python might make things much faster, as it is well suited to code that uses Python built-ins and also many loops. It optimises these cases very well, through tricks such as garbage collection. The latest version also supports Python3.5 and 3.6 as well as NumPy.</p>

<p>From their website the main benefits:</p>

<blockquote>
  <p><strong>Speed</strong>: thanks to its Just-in-Time compiler, Python programs often run
  faster on PyPy. (What is a JIT compiler?)</p>
  
  <p>“If you want your code to run faster, you should probably just use
  PyPy.” — Guido van Rossum (creator of Python)</p>
  
  <p><strong>Memory usage</strong>: memory-hungry Python programs (several hundreds of MBs
  or more) might end up taking less space than they do in CPython.</p>
  
  <p><strong>Compatibility</strong>: PyPy is highly compatible with existing python code. It
  supports cffi and can run popular python libraries like twisted and
  django.</p>
  
  <p><strong>Stackless</strong>: PyPy comes by default with support for stackless mode,
  providing micro-threads for massive concurrency.</p>
</blockquote>
","1","2","45264","13013"
"58928","<p>Assuming your samples have a fixed size and you use constant batch size, you should never run out of memory - that is: as soon as you have managed to train a full batch (forward and backward propagation complete) you should be fine to assume it will train until completion!</p>

<hr>

<h3>Extra info / tips</h3>

<p>The total memory requirement will be a product of the size of the model (number of parameters) along with the batch size. Each single sample that gets sent through the network will effectively influence each of the <span class=""math-container"">$N$</span> weights. A batch of size <span class=""math-container"">$B$</span> will mean that there needs to be roughly <span class=""math-container"">$N \times B$</span> memory available.</p>

<p>If you model is already fixed, you can start with a large batch size, decreasing it slowly until you no longer receive an <code>outOfMemory</code> error. You could also try computing exactly what your memory requirement are, but there are some other factors that might make that a little difficult, so it is faster to try the first approach.</p>

<p>If you are using a GPU, you ca look at the output of the terminal command <code>nvidia-smi</code>, you can see the available memory of the GPUs. You will notice it essentially becomes all used as soon as training begins. This is because Tensorflow, by default, will occupy all available memory. There are ways around that, so search for <code>allow_growth</code> in the Tensorflow docs for relevant information and configuration tactics. </p>
","1","2","45264","13013"
"60181","<p>I think you have listed the main ways of doing it - you can achieve it by iterating or merging. What is ""best"" will depend on your use case.</p>

<p>Here is a way to do it by iterating over a dataframe. This way gives you more control about what gets filled i.e. you could add more conditions on which values to fill in. I first make a new dataframe that has all the complete dates and alternating <code>A B A B</code> values in column <code>C1</code>:</p>

<pre><code>import pandas as pd
import numpy as np

dates = pd.date_range(start=""1/1/2019"", end=""1/10/2019"")
repeated_dates = np.repeat(dates, 2)
df = pd.DataFrame(index=repeated_dates, columns=[""C1"", ""C2""])
df[""C1""] = (len(df) // 2) * [""A"", ""B""]

# See first 5 rows
print(df.head())

           C1   C2
2019-01-01  A  NaN
2019-01-01  B  NaN
2019-01-02  A  NaN
2019-01-02  B  NaN
2019-01-03  A  NaN
</code></pre>

<p>We will later fill the values of the <code>C2</code> column.</p>

<p>Next make a dataframe (it would actually be your starting data), by dropping a few rows from our ""results"" dataframe above:</p>

<pre><code>df_missing = df.drop(df.index[[3, 9]])

C2_col = []

# grouping by the index (i.e. the date) gives us two rows at a time
for date, group in df.groupby(df.index):
    try:
        # see which values your data has for this data and extract them
        day = df_missing.loc[date, [""C1"", ""C2""]]
        C2_A, C2_B = day.C2.values

    # If the date wasn't there, we can catch the error and give any values we want
    except KeyError as e:
        # Could now use more condition e.g. on the date or previous values, etc.
        C2_A = C2_B = ""was_missing""

    # Keep the values in a list
    C2_col.extend([C2_A, C2_B])

# Overwrite the column that was full of NaN values
df[""C2""] = C2_col
</code></pre>

<p>We can see in the final result that all dates are present, along with the <code>A B A</code> pattern, and we could insert whatever we wanted into those dates with missing values:</p>

<pre><code>print(df)

           C1           C2
2019-01-01  A          NaN
2019-01-01  B          NaN
2019-01-02  A  was_missing
2019-01-02  B  was_missing
2019-01-03  A          NaN
2019-01-03  B          NaN
2019-01-04  A          NaN
2019-01-04  B          NaN
2019-01-05  A  was_missing
2019-01-05  B  was_missing
2019-01-06  A          NaN
2019-01-06  B          NaN
2019-01-07  A          NaN
2019-01-07  B          NaN
2019-01-08  A          NaN
2019-01-08  B          NaN
2019-01-09  A          NaN
2019-01-09  B          NaN
2019-01-10  A          NaN
2019-01-10  B          NaN
</code></pre>
","2","2","45264","13013"
"60269","<p>There are a few ways to do this a little more efficiently:</p>

<h2>JSON module, then into Pandas</h2>

<p>You could try reading the JSON file directly as a JSON object (i.e. into a Python dictionary) using the json module:</p>

<pre><code>import json
import pandas as pd

data = json.load(open(""your_file.json"", ""r""))
df = pd.DataFrame.from_dict(data, orient=""index"")
</code></pre>

<p>Using <code>orient=""index""</code> might be necessary, depending on the shape/mappings of your JSON file.</p>

<p>check out this in depth tutorial on <a href=""https://realpython.com/python-json/"" rel=""nofollow noreferrer"">JSON files with Python</a>.</p>

<h2>Directly using Pandas</h2>

<p>You said this option gives you a memory error, but there is an option that should help with it. Passing <code>lines=True</code> and then specify how many lines to read in one chunk by using the <code>chunksize</code> argument. The following will return an object that you can iterate over, and each iteration will read only 5 lines of the file:</p>

<pre><code>df = pd.read_json(""test.json"", orient=""records"", lines=True, chunksize=5)
</code></pre>

<p>Note here that the JSON file must be in the <code>records</code> format, meaning each line is <em>list like</em>. This allows Pandas to know that is can reliably read <code>chunksize=5</code> lines at a time. Here is the <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json"" rel=""nofollow noreferrer""><strong>relevant documentation on line-delimited JSON files</strong></a>. In short, the file should have be written using something like: <code>df.to_json(..., orient=""records"", line=True)</code>.</p>

<p>Not only does Pandas abstract some manual parts away for you, it offers a lot more options, such as converting dates correctly, specifying data type of each column and so on. <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html"" rel=""nofollow noreferrer"">Check out the relevant documentation</a>.</p>

<p>Check out a little code example in <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json"" rel=""nofollow noreferrer"">the Pandas user guide documentation</a>.</p>

<h2>Another memory-saving trick - using Generators</h2>

<p>There is a nice way to only have one file's contents in memory at any given time, using Python generators, which have lazy evaluation. Here is a <a href=""https://realpython.com/introduction-to-python-generators/"" rel=""nofollow noreferrer"">starting place</a> to learn about them.</p>

<p>In your example, it could look like this:</p>

<pre><code>import os

# Get a list of files
files = sorted(os.listdir(""your_folder""))
# Load each file individually in a generator expression
df = pd.concat(pd.read_json(file, orient=""index"") for f in files, ...)
</code></pre>

<p>The concatenation happens only once all files are read. Add any more parameters that are required where I left the <code>...</code>. The <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html"" rel=""nofollow noreferrer"">documentation for <code>pd.concat</code> are here</a>.</p>
","3","2","45264","13013"
"60365","<h3>One-liner:</h3>

<pre><code>result = data.loc[data.x == 4, ""time""]    # assuming you expect just the time as output
</code></pre>

<hr>

<h3>Explanation</h3>

<p>To describe your problem, first in English (then code):</p>

<blockquote>
  <p>You want the rows of the data frame, where the value of the <code>X</code> column is 4, and all columns should be included in the result.</p>
</blockquote>

<p>To check where coilumn <code>X</code> is 4, we can do this:</p>

<pre><code>data.X == 4
</code></pre>

<p>This returns <code>True</code> or <code>False</code> for every single row. Now we can use this as a filter, by passing it to the pandas DataFrame's <code>.loc</code> accessor (<code>loc</code> = ""locate"").</p>

<pre><code>result = data.loc[data.X == 4, :]
# Notation: dataframe[rows_we_want, columns_we_want]
</code></pre>

<p>Because we want all columns, we have left the <em>colon</em> <code>:</code> in there after the comma. Pandas assumes you want all columns by default, so we can actually leave it out to get the <em>one-liner</em> given above.</p>

<p>So if you just want to time to be given as output, you can filter for that by putting the name of the column after the comma:</p>

<pre><code>result = data.loc[data.X == 4, ""time""]
</code></pre>
","1","2","45264","13013"
"60664","<p>It seems to be because the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB.predict"" rel=""nofollow noreferrer""><strong>predict method on your <code>Pipeline</code> object</strong></a> requires the input to match the input of the first object in your pipeline, which is the <code>CountVectorizer</code>. It any case, it only requires an iterable object, which your 1d array indeed is.</p>

<p>The classifier you train without the pipeline ends up being a MultinomialNB object, whose <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.predict"" rel=""nofollow noreferrer""><strong>predict method</strong></a> actually requires a 2d array of shape <code>num_samples * num_features</code>.</p>

<p>Maybe you need to pass your test data through each of the individual steps manually before feeding anything into the final classifier?</p>
","1","2","45264","13013"
"60872","<p>I would suggest using the well-known and tested <code>train_test_split</code> function from <code>sklearn</code>. <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"" rel=""nofollow noreferrer""><strong>Here is the documentation</strong></a>.</p>

<p>You start with arrays in your example, so no need to use tuples.</p>

<p>Because you want train, test and validation sets, you will need to split the data twice though. Here is an example, where I split the into <code>train_and_val</code> and <code>test</code>, then split the <code>train_and_val</code> part into (final) <code>train</code> and <code>val</code>. The test remains from the first split to leave you with a final 60-20-20 split:</p>

<pre><code>from sklearn.model_selection import train_test_split

# First split 80-20 - using your original data: x_sample and label
X_train_and_val, X_test, y_train_and_val, y_test = train_test_split(
     x_sample, label, test_size=0.2, random_state=666)
</code></pre>

<p>To split the 80 from above into 60-20 of the entire dataset, we need to use a <code>test_size</code> of <span class=""math-container"">$20 / (60 + 20) = 0.25$</span>.</p>

<pre><code>X_train, X_val, y_train, y_val = train_test_split(
     X_train_and_val, y_train_and_val, test_size=0.25, random_state=666)
</code></pre>

<p>Now you have <code>X_train</code>, <code>y_train</code> (60%), <code>X_val</code>, <code>y_val</code> (20%) and <code>X_test</code>, <code>y_test</code> (60%).</p>
","0","2","45264","13013"
"61117","<p>As others have pointed out, you could retrospectively compute those values if you know enough about the data.</p>

<p>If you literally only know the <em>accuracy</em>, <em>precision</em> and <em>recall</em> then it wouldn't be possible. It would be like someone telling you the answer is 0.79, and asking how it was computed... there are infinitely many ways.</p>

<p>I would suggest looking into the code where those metrics are computed and intercept the raw predictions and labels. You could sum up the values in the confusion matrix (TP, FP, FN) during inference, then just use something like the  <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html"" rel=""nofollow noreferrer""><strong>sklearn.metrics.precision_recall_fscore_support function from Sci-kit Learn</strong></a>.</p>

<p>Depending on what method you are using to get the metrics, there might even already be an argument to the function that will also return the full confusion matrix.</p>
","2","2","45264","13013"
"61415","<p>Assuming they can both do what you want them for (can perform the correct splits for your needs), I would recommend simply using the one that fits best with your pipeline. E.g. the data is being fed straight into a Keras model, just do as much as possible in Keras -> use the <a href=""https://keras.io/preprocessing/sequence/#timeseriesgenerator"" rel=""nofollow noreferrer"">TimeseriesGenerator</a>. Otherwise, stick to Sci-kit Learn.</p>

<p>The both <strong>generate</strong> the splits, meaning they only actually create each <em>split</em> as you loop through the data. This saves memory.</p>

<p>Looking at <a href=""https://github.com/keras-team/keras/blob/f295e8ee39d4ba841ac281a9337d69c7bc5e0eb6/keras/utils/data_utils.py#L305"" rel=""nofollow noreferrer""><strong>the source code of the Keras variant</strong></a>, it doesn't seem to support running on the GPU if you combine e.g. with your Tensorflow graph - meaning there really isn't a big difference between the two, just the APIs/functionality.</p>
","2","2","45264","13013"
"61429","<p>Electricity prices are essentially the same as stock prices: best modelled by a random walk, where the best prediction for tomorrow is the price today. Therefore I am not really surprised that you get worse results using more historical data.
 Some versions of ARIMA will also include regularisation, which will punish your model for including more and more data - inclusion of new data must be justified by contributing to a lower residual error to be ""worth"" inclusion.</p>

<p>Other models that tend to be a little more robust use features other than the actual target, here the price. For example, trying to predict the volatility of the price might  prove to be more accurate. For this there are GARCH models (Generalised AutoRegressive Conditional Heteroskedasticity).</p>

<p>Another thing you might consider is to include external data... for example, electricity consumption is heavily influenced by the weather - if it is cold outside, a lot of people heat their homes using electrical heaters, they also drink more hot drinks etc.</p>
","1","2","45264","13013"
"61481","<p>Sublime Text is a text editor - it is independent of your Python setup. I would suggest following a tutorial on setting up Anaconda on your machine, then follow a second tutorial on making sublime text find the Anaconda Python executable.</p>

<ul>
<li>Here is a tutorial for <a href=""https://www.datacamp.com/community/tutorials/installing-anaconda-mac-os-x"" rel=""nofollow noreferrer""><strong>setting up Python with Anaconda on a Mac</strong></a>.</li>
<li>Here is <a href=""https://gist.github.com/zaemiel/4fbd8b5125fda7a140be"" rel=""nofollow noreferrer""><strong>something I found with a simple search</strong></a> (disclaimer: I don't know anything about sublime text!)</li>
</ul>

<p>Once you have anaconda installed, you would need to run this command in terminal to install the packages you need:</p>

<pre><code>conda install numpy pandas xlrd matplotlib
</code></pre>
","0","2","45264","13013"
"61958","<p>A basic decision tree is <em>pruned</em> to reduce the likelihood of over-fitting to the data and so help to generalise. <strong>Random forests</strong> don't usually require pruning because each individual tree is trained on a random subset of the features and when trees are combined, there is little correlation between them, reducing the risk of over-fitting and building dependencies between trees.</p>

<p>There could be a few reasons why you get this unexpected improved performance, mostly depending on how you trained the random forest. If you did any of the following, you potentially allowed overfitting to creep in:</p>

<ul>
<li>a small number of random trees was used</li>
<li>trees with high <em>strength</em> were used; meaning very deep, learning idiosyncrasies of the training set </li>
<li>correlation between your features</li>
</ul>

<p>and so removing features, you have allowed your model to generalise slightly more and so improve its performance.</p>

<p>It might be a good idea to remove any features that are highly correlated e.g. if two features have a pairwise correlation of >0.5, simply remove one of them. This would essentially be what you did (removing 3 features), but in a more selective manner.</p>

<h2>Overfitting in Random Forests</h2>

<p>You can read a bit more about the reasons above <a href=""https://en.wikipedia.org/wiki/Random_forest#Algorithm"" rel=""nofollow noreferrer""><strong>on Wikipedia</strong></a> or in some papers about random forests that discuss issues:</p>

<ol>
<li><p><a href=""https://link.springer.com/article/10.1023%2FA%3A1010933404324"" rel=""nofollow noreferrer""><strong>Random forest paper by Leo Breiman</strong></a> - states in the conclusion section:</p>

<blockquote>
  <p>Because of the Law of Large Numbers, they do not overfit.</p>
</blockquote>

<p>but also mentions the requirement of appropriate levels of randomness.</p></li>
<li><p><a href=""https://web.stanford.edu/~hastie/ElemStatLearn//"" rel=""nofollow noreferrer""><strong>Elements of Statistical Learning by Hastie et. al</strong></a> (specifically <em>section 15.3.4 Random Forests and Overfitting</em>) gives more insight, referring to the increase of the number of data samples taken from your training set:</p>

<blockquote>
  <p>at the limit, the average of fully grown trees can result in too rich a model, and incur unnecessary variance</p>
</blockquote></li>
</ol>

<p>So there is a trade-off perhaps, between the number of features you have, the number of trees used, and their depths. Work has been done to control the depth of trees, with some success - I refer you to <em>Hastie et. al</em> for more details and references. </p>

<p>Here is an image from the book, which shows results of a regression experiment controlling the depth of trees via <em>minimum Node Size</em>. So requiring larger nodes effectively restricts your decision trees from being grown too far, and therefore reducing overfitting.</p>

<p><a href=""https://i.stack.imgur.com/qWaXD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qWaXD.png"" alt=""Node size versus test error""></a></p>

<p>As a side note, <em>section 15.3.2</em> addresses variable importance, which might interest you.</p>

<hr>

<p><em>I assume that you trained (""grew"") your random forest on some training data and tested the performance on some hold-out data, so the performance you speak of is valid.</em></p>
","3","2","45264","13013"
"61963","<p>I tool like MS Excel or Libre Office Calc (open source) is nice to view data in a table and has a low barrier to entry - after playing around for 30 minutes, you can probably get most basic tasks done.</p>

<p>Using a programming language like R or Python opens up many many other opportunities for more advanced analysis. People write packages that will do a lot of work for you. Have a look at <a href=""https://towardsdatascience.com/data-analysis-and-visualisations-using-r-955a7e90f7dd"" rel=""nofollow noreferrer"">these examples R</a>, or <a href=""https://www.kaggle.com/kashnitsky/topic-1-exploratory-data-analysis-with-pandas"" rel=""nofollow noreferrer"">Pandas for Python</a> for examples, both for some simple data analysis.</p>

<p>Detecting outliers can be done in a lot of ways:</p>

<ul>
<li>Excel, using <a href=""https://www.howtogeek.com/400211/how-and-why-to-use-the-outliers-function-in-excel/"" rel=""nofollow noreferrer"">quantiles of your data's distribution</a></li>
<li>R, a <a href=""https://datascienceplus.com/outlier-detection-and-treatment-with-r/"" rel=""nofollow noreferrer"">more detailed example using quantiles</a>, different distributions and package designed for that</li>
<li>Python, a package for outlier detection, <a href=""https://github.com/yzhao062/pyod#quick-start-for-outlier-detection"" rel=""nofollow noreferrer"">here using clustering as the method</a> </li>
</ul>
","1","2","45264","13013"
"61965","<p>you need to add some more explanation to what you are doing. Without guessing, we can tell you only what the error message already tells you: you are trying to get the <code>month</code> attribute of the <code>date1</code> variable, in your loop.</p>

<p>My only guess would be to convert the data to date type somehow, but this might also fail, as I don't know what the values look like in <code>RD[""LastFullStartTime""]</code>.</p>

<p>You could change this line, adding the converter <code>pd.to_datetime()</code>:</p>

<pre><code>NEW = {""Cell"": RD['Cell'], ""LastFullResult"": pd.to_datetime(RD[""LastFullResult""]), ""LastFullStartTime"": RD[""LastFullStartTime""], ""status code"": RD[""status code""]}
NEW = pd.DataFrame(NEW)
date = pd.datetime.now()
NEW_LastFullStartTime = []
</code></pre>
","0","2","45264","13013"
"62139","<p>Getting varying results at the level of F1 score is possible but should be high, as your are essentially taking the average over many values (assuming a reasonably sized dataset).</p>

<p>There are however many sources of random behaviour that could be creeping in. From most obvious to most obscure:</p>

<ol>
<li>Are the definitions and parameters of each of your models identical each time?</li>
<li>Do you use the same train/validation/test splits for each session?</li>
<li>Are your training hyper-parameters (number of epochs, learning rate, early stopping) the same each time?</li>
<li>Do you set the random seed in Numpy/Keras/Tensorflow?</li>
<li>Did you set any Nvidia CUDA flags for determinism?</li>
</ol>

<p>Regarding the final point, GPU determinism - this is a very new thing in the context of deep learning and many people believed it was impossible, but there is good progress being made. Check out <a href=""https://developer.nvidia.com/gtc/2019/video/S9911"" rel=""nofollow noreferrer"">these video/slides from Nvidia GTC</a> conference this year.</p>

<p>These are really just the components of each individual building block i.e. each deep model. You could also introduce random behaviour perhaps in the way you chain these models together to create your ensemble.</p>
","1","2","45264","13013"
"62363","<p>On the page you linked there is actually a Python example on how to get the data. It is in Python 2, but I will show you how to make it work in Python 3.</p>

<pre><code>import urllib
import json                 # Used to load data into JSON format
from pprint import pprint   # pretty-print

url = ""https://data.sa.gov.au/data/api/3/action/datastore_search?resource_id=fec742c1-c846-4343-a9f1-91c729acd097&amp;limit=5&amp;q=title:jones""
response = urllib.request.urlopen(url)
print(response)
# Just an object:   &lt;http.client.HTTPResponse at 0x7f2618123e10&gt;
</code></pre>

<p>We get the text data out by using the <code>read()</code> method:</p>

<pre><code>text = response.read()
</code></pre>

<p>The response in this case is a raw string. We can use the <code>json</code> module's function <code>loads</code> to <strong>load</strong> a <strong>s</strong>tring):</p>

<pre><code>json_data = json.loads(text)

pprint(json_data)
</code></pre>

<p>returns the following JSON data:</p>

<pre><code>{'help': 'https://data.sa.gov.au/data/api/3/action/help_show?name=datastore_search',
 'result': {'_links': {'next': '/api/3/action/datastore_search?q=title%3Ajones&amp;offset=5&amp;limit=5&amp;resource_id=fec742c1-c846-4343-a9f1-91c729acd097',
                       'start': '/api/3/action/datastore_search?q=title%3Ajones&amp;limit=5&amp;resource_id=fec742c1-c846-4343-a9f1-91c729acd097'},
            'fields': [{'id': '_id', 'type': 'int4'},
                       {'id': 'LGA Name', 'type': 'text'},
                       {'id': 'Tenure type', 'type': 'text'},
                       {'id': 'Very low income &lt;<span class=""math-container"">$603 per wk', 'type': 'numeric'},
                       {'id': 'Low income $603-$964 per wk', 'type': 'numeric'},
                       {'id': 'Moderate income $965-$</span>1446 per wk',                            'type': 'numeric'},
                       {'id': 'Total', 'type': 'numeric'},
                       {'id': '_full_count', 'type': 'int8'},
                       {'id': 'rank', 'type': 'float4'}],
            'limit': 5,
            'q': 'title:jones',
            'records': [],
            'resource_id': 'fec742c1-c846-4343-a9f1-91c729acd097'},
 'success': True}
</code></pre>

<p>I would suggest using Pandas, which can do a lot of the tedious work for you very easily. It can read straight from a JSON string (our <code>text</code> above). The issue is that is will parse it a little strangely.</p>

<p>There is no simple way to write this directly to a CSV file, because there are nested structures: e.g. under ""result"" there ""fields"" and then more values, and CSV files can't display that directly. You need to essentially <em>flatten</em> the structure yourself, perhaps decide what is important or what you want to can leave out.</p>

<p>You could take the JSON formatted <code>json_data</code> above and unpack it manually, removing nested parts, which means looking through the response and making your own Python dictionary with only single level i.e. no nesting. Let's say you do that and have a new <em>flattened</em> dictionary named <code>r</code>. Once you have done that, you can do the following using Pandas to write the CSV file:</p>

<pre><code>import pandas as pd

df = pd.read_json(r)
df.to_csv(""output.csv"")
</code></pre>
","3","2","45264","13013"
"62939","<p>An alternative to other (correct) answers: it seems the instructions columns are the same for both, so you could just add the <code>opt</code> columns to the dataframe as a new columns and write that. That way you have all information still (and you said you want <em>something like</em> <code>compiler</code>, <code>opt</code>  ;)</p>

<pre><code>sub_compiler[""opt""] = sub_opt.opt
</code></pre>

<p>Then write to disk as before:</p>

<pre><code>sub_compiler.to_csv(""compiler_opt.csv"")
</code></pre>
","1","2","45264","13013"
"64117","<p>You could have a look at the <a href=""https://www.automl.org/"" rel=""nofollow noreferrer""><strong>AutoML</strong></a> work, which offers a few different ways to optimise a model of parameter space and goes well beyond Sklearn. They even have a tool that wraps around Sklearn!</p>

<p>Here is a summary from their homepage:</p>

<p><a href=""https://i.stack.imgur.com/Tez07.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tez07.png"" alt=""enter image description here""></a></p>

<p>Here is <a href=""https://www.automl.org/automl/"" rel=""nofollow noreferrer""><strong>a longer description</strong></a>, and here is a <a href=""https://automl.github.io/HpBandSter/build/html/auto_examples/example_5_keras_worker.html"" rel=""nofollow noreferrer""><strong>full example based on Keras</strong></a>.</p>

<p>These libraries can essentially combine the best known practical methods of optimisation, such as:</p>

<ul>
<li>standard grid-search over parameters</li>
<li>Bayesian optimisation of parameters</li>
<li>a combination of the two methods: <a href=""https://automl.github.io/HpBandSter/build/html/quickstart.html"" rel=""nofollow noreferrer""><strong>HpBandSter (HyperBand on STERoids)</strong></a></li>
</ul>
","2","2","45264","13013"
"66602","<h1>Simple explanation with images</h1>

<p>We know that an activation is required between matrix multiplications to afford a neural network the ability to model non-linear processes.</p>

<p>A classical LSTM cell already contains quite a few non-linearities: three <code>sigmoid</code> functions and one hyperbolic tangent (<code>tanh</code>) function, here shown in a sequential chain of repeating (unrolled) recurrent LSTM cells:</p>

<p><a href=""https://i.stack.imgur.com/1u630.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1u630.png"" alt=""Sequential (unrolled) LSTM cells""></a>
<a href=""https://i.stack.imgur.com/ubHaZ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ubHaZ.png"" alt=""Symbol legend""></a>
<a href=""https://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""noreferrer"">Images borrowed from ""colah's blog""</a></p>

<p>So far this is just a single LSTM layer, and here we see that the cell output is <em>already the multiplication</em> of two activations (a sigmoid and a hyperbolic tangent). In this case, you could agree there is no need to add another activation layer after the LSTM cell.</p>

<p>You are talking about <em>stacked</em> layers, and if we put an activation between the hidden output of one layer to the input of the stacked layer. Looking at the central cell in the image above, it would mean a layer between the purple (<span class=""math-container"">$h_{t}$</span>) and the stacked layer's blue <span class=""math-container"">$X_{t}$</span>. You will notice then, that the output in this case, like with the sequential output, has already been activated, as it is the exact same output as the black left-to-right arrow (know as <span class=""math-container"">$f_{t}$</span>). What is more, the first thing the input would do in the stacked layer is to be passed through the sigmoids and hypoerbolic tangents of the forget/input/output gates.</p>

<p>So there are plenty of non-linearities being used, meaning it is unnecessary to add yet another between the stacked LSTM layers. You might like to think of it as simply applying two <code>ReLU</code> layers after a fully-connected layer. The results might be slightly different compared to just using one, but not much; as in your experiements with stacked LSTMs.</p>

<h1>Documentation</h1>

<p>If you look at the Tensorflow/Keras <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell"" rel=""noreferrer"">documentation</a> for LSTM modules (or any recurrent cell), you will notice that they speak of two activations: an (output) activation and a recurrent activation. It is here that you can decide which activation to use and the output of the entire cell is then already <em>activated</em>, so to speak. <a href=""https://pytorch.org/docs/stable/nn.html#lstm"" rel=""noreferrer"">PyTorch</a> doesn't seem to (by default) allow you to change the default activations.</p>

<h1>Real world stacked models</h1>

<p>Common applications of recurrent networks are found in NLP, for example the ELMo model. If you look through the <a href=""https://github.com/codertimo/ELMO-tf/blob/master/models/elmo.py#L14"" rel=""noreferrer"">network design code</a>, you see only basic LSTM cells being used, without additional activation laters. They only mention adding activations for the fully-connected layers (namely a ReLU) at the final output. </p>

<p>The first usage of stacked LSTMs (that I know of) was applied to <a href=""https://arxiv.org/abs/1303.5778"" rel=""noreferrer"">speech recognition (Graves et. al)</a>, and the authors also do not mention the need for activation layers between the LSTM cells; only at the final output in conjunction with a fully-connected layer.</p>
","12","2","45264","13013"
"67503","<p><strong>Yes, by default it does shuffle.</strong></p>
<p>Here is the <a href=""https://keras.io/api/models/model_training_apis/#fit-method"" rel=""nofollow noreferrer"">documentation</a>.</p>
<p>The default call signature:</p>
<pre><code>fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None,
    validation_split=0.0, validation_data=None, shuffle=True, 
    lass_weight=None, sample_weight=None, initial_epoch=0,
    steps_per_epoch=None, validation_steps=None, validation_freq=1,
    ax_queue_size=10, workers=1, use_multiprocessing=False)
</code></pre>
<p>where the description for <code>shuffle</code> is:</p>
<blockquote>
<p>shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). This argument is ignored when x is a generator. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None.</p>
</blockquote>
<p>This means <em><strong>it will not shuffle</strong></em> if you have set a value for <code>steps_per_epoch</code>.</p>
","4","2","45264","13013"
"67827","<p>Interesting question  :)</p>

<p>Using <a href=""https://en.wikipedia.org/wiki/Spectrogram"" rel=""nofollow noreferrer"">spectrograms</a> means you are essentially using images (of frequencies varying over time). I understand the content is in understood like the graph i.e. with axes <em>time</em> and <em>frequency</em>, but as far as the network knows, you are giving it black and white images; assuming your last dimension <code>(=1)</code> is the channels dimension.</p>

<p>You normally want to take the batch-norm over the features, so it could depend on what you see as a feature. Do you care about the shape of the peaks/troughs of the spectra, or the values encoded in the final channel? The meaning of <code>axis</code> when used in terms of <code>BatchNormalization</code> might be a little confusing. Have a look at <a href=""https://stackoverflow.com/questions/47538391/keras-batchnormalization-axis-clarification"">these explanations</a> and some of <a href=""https://forums.fast.ai/t/batchnormalization-axis-1-when-used-on-convolutional-layers/214"" rel=""nofollow noreferrer"">the points here</a>.</p>

<p>So as far as pure images go, I would recommend keeping the default <code>axis=-1</code>.</p>

<p>Remember that a 2d convolution operation is looking for spatial correlations over the image itself - the kernel slides from left to right, top to bottom. So mixing that idea with then taking batch norm over your second axis (<code>frequency</code>) is literally an orthogonal idea.</p>

<p>I don't know if it will work out, but I think it might make most sense if your spectrograms are aligned in time, that is they cover the same nominal time period and so for two of them with the same size, the x-axis has the same scale. Otherwise you would be taking batch-normalization over different time-frames and therefore stirring up temporal information between the samples.</p>
","2","2","45264","13013"
"67833","<p>Generally speaking, you need a string to be executed that contains placeholders to accept the values contained within the tuple. Also, what you are calling your ""tuple"" is actually a Python dictionary, but that isn't a problem.</p>

<p>From the <a href=""https://docs.python.org/3.8/library/sqlite3.html"" rel=""nofollow noreferrer""><strong>sqlite3 documentation</strong></a>, we can see there are two ways to correctly use the <a href=""https://docs.python.org/3.8/library/sqlite3.html#sqlite3.Cursor.execute"" rel=""nofollow noreferrer""><strong><code>execute()</code></strong></a> method:</p>

<p>an example command is given as:</p>

<blockquote>
<pre><code># This is the qmark style:
cur.execute(""insert into people values (?, ?)"", (who, age))

# And this is the named style:
cur.execute(""select * from people where name_last=:who and age=:age"", {""who"": who, ""age"": age})
</code></pre>
</blockquote>

<p>And the second example, the <em>named style</em> is exactly what would work with your situation. You can try the following, using the keys of the dictionary as the named placeholders:</p>

<pre><code>cmd = """"""insert into YOUR_TABLE values Marke=:Marke, Kilometerstand=:Kilometerstand, Erstzulassung=:Erstzulassung, Kraftstoffart=:Kraftstoffart, ""Leistung (PS)""=:""Leistung (PS)"", Getriebe=:Getriebe""""""
cur.execute(cmd, your_dictionary)
</code></pre>

<p>So each of the keys of your dictionary matches the name of the corresponding column in the target table.</p>

<p>Notice that I put the entire <code>cmd</code> in triple <code>""</code> to make a string literal that can itself contain the <code>""</code> character. I think it will be required, because you have a column name in your table that contains a space: <code>Leistung (PS)</code>.</p>
","0","2","45264","13013"
"68415","<p>I would personally read all files into pandas dataframes, do any cleaning or processing like that, then pass the underlying matrix out for actual training using the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.fit"" rel=""nofollow noreferrer""><strong>fit method</strong></a>. I don't think it is as hard as you fear - have a look at <a href=""https://www.kaggle.com/ahmethamzaemra/mlpclassifier-example"" rel=""nofollow noreferrer""><strong>this example here</strong></a>.</p>

<p>You need to have a single sample as a train matrix with shape <code>512 x 16</code> and a validation matrix that (I understand) would be a single value i.e. the class, which repeats the same value for all rows of EEG data. It matches the class name stored in the file name as well, right?</p>

<p>You will have 180 samples (one per file). You can split those into train and validation using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"" rel=""nofollow noreferrer""><strong><code>train_test_split</code></strong></a> from <code>sklearn</code>.</p>

<pre><code># imports
import os
import pandas as pd
from sklearn.model_selection import train_test_split

# List all files, stored e.g. in the folder called ""data""
files = sorted([file for file in os.listdir(""./data/"") if f.endswith("".txt"")])

# Read into pandas - might need to adapt based on structure of your txt files
dfs = [pd.read_csv(f) for f in files]
# Do any required cleaning on each dataframe - func takes and returns a df
clean_dfs = [func(df) for df in dfs]
</code></pre>

<p>Now get out the matrices using the dataframe <code>.values</code> attribute:</p>

<pre><code>data = [df.values[:, 0:-1] for df in clean_dfs]  # take all but final column (the label)
labels = [df.values[0, -1] for df in clean_dfs]  # take first items in class column
</code></pre>

<p>Now you can create the train/val split:</p>

<pre><code>X_tr, X_te, y_tr, y_te = create_train_test_split(data, values, test_size=0.2)
</code></pre>

<p>These can now be passed into the classifier's <code>fit</code> method~</p>

<pre><code>from sklearn.metrics import accuracy_score, confusion_matrix
clf = MLPClassifier()            # define the classifier
clf.fit(X_tr, y_tr)              # train the classifier
predictions = clf.predict(X_te)  # make predictions on the test set
</code></pre>

<p>You can compute some metrics such as an <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"" rel=""nofollow noreferrer""><strong>accuracy score</strong></a>:</p>

<pre><code>score = accuracy_score(y_te, predictions)
</code></pre>
","2","2","45264","13013"
"68453","<p>The simplest thing to try out is to put the information in an extra <em>channel</em> of the image.</p>

<p>So if you have RGB channels, you could add a four channel which would simply be the location information you have, repeated for every pixel.</p>

<p>This creates a lot of redundancy, of course, but means you can take any standard image classifier and it will still work.</p>
","5","2","45264","13013"
"68549","<p>How you do this will depend a lot on the tools you are already using and the way your model trains. If you are iterating e.g. over a grid of parameters to search, it can be trivial to parallelize. Other setups can be quite difficult.</p>

<p>There are some generic ways to do it using Python: <a href=""https://docs.python.org/3.8/library/multiprocessing.html"" rel=""nofollow noreferrer""><strong>check out the relevant documentation</strong></a>.</p>

<p>One thing to be aware of is that you really must use multiple <strong>processes</strong> to get real parallelism in Python. Using a <strong>thread</strong> in python does not speed things up for the actual computation. Threads are typically only useful in scenarios where you think you will be waiting for responses e.g. in networking/web programming (similar to asynchronous programming in Python).</p>

<p>In your case, doing bioinformatics, I would first check there aren't frameworks that will already do the parallel processing for you. Maybe you could use:</p>

<ul>
<li>A typical Deep Learning framework like <a href=""https://www.tensorflow.org/tutorials"" rel=""nofollow noreferrer""><strong>Tensorflow</strong></a> or <a href=""https://pytorch.org/tutorials/"" rel=""nofollow noreferrer""><strong>PyTorch</strong></a>, which do many things in parallel on the CPU and also offer GPU support, which can be even faster. You can see from <a href=""https://github.com/aymericdamien/TensorFlow-Examples"" rel=""nofollow noreferrer""><strong>all these examples using Tensorflow</strong></a>, that you can create many models, not just neural networks.</li>
<li>There are also other generic frameworks, such as <a href=""https://github.com/ray-project/ray"" rel=""nofollow noreferrer""><strong>Ray</strong></a>, which try to abstract the difficult parts away for you. Here is <a href=""https://towardsdatascience.com/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8"" rel=""nofollow noreferrer""><strong>an introduction tutorial for that tool</strong></a>.</li>
<li>Sci-Kit Learn's tools, some of which have an options to add more workers or can be used together with <a href=""https://joblib.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">joblib</a>.</li>
</ul>
","1","2","45264","13013"
"68659","<p>Change that line to:</p>

<pre><code>coords = df[[""Latitude"", ""Longitude""]].values
</code></pre>
","0","2","45264","13013"
"68996","<p>The index of your <code>test_ip</code> array goes from 0 to 16. So 17 is indeed too big (out of bounds).</p>

<p>You could add a try/except:</p>

<pre><code>for counter, img in enumerate(image_generator1):
    try:
        test_ip[counter] = img
    except IndexError as err:
        print(f""Filled the array with {counter + 1} images"")    # f-string requires &gt;= python 3.6
</code></pre>

<p>Or if you know the length of that generator, just use this:</p>

<pre><code>test_ip = np.array([im for im in image_generator1])
</code></pre>
","1","2","45264","13013"
"71406","<p>Don't worry - your hard-earned Python skills are still important ;)</p>

<p>Tableau is not a replacement - it is essentially a means of sharing your insights/findings. It is a wrapper around your normal toolkit (Pandas, Scikit-Learn, Keras, etc.). It can do some basic analysis (just using <em>basic</em> models from sklearn), but the powerful thing is it can deploy your models to allow people to run inference on stored data/new data, and then play around with it in an interactive dashboard.</p>

<p><a href=""https://youtu.be/nRtOMTnBz_Y?t=829"" rel=""noreferrer""><strong>Watch this video</strong></a> for a good overview of everything it can do, and how it connects to Python (and R/MatLab). There is just a bit of boiler plate code around your normal Python code.</p>

<p>Tableau also offer TabPy to set up a server, allowing nice deployments of your work, but in the end you need their desktop application to view the results (i.e. your customers need it to look at the results). This is not free: <a href=""https://www.tableau.com/pricing/individual"" rel=""noreferrer"">https://www.tableau.com/pricing/individual</a></p>

<p>In summary, I'd say Tableau is more of a business intelligence tool, allowing e.g. your non-data-scientist boss or other stakeholders to interactively explore the data and the results of your modelling. Similar to <a href=""https://docs.microsoft.com/en-us/power-bi/fundamentals/power-bi-overview"" rel=""noreferrer"">Microsoft's PowerBI</a>.</p>
","11","2","45264","13013"
"71492","<p>You start with a VGG net that is <a href=""https://keras.io/applications/#vgg19"" rel=""nofollow noreferrer""><strong>pre-trained on ImageNet</strong></a> - this likely means the weights are not going to change a lot (without further modifications or drastically increasing the learning rate, for example).</p>

<p>If you are expecting the performance to increase on a pre-trained network, you are performing <em>fine-tuning</em>. There is a section on <a href=""https://keras.io/applications/#fine-tune-inceptionv3-on-a-new-set-of-classes"" rel=""nofollow noreferrer""><strong>fine-tuning the Keras implementation of the InceptionV3 network</strong></a>, but the principals are the same: you should freeze some of the earlier <em>feature-extraction</em> layers, leaving only some of the final layers marked as <em>trainable</em>. This is the example given in the docs, where they add new layers to the base model, train <strong>only those layers</strong> for a while, then additionally unfreeze some of the base model.</p>

<pre><code># create the base pre-trained model
base_model = InceptionV3(weights='imagenet', include_top=False)   # &lt;--- leave out final layers!
# first: train only the top layers (which were randomly initialized)
# i.e. freeze all convolutional InceptionV3 layers
for layer in base_model.layers:
    layer.trainable = False

-&gt; run training for a few epochs
</code></pre>

<p>The some time later, unfreeze the part of the base model</p>

<pre><code># we chose to train the top 2 inception blocks, i.e. we will freeze
# the first 249 layers and unfreeze the rest:
for layer in model.layers[:249]:
   layer.trainable = False
for layer in model.layers[249:]:
   layer.trainable = True
</code></pre>

<p>Please looked at the full documentation for more details.</p>
","1","2","45264","13013"
"71493","<p>By having seen the results on the held-out test set, learning something and applying that to the model, you have essentially added some bias to your model. So if we are being strict, yes... You cheated a little bit  ;) </p>

<p>Without having a solid (meaning <strong><em>unbiased</em></strong>) validation metric, you cannot objectively compare the two results. You used prior information to make an improved modelling decision, which is great :)   However, that information flowed directly from your test data, which introduced the bias. </p>

<p>It can be difficult when you have limited data, I know. You could maybe look into data augmentation to synthetically increase your dataset. In your case, adding random rotations and mirrored images could help a great deal!</p>
","0","2","45264","13013"
"71495","<p>You could compare the relative values over time quite easily with a ""stacked"" graph, where each of the processes is a layer in the stack. In Pandas' plotting, it is referred to as an <code>area</code> plot.</p>

<p>Here is an example with some dummy data:</p>

<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

p1, p2, p3 = np.random.random(100), np.random.random(100), np.random.random(100)

df = pd.DataFrame([p1, p2, p3]).T
df.head(2)
# output
#              0         1         2
#    0  0.542490  0.099974  0.831589
#    1  0.988922  0.035026  0.752813

df.plot(kind=""area"", stacked=True); plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/7Dj7e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Dj7e.png"" alt=""stacked/area plot""></a></p>

<p>I think this might allow you to see the relative swings in each of the processes quite clearly - you could also shift the slightly to be in phase, which will make things easier.</p>
","0","2","45264","13013"
"71510","<p>Although there are valid points in the accepted answer, I believe it is incorrect in this case.</p>

<p>The timing differences mentioned were between the first epoch of training and the remaining epochs. The model and so the computational graph is compiled only once, when you call <code>model.compile()</code>, which is not part of the training itself.</p>

<p>The difference in timings is due to the data being generated and loaded into memory <strong>before and during</strong> the first epoch. Keras (or its backend) caches this data as much as it can, meaning all subsequent epoch train faster.</p>
","1","2","45264","13013"
"71571","<p>Those symbols you are talking about are <code>hex</code> values. They could actually be useful to keep in the dataset, as the <code>tcpdump</code> tool outputs them if the right options are set.</p>

<p>Despite that, you can replace occurrences of them using <code>pandas</code> and a simple regular expression. A regular expression that matches <code>hex</code> values is: <code>0[xX][0-9a-fA-F]+</code> - see a <a href=""https://regex101.com/r/VIdGBg/1"" rel=""nofollow noreferrer"">live demo of that here</a>.</p>

<p>I downloaded one of the CSV files and did it like this:</p>

<pre><code>import pandas as pd

filename = ""./UNSW-NB15_1.csv""
df = pd.read_csv(filename, nrows=5000)     # read first 5000 lines
df.to_html(""with_hex.html"")                # open in a browser to inspect (e.g. line 211)
</code></pre>

<p>Now use the DataFrame <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html"" rel=""nofollow noreferrer""><strong><code>replace</code></strong></a> and set the regex flag, and insert whatever you want to replace the matches with (here <code>""NaN""</code>).</p>

<pre><code>clean = df.replace(""0[xX][0-9a-fA-F]+"", value=""NaN"", regex=True)
</code></pre>

<p>I used this to have a look at the cleaned versions of the data quickly in a browser (notice line 211):</p>

<pre><code>clean.to_html(""clean.html"")
</code></pre>

<hr>

<p>You can use the same trick to replace almost anything, so your fields with <code>'-'</code> can be replaced with:</p>

<pre><code># ""inplace"" means it overwrites the values, no new dataframe is returned
clean.replace(""-"", ""NaN"", regex=False, inplace=True)
</code></pre>
","5","2","45264","13013"
"71596","<p>It really only tracks the value of the metric you selected, there is no tolerance option. In <a href=""https://keras.io/callbacks/#modelcheckpoint"" rel=""nofollow noreferrer""><strong>the relevant documentation</strong></a>, the definition is given:</p>

<blockquote>
  <p>save_best_only: if save_best_only=True, the latest best model
  according to the quantity monitored will not be overwritten.</p>
</blockquote>

<p>In your case, combing the callback with the <a href=""https://keras.io/callbacks/#earlystopping"" rel=""nofollow noreferrer""><strong>EarlyStopping callback</strong></a> would be the best option. This callback <em>does</em> offer some kind of tolerance, i.e. it requires the improvement of the metric to be at least as good as <code>min_delta</code> to be considered an improvement.</p>

<hr>

<p>You could combine both callbacks with something like this:</p>

<pre><code>from keras.callbacks.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor='val_loss', min_delta=0.05, patience=3, verbose=1, mode='auto')
checkpoints = ModelCheckpoint('model_weights.h5', monitor='val_acc', save_best_only=True)

my_callbacks = [early_stop, checkpoints]
...
model.fit(..., callbacks=my_callbacks)
</code></pre>
","1","2","45264","13013"
"71794","<p>I assume you a list of the filenames called <code>movie_images</code></p>

<pre><code># Could get filenames with:
# import os;  movie_images = os.listdir(""./folder/with/images/"")
movie_filenames = [""11_lion_king.jpg"", ""22_avengers.jpg""]
</code></pre>

<p>First create a mapping between the ID values and the filenames:</p>

<pre><code># Use the ""_"" to split the filename and take the first items, the ID
mapping = {f.split(""_"")[0]: f for f in movie_filenames}    # &lt;-- a dictionary-comprehension
</code></pre>

<p>Now add a column of some empty values (whatever you like) that will hold the <code>movie_image</code> values:</p>

<pre><code>data_movie[""movie_image""] = pd.Series()    # will be filled with NaN values until populated
</code></pre>

<p>Now iterate over this mapping, inserting the movie filenames for the correct movie IDs:</p>

<pre><code>for movie_id, movie_image_filename in mapping.items():
    df.loc[df.movie_id == movie_id, ""movie_image""] = movie_image_filename
</code></pre>

<p>This should produce the output dataframe you described.</p>

<p>As a side note (in case you are ever tempted): never load the actual images into a  pandas dataframe. It is best to load them as NumPy arrays or something similar. Pandas DataFrames are in essence just annotated NumPy arrays anyway.</p>
","2","2","45264","13013"
"71872","<p>It is hard to know what is happening from just that screenshot and no code.</p>

<p>The training and validation plots are usually separated on the page, not lines on the same graph.</p>

<p>If you are using Tensorflow 2.0, there is a <a href=""https://github.com/tensorflow/tensorboard/issues/2911"" rel=""nofollow noreferrer""><strong>known issue</strong></a>, regarding the syncing of TB and the <code>tfevent</code> file (where logs are stored). A couple of things to try:</p>

<ol>
<li>Try adding the <code>TensorBoard</code> callback with the argument: <code>profile_batch=0</code></li>
<li>Try restarting the tensorboard a few times... reading can fail or be very slow to load</li>
</ol>

<p>I am referring to the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard"" rel=""nofollow noreferrer""><strong><code>tensorflow.keras</code> API</strong></a>:</p>

<pre><code>tf.keras.callbacks.TensorBoard(
    log_dir='logs', update_freq='epoch',
    profile_batch=0,                      # &lt;-- default value is 2
)
</code></pre>
","3","2","45264","13013"
"71902","<p><code>(0, 0)</code>  is top left.</p>

<p>Here an <a href=""https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/"" rel=""nofollow noreferrer""><strong>a helpful blog</strong></a> that goes through all the features in the output vector.</p>

<p>This is common in image processing. There are a few reasons that this is the convention in computer vision. <a href=""https://gamedev.stackexchange.com/questions/83570/why-is-the-origin-in-computer-graphics-coordinates-at-the-top-left""><strong>Check here for a list</strong></a>, the accepted answer states:</p>

<blockquote>
  <p>This is caused in the history. Early computers had Cathode Ray Tubes (CRTs) which ""draw"" the image with a cathode ray from the upper left corner to the lower right.</p>
</blockquote>
","2","2","45264","13013"
"72015","<p>Your suggested method should work, assuming the two column are in the same dataframe with the same timestamps as the  index. This means that they are already aligned in time and therefore the <em>time</em> is implicitly encoded.</p>

<p>If there are any missing datapoints, you could impute them (<a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html"" rel=""nofollow noreferrer""><strong>fill them</strong></a>) with something like the previous value, which is usually best for time-series data. Replacing with e.g. the mean of the entire time-series might be misleading because there are temporal fluctuations/cycles involved.</p>

<pre><code>df.fillna(method='ffill')
</code></pre>

<p>You could also simply plot them to see which kind of correlation value you might expect, doing something like this:</p>

<pre><code>import matplotlib.pyplot as plt    # &lt;-- needed if running in script (not a Jupyter notebook)
df[[""sensor1"", ""sensor2""]].plot()
plt.plot()
</code></pre>
","0","2","45264","13013"
"72026","<p>I would recommend the pre-processed datasets for language translation in the Stanford NLP group: <strong><a href=""https://nlp.stanford.edu/projects/nmt/"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/projects/nmt/</a></strong></p>

<p>There are three datasets with sizes 0.13Mb, 4.5Mb and 15Mb - something for everybody  :)</p>

<hr>

<p>Translation is something that is very difficult to solve for normal FF-networks and something that transformers dramatically improved due to attention mechanisms.</p>

<p>Also check out the related explanations of those ideas in <a href=""https://nlp.seas.harvard.edu/2018/04/03/attention.html"" rel=""nofollow noreferrer""><strong>The Annotated Transformer</strong></a> and the <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer""><strong>Illustrated Transformer</strong></a></p>
","1","2","45264","13013"
"72034","<p>Instead of using the <code>score</code> method on your trained model, you should use the <code>predict</code> method.</p>

<p>You can then pass the results into the confusion matrix function from sklearn:</p>

<pre><code>from sklearn.metrics import confusion_matrix
y_pred = svmObject.predict(X)
cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight,
                      labels=labels, normalize=normalize)
</code></pre>

<p>There is also a nice function called <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html#sklearn.metrics.plot_confusion_matrix"" rel=""nofollow noreferrer""><strong>plot_confusion_matrix</strong></a>:</p>

<pre><code>from sklearn.metrics import plot_confusion_matrix

plot_confusion_matrix(svmObject, testXtr, testY)
plt.show()
</code></pre>
","2","2","45264","13013"
"72142","<p>If RAM memory is a problem during training, and you have run for 1500 episodes, it would suggest to me that you are saving some information in each episodes (e.g. appending to a <code>list</code>). That will slowly add up over time and cause a crash.</p>

<p>After a quick look at your code, it looks like the <code>scores</code> and <code>ep_history</code> grow with every loop. You could consider writing that information to disk e.g. to a <code>json</code>, file or <code>pickle</code> object, with something like this</p>

<pre><code>import pickle

if episode % 1000 == 0:
    with open(f""./results_dir/scores_{episode}.pkl"", ""wb"") as file_handle:
        pickle.dump(scores, file_handle)
</code></pre>

<p>You should of course retain the amount of history required for your <code>print()</code> statements and computing average scores over time.</p>

<hr>

<p>You rarely need to worry about freeing up some RAM yourself, as Python (and then afterwards, the operating system itself) will start to allow memory to be overwritten. The results is the same, they are essentially being deleted.</p>
","2","2","45264","13013"
"72397","<p>The Pearson correlation coefficient does indeed quantify the <strong>linear relationship</strong> between two variables.</p>

<p>Have a look at one of the many mathematical formulas to compute it, based on a sample of data from two variables <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span>:</p>

<blockquote>
  <p><a href=""https://i.stack.imgur.com/tI7Rj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tI7Rj.png"" alt=""Pearson correlation coefficient""></a></p>
</blockquote>

<p>I like to read this loosely in terms of variance (or <em>spread</em> across each variable). It is asking, how do <span class=""math-container"">$x$</span> and <span class=""math-container"">$y$</span> scale together? This is the covariance, top part of the fraction. This is scaled by the product of their individual standard deviations, like <strong>normalizing the covariance by the magnitude of the individual variations</strong> <span class=""math-container"">$\rightarrow$</span> bringing the final value into the range <span class=""math-container"">$[-1, +1]$</span>.</p>

<p>The <a href=""https://en.wikipedia.org/wiki/Pearson_correlation_coefficient#Geometric_interpretation"" rel=""nofollow noreferrer""><strong>geometric interpretation</strong></a> is also quite intuitive and worth reading.</p>

<p>You could also notice the power of the values. The numerator has <span class=""math-container"">$x^1 * y^1$</span> (so a like a squared term), and the denominator multiplied <strong>two</strong> terms that are each the square root of a squared term, also meaning <span class=""math-container"">$x^1 * y^1$</span>. So the final dimensionality is a linear value, as we divide one <em>squared terms</em> by another. <em>(This naïve argument of course neglects the units of <span class=""math-container"">$x$</span> and <span class=""math-container"">$y$</span>.)</em></p>

<hr>

<p>Here is a nice comparison of many values to gain an intuition - try imagining the linear line of best fit through each of these sets of points, and then how far away the points are from that line. This further away they are, the larger the value of the Pearson correlation coefficient <span class=""math-container"">$\rho$</span>. As with normally regression, the sign of the gradient also gives a sign the <span class=""math-container"">$\rho$</span>:</p>

<p><a href=""https://i.stack.imgur.com/Rpor8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rpor8.png"" alt=""enter image description here""></a>
<a href=""https://www.spss-tutorials.com/pearson-correlation-coefficient/"" rel=""nofollow noreferrer"">Image source</a></p>

<p>What value do you think the Pearson Correlation would be for a group of points almost all lying on a vertical line parallel to the y-axis?  :)</p>

<p><a href=""https://stats.stackexchange.com/questions/32464/how-does-the-correlation-coefficient-differ-from-regression-slope""><strong>Compare correlation to regression here</strong></a>.</p>
","1","2","45264","13013"
"72421","<p>In Python 3, you shouldn't really specify the <code>np.string_</code> dtype, as it is left there for backwards compatibility with Python 2. The <code>S</code> type you see using <code>np.dtype</code> is a map to the <code>bytes_</code> type, a zero-terminated string buffer, which shouldn't be used.</p>
<p>The <code>S</code> just means <code>string</code> and the number gives the number of bytes.</p>
<pre><code>In [1]: s = &quot;&quot;        # start with an empty string                                                                                                

In [2]: for i in range(5):    # make the string larger
            s += str(i) 
            a = np.array([s], dtype=np.string_) 
            print(f&quot;{a}\t{a.dtype}&quot;) 
                                                                                                                       
    [b'0']  |S1
    [b'01'] |S2
    [b'012']    |S3
    [b'0123']   |S4
    [b'01234']  |S5
    [b'012345'] |S6
</code></pre>
<p>For python 3 you should instead use <code>np.unicode</code>:</p>
<pre><code>In [1]: a = np.array([&quot;hello&quot;, &quot;world&quot;], dtype=np.unicode)                                                             

In [2]: type(a)                                                                                                       
Out[2]: numpy.ndarray

In [3]: b.dtype                                                                                                       
Out[3]: dtype('&lt;U5')
</code></pre>
<ul>
<li><code>&lt;</code> means <a href=""https://en.wikipedia.org/wiki/Endianness"" rel=""nofollow noreferrer""><em>little-endian</em></a></li>
<li><code>U</code> means a unicode string</li>
<li><code>5</code> relates to the number of bytes used to hold the string. If you had really long strings, that number would then increase.</li>
</ul>
<p>Have a look at <a href=""https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html"" rel=""nofollow noreferrer"">this documentation</a>, which states:</p>
<blockquote>
<p>Note on string types:</p>
<p>For backward compatibility with Python 2 the S and a typestrings remain zero-terminated bytes and np.string_ continues to map to np.bytes_. To use actual strings in Python 3 use U or np.unicode_.</p>
</blockquote>
","1","2","45264","13013"
"72476","<p>First let's see some data: typical cough data for someone with pneumonia:</p>

<p><a href=""https://i.stack.imgur.com/EFiMz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EFiMz.png"" alt=""enter image description here""></a></p>

<p><a href=""https://www.researchgate.net/publication/237069878_Cough_Sound_Analysis_Can_Rapidly_Diagnose_Childhood_Pneumonia"" rel=""nofollow noreferrer"">image source</a>. This source paper also contains more information on processing cough data.</p>

<hr>

<p>The <strong><em>simplest</em></strong> way I can think of would be to analyse and extract the average/base background noise of an many samples as you can, then simply <strong>subtract the background noise from the entire sound wave</strong>. This background noise is essentially the area outside of the <em>start</em> and <em>end</em> lines indicated on the image above. </p>

<p>Outline of steps will need to:</p>

<ol>
<li>define the start and end of the cough itself (as per the image)</li>
<li>measure the average magnitude of the background: e.g. <code>np.mean(np.abs(background))</code></li>
<li>subtract the average background noise from the sound clip</li>
</ol>

<p>If you can plot the data again, you should notice that the lines outside of <em>start</em> and <em>end</em> are almost flat (but not exactly flat, we only subtracted the <em>average</em> background noise)</p>

<h2>Links:</h2>

<ul>
<li><p>Have a look at <a href=""https://stackoverflow.com/questions/51431859/how-do-i-get-the-frequency-and-amplitude-of-audio-thats-being-recorded-in-pytho""><strong>this answer</strong></a>, which goes through similar steps in more detail</p></li>
<li><p><a href=""https://realpython.com/playing-and-recording-sound-python/"" rel=""nofollow noreferrer""><strong>Here is a great list of libraries</strong></a> you can use to work with sound data in Python</p></li>
<li><p><a href=""https://alphamanual.audacityteam.org/man/Scripting"" rel=""nofollow noreferrer""><strong>Audacity (software package)</strong></a> has a lot of tools and <a href=""https://alphamanual.audacityteam.org/man/Scripting"" rel=""nofollow noreferrer"">is scriptable</a>, using Python</p></li>
</ul>
","1","2","45264","13013"
"72484","<p>Assuming that example is working for you, trying to write the data every 3 seconds, you need to just change the scheduling to be</p>

<pre><code>schedule.every(1).hour.do(write_to_excel)
</code></pre>

<p>You are currently writing the data at each interval to the same file, so you will overwrite the file every time. You could do a few things here:</p>

<ul>
<li>Open the excel file (e.g. into a pandas DataFrame), <code>append</code> the new data and save it all back to disk. This is pretty inefficient though and will have problems once you have a lot of data.</li>
<li>Write the data to a database, extending it every hour. This is the most professional solution.</li>
<li>Write a new file to disk each hour, including e.g. the timestamp of the hour in the filename to make each file unique. This is simple and just means you iterate over the files one-by-one when reading them later to do analysis or plotting etc.</li>
</ul>

<p>You could change your function to be like this, implementing the first option above:</p>

<pre><code>def request_data_and_save(excel_file: str = ""air_quality.xlsx""):
    request = requests.get(""https://api.waqi.info/feed/paris/?token=?"")
    request_text = request.text
    JSON = json.loads(request_text)

    filterJSON = {
        'time': str(JSON['data']['time']['s']),
        'co': str(JSON['data']['iaqi']['co']['v']),
        'h': str(JSON['data']['iaqi']['h']['v']),
        'no2': str(JSON['data']['iaqi']['no2']['v']),
        'o3': str(JSON['data']['iaqi']['o3']['v']),
        'p': str(JSON['data']['iaqi']['p']['v']),
        'pm10': str(JSON['data']['iaqi']['pm10']['v']),
        'pm25': str(JSON['data']['iaqi']['pm25']['v']),
        'so2': str(JSON['data']['iaqi']['so2']['v']),
        't': str(JSON['data']['iaqi']['t']['v']),
        'w': str(JSON['data']['iaqi']['w']['v']),
    }

    # Get a writer and append data to the file
    with pd.ExcelWriter(excel_file, mode=""a"") as xl:
        df = pd.DataFrame.from_dict(filterJSON)
        df.to_excel(xl)

if __name__ == ""__main__"":
    schedule.every().hour.do(write_to_excel)
    while True:
        schedule.run_pending()
</code></pre>

<p><em>NOTE: this code is not tested</em></p>
","0","2","45264","13013"
"72654","<p>The best way to know is to train two epochs. The first epoch often takes longest, because data loading takes place with some caching.</p>

<p>The second epoch will give you an accurate time for each epoch.</p>

<hr>

<p>Things to help the guesses that you might include in your question:</p>

<ul>
<li>What are the specifications of <code>p2.xlarge</code>?</li>
<li>Where are your images stored? on the instance EBS storage? s3 storage? A separate file-system?</li>
<li>In which format is your data stored?</li>
<li>Perhaps the number of parameters in the model, but that can also be misleading!</li>
</ul>
","0","2","45264","13013"
"72712","<p>You could try just using your own solution, replacing <code>np.float</code>:</p>

<pre><code>dtype={'5': pd.to_numeric, '37': np.float, ....}
</code></pre>

<p>Or make a function that does what you want:</p>

<pre><code>def convert(val):
    try:
        return np.float(val)
    except:
        return float(val)
    except:
        return pd.to_numeric(val)

    return val
</code></pre>

<p>Then:</p>

<pre><code>dtype={'5': convert, '37': np.float, ....}
</code></pre>

<p>That is a bit exaggerated, but you get the idea  :)</p>
","0","2","45264","13013"
"72869","<ul>
<li>Reduce the size e.g. using <code>cv2.resize()</code></li>
<li>Compress the image (it is not lossless) e.g. <code>cv2.imencode()</code></li>
<li>Lower the frame rate</li>
<li>Use lower precision - images are <code>uint8</code> when loaded, but the deep learning frameworks use <code>float32</code> by default. You could try <code>float16</code> or mixed precision.</li>
</ul>

<p>Using JPEG compression has been shown to be fairly good in terms of the reduction in memory and minimal loss of performance. Have a look a <a href=""https://arxiv.org/abs/1604.04004"" rel=""nofollow noreferrer""><strong>this research</strong></a>.</p>

<p>You could also drop the frame rate, so say 10 FPS. The actually values could be computed based on the expected velocity of the moving objects -> do you <em>really require</em> 24 FPS for the task?</p>

<p>Otherwise, the hardware you are using will determine which steps to take afterwards. Memory, number of operations, inference speed etc. will change how you optimise the process.</p>

<hr>

<p>You mentioned ""dataframe"", so I will just point out that using Pandas Dataframes to hold raw image data, whilst looking easy, it generally very inefficient due to the number of data-points involved (pixels), and the fact that Pandas DataFrames are essentially annotated NumPy arrays - the annotations take a lot of space. Better to load into pure numpy arrays and use OpenCV for things suchs as making gray-scale (black and white) images from RGB, resizing them, normalising pixel values, and so on.</p>
","1","2","45264","13013"
"72905","<p>Your <code>images_dir</code> actually seems to be the path to a single image... but nevertheless,</p>

<p>I would simply create a single number array with shape: <code>(num_images, height, width, channels)</code> by doing the following:</p>

<pre><code>import os
import numpy as np

# Root directory holding all images (I recommend removing the space in ""Atom projects"")
images_dir = ""/home/awesome_ruler/Documents/Atom projects/Compression_enc/Images/IMG/""

# Number of images you want to load
N= 125

# Get all paths and take the first N
n_image_paths = sorted([f.path for f in os.scandir(data_root)])[:N]

# Load the images using on of the variants of loading images
images = np.array([Image.open(f) for f in n_image_paths])

# Be careful with variants: the order of channels is different for different methods!
# images = np.array([plt.imread(f) for f in n_image_paths])    # Matplotlib
# images = np.array([cv2.imread(f) for f in n_image_paths])    # OpenCV
</code></pre>

<p>Now images can be passed directly to your model:</p>

<pre><code>autoencoder.fit(x=images,
                epochs=50, ...)
</code></pre>
","1","2","45264","13013"
"72973","<blockquote>
  <p>other than using a cronjob because it will be unnecessary to pay for the time and resources when it will not be in use</p>
</blockquote>

<p>It sounds like you want to use cloud compute.</p>

<p>I would suggest looking at the <strong>Events</strong> offered by AWS in their CloudWatch tool. You can set up a job and then schedule it to run at any time you like - they even have a cron-job interface to specify something like: <em>every hour</em>.</p>

<p>Have a look at <a href=""https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html"" rel=""nofollow noreferrer""><strong>relevant AWS documentation for Events</strong></a></p>

<p>You could probably set this up based on a single AWS instance, checking that you meet three requirements for your use case:</p>

<ol>
<li>It has enough network bandwidth to receive your BigQuery results in a reasonable time</li>
<li>It has the required hardware to execute your model (e.g. is a GPU necessary?)</li>
<li>It can hold or link to your own <em>mySQL</em> database</li>
</ol>

<p>There are probably even nice AMI (Amazon Machine Image - with pre-installed software) that will be ready to go for your use case!</p>
","0","2","45264","13013"
"73290","<p>What do you mean exactly with <em>competitive</em> as a data scientist? Unfortunately, many employers will have different expectations of someone they hire to be a Data Scientist, so there isn't a single answer!</p>

<p>In any case, I think it is a good idea to know three components to be <em>effective</em> with databases:</p>

<ol>
<li><p><strong>Managing a connection</strong>: how to create and connect to a database using a simply library in Python or R. This sort of workflow should look familiar to you:</p>

<pre><code>import mysql
connection = mysql.connector.connect(
                 host=""127.0.0.1"",
                 user=""mj_whales"",
                 passwd=""somethingsecret"",
             )

query = ""CREATE DATABASE my_table""
cursor = connection.cursor()
cursor.execute(query)
</code></pre></li>
<li><p><strong>Getting data</strong>: understand basic syntax of a standard SQL-style query, for example: </p>

<pre><code>FROM my_table SELECT my_feature WHERE my_other_feature &gt; 0
</code></pre></li>
<li><p><strong>Writing data</strong>: understand how to insert data efficiently. This will depend on the database type e.g. document database, SQL-style, InfluxDB for time-series data. </p></li>
</ol>

<p>Here is <a href=""https://realpython.com/python-sql-libraries/"" rel=""nofollow noreferrer""><strong>a really good introduction to databases with Python</strong></a> that might help you understand your current strengths and weaknesses on the subject.</p>
","3","2","45264","13013"
"73477","<p>Your <code>month</code> column is really a partial date (i.e. the <em>year</em> is missing), but you could still just convert that column to a proper date format and then sort.
You will have to specify a format, so the parser knows roughly what to do. </p>

<pre><code>df[""date""] = pd.to_datetime([""20-Apr""], format=""%d-%b"")
df.sort_values(""date"")   # ascending by default
</code></pre>

<p>Without any year information, it will add the year as 1900.</p>

<hr>

<p>If you want to add e.g. this year, you could do the following, adding <code>12*20</code> months to the new date column:</p>

<pre><code>df[""fixed_date""] = df.date.apply(lambda x: pd.tseries.offsets.shift_month(x, 12*20))
</code></pre>
","4","2","45264","13013"
"73600","<p>Logistic regression is a standard method of performing <em>binary classification</em>, which matches your task here. Categorical variables can be dealt with, depending on the model you choose.</p>

<p>You can see from the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"" rel=""nofollow noreferrer""><strong>Scikit-Learn documentation on logistic regression</strong></a>, that your data only really needs to be of a certain shape: <code>(num_samples, num_features)</code>. It might ignore the columns that are non-numerical, so you should convert e.g. strings to class IDs (e.g. integers) - see below.</p>

<hr>

<p>Computing the correlation can make sense for categorical values, but to compute these, you need to provide numerical values; strings like ""bbc.com"" or ""US"" won't work. </p>

<p>You can map each of the values to a numerical value and make a new column with that data using <code>pd.factorize</code> like this:</p>

<pre><code>df[""Country_id""] = pd.factorize(df.Country)[0]   # taking the first return element: the ID values
df[""Name_id""] = pd.factorize(df.Name)[0]
</code></pre>

<p>You don't need to do it really for the <code>approved</code> and <code>political</code> columns, because they hold <code>boolean</code> values, which are seen by Python as <code>0</code> and <code>1</code> for <code>False</code> and <code>True</code>, respectively.</p>

<p>Now you can do something like this to see a correlation plot:</p>

<pre><code>import matplotlib.pyplot as plt    # plotting library: pip install matplotlib

# compute the correlation matrix
corr_mat = df[[""Name_id"", ""Country_id"", ""approved"", ""political""]].corr()

# plot it
plt.matshow(corr_mat)
plt.show()
</code></pre>
","0","2","45264","13013"
"73661","<p>The code looks OK to me... I can only think that you have somehow exhausted the ImageDataGenerator object, because you use the same on during training and then for the prediction. (I do realise this should never happen).</p>

<p>To rule this out, you could create a separate generator for testing.</p>

<p>This would also have the advantage of using some of your data for a hold-out test set, so your final predictions are <em>out-of-sample</em>, giving you a better idea of performance  :)</p>

<p>For example, you might change your code to be like this:</p>

<pre><code>validation_directory = 'D:\D_data\Rock_Paper_Scissors\Test'
holdout_directory =  'D:\D_data\Rock_Paper_Scissors\Holdout'   # make this new directory
</code></pre>

<p>Move e.g. 10 files into the new holdout directory</p>

<pre><code>validation_datagen = ImageDataGenerator(rescale= 1./255)
validation_generator = validation_datagen.flow_from_directory(
    validation_directory,
    target_size = (28,28),
    class_mode = 'categorical'
    )

# new generator for predictions
holdout_generator = validation_datagen.flow_from_directory(
    holdout_directory,
    target_size = (28,28),
    class_mode = 'categorical'
    )


model.fit_generator(training_generator, epochs=20, validation_data=validation_generator)
</code></pre>

<p>Set the number <code>N</code> accordingly, e.g. to the 10 files in the new directory</p>

<pre><code>predict = model.predict_generator(holdout_generator, batch_size=N)

print(predict.shape)    # this should return the shape of the numpy array: (N, H, W, C)
print(predict()        # the actual rpedictions
</code></pre>
","0","2","45264","13013"
"73790","<p>The idea of tracking missing data can be useful, so you know which data-points were imputed or dropped for training a model. It can give you an idea of feature importance/reliability.</p>

<p>However, there are many way to do this and if your are using e.g. a Pandas DataFrame, the <code>MissingIndicator</code> class is redundant for the single purpose of tracking missing values in my opinion.</p>

<p>The same can be achieved as follows. Say I start with this dataframe:</p>

<pre><code>import pandas as pd
import numpy as np

In [1]: df                                                                                                           
Out[1]: 
            A         B          C         D
0    1.095564 -0.225533   0.441428  0.099792
1    1.198053  0.523837   -1.53928  0.871108
2         NaN  0.336165  NOT_FOUND -1.881777
3   -0.077794  0.175203   -1.76324  1.172351
4   -1.167858  0.340200   0.369765       NaN
5    0.514393 -0.045929   0.771916  0.130821
6   -0.065623  0.978825  -0.668706 -0.703892
</code></pre>

<p>So there are a few possible missing values:</p>

<pre><code>In [2] missing_vals = [""NOT_FOUND"", np.NaN]

In [3] missing_mask = df.isin(missing_vals)   # boolean mask of True where missing values found

In [4]: missing_mask                                                                                                 
Out[4]: 
       A      B      C      D
0  False  False  False  False
1  False  False  False  False
2   True  False   True  False
3  False  False  False  False
4  False  False  False   True
5  False  False  False  False
6  False  False  False  False
</code></pre>

<p>But as this is a DataFrame, you have all the power of that if you need it. See how many values are missing per feature:</p>

<pre><code>In [5]: missing_mask.sum()
Out[5]: 
A    1
B    0
C    1
D    1
dtype: int64
</code></pre>

<p>Replace all missing values with a single marker e.g. ""MISSING""</p>

<pre><code>In [6]: df.where(~missing_mask, ""MISSING"")
Out[6]: 
           A          B         C          D
0    1.09556  -0.225533  0.441428  0.0997919
1    1.19805   0.523837  -1.53928   0.871108
2    MISSING   0.336165   MISSING   -1.88178
3 -0.0777938   0.175203  -1.76324    1.17235
4   -1.16786     0.3402  0.369765    MISSING
5   0.514393 -0.0459287  0.771916   0.130821
6 -0.0656233   0.978825 -0.668706  -0.703892
</code></pre>

<hr>

<p>If you want to understand your missing data, I suggest having a look at this great package called <a href=""https://github.com/ResidentMario/missingno"" rel=""nofollow noreferrer""><strong><code>missingno</code></strong></a>, which was built for that purpose  :)</p>
","1","2","45264","13013"
"74507","<p>You say you have managed to make your data stationary, so I would probably say an additive model is your best starting point. You could simply plot your stationary data and check that the variance doesn't increase with the nominal values. High variance will mean higher errors for a linear regression.</p>

<p>When we have a basic regression model, like the following:</p>

<p><span class=""math-container"">$$
y = \beta_1x_1 + \beta_2x_2 + \epsilon
$$</span></p>

<p>the residual error <span class=""math-container"">$\epsilon$</span> is hypothesized to be constant (assuming the model itself is accurate). <span class=""math-container"">$\epsilon$</span> should not get larger when there are larger values for the covariates <span class=""math-container"">$x_1$</span> <span class=""math-container"">$x_2$</span>. In other words, you expect <a href=""https://en.wikipedia.org/wiki/Homoscedasticity"" rel=""nofollow noreferrer"">homoscedasticity</a>: that the error term is the same across all values of the model covariates. This is what you will easily spot on the plot of your processed (stationary) data.</p>

<p>If you made you time series stationary by taking the logarithms (a.k.a <em>differencing</em>), then an additive model of the <code>log</code>-ed variables would almost correspond to a multiplicative model.</p>

<p>Just to be clear, if you still seem to have <em>heteroscedasticity</em> with <span class=""math-container"">$\epsilon$</span> varying greatly, this might imply that your model itself is ill-formed e.g. that an important factor is missing.</p>
","1","2","45264","13013"
"74832","<p>Seems you have a seasonal correlation - every seventh day. You can look at the <a href=""https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_sarimax_stata.html"" rel=""nofollow noreferrer""><strong><code>SARIMAX</code> models in <code>statsmodels</code></strong></a> to try to compensate for that. It adds a fourth component to your ARIMA, so on top of <code>p</code>, <code>d</code> and <code>q</code>, you get a fourth term, <code>s</code>. Here is <a href=""https://machinelearningmastery.com/time-series-seasonality-with-python/"" rel=""nofollow noreferrer""><strong>a walkthrough</strong></a> on how to work with that.</p>

<p>Using an ensemble of models does help to remove noise a little further, as mentioned by <a href=""https://datascience.stackexchange.com/a/74821/45264"">@ThomasG</a>. This is used in many modelling situations to eek out the last few accuracy points, but won't solve an underlying modelling issue.</p>

<p>Maybe you could create a stratified train/validation/test split, such that the model is trained (and then evaluated) on all kinds of web traffic throughput: increasing, decreasing, stable. Stratification really just means that those three phases are equally represented in the splits you create.</p>
","1","2","45264","13013"
"74999","<p>You could use Tensorboard, via the <a href=""https://github.com/lanpa/tensorboardX"" rel=""nofollow noreferrer""><strong>tensorboardX</strong></a> interface.</p>

<p>That allows you to load models from PyTorch (and Chainer, MXNet etc.) into Tensorboard. This will then show you the full graph interactively.</p>

<p>From the homepage:</p>

<p><a href=""https://i.stack.imgur.com/Ws9C6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ws9C6.png"" alt=""enter image description here""></a></p>
","0","2","45264","13013"
"75180","<p><strong>No, it does not make sense to do this</strong>.</p>

<p>You model has learned how to map one input space to another, that is to say it is itself <em>function approximation</em>, and will likely not know what to for the unseen data.</p>

<p>By not performing the same scaling on the test data, you are introducing systematic errors in the model. This was pointed out in the comments by nanoman - see that comment for a simple transformation example.</p>

<p>To exaggerate the point, imagine you have a language model, translating text from English to French. You apply a single transformation on your English data: you first translate it to Spanish. Now the model is trained to translate Spanish to French, with some accuracy. Now you move on to the  test data - still in English - and you do not apply the same transformation as you did to your training data. You are asking the model to translate directly from English to French, instead of Spanish to French, and it is obvious that the results won't be good.</p>

<p>In principal, this idea is the same as with any other model and transformations, just that the impact might not always be so visible i.e. you might get really lucky and not notice a large impact.</p>

<p>The language model might have learnt some elementary linguistics common to all three languages (e.g. overlapping vocabulary or sentence structuring), but we cannot expect the model to perform well, translating English to French.</p>

<h2>Practical Note</h2>

<p>You should only compute the transformation statistics (e.g. mean and variance for normalisation) <strong>only on training data</strong> and use these values to then transform the training data itself, and then the same values to transform the test data. </p>

<p>Including the test dataset in the transform computation will allow information to flow from the test data to the train data and therefore to the model that learns from it, thus allowing the model <em>to cheat</em> (introducing a bias).</p>

<p>Also, it is important not to confuse transformations with augmentations. Some ""transformations"" might be used to synthetically create more training data, but don't have to be used at test time. For example, in computer vision, deleting regions of an image. <strong><em>Test time augmentation</em></strong> is something you could read about.</p>

<h2>Extra discussion</h2>

<p>More complicated models (ones with many many more parameters) might be able to perform some kind of interpolation, especially if your dataset if N-dimensions with a large N (e.g. > 10).</p>

<p>This has recently been seen with extremely large models, such as <a href=""https://openai.com/blog/better-language-models/"" rel=""nofollow noreferrer"">Open AI's GPT-3</a>, which has <strong>175 BILLION</strong> parameters, and is therefore even able to perform quite well on completely different tasks, let alone the given problem in the training set range.</p>
","5","2","45264","13013"
"75418","<p>There are a few approaches that allow you to do basic ML modelling using a GPU.</p>

<p>First of all, in the code as you presented it, the tensorflow <code>MirroredStrategy</code> unfortunately has no effect. It will only work with tensorflow models themselves, not those from <code>sklearn</code>. In fact, <code>sklearn</code> does not offer any GPU support at all.</p>

<h2>1. <a href=""https://github.com/rapidsai/cuml"" rel=""nofollow noreferrer""><strong>CUML</strong></a></h2>

<p>An Nvidia library that provides some basic ML model types and other things, often offering the exact same API (classes and functions) as SciKit-Learn. Coming from Nvidia, this of course means everything is built with the GPU in mind.</p>

<p>This is part of the larger <a href=""https://rapids.ai/index.html"" rel=""nofollow noreferrer""><strong>RAPIDS toolset</strong></a> (incubated by Nvidia). Maybe there are other tools there that can be helpful, like their <a href=""https://rapids.ai/xgboost.html"" rel=""nofollow noreferrer""><strong>XGBoost library</strong></a>.</p>

<h2>2. <a href=""https://www.tensorflow.org/tutorials"" rel=""nofollow noreferrer""><strong>Tensorflow</strong></a> / <a href=""https://pytorch.org/"" rel=""nofollow noreferrer""><strong>PyTorch</strong></a> + NumPy</h2>

<p>These framework are not just for complicated <em>Deep Learning</em>, you can really use them to perform any basic modelling and leverage their GPU support. Their documentation contains examples, otherwise something like Hands On Machine Learning (<a href=""https://github.com/ageron/handson-ml2"" rel=""nofollow noreferrer""><strong>a book with an accompanying set of Jupyter notebooks</strong></a>) is a nice way to dig in.</p>

<p>These frameworks work well with the normal scientific stack in Python (such as NumPy, Scipy, Pandas) because numpy arrays and the frameworks' <code>Tensor</code> objects are plug-and-play for most cases.</p>

<hr>

<h3>3. Another option:</h3>

<p>Stick to <strong><code>sklearn</code></strong> while you are learning about the models, how they work and so on. If you want to just do anything with the goal of learning about about GPU usage, the two options above are the most modern ways to get started.</p>
","0","2","45264","13013"
"76084","<p>Churn models often simply <a href=""https://towardsdatascience.com/hands-on-predict-customer-churn-5c2a42806266"" rel=""nofollow noreferrer"">predict a binary output</a>: will the student churn? Yes or No, <code>1</code> or <code>0</code>. In your case there is an added component, namely the time factor of 6 weeks, so the question is more <strong><em>""when is the student likely to churn?""</em></strong>.</p>

<p>Does your dataset include how long students stayed on the course before churning (i.e. leaving the course)?</p>

<hr>

<p>At the highest level, you could model this problem as either a classification problem or a regression problem (with some post processing).</p>

<h2>Classification approach</h2>

<p>If you choose classification, you should form your target variable (the actual churn information) for each student into several discrete classes. For example, you could create 3 classes:</p>

<ul>
<li><code>0</code> -> the student didn't churn</li>
<li><code>1</code> -> the student churned later than 6 weeks</li>
<li><code>2</code> -> the student churned within 6 weeks</li>
</ul>

<p>Then you can select any model that can consume your data (<em>input features</em>) and classify each case as one of those three cases. Something like <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"" rel=""nofollow noreferrer""><strong>a decision tree</strong></a> might work well as a base line model. You could then perhaps try <a href=""https://scikit-learn.org/stable/modules/svm.html#classification"" rel=""nofollow noreferrer""><strong>an SVM model</strong></a>.</p>

<h2>Regression approach</h2>

<p>In this case, you would be predicting the exact time each student will churn. For this, your dataset must contain e.g. the number of days or weeks into the course that churn students left the course.</p>

<p>In this case, you target variable is then simply this information, and your model will predict numbers, like 3.7 weeks or 26 weeks; you will then need to simply post-process these results into your 3 categories (as listed above).</p>

<p>In this case, again, you could try decision tree (<a href=""https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html"" rel=""nofollow noreferrer""><strong>regression variant</strong></a>) or an <a href=""https://scikit-learn.org/stable/modules/svm.html#regression"" rel=""nofollow noreferrer""><strong>SVM model</strong></a>.</p>
","1","2","45264","13013"
"77032","<p>Assuming you have the ground truth results <code>y_true</code> and also the corresponding model predictions <code>y_pred</code>, you can use SciKit-Learn's <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html"" rel=""nofollow noreferrer""><strong>precision_recall_fscore_support</strong></a>.</p>
<pre><code>from sklearn.metrics import precision_recall_fscore_support

y_true = ...                             # read from a file or wherever
y_pred = logistic_model.predict(x_test)  # your model that has been &quot;fit&quot; using e.g. x_train

# Perform any further steps required to classify predictions as TP, FP, etc.

# Compute precision, recall, f1_score and number of occurences of each label
precision, recall, f1, counts = precision_recall_fscore_support(y_true, y_pred)
</code></pre>
","0","2","45264","13013"
"77201","<p>You can compare the predictions with the expected results directly, using simple comparisons, in this case just <code>==</code>. This returns boolean values - <code>True</code> or <code>False</code>, which you can sum up because <code>True == 1</code> and <code>False = 0</code>.</p>
<p>Here is an example for your case using some randomly generated dummy data:</p>
<pre><code>In [1]: import numpy as np                                                                                                           

In [2]: y = np.random.choice([-1, 0, 1], 10)                                                                                         

In [3]: preds = np.random.choice([-1, 0, 1], 10)                                                                                     

In [4]: y                                                                                                                            
Out[4]: array([ 1,  1,  1, -1,  1, -1, -1,  1,  1,  0])

In [5]: preds                                                                                                                        
Out[5]: array([ 0, -1,  1,  0,  1,  1, -1,  1, -1,  0])
</code></pre>
<p>The real part that checks where your predictions are correct is then done using two checks:</p>
<ol>
<li>where are the predictions equal to the ground truth i.e. where were you correct in predicting the direction, <strong>and</strong></li>
<li>where are the predictions equal to the direction you are interested in i.e. downwards (<code>-1</code>), no change (<code>0</code>) or upwards (<code>1</code>)</li>
</ol>
<p>This can be done as follows:</p>
<pre><code>In [6]: (y == preds) &amp; (preds == 0)   #   &amp;   means we needs both checks to be True 
Out[6]:
array([False, False, False, False, False, False, False, False, False, True])
</code></pre>
<p>We can see only the final position is True after both these checks, because that is the only place that the prediction was <code>True</code> and the value direction was <code>0</code>.</p>
<p>You can then write a loop to check all values and do something with them:</p>
<pre><code>In [7]: n = len(preds)    # the number of test samples (= 10 in my dummy example)

In [8]: for direction in [-1, 0, 1]:
            score = sum((y == preds) &amp; (preds == 0)) 
            accuracy = score / n 
            print(f&quot;Direction {direction:&gt;2}: {score}/{n} = {accuracy * 100:.1f}%&quot;)
</code></pre>
<p>Which gives:</p>
<pre><code>Direction -1: 1/10 = 10.0%
Direction  0: 1/10 = 10.0%
Direction  1: 1/10 = 10.0%
</code></pre>
","5","2","45264","13013"
"77265","<blockquote>
<p>what is the difference between slope of line and slope of curve</p>
</blockquote>
<p>It is really a matter of perspective. The slope of a line is the same over the entire span of that line, i.e. until the line changes direction. The slope of a curve is like the slope of millions of <strong>tiny lines</strong> all connected, so the slope is only the same value over <em>tiny spans</em>. So we can only talk about the slope of a curve at a give point (e.g. a given x value) and then we normally talk about the <em>gradient</em> of the line at that point.</p>
<hr />
<p>Using your words, the gradient computed by <a href=""https://numpy.org/doc/1.18/reference/generated/numpy.gradient.html"" rel=""nofollow noreferrer""><strong>numpy.gradient</strong></a> is the <strong>slope</strong> of a <em>curve</em>, using the differences of consecutive values.</p>
<p>However, you might like to imagine that your changes, when measured over smaller and smaller distances, <em>become</em> the <strong>slope</strong> (by your definitions). So when e.g. <code>x2 - x1</code> almost reaches zero, you have the same meaning as <span class=""math-container"">$dx$</span>.</p>
<h2>Coarse example with 10 points</h2>
<p>Here is a coarse example of <code>numpy.gradient</code>, where <span class=""math-container"">$dx$</span> has s size of <code>1</code> (equally spaced values):</p>
<pre><code>In [1]: import matplotlib.pyplot as plt
In [2]: import numpy as np
In [3]: N = 10                                  # Use ten samples
In [4]: x = np.linspace(0, np.pi*2, N)          # Equally spaced x values
In [5]: y = np.sin(x)                           # Corresponding sine values
In [6]: grads = np.gradient(y)                  # compute the gradients
</code></pre>
<p>Plotting the values and the gradients - I shifted the gradients by <code>0.5</code> to the right, so their values line up with the middle of the segment for which they describe the slope:</p>
<pre><code>In [7]: fig, ax = plt.subplots()           
In [8]: ax.plot(x, y, &quot;-b&quot;, label=&quot;values&quot;)                  # the y values
        ax.plot(x + 0.5, grads, &quot;--r&quot;, label=&quot;gradients&quot;)    # the computed gradients
        plt.legend()
In [9]: plt.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/l1JrW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l1JrW.png"" alt=""Gradients with only 10 sample points"" /></a></p>
<h2>Fine example with 1,000,000 points</h2>
<p>Now we do example same as before, just use one million points: <code>N = 1_000_000</code>.
The blue line would look much more like a true sine wave, but the red line is now measuring the gradient with a much higher resolution than before, giving us the exact value at 1,000,000 points - for each tiny line segment of the blue line.</p>
<p><a href=""https://i.stack.imgur.com/VSsx5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VSsx5.png"" alt=""fine grained gradients"" /></a></p>
<p>So the gradient values look like they are all zero! Well this is just because we made <code>x2-x1</code> become almost zero (<code>1 / 1e6</code>), so the values of <code>y2 - y1</code> also were essentially zero! We have started to approximate <span class=""math-container"">$\frac{dy}{dx}$</span>.</p>
<p>Let change the axis scale to see that the gradients do still match the pattern we might expects, and are indeed very smooth - looking like a curve:</p>
<p><a href=""https://i.stack.imgur.com/6xl09.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6xl09.png"" alt=""Fine grained gradients"" /></a></p>
<p>Much better  :)</p>
<p>(Notice the scale difference - <code>1e-6</code>).</p>
<hr />
<p>Here are a few explanations of <a href=""https://stackoverflow.com/questions/24633618/what-does-numpy-gradient-do""><strong>what <code>np.gradient</code> really does</strong></a>.</p>
","1","2","45264","13013"
"86231","<p>You specifically talk about <em>account names</em>, and so I assume they can be treated as strings.</p>
<p>One way to compare closeness of strings is the <a href=""https://en.wikipedia.org/wiki/Levenshtein_distance"" rel=""nofollow noreferrer"">Levenshtein distance</a>, defined as:</p>
<blockquote>
<p>the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other.</p>
</blockquote>
<p>It just so happens there is a nice library that implements this kind of <em>fuzzy matching</em> - <a href=""https://github.com/seatgeek/fuzzywuzzy"" rel=""nofollow noreferrer""><strong>fuzzywuzzy</strong></a>. They have some usage examples on the homepage.</p>
<hr />
<h2>Ideas for processing the data</h2>
<p>In your case, if you know the correct account names, you could compute the similarity of just those correct ones to each of the actual entries, and use a threshold value to turn all close-matches into the correct account name.</p>
<p>Alternatively, you could compute pairwise similarities pair up the highest scores, reducing each pair to a single name. Iterate on this approach until you have no name-pairs with a similarity above a given threshold.</p>
<p>For the thresholds, in either case, you'd have to probably use a heuristic value.</p>
","1","2","45264","13013"
"87774","<p>The short answer, is that you want the fastest way to reach the performance you expect/desire. This would mean first playing with some hyperparameters like learning rate, initialisation strategy, trying different optimizers, etc.</p>
<p>Having said that, if you managed to write your training process such that it is trivial to swap out the model completely for another one, you can also give that a go.</p>
<hr />
<p>If you have a model that is known to work well for a given type of task, but is really under-performing (e.g. not converging or a large difference to results in the literature), then I would first check for some common issues in the training process:</p>
<ul>
<li><strong>data quality</strong>: are the inputs scaled correctly? Should you add some</li>
<li><strong>data volume</strong>: if you don't have many samples, can you get more or use augmentations?</li>
<li><strong>loss function</strong>: are the outputs of the model being correctly consumed and the losses correctly computed? are you even using the right loss formulation?</li>
</ul>
<p>This is essentially debugging your training setup.</p>
<p>I would convince myself as far as possible that none of these exist before building a different architecture or a radically different approach. The main reason for this, is that I assume you did some research into the problem and the type of models that are known to perform well - so if this doesn't work, what makes you think another architecture will?  :)</p>
","2","2","45264","13013"
"87775","<p>No - the input and outputs sizes are independent from one another in a deep full-connected network. You could e.g. have input matrix shape <code>(100, 100, 100)</code> and output shape <code>(1,)</code>.</p>
","0","2","45264","13013"
"87776","<p>You could try using the <code>flow_from_directory()</code> method on your <code>ImageDataGenerator</code> class, which does the augmentation - only a small change is necessary:</p>
<pre><code>H = model.fit(
    aug.flow_from_directory(trainX, trainY, batch_size=BS),
    ...)
</code></pre>
<p>If you start using a <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer""><strong><code>tf.data.Dataset</code></strong></a> directly, you will get more control over how the data is read from disk (caching, number of threads etc.), but you will lose the easy image augmentation you get with the <code>ImageDataGenerator</code>.</p>
","1","2","45264","13013"
"88025","<blockquote>
<p>reward_threshold (float) – Gym environment argument, the reward threshold before the task is considered solved   <a href=""https://tensorforce.readthedocs.io/en/latest/environments/openai_gym.html"" rel=""nofollow noreferrer"">[1]</a></p>
</blockquote>
<p>Just from that one sentence definition, it sounds like a total reward that e.g. an agent must earn, before the task is complete, and so ends.</p>
<p>If this were for example a task for the cartpole agent to stay upright/vertical, it might be formulated as the number of frames, so 1 frame = 1 point reward and if <code>reward_threshold = 200</code>, the agent must balance the pole for 200 frames to succeed.</p>
<p>Have a look at the <a href=""https://gym.openai.com/evaluations/eval_XZ8AjUEKRBaCFzXKThnRg/"" rel=""nofollow noreferrer"">example of cartpole on the OpenAI Gym website</a>:</p>
<pre class=""lang-py prettyprint-override""><code>while True:
    candidate_model = model.symmetric_mutate()
    rewards = [run_one_episode(env, candidate_model, False) for _ in range(5)]
    reward = np.mean(rewards)
    if reward &gt;= env.spec.reward_threshold:
        print &quot;Reached reward threshold!&quot;
        rewards2 = [run_one_episode(env, candidate_model, False) for _ in range(env.spec.trials)]
        if np.mean(rewards2) &gt;= env.spec.reward_threshold:
            break
        else:
            print &quot;Oops, guess it was a fluke&quot;
</code></pre>
<p>So the agent runs for 5 episodes (<code>for _ in range(5)</code>) and each episode returns an award. We compute the mean reward over 5 episodes (<code>reward = np.mean(rewards)</code>) and then introduce the desired control flow, based on that result.</p>
<p>In this case, they use the default <em>reward_threshold</em> from the environment (<code>env.spec.reward_threshold</code>), print a success message, and do a final check to see if the agent really has learnt something consistent or not, by checking once again on a new set of episode (<code>for _ in range(env.spec.trials)</code>).</p>
","0","2","45264","13013"
"88028","<p>How do you get to the second picture from that code? I wil give you rough set of steps to follow along with the main function names. you can search through the pandas documentation for more details.</p>
<ol>
<li>initialise some empty lists:  <code>ticker_names=[]</code> <code>dataframes=[]</code></li>
<li>Loop over the files and read them using <code>df = pd.read_csv(file_name)</code></li>
<li>Take just the <code>Close</code> column of each single ticker's dataframe: <code>df = df[&quot;Close&quot;]</code></li>
<li>Keep track of the tickers' names in a list: <code>ticker_names.append(ticker)</code></li>
<li>Bring them all together into one dataframe: <code>final_df = pd.concat(dataframes, left_index=True, right_index=True)</code>. We uses the indices because you have date-time index values, and we want to keep them all.</li>
<li>Then give each column the correct name: <code>final_df.columns = ticker_names</code></li>
</ol>
<p>You can play around with each of these steps, and I would suggest looking at the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html#pandas.DataFrame.merge"" rel=""nofollow noreferrer""><code>**pd.merge**</code></a> documentation.</p>
","0","2","45264","13013"
"88038","<p>I think these are often used colloquially as synonyms, but let's try to find the differences.</p>
<p>Each of them begins with &quot;Time Series&quot; (TS). So the difference lies in the three following terms. here with my interpretation:</p>
<ul>
<li><strong>Analysis</strong> - wanting to describe and understand characteristics the observed data coming from the generating function<span class=""math-container"">$^1$</span>.</li>
<li><strong>Forecasting</strong> - wanting to <em>somehow</em> predict the future output of the generating function.</li>
<li><strong>Modeling</strong> - wanting to describe and understand the generating function itself, via an approximation.</li>
</ul>
<p>So as you see, I think <em>time-series analysis</em> and <em>time-series modelling</em> are almost the same thing. I just feel the latter most likely involves really using a real model, with parameters. That doesn't mean specifically <a href=""https://en.wikipedia.org/wiki/Parametric_model"" rel=""nofollow noreferrer""><strong>parametric</strong></a> or a <a href=""https://en.wikipedia.org/wiki/Nonparametric_statistics"" rel=""nofollow noreferrer""><strong>non-parametric</strong></a> approaches.</p>
<p>I personally wouldn't draw a line between them in terms of &quot;uses statistics&quot; vs. &quot;uses machine learning&quot;, as your question suggests.</p>
<hr />
<p>A natural progression in approaching a time-series problem (in my opinion) would be:</p>
<ol>
<li>understand the data (TS-analysis)</li>
<li>understand the underlying function (TS-modelling)</li>
<li>test how well I have understood the function and its output (TS-forecasting)</li>
</ol>
<hr />
<p><span class=""math-container"">$^1$</span>
With the term &quot;generating function&quot;, I don't mean the strict mathematical definition, rather just the underlying process or system that is generating the observed data</p>
","1","2","45264","13013"
"88202","<p><em><strong>Disclaimer: this is massively subjective topic, and below is just my opinion.</strong></em></p>
<hr />
<p>I would say it depends on your focus. You could do work in these areas with any major programming language, but some have better suited capabilities and/or better support from a community.</p>
<p>My biased and very simple answer would be to use <strong>Python</strong> for basically everything, and learn <strong>C++</strong> for times when performance is important. And then, if you get really serious, you'll need to learn <strong>CUDA</strong> (extensions to C++ to programme directly for Nvidia GPUs). If you are proficient at Python and C++, there isn't really anything you couldn't do.</p>
<h2>In summary</h2>
<ul>
<li><p>If you want to do research, you could probably do everything with just Python. There are libraries such a NumPy/PyTorch/Tensorflow which do all the heavy lifting for you.</p>
</li>
<li><p>If you want to go the way of robotics and embedded hardware/software, you'll likely need C/C++.</p>
</li>
<li><p>If you want to make models to deploy in a browser, perhaps Javascript would be useful.</p>
</li>
</ul>
","6","2","45264","13013"
"88486","<h2>Simplest approach</h2>
<p>First a very fast and perhaps practical approach: just remove them <strong>without</strong> replacing them!</p>
<p>From your bar chart, it seems you have a lot of transactions - several hundred thousand. Removing a few hundred (I can't even see a bar for the &gt; $600 transactions) and <strong>not</strong> replacing them wouldn't mean the remaining data is unusable. Replacing those transaction e.g. using the mean of the distribution, is basically trying to make sure they don't have any significant effect on the model; so what is the point in replacing them?</p>
<p>Looking at your frequency chart, I would be interested to know what appears in the final large peak. Could you just remove those transaction?</p>
<p>You could give this a go and move onto the modelling, see if you get sensible results - if not, come back to investigate further.</p>
<hr />
<h2>Back to statistical methods</h2>
<p>Using 3 standard deviations isn't a bad approach - assuming your data is normally distributed, it means <a href=""https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule"" rel=""nofollow noreferrer"">you only remove 0.3% of the data</a>. The issue is perhaps that your distribution of transaction amounts does not look normally distributed - it looks more like a <a href=""https://en.wikipedia.org/wiki/Beta_distribution"" rel=""nofollow noreferrer"">beta distribution</a>(the orange line):</p>
<p><a href=""https://i.stack.imgur.com/u1RTR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u1RTR.png"" alt=""beta distribution from Wikipedia"" /></a></p>
<p>You could therefore try your approach of removing data outside the distribution, by computing the parameters of the <em>beta distribution</em>, and using those instead of the <em>normal</em> mean/std.</p>
<p>To do this, check out <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html"" rel=""nofollow noreferrer"">scipy.stats.beta</a>, and be aware that you should probably normalise your transactions to the range <code>[0, 1.0]</code> (see Scikit-Learn's <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"" rel=""nofollow noreferrer"">StandardScaler</a>). Here is <a href=""https://stats.stackexchange.com/questions/68983/beta-distribution-fitting-in-scipy"">a thread on understanding the output</a> of your beta distribution.</p>
<hr />
<h1>Impute</h1>
<p>If you simple treat your outliers as missing data, there are some nice ideas and explanations about filling missing gaps in your data (a.k.a. <em>data imputation</em>) in this well-known book: <a href=""https://web.stanford.edu/%7Ehastie/Papers/ESLII.pdf"" rel=""nofollow noreferrer"">Elements of Statistical Learning (pdf)</a><span class=""math-container"">$^1$</span> - see section 9.6.</p>
<p><span class=""math-container"">$^1$</span>Authors: Trevor Hastie, Robert Tibshirani, Jerome Friedman</p>
","1","2","45264","13013"
"88637","<p>Most of the common libraries you would use for data manipulation do actually use C (or C++ or Fortran, etc.) under the hood.</p>
<p>There are even libraries such as <a href=""https://cupy.dev/"" rel=""nofollow noreferrer"">CuPy</a>, which offers <a href=""https://docs.cupy.dev/en/stable/tutorial/basic.html"" rel=""nofollow noreferrer"">the entire NumPy API</a>, but can run your code on a GPU. Using GPUs for speed is a much more common use case these days (in my experience), compared to writing the C/C++ version.</p>
<p><em><strong>EDIT: <a href=""https://datascience.stackexchange.com/questions/88201/what-s-the-most-suitable-programming-language-for-ai-development/88202#88202"">here is a related answer</a></strong></em>, about which programming languages are most competitive for AI, ML, DataScience, etc.</p>
<hr />
<p>In my opinion, you might need to &quot;do it yourself&quot; in one of 3 cases:</p>
<h3>1. Speed</h3>
<p>You need it to <strong>run faster</strong> than current libraries offer - e.g. if the clustering algorithms in Scikit-Learn are too slow</p>
<h3>2. Memory</h3>
<p>You need to use <strong>less memory</strong> that existing algorithms - perhaps a specific method on your Pandas DataFrame uses more memory that you have available</p>
<h3>3. New Algorithms</h3>
<p>You need something that is fairly fast or very low level, and <strong>no existing library</strong> offers it. I would normally suggest trying your idea first using NumPy though, before trying to roll your own binaries.</p>
","2","2","45264","13013"
"88640","<p>You get a view of the matrix, with <strong>only the the axes transposed</strong>. So your <code>B</code> doesn't get transposed.</p>
<p>More nuances depend on the shape of your original array:</p>
<pre><code>In  [1]: import numpy as np

In  [2]: np.ndarray.transpose??
Out [2]:

Returns a view of the array with axes transposed.
For a 1-D array, this has no effect. (To change between column and
row vectors, first cast the 1-D array into a matrix object.)                                                      
For a 2-D array, this is the usual matrix transpose.                                                              
For an n-D array, if axes are given, their order indicates how the                                               
axes are permuted (see Examples). [truncated]

</code></pre>
<p>NOTE: there is a small difference between, <code>a.T</code> and <code>a.transpose()</code>:</p>
<blockquote>
<p>Same as self.transpose(), except that self is returned if self.ndim &lt; 2.</p>
</blockquote>
<hr />
<p>There are other methods, such as <code>np.ndarray.reshape()</code>, which might or might not return a copy of the data, also depending on things like the shape of the array.</p>
","0","2","45264","13013"
"88663","<h2>Efficient use of resources</h2>
<p>It is a balancing game with the learning rate, and one reason you don't normally see people do this is that you want to utilise as much of the GPU as possible.</p>
<p>It is commonly preferred to start with the maximum batch size you can fit in memory, then increase the learning rate accordingly. This applies to &quot;effective batch sizes&quot; e.g. when you have 4 GPUs, each running with <code>batch_size=10</code>, then you might have a global learning rate of <code>nb_gpu * initial_lr</code> (used with sum or average of all 4 GPUs).</p>
<p>The final &quot;best approach&quot; is usually problem specific - small batch sizes might not work for GAN type models, and large batch sizes might be slow and sub-optimal for certain vision based tasks.</p>
<h3>Friends don't let friends use large batch sizes</h3>
<p>There is literature <a href=""https://arxiv.org/abs/1804.07612"" rel=""noreferrer"">to support usage of small batch sizes</a> at almost all times. Even though this idea was supported by Yann Lecun, there are <a href=""https://twitter.com/Tim_Dettmers/status/989767141388795905"" rel=""noreferrer"">differences of opinion</a>.</p>
<h3>Super convergence</h3>
<p>There are also other tricks that you might consider, if you are interested in faster convergence, <a href=""https://arxiv.org/abs/1708.07120"" rel=""noreferrer"">playing with learning rate cycling</a>.</p>
","6","2","45264","13013"
"88708","<p>If you know there are only specific variants, you can obviously make a look-up table yourself (i.e. a Python dictionary).</p>
<p>Otherwise you could try using a fuzzy matching library, like <a href=""https://github.com/seatgeek/fuzzywuzzy"" rel=""nofollow noreferrer"">fuzzywuzzy</a>.</p>
<p>This will give you a &quot;closeness&quot; score for your search term, based on your list of parameters (measurements). Here is an example of how you could use it:</p>
<pre class=""lang-py prettyprint-override""><code>In [1]: from fuzzywuzzy import process

In [2]: measurements = [&quot;Voltage&quot;, &quot;Current&quot;, &quot;Resistance&quot;, &quot;Power&quot;]

In [3]: variants = [&quot;VOLT&quot;, &quot;voltage_in&quot;, &quot;resistnce&quot;, &quot;pwr&quot;, &quot;amps&quot;] # notice typos etc.

In [4]: for variant in variants:
   ...:     results = process.extract(variant, measurements, limit=2)
   ...:     print(f&quot;{variant:&lt;11} -&gt; {results}&quot;)  # See which two were found to be closest 
   ...:     best = results[0]                     # Take the best match by score (first in the list)
   ...:     if best[1] &lt; 70:                      # Set a threshold at 70%
   ...:         print(f&quot;Rejected best match for '{variant}': {best}&quot;)

VOLT        -&gt; [('Voltage', 90), ('Current', 22)]
voltage_in  -&gt; [('Voltage', 82), ('Resistance', 30)]
resistnce   -&gt; [('Resistance', 95), ('Current', 38)]
pwr         -&gt; [('Power', 75), ('Current', 30)]
amps        -&gt; [('Voltage', 26), ('Resistance', 22)]
Rejected best match for 'amps': ('Voltage', 26)
</code></pre>
<p>So most worked out pretty well, including the typo example.</p>
<p>Obviously this does not kind of <code>semantic</code> search, as so <code>amps</code> do not get related to <code>Current</code> in any way.</p>
<hr />
<p>To go the way of semantic encodings, you might want to look into &quot;word embeddings&quot;, which do indeed try to match the real meaning of words, based on their semantic meaning. To start here, you could look into <code>Word2Vec</code> or GloVe` embeddings. Perhaps there is even a tool or library that already offers this capability.</p>
<p>These approaches will not inherently deal with things like typos, so for best results, you could even combine the two approaches.</p>
","4","2","45264","13013"
"89003","<p>The easiest way to do it for your case is something like this:</p>
<pre class=""lang-py prettyprint-override""><code>
In [1]: import pandas as pd                                                                                                         

In [2]: df1 = pd.DataFrame([0, 1, 2, 3, 1, 2, 3, 1, 2], columns=[&quot;A&quot;])                                                              

In [3]: df2 = pd.DataFrame([9] * len(df1), columns=[&quot;A&quot;])                                                                           

In [4]: df1                                                                                                                         
Out[4]: 
   A
0  0
1  1
2  2
3  3
4  1
5  2
6  3
7  1
8  2

In [5]: df2                                                                                                                         
Out[5]: 
   A
0  9
1  9
2  9
3  9
4  9
5  9
6  9
7  9
8  9
</code></pre>
<p>Wherever the row of column <code>A</code> in <code>df1</code> is equal to <code>1</code>, insert <code>1</code> into the same row of column A in <code>df2</code></p>
<pre class=""lang-py prettyprint-override""><code>In [6]: df2.A[df1.A == 1] = 1                                                                                                       

In [7]: df2                                                                                                                         
Out[7]: 
   A
0  9
1  1
2  9
3  9
4  1
5  9
6  9
7  1
8  9

</code></pre>
<hr />
<p>You could also use the <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.where.html"" rel=""nofollow noreferrer"">pd.DataFrame.where</a> method, to replace the part <code>df1.A == 1</code>.</p>
<p>There are also many other ways to get the same thing done, but my example is the most straighforward :)</p>
","0","2","45264","13013"
"89036","<p>Because on each loop you assign a single value to the entire column:</p>
<pre class=""lang-py prettyprint-override""><code>   tots_df['orc_4_totals'] = s4    # entire column orc_4_totals = s4!
</code></pre>
<p>(the same thing for the other column.</p>
<p>It equals all ones then, just because your final loop value fills it with <code>1</code>.</p>
<p>You need to insert single values instead, for each row of your target dataframe.
However, because I don't know if your target dataframe <code>tots_df</code> has an existing index, I would suggest first saving the values in a list, then assigning them:</p>
<pre class=""lang-py prettyprint-override""><code>orc_4_values = []
orc_5_values = []
for name in pros_unique:
    # perform your checks, then assign using:
    s4 = df.loc[(df.pros_split == name) &amp; (df.orc_4 == 1)].orc_4.count()
    s5 = df.loc[(df.pros_split == name) &amp; (df.orc_5 == 1)].orc_4.count()
    orc_4_values.append(s4)
    orc_5_values.append(s5)

# Add to dataframe after the loop
tots_df['orc_4_totals'] = orc_4_values
tots_df['orc_5_totals'] = orc_5_values
</code></pre>
","1","2","45264","13013"
"89054","<p>One way to do it (without getting into lots of the inner-workings of Jupyter notebooks), it to use two matplotlib <code>Axes</code> in one plot. Then you show one image in each of these:</p>
<pre><code>from skimage import data
image_coffee = data.coffee()
image_horse = data.horse()
fig, axs = plt.subplots(1, 2, figsize=(15, 8))  # one row of Axes, two columns = 2 plots
</code></pre>
<p>The <code>axs</code> variables is a list containing the two <code>Axes</code>, so just access each one and plot your image on it like this:</p>
<pre class=""lang-py prettyprint-override""><code>axs[0].imshow(image_coffee)
axs[1].imshow(image_horse)
</code></pre>
<p>If the plots don't pop automatically, either run <code>plt.show()</code> or make sure your notebook has executed <code>%matplotlib inline</code> in a cell.</p>
","0","2","45264","13013"
"89591","<p>If you just multiply the two dataframes, the missing rows will be filled with <code>NaN</code> values (missing values. You can then simply replace all these with <code>0.0</code>, or any value.</p>
<p>Here is an example:</p>
<pre class=""lang-py prettyprint-override""><code>In [1]: import pandas as pd

In [2]: df1 = pd.DataFrame(range(6), columns=[&quot;A&quot;])

In [3]: df2 = pd.DataFrame(range(8), columns=[&quot;A&quot;])   # different length

In [4]: df3 = df1 * df2

In [5]: df3   # Look at the Not-a-Number values
Out[5]:
      A
0   0.0
1   1.0
2   4.0
3   9.0
4  16.0
5  25.0
6   NaN
7   NaN

In [6]: df3.fillna(0.0)   # fill those NaN values with zero
Out[6]:
      A
0   0.0
1   1.0
2   4.0
3   9.0
4  16.0
5  25.0
6   0.0
7   0.0
</code></pre>
","0","2","45264","13013"
"89704","<p>Given you already have the <code>tf.data.Dataset</code>, one way to do it would be to iterate over the dataset and each time you come across a new label, save that e.g. to a dictionary, otherwise skip an already seen label.</p>
<p>Here is a short example just using the MNIST dataset that comes with tensorflow:</p>
<pre class=""lang-py prettyprint-override""><code>import matplotlib.pyplot as plt
import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
</code></pre>
<p>Now we have the dataset, we can shuffle it and also create a batch size of one, so each iteration will return one image and one label:</p>
<pre class=""lang-py prettyprint-override""><code># Shuffle and make batch_size 1 to iterate over single example
dataset = dataset.shuffle(buffer_size=2048).batch(1)
</code></pre>
<p>For this MNIST dataset, the labels are known to be integers from 0-9, one for each single digit:</p>
<p><code>known_labels = list(range(10))   # in the case of MNIST used here</code></p>
<p>Now we can create an empty dictionary into which we will insert each class with its image as we come across it:</p>
<p><code>label_to_images = {}</code></p>
<p>And iterate over our dataset. If the label is not already in the dictionary, we add it. We also check that we have all labels and then break the cycle if we have:</p>
<pre class=""lang-py prettyprint-override""><code>for image, label in dataset:
    label_as_int = int(label.numpy())
    if label_as_int not in label_to_images.keys():
        print(f&quot;Found label: {label_as_int}&quot;)
        label_to_images[label_as_int] = image

    # Sort before comparison as keys are sorted according to insertion order
    if sorted(list(label_to_images.keys())) == known_labels:
        print(f&quot;Got all labels! -&gt; {list(label_to_images.keys())}&quot;)
        break

# OUTPUT
Found label: 1
Found label: 4
Found label: 7
Found label: 9
Found label: 2
Found label: 0
Found label: 6
Found label: 3
Found label: 5
Found label: 8
Got all labels! -&gt; [1, 4, 7, 9, 2, 0, 6, 3, 5, 8]
</code></pre>
<p>Now we can plot them all as follows:</p>
<pre class=""lang-py prettyprint-override""><code>fig, axs = plt.subplots(2, 5)   # 2 rows, 5 columns

plt.gray()   # Need this because the MNIST images are all grayscale

for ax, (label, image_tensor) in zip(axs.flatten(), label_to_images.items()):
    image = image_tensor.numpy()[0]   # remove channel dimension, we don't have RGB
    ax.set_title(f&quot;Class: {label}&quot;)
    ax.imshow(image)
    ax.axis(&quot;off&quot;)    # we don't need to see the pixel numbers as axes

    
fig.tight_layout()
</code></pre>
<p>Which gives the following output:</p>
<p><a href=""https://i.stack.imgur.com/GnFRD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GnFRD.png"" alt=""MNIST digits"" /></a></p>
<p>Because we added the <code>shuffle()</code> method to our dataset, if you run the code again, you'll get a different result.</p>
","0","2","45264","13013"
"90004","<p>Let's say you pass in output_shape as a tuple <code>(50, 50, 10)</code> where we can call the values (height, width, channels)` to the lambda layer:</p>
<pre><code>your_layer = tf.keras.layers.Lambda(lambda x: x, output_shape=(50, 50, 3))
</code></pre>
<p>The part of the documentation:</p>
<blockquote>
<p>If a tuple, it only specifies the first dimension onward;</p>
</blockquote>
<p>means that the <code>batch</code> dimensions itself is simple carried forward, unchanged.</p>
<p>If you have e.g. <code>batch_size=3</code> during training, the incoming tensor to <code>your_layer</code> might be <code>(3, n, p, q)</code>, where <code>n</code> <code>p</code> and <code>q</code> could be anything, but the layer is expected to produce a shape <code>(3, 50, 50, 10)</code>. So the <code>0</code> dimension remains unchanged, and we have concatenated it with your <code>output_shape</code>:</p>
<pre><code>(3,) + (50, 50, 10) -&gt; (3, 50, 50, 10)
</code></pre>
<p>This corresponds to the expression: <code>output_shape = (input_shape[0], ) + output_shape</code>, so we see that <code>input_shape</code> is the true shape of the <em><strong>incoming batch tensor</strong></em> during training, as we only took the batch dimension to produce the layer's <em><strong>outgoing batch tensor</strong></em>.</p>
<p>For the second expression it is really just the same thing, but if you haven't provided a batch shape, Tensorflow &amp; Keras represent that as something that could be anything, and store it as <code>None</code>. So in that case you get:</p>
<pre><code>(None,) + (50, 50, 10) -&gt; (None, 50, 50, 10)
</code></pre>
","1","2","45264","13013"
"90087","<p>Assuming you cannot add more memory to your computer (or free up some of the memory), you could try 2 general approaches:</p>
<ol>
<li><p>Read only some of the data into memory e.g. a subset of the rows or columns.</p>
</li>
<li><p>reduce the precision of the data from <code>float64</code> to <code>float32</code>.</p>
</li>
</ol>
<p>From your error, it looks like you are loading data into a numpy array, so somewhere in your code, you would need to add this argument to the array creation step e.g.  <code>np.array(your_data, dtype=np.float32)</code>.</p>
<h3>EDIT:</h3>
<p>I don't think this is a rate limiting problem, or a <code>max_buffer_size</code> issue from Tornado (the library behind Jupyter).</p>
<p>You can try to see if your machine is actually running out of memory by using a tool called <code>htop</code> - just execute <code>htop</code> in a terminal (or first <code>sudo apt install htop</code> if it isn't already installed).  That shows the total amount of memory (RAM) available on your machine, it looks something like this:</p>
<p><a href=""https://i.stack.imgur.com/Voj85.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Voj85.png"" alt=""htop example"" /></a>
This example shows the machine has 16 cores and 62.5 Gb memory - 6.14Gb of that is occupied.</p>
<p>Watch that view while you run your code. If the memory bar becomes full before the crash, you know you ran out of RAM.</p>
","3","2","45264","13013"
"90658","<p>Your train/validation loss curves are a classic example of overfitting.</p>
<p>It looks like you have 1425 data samples to train a model with &gt; 28 million parameters.</p>
<p>I would suggest trying any/all of the follow:</p>
<ol>
<li><strong>use a smaller model</strong> -&gt; less parameters means less complexity and less ability to overfit</li>
<li><strong>data augmentation</strong> -&gt; more data samples means more variation in the dataset for the model to capture</li>
<li><strong>early-stopping</strong> -&gt; use something like the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping"" rel=""nofollow noreferrer""><strong>Keras callback</strong></a>, which will stop the model training once the validation loss doesn't decrease for a number of epochs</li>
</ol>
<p>If you happen to be using image data, you might take a look at the Keras <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"" rel=""nofollow noreferrer""><strong>ImageDataGenerator</strong></a>, which can do things like flip/rotate your images.</p>
","2","2","45264","13013"
"94245","<p><strong>TL;DR</strong> - use the <code>max_depth</code> and <code>max_samples</code> arguments to <code>ExtraTreesRegressor</code> to reduce the maximum tree size. The sizes you pick might depend on the distribution of your data. As a starting point, you could start with max_depth=5 and <code>max_samples=0.1*data.shape[0]</code> (10%), and compare results to what you have already. Tweak as you see fit.</p>
<hr />
<p>Apart from the fairly large input space, the data structure built by the <code>ExtraTreeRegressor</code> is the main issue. It will continue to expand the tree size until each leaf reaches your criteria, namely <code>min_samples_leaf=1</code>. This means every single data point of your input dataset must end up in its own leaf. Apart from probably overfitting, this is going to lead to high memory consumption.</p>
<p>See the <code>Note:</code> in <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html"" rel=""nofollow noreferrer""><strong>the relevant documentation</strong></a>:</p>
<blockquote>
<p>The default values for the parameters controlling the size of the trees (e.g. <code>max_depth</code>, <code>min_samples_leaf</code>, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.</p>
</blockquote>
<p>Each <code>ExtraTreesRegressor that you create looks like it might make a full copy of your dataset, according to the documentation for </code>max_samples`:</p>
<pre><code>    max_samples : int or float, default=None
        If bootstrap is True, the number of samples to draw from X
        to train each base estimator.
        - If None (default), then draw `X.shape[0]` samples.
</code></pre>
<p>To gain a deeper understanding of how you might tune your memory usage, you could take a look at <a href=""https://github.com/scikit-learn/scikit-learn/blob/15a949460/sklearn/ensemble/_forest.py#L1807"" rel=""nofollow noreferrer""><strong>the source code of the ExtraTreesRegressor</strong></a>.</p>
","1","2","45264","13013"
"42478","<h2><a href=""https://pytorch.org/"" rel=""nofollow noreferrer""><strong>PyTorch</strong></a></h2>

<p>It is gaining a lot of support due to its ease of use and similarity with basic Python.</p>

<p>It runs ""line-by-line"" (via dynamic graphs), just like normal Python and can be easily debugged - even using standard print statements. It also integrates very well with NumPy and other well-known Python linbraries, like Scikit Learn.</p>

<p>Because it makes modelling easy, it is great for prototyping and exploring new ideas in general.</p>

<p>It supports multiple GPUs and does so in a really easy way.</p>

<p>Check out <a href=""https://pytorch.org/features"" rel=""nofollow noreferrer"">more features here</a>.</p>

<hr>

<p>Although many of the benefits above make PyTorch much nicer to use than other commonly used libraries, it is worth mentioning that the upcoming major release of Tensorflow will, by default, also use dynamic graph creation (a.k.a. <em>eager mode</em>). This will make it comparable to PyTorch in usage. </p>
","4","2","45264","13013"
"17794","<p>I was reading about <code>PAC framework</code> and faced the definition of <code>Generalization Error</code>. The book defined it as: </p>

<blockquote>
  <p>Given a hypothesis h ∈ H, a target concept c ∈ C, and an underlying distribution
  D, the generalization error or risk of h is defined by <br />
  <a href=""https://i.stack.imgur.com/BbKi5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BbKi5.jpg"" alt=""enter image description here""></a>
  <br />
  The generalization error of a hypothesis is not directly accessible to the learner
  since both the distribution D and the target concept c are unknown. However, the
  learner can measure the empirical error of a hypothesis on the labeled sample S.</p>
</blockquote>

<p>I can not understand the equation. Can anyone please tell me how it can be interpreted? Also what is x~D?</p>

<p>Edit:
How do I formally write this term? Is something like $$\mathbb{E}_{x \sim D} [1_{h(x)\neq c(x)}] = \int_X 1_{h(\cdot) \neq c(\cdot)} (\omega) dD(\omega)$$
correct or do I need to define some random variable? Also, to show that the empirical error 
$$ \hat{R}(h) = \frac{1}{m} \sum_{i =1}^m 1_{h(x_i)\neq c(x_i)} $$
is unbiased, we have
$$\mathbb{E}_{S \sim D^m} [\hat{R}(h)] = \frac{1}{m} \sum_{i =1}^m   \mathbb{E}_{S \sim D^m} ~ \left[ 1_{h(x_i)\neq c(x_i)} \right] = \frac{1}{m} \sum_{i =1}^m   \mathbb{E}_{S \sim D^m} ~ \left[ 1_{h(x)\neq c(x)} \right]$$,
but how do we formally get
$$ \mathbb{E}_{S \sim D^m} ~ \left[ 1_{h(x)\neq c(x)} \right]= \mathbb{E}_{X \sim D} ~ \left[ 1_{h(x)\neq c(x)} \right] = R(h)$$</p>

<p>I think that I understand it intuitionally, but I can't write it down formally. Any help is much appreciated!</p>
","3","1","28175","12588"
"17839","<p>In the context of <em>Machine Learning</em>, I have seen the term <em>Ground Truth</em> used a lot. I have searched a lot and found the following definition in <a href=""https://en.wikipedia.org/wiki/Ground_truth"" rel=""noreferrer"">Wikipedia</a>:</p>
<blockquote>
<p>In machine learning, the term &quot;ground truth&quot; refers to the accuracy of the training set's classification for supervised learning techniques. This is used in statistical models to prove or disprove research hypotheses. The term &quot;ground truthing&quot; refers to the process of gathering the proper objective (provable) data for this test. Compare with gold standard.</p>
<p>Bayesian spam filtering is a common example of supervised learning. In this system, the algorithm is manually taught the differences between spam and non-spam. This depends on the ground truth of the messages used to train the algorithm – inaccuracies in the ground truth will correlate to inaccuracies in the resulting spam/non-spam verdicts.</p>
</blockquote>
<p>The point is that I really can not get what it means. Is that the <em>label</em> used for each <em>data object</em> or the <em>target function</em> which gives a label to each <em>data object</em>, or maybe something else?</p>
","41","1","28175","12588"
"18123","<ul>
<li><p>What are the meanings of <em>Bias</em>?</p></li>
<li><p>And is <em>Under fitting</em>, which is used in <em>machine learning</em> contexts, the same as ""Bias""?</p></li>
</ul>

<p>I have faced <em>biased data</em> in sampling in statistics but it seems this is a different thing to <em>bias</em> in learning concepts.</p>

<p>I have heard that some data sets are <em>biased</em>, also have heard the model (for example neural network) has <em>low bias</em> or e.g. 'high bias' problem. Are these uses of <em>bias</em> different?</p>
","3","1","28175","12588"
"18802","<p>In <code>linear regression</code> we use the following cost function which is a convex function:<br /> <a href=""https://i.stack.imgur.com/aFBeP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/aFBeP.png"" alt=""enter image description here""></a>
<br />
We Use the following cost function<br /> 
<a href=""https://i.stack.imgur.com/JX0jv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JX0jv.png"" alt=""enter image description here""></a>
<br />
in <code>logistic regression</code> because the preceding cost function is not <code>convex</code> whenever the hypothesis (h) is logistic function. We have changed the equation of cost function to have a convex shape to find its global (the only one which exists). There is a fact that I can not understand. In <code>Multi Layer Perceptrons</code> ANNs I have seen a lot that they can be stuck in local minimums. Why is that? We have used this cost function for each perceptron and gotten the rules for updating the values for the weights in back propagation algorithm; So why do we stuck?</p>
","6","1","28175","12588"
"21896","<p>I know that <code>Polynomial Logistic Regression</code> can easily learn a typical data like the following image: <br />
<a href=""https://i.stack.imgur.com/AM2zD.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/AM2zD.png"" alt=""first image""></a>
<br />
I was wondering whether the following two data also can be learned using <code>Polynomial Logistic Regression</code> or not. <br />
<hr />
<a href=""https://i.stack.imgur.com/M6FjC.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/M6FjC.png"" alt=""enter image description here""></a> 
<hr />
<a href=""https://i.stack.imgur.com/pQwKQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/pQwKQ.png"" alt=""enter image description here""></a> </p>

<p>I guess I have to add more explanation. Assume the first shape. If we add extra polynomial features for this 2-D input (like x1^2 ...) we can make a decision boundary which can separate the data. Suppose I choose X1^2 + X2^2 = b. This can separate the data. If I add extra features I will get a wavy shape (maybe a wavy circle or wavy ellipsis) but it still can not separate the data of the second graph, can it? </p>
","9","1","28175","12588"
"21916","<p>Use <code>drop_duplicates()</code> of <code>pandas</code>:</p>

<pre><code>import pandas as pd    
df = pd.DataFrame({'userid':[1,1,1,1, 2,2,2],
                   'itemid':[1,1,3,4, 1,2,3] })
print(df)
print()
print(df.drop_duplicates())
</code></pre>

<p>Consider that drop won't change the df itself and just pass a new data frame which has dropped the specified row(s). If you want to change the <code>df</code> itself set <code>inside</code> parameter to <code>True</code>.</p>
","1","2","28175","12588"
"22952","<p>I have <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjBkd3C0Z3WAhVMsBQKHfZ1DVQQFggnMAA&amp;url=https%3A%2F%2Fwww.coursera.org%2Flearn%2Fpython-data-analysis%2Flecture%2FLn156%2Fpandas-idioms&amp;usg=AFQjCNHKDkwSCZabMMM42aghkOrMrQKhsw"" rel=""nofollow noreferrer"">seen</a> the term <code>pandorable</code> but I cannot find any accurate definition for that. What does that mean?</p>
","4","1","28175","12588"
"23211","<p>I have a set of images which are loaded from an <code>h5</code> file. I checked their dimensions and I get <code>(209, 64, 64, 3)</code>.</p>

<pre><code>read = h5py.File('datasets/train_catvnoncat.h5', 'r')
read['train_set_x'].shape
</code></pre>

<blockquote>
  <p>(209, 64, 64, 3)</p>
</blockquote>

<p>it means that there are 209 images but the point that I cannot understand is that, what is (64, 64, 3)? <br />
I have used the following code for plotting: <br /></p>

<pre><code>import matplotlib.pyplot as plt

plt.imshow(read['train_set_x'][1])
plt.show()
</code></pre>

<p>and I get a colored image which is 64 by 64. before this, I thought for (., ., .) shapes, the second number specifies the number of lines and the third one specifies the number of rows. also the first one specifies the number of the mentioned (row and column) arrays.<br />
My question is that in <code>numpy</code> if you have a three dimensional array, for accessing rows and columns you have to change the second and third entries in the indexing operator; Why this is different in images and rows and columns are arranged differently in images. Shouldn't it be (3, 64, 64)?</p>
","0","1","28175","12588"
"23287","<p>Why  weights with large values cause neural networks to be overfitted, and consequently we use approaches like regularization to neutralize weights with large values?</p>
","9","1","28175","12588"
"23493","<p><a href=""https://datascience.stackexchange.com/questions/15484/sigmoid-vs-relu-function-in-convnets"">Here</a> the answer refers to vanishing and exploding gradients that has been in <code>sigmoid</code>-like activation functions but, I guess, <code>Relu</code> has a disadvantage and it is its expected value. there is no limitation for the output of the <code>Relu</code> and so its expected value is not zero. I remember the time before the popularity of <code>Relu</code> that <code>tanh</code> was the most popular amongst machine learning experts rather than <code>sigmoid</code>. The reason was that the expected value of the <code>tanh</code> was equal to zero and and it helped learning in deeper layers to be more rapid in a neural net. <code>Relu</code> does not have this characteristic, but why it is working so good if we put its derivative advantage aside. Moreover, I guess the derivative also may be affected. Because the activations (output of <code>Relu</code>) are involved for calculating the update rules.</p>
","18","1","28175","12588"
"23613","<p>I have seen <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwin9efZmeHWAhWRJVAKHfX9DgoQFgguMAI&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FProbably_approximately_correct_learning&amp;usg=AOvVaw0-7Yenwwy4WysCK70mZL-5"" rel=""nofollow noreferrer"">here</a> but I really cannot realize that.</p>

<blockquote>
  <p>In this framework, the learner receives samples and must select a generalization function (called the hypothesis) from a certain class of possible functions. The goal is that, with high probability (the ""probably"" part), the selected function will have low generalization error.</p>
</blockquote>

<p>Actually we do that in every machine learning situations and we do latter part for avoiding over-fitting. Why do we call it PAC-learning?<br />
I also have not get the meaning of the math too. Is there anyone that can help?</p>
","3","1","28175","12588"
"23750","<p>I am reading <a href=""https://arxiv.org/pdf/1506.02025.pdf"" rel=""nofollow noreferrer"">this</a> paper. </p>

<blockquote>
  <p>Convolutional Neural Networks define an exceptionally powerful class of models,
  but are still limited by the lack of ability to be spatially invariant to the input data
  in a computationally and parameter efficient manner. In this work we introduce a
  new learnable module, the Spatial Transformer, which explicitly allows the spa-
  tial manipulation of data within the network. This differentiable module can be
  inserted into existing convolutional architectures, giving neural networks the abil-
  ity to actively spatially transform feature maps, conditional on the feature map
  itself, without any extra training supervision or modification to the optimisation
  process. We show that the use of spatial transformers results in models which
  learn invariance to translation, scale, rotation and more generic warping, result-
  ing in state-of-the-art performance on several benchmarks, and for a number of
  classes of transformations.</p>
</blockquote>

<p>Spatial Transformers are used in <code>CNNs</code> to have spatial invariant transformations and consequently the process of learning would be so easier and the networks would have better performance for data with different kind of distributions (noisy data). In this paper I don't realize the meaning of <code>differentiable module</code>. These so called differentiable modules are used in neural networks. But what is the meaning of <em>differentiable</em>?</p>
","1","1","28175","12588"
"24265","<p>If you use just one neuron (linear->sigmoid), you can find the minimum with too much small error or maybe reach to the minimum using approaches like gradient descent or other optimization algorithms. The reason for finding the minimum is that cross entropy is a non-convex shape, but if you use sigmoid function as the activation function of logistic regression, the cross entropy cost function becomes convex and it is easy to find the only global minimum.</p>
","0","2","28175","12588"
"24319","<p>I want to train a deep model with a large amount of training data, but my desktop does not have that power to train such a deep model with these abundant data. </p>

<p>I'd like to know whether there are any free cloud services that can be used for training machine learning and deep learning models? </p>

<p>I also would like to know if there is a cloud service, where I would be able to track the training results, and the training would continue even if I am not connected to the cloud.</p>
","36","1","28175","12588"
"24454","<p>In perspective of storing data in databases, storing correlated features is somehow similar to storing redundant information which it may cause wasting of storage and also it may cause inconsistent data after updating or editing tuples. </p>

<p><strong>If we add so much correlated features to the model we may cause the model to consider unnecessary features and we may have curse of high dimensionality problem</strong>, <em>I guess this is the reason for worsening the constructed model.</em></p>

<p>In the context of machine learning we usually use <code>PCA</code> to reduce the dimension of input patterns. This approach considers removing correlated features by someway (using <code>SVD</code>) and is an unsupervised approach. This is done to achieve the following purposes:</p>

<ul>
<li><a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwju5tH86KzXAhXCDpoKHfW-BBAQFggsMAE&amp;url=http%3A%2F%2Fwww.scielo.br%2Fscielo.php%3Fscript%3Dsci_arttext%26pid%3DS1679-45082012000200004&amp;usg=AOvVaw0fHz2y5dPeaNeqyqtfbw4v"" rel=""noreferrer"">Compression</a></li>
<li><a href=""https://stats.stackexchange.com/questions/268704/why-do-we-use-pca-to-speed-up-learning-algorithms-when-we-could-just-reduce-the"">Speeding up learning algorithms</a></li>
<li><a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwi2y8G46qzXAhVGEJoKHesVAVcQFggrMAA&amp;url=http%3A%2F%2Fnghiaho.com%2F%3Fpage_id%3D1030&amp;usg=AOvVaw01gd1IN4lVYY7HxilQCtcn"" rel=""noreferrer"">Visualizing data</a></li>
<li><a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjd07nf6qzXAhWCE5oKHew2DAYQFggtMAE&amp;url=http%3A%2F%2Fwww.cs.haifa.ac.il%2F~rita%2Fuml_course%2Fadd_mat%2FPCA.pdf&amp;usg=AOvVaw3GdiMQK2c5GTbXDMUYW34Z"" rel=""noreferrer"">Dealing with curse of high dimensionality</a></li>
</ul>

<p>Although this may not seem okay but I have seen people that use removing correlated features in order to avoid overfitting but I don't think it is a good practice. For more information I highly recommend you to see <a href=""https://stats.stackexchange.com/a/268719/179078"">here</a>.</p>

<p>Another reason is that in deep learning models, like <code>MLPs</code> if you add correlated features, you just add unnecessary information which adds more calculations and parameters to the model.</p>
","9","2","28175","12588"
"24494","<p>Should we add bias to each entry of the convolution then sum, or add bias once at end of calculating the convolution in CNNs?</p>
","4","1","28175","12588"
"24511","<p>In machine learning tasks it is common to shuffle data and normalize it. The purpose of normalization is clear (for having same range of feature values). But, after struggling a lot, I did not find any valuable reason for shuffling data.</p>

<p>I have read this post <a href=""https://stats.stackexchange.com/a/180847/179078"">here</a> discussing when we need to shuffle data, but it is not obvious why we should shuffle the data. Furthermore, I have frequently seen in algorithms such as Adam or SGD where we need batch gradient descent (data should be separated to mini-batches and batch size has to be specified). It is vital according to this <a href=""https://stats.stackexchange.com/questions/245502/shuffling-data-in-the-mini-batch-training-of-neural-network"">post</a> to shuffle data for each epoch to have different data for each batch. So, perhaps the data is shuffled and more importantly changed.</p>

<p>Why do we do this?</p>
","49","1","28175","12588"
"24537","<p>Gradient Descent is an algorithm which is designed to find the optimal points, but these optimal points are not necessarily global. And yes if it happens that it diverges from a local location it may converge to another optimal point but its probability is not too much. The reason is that the step size might be too large that prompts it recede one optimal point and the probability that it oscillates is much more than convergence.</p>

<p>About gradient descent there are two main perspectives, machine learning era and deep learning era. During machine learning era it was considered that gradient descent will find the local/global optimum but in deep learning era where the dimension of input features are too much it is shown in practice that the probability that all of the features be located in there optimal value at a single point is not too much and rather seeing to have optimal locations in cost functions, most of the time saddle points are observed. This is one of the reasons that training with lots of data and training epochs cause the deep learning models outperform other algorithms. So if you train your model, it will find a detour or will find its way to go downhill and do not stuck in saddle points, but you have to have appropriate step sizes.</p>

<p>For more intuitions I suggest you referring <a href=""https://datascience.stackexchange.com/q/18802/28175"">here</a> and <a href=""https://www.coursera.org/learn/deep-neural-network/lecture/RFANA/the-problem-of-local-optima"" rel=""noreferrer"">here</a>.</p>
","40","2","28175","12588"
"25085","<p>Like the update rule for bias terms in dense layers, in convolutional nets the bias gradient is calculated using the sum of derivatives of Z terms:</p>

<p>$$ dJ / db = \sum_h \sum_w dZ_{hw} $$</p>

<p>which J is the cost function, w is the width of the activation after convolution and h is the height of the activation after convolution.</p>

<p>db is computed by summing  dZs. It means you are summing over all the gradients of the conv output (Z) with respect to the cost.</p>

<p>Calculating the error of the net depends on the cost function that you have used. Depending on using cross entropy or mean-squared-error or other justified cost functions, you may have different update rule for other parameters of your net. But if you use cross entropy which is common for variants of classification tasks, the above update rule is used for updating bias terms.</p>
","1","2","28175","12588"
"25137","<p>I guess you have a learning problem that has low number of training data. I recommend you a two-step solution.<br /></p>

<ol>
<li>First you need to do error analysis. Find the images that your classifier does not recognize and try to compare with those which your classifier correctly classified them. find the features that your classifier can not do well on them and try to add features that your classifier can discriminate those, I mean try to find yourself what prompts your classifier to make mistake on those images and why you don't make mistake during recognizing them. Try to add those features that you care but your classifier does not and add them as input. </li>
<li>Try to augment your data using data augmentation for both you mistake and those which you don't. There is an important fact here. don't change the distribution of your data. I mean if one class has two times bigger population than the other class, after augmentation try to keep this ratio. The reason is that maybe after equalizing the number of population of classes you reduce the Bayes error but at borders of classes the answers would be random.</li>
</ol>
","1","2","28175","12588"
"25241","<p>In storing floating point values both overflow and underflow problems cause loss of data. In machine learning tasks underflow is a common problem. I wanted to know if <code>double</code> is better than <code>float64</code> in TensorFlow or not and if there is any difference between them?</p>
","0","1","28175","12588"
"25269","<p><code>val_loss</code> is the value of cost function for your cross-validation data and loss is the value of cost function for your training data. On validation data, neurons using drop out do not drop random neurons. The reason is that during training we use drop out in order to add some noise for avoiding over-fitting. During calculating cross-validation, we are in the recall phase and not in the training phase. We use all the capabilities of the network.</p>

<p>Thanks to one of our dear friends, I quote and explain the contents from <a href=""https://keras.io/models/sequential/"" rel=""noreferrer"">here</a> which are useful.</p>

<blockquote>
  <p><code>validation_split</code>: Float between 0 and 1. The fraction of the training
  data to be used as validation data. The model will set apart this
  fraction of the training data, will not train on it, and will evaluate
  the loss and any model metrics on this data at the end of each epoch.
  The validation data is selected from the last samples in the <em>x</em> and
  <em>y</em> data provided, before shuffling.</p>
  
  <p><code>validation_data</code>: tuple (x_val, y_val) or tuple (x_val, y_val,
  val_sample_weights) on which to evaluate the loss and any model
  metrics at the end of each epoch. The model will not be trained on
  this data. This will override  validation_split.</p>
</blockquote>

<p>As you can see</p>

<pre><code>fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)
</code></pre>

<p><code>fit</code> method used in <code>Keras</code> has a parameter named validation_split, which specifies the percentage of data used for evaluating the model which is created after each epoch. After evaluating the model using this amount of data, that will be reported by <code>val_loss</code> if you've set verbose to <code>1</code>; moreover, as the documentation clearly specifies, you can use either <code>validation_data</code> or <code>validation_split</code>. Cross-validation data is used to investigate whether your model over-fits the data or does not. This is what we can understand whether our model has generalization capability or not.</p>
","23","2","28175","12588"
"25271","<p>I recommend you to use the following example and try to manipulate the arguments and adjust them for your work:</p>

<pre><code>from matplotlib import cm
cmap = cm.get_cmap('gnuplot')
scatter = pd.scatter_matrix(YOUR_TRAINING_DATA, c = YOUR_LABELS_OF_TRAINING, marker = 'o', s = 40, hist_kwds = {'bins':15}, figsize = (12, 12), cmap = cmap)
</code></pre>

<p><a href=""https://i.stack.imgur.com/1fAcf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1fAcf.png"" alt=""Code and image given from coursera data science course""></a></p>
","3","2","28175","12588"
"25284","<p>The table that you are referring to is doing OR operation. whenever you have just a neuron in your net you are able to have one line to separate your data. but for xor data you have to have two line separators. it is common for solving this problem to have two neurons in the first layers which both do OR operation and in the second layer to have one neuron to do and operation. if you stack these two layers, you will get the result. I suggest you to see the shape of <a href=""https://datascience.stackexchange.com/questions/21896/can-the-linearly-non-separable-data-be-learned-using-polynomial-features-with-lo"">this</a> to see the nonlinear decision boundary. Also the second table is doing AND operation.</p>
","1","2","28175","12588"
"25315","<p>In the last layer of <code>CNNs</code> and <code>MLPs</code> it is common to use softmax layer or units with sigmoid activation functions for multi-class classification. I have seen somewhere, I don't remember where, that softmax is used whenever the classes are mutually exclusive and the layer with units containing sigmoid activation function are used in tasks with multiple labels, e.g. recognizing animals in an image which can contain numerous animals. Am I right? Is there any other deduction for distinguishing between them?</p>

<p>I have seen <a href=""https://stackoverflow.com/questions/41409248/softmax-and-sigmoid-function-for-the-output-layer"">here</a> and <a href=""https://stats.stackexchange.com/a/218589/179078"">here</a> but they do not contain what I want.</p>
","8","1","28175","12588"
"25413","<p>It depends on your data. There is something called human level error. Suppose tasks like reading of printed books, humans do not struggle to read and it might not happen to make a mistake unless because of bad quality of printing. In cases like reading hand-written manuscripts, it may happen a lot not to understand all words if the font of the writer is odd to reader. In the first situation the human level error is too low and the learning algorithms can have the same performance but the second example illustrates the fact that in some situations the human level error is so much high and in a usual manner (if you use the same features as humans) your learning algorithm will have so much error ratio.</p>

<p>In statistical learning, there is something called <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwj0p_jek_PXAhWJp6QKHXlVC-sQFggnMAA&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FBayes_error_rate&amp;usg=AOvVaw1n3f-z6MZ5un6DnflDtZ_d"" rel=""nofollow noreferrer""><code>Bayes Error</code></a>, whenever the distribution of classes overlap, the ratio of error is large. without changing the features, the Bayes error of the current distributions is the best performance and can not be reduced at all.</p>

<p>I also suggest you reading <a href=""https://stats.stackexchange.com/questions/302900/what-is-bayes-error-in-machine-learning"">here</a>. Problems with a large amount of Bayes error with appointed features are considered <em>not classifiable</em> with in the space of those features. As another example you can suppose you want to classify cars with lights on. If you try to do that in the morning, you yourself may have lots of errors and if you use same images for training the learning algorithm, that may have too. </p>

<p>Also I recommend you not to change the distribution of your classes. In such cases, the result of classifier near the boundary would be completely random. The distribution of data for training your machine learning algorithm should not be changed and should be as it is in the real condition.</p>
","4","2","28175","12588"
"25414","<p>It uses <strong>same</strong> padding which means the output of max-pooling is padded with zeros in a way that the output of next layer preserves the width and height. for information take a look at <a href=""https://www.coursera.org/learn/convolutional-neural-networks/lecture/MmYe2/classic-networks"" rel=""nofollow noreferrer"">here</a>.</p>
","1","2","28175","12588"
"25480","<p>It uses <a href=""http://deepdish.io/2015/02/24/network-initialization/"" rel=""nofollow noreferrer"">Xavier initialization</a>. If you don't initialize it, by default it uses this method for initialization. If you want to know how to perform that you can use <a href=""https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/"" rel=""nofollow noreferrer"">here</a>. In the first link at the end, link of the paper is also available.</p>

<p><em>This Xavier initialization (after Glorot’s first name) is a neat trick that works well in practice. However, along came rectified linear units (ReLU), a non-linearity that is scale-invariant around 0 and does not saturate at large input values. This seemingly solved both of the problems the sigmoid function had.</em></p>
","2","2","28175","12588"
"25551","<p>I guess you should change the following line to solve the problem:</p>

<pre><code>model.add(Conv2D(64, strides=5, kernel_size=EMBED_DIM, activation=""relu"", padding='valid'))
</code></pre>

<p>instead use this code:</p>

<pre><code>model.add(Conv2D(64, strides=5, kernel_size=EMBED_DIM, activation=""relu"", padding='same'))
</code></pre>

<p>this will keep the dimension of your input. If it does not work, let me know to help you more.</p>
","2","2","28175","12588"
"25642","<p>I recommend you using <code>Keras</code> and employing its pre-trained models. Because of low number of data-set, you should use <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiPoa-KzofYAhUDaVAKHXhbDkAQFggpMAE&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTransfer_learning&amp;usg=AOvVaw2HhdpOpHnaIlHbqcB21o43"" rel=""nofollow noreferrer"">transfer learning</a>. There are lots of researches about that like <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=6&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwiPoa-KzofYAhUDaVAKHXhbDkAQFghJMAU&amp;url=http%3A%2F%2Fcs231n.github.io%2Ftransfer-learning%2F&amp;usg=AOvVaw3meugAV_bCpOZhjgZv3eRW"" rel=""nofollow noreferrer"">here</a>. Based on the data that you have, you should choose a model which is appropriate for your task and have been trained for tasks which are like yours, and then use it. I guess <em>ResNet</em> and <em>GoogleNet</em> already have been trained on <em>ImageNet</em> data set and are in the <code>Keras</code>. You have to freeze the weights of the convolution and dense layers. You should change the soft-max layer with your own. In this kind of learning, the pre-trained model has already the ability to find features. You are just supposed to let it learn how to classify your data. </p>
","1","2","28175","12588"
"25657","<p><code>RNN</code> architectures like <code>LSTM</code> and <code>BiLSTM</code> are used in occasions where the learning problem is sequential, e.g. you have a video and you want to know what is that all about or you want an agent to read a line of document for you which is an image of text and is not in text format. I highly encourage you take a look at <a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""noreferrer"">here</a>.</p>

<p><code>LSTMs</code> and their bidirectional variants are popular because they have tried to learn how and when to forget and when not to using gates in their architecture. In previous <code>RNN</code> architectures, vanishing gradients was a big problem and caused those nets not to learn so much.</p>

<p>Using Bidirectional <code>LSTMs</code>, you feed the learning algorithm with the original data once from beginning to the end and once from end to beginning. There are debates here but it usually learns faster than one-directional approach although it depends on the task. </p>

<p>Yes, you can use them in unsupervised learning too depending on your task. take a look at <a href=""http://deeplearning.stanford.edu/tutorial/"" rel=""noreferrer"">here</a> and <a href=""http://ufldl.stanford.edu/tutorial/selftaughtlearning/SelfTaughtLearning/"" rel=""noreferrer"">here</a>. </p>
","13","2","28175","12588"
"25699","<p>We use freezing to employ <a href=""https://en.wikipedia.org/wiki/Transfer_learning"" rel=""noreferrer"">transfer learning</a>. Deep learning has a great hunger for data. In some tasks you may not have so much data, but there may already be a pre-trained network that can be helpful. In such cases you use the model and its weights and by replacing the soft-max layer, in situations where you have small amount of data, you try to customize the network for you specific task. If you have more data, the more number of layers can be trainable. Take a look at <a href=""http://ruder.io/transfer-learning/"" rel=""noreferrer"">here</a></p>
","6","2","28175","12588"
"25722","<p>Can dropout be applied to convolution layers or just dense layers. If so, should it be used after pooling or before pooling and after applying activation?<br />
Also I want to know whether batch normalization can be used in convolution layers or not.</p>

<p>I've seen <a href=""https://www.reddit.com/r/MachineLearning/comments/42nnpe/why_do_i_never_see_dropout_applied_in/"" rel=""nofollow noreferrer"">here</a> but I couldn't find valuable answers because of lacking reference.</p>
","3","1","28175","12588"
"25747","<p>Depending on your algorithm, it may have different interpretations. </p>

<p>Suppose you are using <strong>SVM</strong> <code>with kernels</code>, it means your input data is exactly between two data points with different classes or if you are using <code>linear kernels</code> in <strong>SVM</strong> it means that <strong>your data</strong> is <strong>on the separator line</strong>. </p>

<p>If you are using <strong>Neural Networks</strong>, this is the <strong>Probability</strong> of each <strong>class</strong>. In such cases there maybe different explanations for this phenomenon. </p>

<ol>
<li>Suppose that you are trying to classify images of green and purple
cats! and suppose that you have two same cats with both colors, then
your classifier may output this result</li>
<li><p>Another interpretation may also exist. suppose that you have
unbalanced data-set for your two classes. suppose that there are
same feature vectors of both classes. in such cases, the
distribution of your samples overlap. A class which has more data samples would be the winner and will occupy the space much more than the class with less samples. In such cases, if you get 50 percent as the output, You can not tell that both classes have same expectations. </p>

<p>If I want to clarify this more, I would do that with this example.
Suppose that you have a <strong>car classifier</strong> for distinguishing
between white and blue cars. during training you had <strong>100</strong> images
of <strong>blue car</strong> and <strong>20</strong> images of <strong>white car</strong>. During <em>recall
phase</em>, if for an arbitrary image you have 50 percent for each
class, you can never say that the image has same probability to be
each one at all</p></li>
</ol>

<p>.</p>
","3","2","28175","12588"
"25748","<p>It depends on your task and the amount of data you have. If you have so much data but you can not find similar tasks to have appropriate architecture you should stack convolution and dense layers yourself. But if you have appropriate amount of data and there exist good architectures then you have to decide what you want and how is your situation. Suppose that you want to have recognition task, there are so many architectures that are applied to <code>ImageNet</code> data-set. You can use transfer learning but there is a point here. Suppose that you want to fine tune <code>GoogleNet</code>. This is a very large network and is capable for recognizing about a thousand distinct classes. If you have a recognition task with 5 classes and you have an agent that should be online, this is not logical to have such a <strong>big</strong> network. You may have similar performance by stacking a few layers and get better time complexity. If you don't have so much data, freezing the layers and applying transfer learning to the last layer maybe a typical solution.</p>
","2","2","28175","12588"
"25751","<p>If you are using two <em>hidden</em> layers, it may mean that your data is not linearly separable. If you just use one unit in a single hidden layer then you can claim that your data is linearly separable, which in your case you may say that there exist a hyper-plane that separates your data linearly. </p>

<p>As the answer to your question, yes. It is possible to reduce the amount of error by adding layers or adding extra units to the current architecture but there are some points that are necessary to be considered. </p>

<ul>
<li>Whenever you add more layers, there will be vanishing and exploding gradients which may cause your network not to learn, or learning may happen so slowly. To avoid, you should use <code>ReLU</code> activation in order to avoid saturation of gradients. Moreover you have to use <em>He</em> or <em>Xavier</em> initialization techniques to avoid having bad random weights. There are other techniques for solving this problem which are called skip connections but at least I've never seen the use of them in <code>MLP</code>s Although they are really helpful for solving the mentioned problem.</li>
<li>Covariat shift is the problem of learning for deeper layers. As a solution you have to use <em>Batch Normalization</em> to somehow normalize the activations of the deeper layers. </li>
<li>Overfitting is a problem that happens for large architectures that are not fed with enough training data. You should use regularization techniques. There are so many papers about this but in your case which is using <code>MLP</code> I highly recommend you using Dropout technique which is invented by the so called <em>God Father</em> of deep learning. </li>
</ul>
","2","2","28175","12588"
"25755","<p>During back propagation both dense layers and convolution layers get updated but max-pooling layers do not have any weight to be updated. Dense layers are updated to help the net to classify. Convolution layers get updated to let the network learn the features itself. Because you have not asked in the question I just add a link for you if you want to know more. There is a very good explanation for back propagation <a href=""http://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/"" rel=""nofollow noreferrer"">here</a> which can be useful for you.</p>
","3","2","28175","12588"
"25849","<p>As you can see <a href=""https://keras.io/models/sequential/"" rel=""noreferrer"">here</a> <code>Keras</code> models contain predict method but they do not have the method <code>predict_proba()</code> you have specified and they actually do not need it. The reason is that <code>predict</code> method itself returns the probability of membership of the input to each class. If the last layer is <code>softmax</code> then the probability which is used would be mutually exclusive membership. If all of the neurons in the last layer are <code>sigmoid</code>, it means that the results may have different labels, e.g. existence of dog and cat in an image. For more information refer <a href=""https://datascience.stackexchange.com/q/25315/28175"">here</a>.</p>

<p>For more information as stated <a href=""https://stackoverflow.com/questions/40747679/keras-what-is-the-difference-between-model-predict-and-model-predict-proba"">here</a> <em>in the recent version of keras, predict and predict_proba are the same i.e. both give probabilities. To get the class labels use predict_classes. The documentation is not updated. (adapted from Avijit Dasgupta's comment)</em></p>
","6","2","28175","12588"
"25851","<p>You have to normalize your data to accelerate learning process but based on experience its better to normalize your data in the standard manner, mean zero and standard deviation one. Although mapping to other small intervals near to zero may also be fine but the latter case usually takes more time than the other. If you use <code>ReLU</code>, again based on experience, you have to normalize your data and use standard initialization techniques for your weights, like <a href=""https://datascience.stackexchange.com/q/13061/28175"">He or Glorot</a> methods. The reason is that your should avoid each activation to be so large, because your net would be so much dependent to that activation, and you may have overfitting problem. When you use <code>ReLU</code> because there is no limit for its output, you have to normalize the input data and also use initialization techniques that avoid having large values for weights. For more information I encourage you taking a look at <a href=""https://datascience.stackexchange.com/q/23493/28175"">here</a> and <a href=""https://datascience.stackexchange.com/q/23287/28175"">here</a>.</p>
","5","2","28175","12588"
"25856","<p>Using machine-learning techniques, nowadays computers can automatically see and understand better than years ago. Tasks like recognition, localization, detection, semantic segmentation and related things have better performance today. This progress has promoted other fields too. Automatic cars, using autonomous driving benefit these advantages for pedestrian detection or car detection. </p>

<p>Recommender systems are so popular today. They suggest you exactly what you need based on your actions. On Facebook, you can find your old friends without even trying to. You can see a lot of movies which you may be sure that will like them. </p>

<p>During searching in search engines, you can see exactly what you are looking for without even knowing how the learning algorithm is ranking the results to show you the appropriate result.</p>

<p>Your email service finds spam and junk mails automatically and delete them for you and you can be confident that you won't loose any important thing. </p>

<p>OCR systems transform images of books and make them as text files. Using this, you can search in texts and find what you want without searching a lot of images.</p>

<p>Tasks which are difficult for finding algorithms, like walking of robots, are so better than before, using data and machine-learning.</p>

<p>There have been designed agents which can defeat human brain in famous games, like chess. Even in AAA computer games, the agents use reinforcement learning to learn how to defeat the human rivals using experience of each specific player based on their behavior. </p>

<p>Machine learning is EVERYWHERE.</p>
","0","2","28175","12588"
"25929","<p>Suppose that I have an input image with 224 * 224 * 3 dimensions. I pass it through a convolution layer with 64 filters and same convolution operation. Suppose that I want to reconstruct the original image using the output of the convolution layer which has the size 224 * 224 * 64. I have the following questions:</p>

<ul>
<li>Should I use deconvolution? If so, how is the arrangement of deconvolution layer (number of filters and the value of weights. Also when should the activation be applied)?</li>
<li>Are the number of filters and weights in forward pass equal to the backward pass?</li>
<li>Is there any technique other than deconvolution?</li>
<li>Is there any available <code>Keras</code> code for my need?</li>
</ul>

<p>I've seen <a href=""https://datascience.stackexchange.com/q/6107/28175"">here</a> and also <a href=""https://github.com/vdumoulin/conv_arithmetic"" rel=""nofollow noreferrer"">here</a> but they represent a high level abstraction and don't contain appropriate detailed answer.</p>
","1","1","28175","12588"
"25930","<p>There are two possible reasons for this result:</p>

<ul>
<li>Low number of training examples</li>
<li>Using dense layers without batch normalization</li>
</ul>

<p>You have a relatively deep network and your training set size is small. In such cases if you run your model too many times you will certainly overfit the training data. The reason is that whenever you have a powerful model that can learn complicated functions and if you provide low number of training examples, it has capability to fit the data and not learn it. Suppose that you have a fitting problem in calculus and you have 4 points and you also have polynomials with degree of four. In such cases you can fit the data exactly without any error but the point is that you are fitting it, not learning it. In your case your model is powerful and it tries to fit the data, not to learn it. The cure is to provide enough training data.</p>

<p>The reason that your model can not learn is that you are stacking dense layers without batch normalization. each time you update the weights, the output of the deep layers change and this cause the covariat shift. To avoid this problem use batch normalization. </p>

<p>As a solution for you I recommend you to putting just two hidden layers and manipulate the number of units in those layers and provide more training examples. Your task can easily be learned by a two hidden-layer-network.
<hr />
After struggling for about one day finally I want to express my opinion about the problem. Depending on the problems, they may be solved using machine-learning or other techniques. Machine learning problems are those which you can not define an appropriate function to map the input to output or it may be so much hard to do so. Whenever you have the function, it can be used as the hypothesis, the final goal of machine learning algorithms. I tried hard and put so much time on this code and I get the same result. Nothing is going to be learned at all. I have been trying for about one day and still no progress has been seen. To explain the reason, the data is so much hard to be learned! for imagining how difficult it is, I recommend you to write numbers from one to ten in a straight line and put a line between consecutive numbers. The numbers are endless, so you will have no generalization because the boundaries that are going to be found will work just for the two neighbor numbers. This means that if you use the current features, you can not separate, learn, your data. I tried to do, somehow, feature engineering and used the following code to solve the problem: </p>

<pre><code>import keras
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers.normalization import BatchNormalization 
from keras import regularizers 
from keras.optimizers import Adam

def gen(x):
    if (x % 2 == 0):
        return 0; # represents 2
    else:
        return 1; # represents 4


a = []           
for i in range(1,100001):
    temp = np.random.randint(0, 10000000)
    a.append([temp, temp ** 2, temp ** 3, temp ** 4, gen(temp)])

a = np.array(a)


x = a[:, 0: a.shape[1] - 1]
y = a[:, a.shape[1] - 1:]

mean_of_x = np.mean(x, axis = 0, keepdims = True)
std_of_x = np.std(np.float64(x), axis = 0, keepdims = True)
x = (x - mean_of_x) / std_of_x

n_classes = 2
y = keras.utils.to_categorical(y, 2)

percentage = 95 / 100
limit = int(percentage * x.shape[0])

x_train = x[: limit, :]
y_train = y[: limit, :]

x_test = x[limit: , :]
y_test = y[limit: , :]

x_train.shape

model = Sequential()
model.add(Dense(1000, activation='relu', input_shape=(x_train.shape[1],)))
model.add(BatchNormalization())
# model.add(Dropout(0.5))
model.add(Dense(1000, activation='relu'))
model.add(BatchNormalization())
# model.add(Dropout(0.5))

model.add(Dense(n_classes, activation='softmax'))

model.summary()

model.compile(loss = 'categorical_crossentropy', optimizer = keras.optimizers.Adam(lr = 0.0001, decay = 1.5), metrics=['accuracy'])

model.fit(x_train, y_train, batch_size = 128, epochs = 2000, verbose = 1, validation_data=(x_test, y_test), shuffle = True,
          class_weight = {0: 10, 1: 1})
</code></pre>

<p>As you can see, the above code uses high order polynomial. Surprisingly, no progress was seen here too. this model has the mentioned problem for the previous feature in higher dimensions. That's why the learning does not happen here too. </p>

<p>the point here is that although you can not learn using the current feature, number itself, or its high order polynomials, you already have a solution for the problem. Instead of passing the number itself to the learning problem, pass the modulus two of the current number. This feature is so much easy to be learned. you may need just one unit. </p>
","2","2","28175","12588"
"25942","<p>If I get the point, you can use a similar code like the following:</p>

<pre><code>from keras.layers import merge, Convolution2D, MaxPooling2D, Input

input = Input(shape=(256, 256, 3))

seq1 = Dense(1, activation = 'relu')(input)
seq2 = Dense(1, activation = 'sigmoid')(input)
seq3 = Dense(1, activation = 'tanh')(input)

acum = merge([seq1, seq2, seq3], mode='concat', concat_axis=1)
</code></pre>

<p>Depending on your task, specify <code>concat_axis</code>. </p>
","2","2","28175","12588"
"25943","<p>I want to use a pre-trained <a href=""https://keras.io/applications/#vgg16"" rel=""nofollow noreferrer"">VGG16</a> in keras. My question is simple. Should I normalize the input image before predicting its label?</p>
","3","1","28175","12588"
"25978","<p>C value is like lambda, the L2/L1 regularization hyper parameter, but in reverse manner. Whenever C is large, means there is high probability that your model overfit the data in hand. Whenever it is small, your model endure some errors in order to avoid producing small margin and consequently you will have a model less prone to overfitting. The first one is called hard SVM and the latter one is soft SVM. In your case, I guess there will be no difference because your model is linearly separable. If <em>X4</em> was between <em>X2</em> and <em>X3</em> you would have different result for each hard and soft. Soft and hard differ in cases which the data is not linearly separable or maybe they are separable but the margin would be narrow or wide if SVM discards some samples. <em>Soft</em> tries to make the separator space, margin, as large as possible by discarding those minority data which hinder a large margin to be crated but <em>hard</em> tries to separate the data to reduce the error as much as possible. In your case, you will get same result.</p>

<p><a href=""https://i.stack.imgur.com/vIGFw.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vIGFw.jpg"" alt=""enter image description here""></a></p>

<p>For illustrating more, look at the left figure. It tries to discard the sample which hinders the large margin to be created but large margin, considers all the samples.</p>
","2","2","28175","12588"
"25993","<p>Using <code>perceptron</code>, you specify a cost function, <code>Mean Squared Error</code> for regression tasks or maybe <code>Cross Entropy</code> for classification tasks. The input data are the constants and the weights are the parameters of your learning problem. When you specify the cost function, if you have error, the cost would be non-zero. You use algorithms like gradient descent to decrease the cost value. This is an optimization problem which you try to decrease the value of error. When we say <code>Perceptron</code> finds the optimal point, the reason is that the shape of cost function, e.g. <code>MSE</code> is convex and there is just one optimal point which gradient is zero there and the cost has the least possible value there. If you use neural networks, the cost with respect to its parameters, weights, is not convex and you usually can not find the optimal point.</p>

<p>I suggest you looking <a href=""https://datascience.stackexchange.com/q/18802/28175"">here</a> and <a href=""https://datascience.stackexchange.com/a/24537/28175"">here</a> for understanding more neural nets optimality.</p>
","-1","2","28175","12588"
"26007","<p>I had a similar problem. I guess the reason is incompatibility of <code>Keras</code> with your current anaconda. Just update it. </p>
","1","2","28175","12588"
"26018","<p>If you do not specify them, as it is clear in the signature of the functions you are referring to, the function will use the default value for them. For instance, you can see that the default value for stride is (2, 2) which means if you don't define the value of stride the method will use the mentioned value as the default value for the stride. Consequently, if you don't specify it, it does not mean there isn't such thing. In programming this approach helps programmers not to define many different overloads of a typical function.</p>

<p>As the response for the second question, again, if you don't specify the initial weights, <code>TensorFlow</code> itself will use <code>Glorot</code> method for initialization. So, definitely the filters will have values called weights in order to operations like convolution, actually cross correlation, be applicable.</p>

<p>As a recommendation, I highly suggest you taking a look at <a href=""http://yosinski.com/deepvis"" rel=""nofollow noreferrer"">here</a> and <a href=""https://gist.github.com/akiross/754c7b87a2af8603da78b46cdaaa5598"" rel=""nofollow noreferrer"">here</a>.</p>
","0","2","28175","12588"
"26042","<p>Before facing <a href=""https://datascience.stackexchange.com/q/25925/28175"">this</a> question, I always thought non-learnable problems are those which the provided data for the problem has high amount of outliers, those which don't have sufficient features or those for which the Bayes error is large because of having same features with different labels. As you can see, it seems that the data is fine because the learning should be comparable with human level inference. A human can distinguish between even or odd numbers by just looking at them. I know that we as human begins, do modulus two operation in our mind to decide whether a number is even or odd, the feature extraction part, but we are doing that with just the number itself. It is clear that we can not find a decision boundary to be able to generalize because the inputs have alternative behavior. 1 is even 2 is odd, 3 is even 4 is odd and all the other numbers in this manner. I want to know this kind of problem ,which does not have the mentioned problems which may cause an algorithm not to learn, has any special name?</p>
","0","1","28175","12588"
"26049","<p>Instead of calling <code>answer_two</code>, call <code>answer_two()</code>. you are referring to the function object now. You have to call the function.</p>
","1","2","28175","12588"
"26072","<p>Definitely they are different. Very deep nets have exploding/vanishing gradient problem. The authors of <code>ResNet</code> paper had seen that by stacking many layers of convolution and dense layers, the learning did not increase although they used <code>ReLU</code> activation and batch normalization. They used a concept named skip connection which helped the nets to <em>learn</em> whether the input to a typical layer should be preserved or it should be transformed by that layer. Using this concept allowed them to increase the number of layers without hesitating whether they would have vanishing/exploding gradients. The concept of residual nets was originally this. The paper uses this concept for spatial data but recently I've seen people debating using them in temporal cases too __time series data. </p>

<p>Recurrent nets are used in temporal domains. Tasks like sequence classification are examples of their usage. In this domain the net should know the information of previous seen data. Well known examples of these nets are <a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""noreferrer"">LSTMS</a>. Early recurrent nets had vanishing/exploding gradient problem too. but after years <code>LSTM</code>s get popular amongst deep-learning practitioners. They defined a concept named gates which could learn when to forget and when to keep the previous data. </p>
","8","2","28175","12588"
"26114","<p>Adam uses mini batches to optimize. During optimization, you may need go down hill, the cost function, so quickly using a high learning rate. When you reach to points which are near to relatively optimal point you have to reduce the learning rate in order not miss the optimal point. In other words you have to decay learning rate to have more accurate steps by reducing the learning rate. Mini-batch optimizers have multiple steps during one epoch, which all of them may not be true but because they try to minimize the cost for each batch of input data, they finally reach to the relative optimal points. </p>

<p>For each epoch, <code>TensorFlow</code> uses same learning rate and after finishing epoch, the next epoch will be started using the current learning rate divided by the decay parameter. It should not be negative because you are using gradient <em>descent</em> which implies moving toward low-level places. 
<hr />
Recently I was looking the code of optimizers in <code>Keras</code> and I found that as the following code:</p>

<pre><code>if self.initial_decay &gt; 0:
       lr *= (1. / (1. + self.decay * K.cast(self.iterations,
                                             K.dtype(self.decay))))
</code></pre>

<p>learning changes each iteration and not each epoch.</p>
","3","2","28175","12588"
"26115","<p>I suggest you using the pre-trained model and freezing all the convolution layers. You should just train the weights of your dense layers in this situation. So use <code>transfer learning</code> and freeze the convolution layers and replace the dense layer with your desired one and also the last layer which is <code>softmax</code>. The reason is that the image-net already can find the features of a basketball play very well. You just need to classify each game. If you want to see an explanation, read <a href=""https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2812140"" rel=""nofollow noreferrer"">this</a> paper.</p>
","0","2","28175","12588"
"26161","<p>They have the same meaning in this context although during training using <code>Rosenblatt</code> update rule, the former may have great changes during each update. <code>Perceptron</code> is used for binary classification which means there are two possible classes to classify. If the result of inner product, here dot product, is greater than or equal to zero, the class of inputs will be the first class and if it's smaller than zero the class of inputs would be the other class. <code>Perceptron</code> just has one neuron. It's a simple linear classifier. The value of threshold is just important. Means that if the product is greater than zero, the input belongs to e.g. positive class and if is negative it belongs to negative class. The step functions and <code>Rosenblatt</code> update rule are not used any more. They have so much oscillation. Today networks learn using gradient descending algorithms which uses the concept of derivative. </p>

<p>When you progress, you will see that neural nets which use other activation functions like <code>Sigmoid</code> or <code>Tanh</code> are different. the former has <code>0.5</code> expected value and the latter has <code>0</code> expected value which causes the second learn so much faster. Although now a days <code>ReLU</code> is more popular among other activation functions.</p>
","0","2","28175","12588"
"26291","<p>Actually I guess the question is a bit broad! Anyway. </p>

<h1>Understanding Convolution Nets</h1>

<p>What is learned in <code>ConvNets</code> tries to minimize the cost function to categorize the inputs correctly in classification tasks. All parameter changing and learned filters are in order to achieve the mentioned goal. </p>

<h3>Learned Features in Different Layers</h3>

<p>They try to reduce the cost by learning low level, sometimes meaningless, features like horizontal and vertical lines in their first layers and then stacking them to make abstract shapes, which often have meaning, in their last layers. For illustrating this fig. 1, which has been used from <a href=""http://yosinski.com/deepvis#toolbox"" rel=""noreferrer"">here</a>, can be considered. The input is the bus and the gird shows the activations after passing the input through different filters in the first layer. As it can be seen the red frame which is the activation of a filter, which its parameters have been learned, has been activated for relatively horizontal edges. The blue frame has been activated for relatively vertical edges. It is possible that <code>ConvNets</code> learn unknown filters that are useful and we, as e.g. computer vision practitioners, have not discovered that they may be useful. The best part of these nets is that they try to find appropriate filters by their own and don't use our limited discovered filters. They learn filters to reduce the amount of cost function. As mentioned these filters are not necessarily known. </p>

<p><a href=""https://i.stack.imgur.com/j7QHf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/j7QHf.png"" alt=""**Figure 1.** *Low level activations*""></a></p>

<p>In deeper layers, the features learned in previous layers come together and make shapes which often have meaning. In <a href=""http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf"" rel=""noreferrer"">this paper</a> it has been discussed that these layers may have activations which are meaningful to us or the concepts which have meaning to us, as human beings, may be distributed among other activations. In fig. 2 the green frame shows the activatins of a filter in the fifth layer of a <code>ConvNet</code>. This filter cares about the faces. Suppose that the red one cares about hair. These have meaning. As it can be seen there are other activations that have been activated right in the position of typical faces in the input, the green frame is one of them; The blue frame is another example of these. Accordingly, abstraction of shapes can be learned by a filter or numerous filters. In other words, each concept, like face and its components, can be distributed among the filters. In cases where the concepts are distributed among different layers, if someone look at each of them, they may be sophisticated. The information is distributed among them and for understanding that information all of those filters and their activations have to be considered although they may seem so much complicated.</p>

<p><a href=""https://i.stack.imgur.com/3cGMQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3cGMQ.png"" alt=""**Figure 2.** *High level activations*""></a></p>

<p><code>CNNs</code> should not be considered as black boxes at all. <em>Zeiler et all</em> in <a href=""http://arxiv.org/pdf/1311.2901v3.pdf"" rel=""noreferrer"">this amazing paper</a> have discussed the <em>development of better models is reduced to trial and error</em> if you don't have understanding of what is done inside these nets. This paper tries to visualize the feature maps in <code>ConvNets</code>.</p>

<h3>Capability to Handle Different Transformations to Generalize</h3>

<p><code>ConvNets</code> use <code>pooling</code> layers not only to reduce the number of parameters but also to have the capability to be insensitive to the exact position of each feature. Also the use of them enables the layers to learn different features which means first layers learn simple low level features like edges or arcs, and deeper layers learn more complicated features like eyes or eyebrows. <code>Max Pooling</code> e.g. tries to investigate whether a special feature exists in a special region or not. The idea of <code>pooling</code> layers is so useful but it is just capable to handle transition among other transformations. Although filters in different layers try to find different patterns, e.g. a rotated face is learned using different layers than a usual face, <code>CNNs</code> by there own do not have any layer to handle other transformations. To illustrate this suppose that you want to learn simple faces without any rotation with a minimal net. In this case your model may do that perfectly. suppose that you are asked to learn all kind of faces with arbitrary face rotation. In this case your model has to be much more bigger than the previous learned net. The reason is that there have to be filters to learn these rotations in the input. Unfortunately these are not all transformations. Your input may also be distorted too. These cases made <em>Max Jaderberg et all</em> angry. They composed <a href=""https://arxiv.org/abs/1506.02025"" rel=""noreferrer"">this</a> paper to deal with these problems in order to settle down our anger as theirs.</p>

<h3>Convolutional Neural Networks Do Work</h3>

<p>Finally after referring to these points, they work because they try to find patterns in the input data. They stack them to make abstract concepts by there convolution layers. They try to find out whether the input data has each of these concepts or not in there dense layers to figure out which class the input data belongs to. </p>

<p>I add some links which are helpful:</p>

<ul>
<li><a href=""https://github.com/vdumoulin/conv_arithmetic"" rel=""noreferrer"">Understanding convolution operation</a></li>
<li><a href=""https://gist.github.com/akiross/754c7b87a2af8603da78b46cdaaa5598"" rel=""noreferrer"">Understanding arithmetic behind <code>ConvNets</code></a></li>
<li><a href=""http://cs.stanford.edu/people/karpathy/convnetjs/"" rel=""noreferrer"">Useful toolboxes for tracking what happens in these nets</a></li>
</ul>
","27","2","28175","12588"
"26292","<p>We use cost function to have the amount of error for a specified set of weights. We should find the weights which minimize the cost function. The used approach for minimizing the cost function are based on gradients. Means that you should move toward directions which minimize the error. To do so, the cost function have to have derivatives. Absolute function does not have derivatives in some places. Quadratic functions like square, has derivative. Although there are other reasons that we have become to this squared function, the reason it's not absolute was what I referred to.</p>
","0","2","28175","12588"
"26316","<p>There is a rule called <em>there is no free launch</em>. It means that there isn't a learning algorithm that solves all the problems. You as a machine learning practitioner should decide when and how to use which algorithm. Suppose that you want to recognize faces. This problem is a learning problem which if you increase the number of training data, you will get better results. In these cases neural nets and deep nets are highly recommended. In this case it is not logical to use <code>non-linear SVM</code> because it will be so costly and you may not even get good answers. the reason is that deep nets cares about local patterns but <code>SVM</code> considers all the input pattern simultaneously. Actually in your case, I guess your data is categorical. For categorical data, people often use decision trees.</p>

<p>To illustrate an example, once I decided to train a simple MLP to distinguish whether an input pattern is in correct position, to solve 8-queen problem. I solve the game using <code>Genetic algorithm</code> and made data for training the net. The data I brought to net was categorical in some extant. I used it and the net was so good for the trained data, but input features similar to training data which were a bit different had bad recall rate. I trained a decision tree, I get so much better result. </p>

<p>Which algorithm depends on your task and your input features. </p>
","1","2","28175","12588"
"26321","<p>Images are two dimensional signals. The use of <code>Conv1D</code> is for one dimensional signals like voice and sound. CNNs are good for these signals too because of taking care of local input patterns. Definitely there are standard one dimensional filters which are most used in signal processing like <em>high pass filters</em> and <em>low pass filters</em> which are so much popular. In order to show you an example take a look at the following figure which illustrates the convolution of two signals:</p>

<p><a href=""https://i.stack.imgur.com/kTBiy.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kTBiy.gif"" alt=""enter image description here""></a></p>
","1","2","28175","12588"
"26322","<p>If I get the point right based on the title of the question, in your code you are not making multi layer perceptron. You have tried to make somehow a convolutional network. In MLPs you just have to stack dense layers. Do something like the following code snippet which stacks just dense layers:</p>

<pre><code>model = Sequential()
model.add(keras.layers.Dense(10))
model.add(keras.layers.Dense(10))
model.add(keras.layers.Dense(10))
model.add(keras.layers.Dense(10, activation = 'softmax'))
</code></pre>
","0","2","28175","12588"
"26336","<p>Logistic and other activation functions are used to add non linearity to the neural network models.
In logistic model which uses just one neuron it may have a nice interpretation. To interpret what it does other than probability of belonging to each class, its sign shows whether your input data is on the left of the decision boundary or on the right of that. Also, if it has great value, it shows more confidence of belonging to that class.</p>
","1","2","28175","12588"
"26359","<p>You do feature scaling for accelerating learning process. Features may have different scales. One maybe from 1 to 10 and one may be from -100 to 1000. Using normalization, you make the scale of them the same as each other, helps accelerate the learning process. You should find the mean and variance for each feature separately on your training data. then during training and testing each feature should be reduced by the corresponding mean and be divided by the corresponding standard deviation. So yes, for each feature during testing and training you have to provide same values for mean and std which are obtained using training data. I suggest you taking a look at <a href=""https://datascience.stackexchange.com/a/2595/28175"">here</a>.</p>
","1","2","28175","12588"
"26384","<p>Although your input data is three dimensional, you have to use <code>Conv2D</code> for your task. I guess <code>Conv3d</code> is used for data with temporal characteristic, yours is just a simple picture. To illustrate why you should <code>Conv2D</code>, suppose your input image is <code>224 * 224 * 3</code> and you employ a <code>Conv2D</code> layer with 10 filters. You have to specify stride and padding in order to specify the output shape. You have to specify dimensions to illustrate the height and width of you filters, also known as kernels, filter size will affect the output size if you assign padding to 'valid'. Here, there is a point. Suppose you have specified the filter size a <code>10 * 10</code> filter, then if the input shape was <code>224 * 224 * 1</code>, each filter would be of size <code>10 * 10 * 1</code> to fit the input area. Now that the input is of size <code>224 * 224 * 3</code> the size of each kernel is <code>10 * 10 * 3</code> to fit the input <strong>volume</strong>. Consider in all cases the output of each convolution operation, better to say cross correlation, is a scalar. For more information take a look at videos <a href=""https://www.coursera.org/learn/convolutional-neural-networks/home/week/1"" rel=""nofollow noreferrer"">here</a> and for your case I encourage you watching <em>Convolution Over Volume</em>.</p>
","4","2","28175","12588"
"26402","<p><code>W</code> represents the wight matrix. This is what you try to learn, in other words to fix. <code>X</code> is the input data. During training, you have to provide <code>X</code>, feature vector, in order to learn parameters <code>W</code>. You learn this parameter matrix / vector,  in order to have a model which represents your data. This model will be able to classify your seen and unseen data. Suppose that you have the following data. The input feature vector <code>X</code> is a two dimensional vector, horizontal and vertical axes represent the value for each dimension. The input features belong to either class blue or red. You try to find the separator line by learning. You learn how to specify parameter vector <code>W</code> to have a line which separates the data to each class. Using this procedure, after learning, you will be able to classify unseen data based on position of the data. Consider each setting of parameters will result in a different separator line. You try to find the best.</p>

<p><a href=""https://i.stack.imgur.com/IiDeV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IiDeV.png"" alt=""enter image description here""></a></p>
","1","2","28175","12588"
"26413","<p>Actually in these cases I usually plot the data based on features <a href=""https://datascience.stackexchange.com/a/25271/28175"">two by two</a>. This approach is so much similar to watching <code>covariance matrix</code>. In this way you should see the new added features whether they have correlation, whether they change linearly with each of previous features. Suppose that you have already one feature for a classification task. Then you may want to add another feature. You have to plot data with each feature as one axis and investigate whether they have linear correlation or not. If they don't have any correlation or their correlation is near to zero, you should add the feature because that may help you, they provide a kind of knowledge that the previous features didn't provide. If a new feature has correlation with the previous feature, it means that adding that does not help you have a new knowledge or perception of that concept. Although there are debates <a href=""https://datascience.stackexchange.com/q/24452/28175"">here</a> I prefer not to add correlated features because of computation complexity. That will be so time consuming. </p>

<p>For illustrating more, suppose you have a data-set A = {X1, X2, y} in which X1 and X2 are the feature and y is the label and all are binary values. Also suppose that the covariance matrix between these is as follows.</p>

<p><a href=""https://i.stack.imgur.com/2CJKw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2CJKw.png"" alt=""enter image description here""></a></p>

<ul>
<li><p>We may infer that we can ignore X1 in the process of classification without loss of accuracy, if e is equal to zero which means label y is independent of X1, since they are binary valued. We can ignore X1 in classification, regardless of other values. </p></li>
<li><p>We may infer that we can ignore either one of features in the process of classification without loss of accuracy, if |d| >> 0 which means there is high correlation between X1 and X2 and hence information redundancy. So we may ignore one of them without problem.</p></li>
<li><p>We may infer that we can not ignore neither X1 nor X2 in the process of classification, if d = 0 and none of e or f are zero. Then the feature are independent and both are influential on label. So we may not ignore any of them.</p></li>
</ul>
","2","2","28175","12588"
"26476","<blockquote>
  <p>I understand the advantages of ReLU, which is avoiding dead neurons during backpropagation.</p>
</blockquote>

<p>This is not completely true. The neurons are not dead. If you use sigmoid-like activations, after some iterations the value of gradients saturate for most the neurons. The value of gradient will be so small and the process of learning happens so slowly. This is vanishing and exploding gradients that has been in sigmoid-like activation functions. Conversely, the dead neurons may happen if you use <code>ReLU</code> non-linarity, which is called <a href=""https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks""><em>dying ReLU</em></a>. </p>

<blockquote>
  <p>I am not able to understand why is ReLU used as an activation function if its output is linear</p>
</blockquote>

<p>Definitely it is not linear. As a simple definition, linear function is a function which has same derivative for the inputs in its domain.</p>

<blockquote>
  <p><a href=""http://www.columbia.edu/itc/sipa/math/linear.html"" rel=""noreferrer"">The linear function</a> is popular in economics. It is attractive because it is simple and easy to handle mathematically. It has many important applications. Linear functions are those whose graph is a straight line. A linear function has the following property:<br /><br />
  <span class=""math-container"">$f(ax + by) = af(x) + bf(y)$</span></p>
</blockquote>

<p><a href=""https://www.quora.com/Why-is-ReLU-non-linear"" rel=""noreferrer""><code>ReLU</code> is not linear</a>. <em>The simple answer is that <code>ReLU</code>'s output is not a straight line, it bends at the x-axis. The more interesting point is what’s the consequence of this non-linearity. In simple terms, linear functions allow you to dissect the feature plane using a straight line. But with the non-linearity of <code>ReLU</code>s, you can build arbitrary shaped curves on the feature plane.</em></p>

<p><code>ReLU</code> may have a disadvantage which is its expected value. There is no limitation for the output of the <code>Relu</code> and its expected value is not zero. <code>Tanh</code> was more popular than <code>sigmoid</code> because its expected value is equal to zero and learning in deeper layers occurs more rapidly. Although <code>ReLU</code> does not have this advantage <code>batch normalization</code> solves <a href=""https://datascience.stackexchange.com/q/23493/28175"">this problem</a>. </p>

<p>You can also refer <a href=""https://stats.stackexchange.com/q/299915/179078"">here</a> and <a href=""https://stats.stackexchange.com/q/141960/179078"">here</a> for more information.</p>
","11","2","28175","12588"
"26494","<p>Actually I'm currently not working on this area but I remember something from past that may help you. JPG files use quantization, it is really difficult for forgery detection but I suggest you reading the following paper.</p>

<p><a href=""http://ieeexplore.ieee.org/document/7453773/?reload=true"" rel=""nofollow noreferrer"">Performance analysis of forgery detection of JPEG image compression</a></p>

<blockquote>
  <p><em>The proposed forensic algorithm to discriminate between original and forged regions in JPEG images, under the hypothesis that the tampered image presents a double JPEG compression, either aligned (A-DJPG) or nonaligned (NA-DJPG). Unlike previous approaches, the proposed algorithm does not need to manually select a suspect region in order to test the presence or the absence of double compression artifacts. Based on an improved and unified statistical model characterizing the artifacts that appear in the presence of both A-DJPG or NA-DJPG, the proposed algorithm automatically computes a likelihood map indicating the probability for each 8 × 8 discrete cosine transform block of being doubly compressed. The validity of the proposed approach has been assessed by evaluating the performance of a detector based on thresholding the likelihood map, considering different forensic scenarios. The effectiveness of the proposed method is also confirmed by tests carried on realistic tampered images. An interesting property of the proposed Bayesian approach is that it can be easily extended to work with traces left by other kinds of processing.</em>  </p>
</blockquote>
","1","2","28175","12588"
"26529","<p>It highly depends on your data. If it's image, I guess it is somehow logical but if not I recommend you constructing covariance matrix and tracking whether features have correlation or not. If you see many features are correlated, it is better to discard correlated features. You also can employ <code>PCA</code> to do this. Correlated features cause larger number of parameters for neural network. </p>

<p>Also I have to say that maybe you can reduce the number of parameters if your inputs are images by resizing them. In popular nets the length and height of input images are usually less than three hundred which makes the number of input features <code>90000</code>. Also you can employ max-pooling after some convolution layers, if you are using convolutional nets, to reduce the number of parameters. Refer <a href=""https://datascience.stackexchange.com/a/26413/28175"">here</a> which maybe helpful.</p>
","4","2","28175","12588"
"26642","<p>The consideration of the number of neurons for each layer and number of layers in <strong>fully connected networks</strong> depends on the feature space of the problem. For illustrating what happens in the two dimensional cases in order to depict, I use 2-d space. I have used images from the works of <a href=""https://scholar.google.com/citations?user=HOauvegAAAAJ&amp;hl=en"" rel=""noreferrer"">a scientist</a>. For understanding other nets like <code>CNN</code> I recommend you taking a look at <a href=""https://datascience.stackexchange.com/a/26291/28175"">here</a>.</p>

<p>Suppose you have just a single neuron, in this case after learning the parameters of the network you will have a linear decision boundary which can separate the space to two individual classes. </p>

<p><a href=""https://i.stack.imgur.com/Yo1rq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Yo1rq.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/gYDvI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gYDvI.png"" alt=""enter image description here""></a></p>

<p>Suppose that you are asked to separate the following data. You will need <code>d1</code> which specifies the upper decision boundary and somehow is doing <code>AND</code> operation to determine whether the input data is on the left side of it or on the right side. Line <code>d2</code> is doing another <code>AND</code> operation which investigates whether the input data is upper than <code>d2</code> or not. In this case <code>d1</code> is trying to understand whether the input is on the left side of line to classify the input as <em>circle</em>, also <code>d2</code> is trying to figure out whether the input is on the right side of the line to classify the input as <em>circle</em>. Now we need another <code>AND</code> operation to wrap up the results of the two lines which are constructed after training their parameters. If the input is on the left side of <code>d1</code> and on the right side of <code>d2</code>, it should be classified as <em>circle</em>.</p>

<p><a href=""https://i.stack.imgur.com/JOB2t.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JOB2t.png"" alt=""enter image description here""></a></p>

<p>Now suppose that you have the following problem and you are asked to separate the classes. In this case the justification is exactly like the above's.</p>

<p><a href=""https://i.stack.imgur.com/1kIMG.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1kIMG.png"" alt=""enter image description here""></a></p>

<p>For the following data:</p>

<p><a href=""https://i.stack.imgur.com/6hLmG.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6hLmG.png"" alt=""enter image description here""></a></p>

<p>the decision boundary is not convex and is more complex than the previous boundaries. First you have to have a sub-net which finds the inner circles. Then you have to have another sub-net which finds the inner rectangular decision boundary which decides the inputs which are inside of the rectangle are not circle and if they are outside, they are circle. After these, you have to wrap up the results and say if the input data is inside the bigger rectangle and outside of the inner rectangle, it should be classified as <em>circle</em>. You need another <code>AND</code> operation for this purpose. The network would be like this:</p>

<p><a href=""https://i.stack.imgur.com/A8FiS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/A8FiS.png"" alt=""enter image description here""></a></p>

<hr />

<p>Suppose that you are asked to find the following <strong>circled</strong> decision boundary.</p>

<p><a href=""https://i.stack.imgur.com/XXFsf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/XXFsf.png"" alt=""enter image description here""></a></p>

<p>In this case your network would be like the following network which was referred to but with much more neurons in the first hidden layer. </p>

<p><a href=""https://i.stack.imgur.com/UM1rH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/UM1rH.png"" alt=""enter image description here""></a></p>
","25","2","28175","12588"
"26645","<p>You have not specified which deep net you are using. You can use any data set which is of the right format in <code>TensorFlow</code>. If you are using convolutional networks, You may need data which locality is important. In this case:</p>

<ul>
<li>MNIST data set </li>
<li>ImageNet data set</li>
</ul>

<p>may help you. If you are doing other tasks I highly recommend you taking a look at <a href=""https://datascience.stackexchange.com/q/155/28175"">this</a> question which is so much popular and has great answers.</p>

<p>If I want to quote, I quote the <a href=""https://datascience.stackexchange.com/a/4997/28175"">following answer</a>. <a href=""http://networkrepository.com"" rel=""nofollow noreferrer"">Network Repository</a> an interactive data repository with over 600+ networks in 20+ collections; from large-scale social networks, web graphs, biological networks, communication and technological networks my <strong>help</strong> you.</p>

<blockquote>
  <h2>Data Sets</h2>
  
  <ul>
  <li><a href=""http://academictorrents.com/"" rel=""nofollow noreferrer"">Academic Torrents</a></li>
  <li><a href=""http://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public"" rel=""nofollow noreferrer"">Quora</a></li>
  <li><a href=""http://hadoopilluminated.com/hadoop_illuminated/Public_Bigdata_Sets.html"" rel=""nofollow noreferrer"">hadoopilluminated.com</a></li>
  <li><a href=""http://catalog.data.gov/dataset"" rel=""nofollow noreferrer"">data.gov</a></li>
  <li><a href=""http://www.quandl.com/"" rel=""nofollow noreferrer"">Quandl</a></li>
  <li><a href=""https://www.freebase.com/"" rel=""nofollow noreferrer"">freebase.com</a></li>
  <li><a href=""http://usgovxml.com/"" rel=""nofollow noreferrer"">usgovxml.com</a></li>
  <li><a href=""http://public.enigma.com/"" rel=""nofollow noreferrer"">enigma.com</a></li>
  <li><a href=""http://datahub.io/"" rel=""nofollow noreferrer"">datahub.io</a></li>
  <li><a href=""http://aws.amazon.com/datasets"" rel=""nofollow noreferrer"">aws.amazon.com/datasets</a></li>
  <li><a href=""http://databib.org/"" rel=""nofollow noreferrer"">databib.org</a></li>
  <li><a href=""http://www.datacite.org"" rel=""nofollow noreferrer"">datacite.org</a></li>
  <li><a href=""https://www.quandl.com/"" rel=""nofollow noreferrer"">quandl.com</a></li>
  <li><a href=""http://figshare.com/"" rel=""nofollow noreferrer"">figshare.com</a></li>
  <li><a href=""http://dev.maxmind.com/geoip/legacy/geolite/"" rel=""nofollow noreferrer"">GeoLite Legacy Downloadable Databases</a></li>
  <li><a href=""http://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public"" rel=""nofollow noreferrer"">Quora's Big Datasets Answer</a></li>
  <li><a href=""http://hadoopilluminated.com/hadoop_illuminated/Public_Bigdata_Sets.html"" rel=""nofollow noreferrer"">Public Big Data Sets</a></li>
  <li><a href=""http://data.ohouston.org/"" rel=""nofollow noreferrer"">Houston Data Portal</a></li>
  <li><a href=""https://www.kaggle.com/wiki/DataSources"" rel=""nofollow noreferrer"">Kaggle Data Sources</a></li>
  <li><a href=""http://www.1000genomes.org/data"" rel=""nofollow noreferrer"">A Deep Catalog of Human Genetic Variation</a></li>
  <li><a href=""https://www.freebase.com/"" rel=""nofollow noreferrer"">A community-curated database of well-known people, places, and things</a></li>
  <li><a href=""http://www.google.com/publicdata/directory"" rel=""nofollow noreferrer"">Google Public Data</a></li>
  <li><a href=""http://data.worldbank.org/"" rel=""nofollow noreferrer"">World Bank Data</a></li>
  <li><a href=""http://nyctaxi.herokuapp.com/"" rel=""nofollow noreferrer"">NYC Taxi data</a></li>
  <li><a href=""http://www.opendataphilly.org/"" rel=""nofollow noreferrer"">Open Data Philly</a> Connecting people with data for Philadelphia</li>
  <li><a href=""http://networkrepository.com"" rel=""nofollow noreferrer"">Network Repository</a> An interactive data repository with over 600+ networks  in 20+ collections; from large-scale social networks, web graphs, biological networks, communication and technological networks, etc.</li>
  <li><a href=""http://ahmetkurnaz.net/en/statistical-data-sources/"" rel=""nofollow noreferrer"">A list of useful sources</a> A blog post includes many data set databases</li>
  </ul>
</blockquote>

<p><a href=""https://github.com/okulbilisim/awesome-datascience#data-sets"" rel=""nofollow noreferrer"">Data Sets</a> From <a href=""https://github.com/okulbilisim/awesome-datascience"" rel=""nofollow noreferrer"">awesome-datascience</a></p>
","1","2","28175","12588"
"26646","<p>Based on <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"" rel=""nofollow noreferrer"">here</a>, use <code>sklearn.model_selection.train_test_split(*arrays, **options)</code> in order to split your data into train and test. Train your model on train-split and use the <code>predict</code> method to see the performance on the test data. As an example take a look at the following code which splits the data to two separate groups.</p>

<pre><code>import numpy as np
from sklearn.model_selection import train_test_split
X, y = np.arange(10).reshape((5, 2)), range(5)
X

array([[0, 1],
       [2, 3],
       [4, 5],
       [6, 7],
       [8, 9]])

list(y)
[0, 1, 2, 3, 4]


X_train, X_test, y_train, y_test = train_test_split(
...     X, y, test_size=0.33, random_state=42)
...
X_train
array([[4, 5],
   [0, 1],
   [6, 7]])
y_train
[2, 0, 3]
X_test
array([[2, 3],
   [8, 9]])
y_test
[1, 4]

train_test_split(y, shuffle=False)
[[0, 1, 2], [3, 4]]
</code></pre>
","0","2","28175","12588"
"26649","<p>Based on the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"" rel=""noreferrer"">documentation</a> <code>cosine_similarity(X, Y=None, dense_output=True)</code> returns <em>an array with shape (n_samples_X, n_samples_Y)</em>. Your mistake is that you are passing [vec1, vec2] as the first input to the method. Also your vectors should be <code>numpy</code> arrays:</p>

<pre><code>from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
vec1 = np.array([[1,1,0,1,1]])
vec2 = np.array([[0,1,0,1,1]])
#print(cosine_similarity([vec1, vec2]))
print(cosine_similarity(vec1, vec2))
</code></pre>

<blockquote>
  <p>X : ndarray or sparse array, shape: (n_samples_X, n_features)
      Input data.</p>
</blockquote>

<p>So you have to specify the dimension. </p>

<pre><code>np.array([1, 2]).shape
</code></pre>

<p>has funny shape:</p>

<blockquote>
  <p>(2, )</p>
</blockquote>
","11","2","28175","12588"
"26697","<p><code>PCA</code> is used to abandon having redundant features. It expands directions which your data is highly distributed in those directions. During this process, it does not care about the labels of your data. In those directions your data will be highly distributed and none of features have correlation. <code>PCA</code> just does the preceding things. In that feature space your data may be easily separable or not. But using this before neural nets is a way to reduce the redundant features which may cause your net have too many parameters. It is a kind of pre-processing for just reducing the correlated features, although there are different reasons that one can apply <code>PCA</code>, like data visualization or understanding data, or even reporting data based on e.g. three main components. I recommend you using that because you may find that in the new space you would classify your data with a more smaller net.</p>

<blockquote>
  <p>Does the combination of a PCA before neural nets makes sense</p>
</blockquote>

<p>Yes, you can do that as a pre-processing stage.</p>

<blockquote>
  <p>... as the neural nets also reduces the information in internal layers? </p>
</blockquote>

<p>neural nets do not necessarily reduce the information in internal layers. In convolutional nets, max-pooling layer somehow reduces the unnecessary information but other usual nets such as convolutional layers or dense layers, try to change the space of the inputs of the layers or equivalently with another interpretation they try to find other features and patterns that the data can be separated in those spaces. <code>PCA</code> actually reduces the correlated features. </p>

<blockquote>
  <p>Does anybody has experiences with such a combination?</p>
</blockquote>

<p>Yes. It all depends on your data. In some applications it worked for me, which I had so many correlated features. But it has happened that it does not work very well in both cases of existing or not existing of correlated features. </p>
","4","2","28175","12588"
"26719","<p>PCA is used to eliminate redundant features. It finds directions which data is highly distributed in. It does not care about the labels of the data, because it is a projections which <strong>represents</strong> data in least-square sense. Multiple Discriminant Analysis, <code>MDA</code> try to find projections which best <strong>separates</strong> the data. The latter considers the label and finds directions that data can be separated the best, although it has some details about the kind of decision that finds. To wrap up, <code>PCA</code> is not a learning algorithm. It just tries to find directions which data are highly distributed in order to eliminate correlated features. Similar approaches like <code>MDA</code> try to find directions in order to classify the data. Although <code>MDA</code> is so much like <code>PCA</code>, but the former is used for classification, it considers the labels, but the latter is not directly used for classification. </p>
","4","2","28175","12588"
"26728","<p>I recommend you using <code>PCA</code>. It finds directions which data is highly distributed in. Using this procedure, the components __ new features __ will be in descending order for the eigenvalues. Each eigenvalue that has greater value than the next eigenvalues, will have much information than them. After using <code>PCA</code> you can use its first three principal components for plotting. Each of the new features is a linear combination of the previous features. Using e.g. first three principal components will have so much information which will be representative of your data. In cases that data is not correlated, the preceding statement may not be always true but in your case that you have so many features, based on experience, you definitely have so many correlated features. For more information take a look at <a href=""https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/principal-components/interpret-the-results/key-results/"" rel=""nofollow noreferrer"">here</a> and <a href=""https://onlinecourses.science.psu.edu/stat505/node/54"" rel=""nofollow noreferrer"">here</a> which may help you.  </p>
","0","2","28175","12588"
"26730","<p>If you have unbalanced data, at first I recommend you try to have real data. I mean do not replicate your data by hand if you don't have balanced data. You should never ever change the distribution of your data. Although it may seem that you reduce the Bayse error, your classifier won't do well in your specified application. I have two suggestions for imbalanced data:</p>

<ul>
<li>Use class weights to improve your cost function. For the rare class use a much larger value than the dominant class.</li>
<li>Use <code>F1</code> score to evaluate your classifier</li>
</ul>

<blockquote>
  <p>For an imbalanced set of data is it better to choose an L1 or L2 regularization</p>
</blockquote>

<p>These are for dealing with over-fitting problem. First of all you have to learn the training data to solve high bias problem. The latter is more common in usual tasks. They are just fine for imbalanced data set but consider the point that first you have to deal with high bias problem, learning the data, then deal with high variance problem, avoiding over-fitting.</p>
","1","2","28175","12588"
"26748","<p>You have label of classes which are not mutually exclusive which means as the label of each sample, your data won't be in one-hot-encoding format. Each output vector may have multiple ones. In such occasions you shouldn't use soft-max as the output layer. You have to use <code>Sigmoid</code> activation function for each neuron in the last layer. Suppose you have ten labels and for a typical movie each of them may be activated. So, in the last layer use a dense layer with ten <code>Sigmoid</code> activation function. You can see <a href=""https://datascience.stackexchange.com/q/25315/28175"">here</a> which may help you.</p>

<p>And as a side answer, each cost function that you are going to use should be categorical, because you have different categories. I respect so much for the other answer and I thank him/her for the answer, but I guess you should use categorical cost function otherwise the code won't work because your output matrix actually is a <em>matrix</em> of samples and not a vector of samples. </p>
","0","2","28175","12588"
"26753","<p>Whenever you have skewed data-set, it means that you know a typical class better than the others. In such cases it means that the data is your <em>knowledge</em> and is not in a way that finds the minimum of the Bayes error because you don't know the distribution of other available classes and consequently you won't be able to find out whether the distribution of different classes overlap in the current feature space. and there are learning algorithms for such occasions.</p>

<p>Consider an important fact here. Suppose that you have feature vectors of conditions of a nuclear company and they describe whether the company is in danger of nuclear radiation or not. In such cases it is clear that it does not happen a lot that you have infected companies so almost, all of you data has label of healthy condition. You have so much knowledge about the healthy class but you don't know much about the infected class because you don't have much data; consequently, you don't know its distribution and you can not estimate it well. Whenever your data is skewed, it means that e.g. you have 1 million feature vectors of negative class and 5 feature vectors of positive class if there is any.</p>

<p>I quote from <a href=""https://datascience.stackexchange.com/a/25413/28175"">here</a> that <em>in statistical learning there is something called Bayes Error, whenever the distribution of classes overlap, the ratio of error is large. without changing the features, the Bayes error of the current distributions is the best performance and can not be reduced at all</em> If the number of samples of each class is equal. In anomaly problems, this is not possible. You can not find balanced samples for each class.</p>
","1","2","28175","12588"
"26754","<p>It seems that after some epochs your training oscillates. I guess the reason is that the learning rate is high. Try to set the decay parameter of <code>Adam</code> optimizer to a number more than one and resume training. </p>
","1","2","28175","12588"
"26855","<p>For model evaluation there are different metrics based on your model:</p>

<ul>
<li><p>Confusion matrix</p>

<p><strong>Classification accuracy</strong>:<br />
(TP + TN) / (TP + TN + FP + FN)<br />
<strong>Error rate</strong>: <br />
(FP + FN) / (TP + TN + FP + FN)<br /></p></li>
<li><p>Paired criteria</p>

<p><strong>Precision</strong>: (or Positive predictive value)<br />
proportion of predicted positives which are actual positive<br />
TP / (TP + FP)<br />
<strong>Recall</strong>: proportion of actual positives which are predicted positive<br />
TP / (TP + FN)<br /></p>

<p><strong>Sensitivity</strong>: proportion of actual positives which are predicted positive<br />
TP / (TP + FN)<br />
<strong>Specificity</strong>: proportion of actual negative which are predicted negative<br />
TN / (TN + FP)<br />
<strong>True positive rate</strong>: proportion of actual positives which are predicted
positive <br />
TP / (TP + FN)<br />
<strong>True negative rate</strong>: proportion of actual negative which are predicted
negative<br />
TN / (TN + FP)<br />
<strong>Positive likelihood</strong>: likelihood that a predicted positive is an actual positive <br />
sensitivity / (1 - specificity)<br />
<strong>Negative likelihood</strong>: likelihood that a predicted negative is an actual
negative<br />
(1 - sensitivity) / specificity<br /></p></li>
<li><p>Combined criteria</p>

<p><strong>BCR</strong>: Balanced Classification Rate<br />
½ (TP / (TP + FN) + TN / (TN + FP))<br />
<strong>BER</strong>: Balanced Error Rate, or HTER<br />
<strong>Half Total Error Rate</strong>: 1 - BCR<br />
F-measure harmonic mean between precision and recall <br />
2 (precision . recall) / (precision + recall)<br />
<strong>Fβ-measure</strong> weighted harmonic mean between precision and recall<br />
(1+β)2 TP / ((1+β)2 TP + β2 FN + FP)<br /></p>

<blockquote>
  <p>The harmonic mean between specificity
  and sensitivity is also often used and
  sometimes referred to as F-measure.</p>
</blockquote></li>
<li><p>Youden's index: arithmetic mean</p>

<p><strong>between sensitivity and specificity</strong><br />
sensitivity - (1 - specificity)<br />
<strong>Matthews correlation</strong>: correlation between the actual and predicted<br /> 
(TP . TN – FP . FN) / ((TP+FP) (TP+FN) (TP + FP) (TN+FN)) ^ (1/2) <br />
comprised between -1 and 1 Discriminant power normalized likelihood index
sqrt(3) / π .
(log (sensitivity / (1 – specificity)) +
log (specificity / (1 - sensitivity)))
&lt;1 = poor, >3 = good, fair otherwise</p></li>
</ul>

<p>You can find much more <a href=""http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf"" rel=""nofollow noreferrer"">here</a>. Also there are some explanations <a href=""https://www.dezyre.com/data-science-in-python-tutorial/performance-metrics-for-machine-learning-algorithm"" rel=""nofollow noreferrer"">here</a> and you can find useful code snippet from <a href=""https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/"" rel=""nofollow noreferrer"">here</a> which are implemented. </p>
","4","2","28175","12588"
"26857","<p>If I've got the point of your question, it means that convolutional nets can learn different filters. You may train a net with random initialization and after training you can find learned-filters which are familiar to you or not. You may see that your model has learned <code>Sobel</code> filter which is a high pass filter or maybe you may see that the weights which are learned are exactly the same as a mean filter which is a low pass filter. In all cases network tries to learn filters that help it find appropriate features of the inputs. The learned filters can be anything. They may be known to you or not. So, yes. They can learn all kind of features like low pass and high pass filters. Also for visualization purposes to fully figure out what is going on inside them I recommend you taking a look at <a href=""https://datascience.stackexchange.com/a/26291/28175"">here</a>.</p>
","2","2","28175","12588"
"26861","<p>First of all don't change the distribution of your data. Your classifier won't perform good at test time if the real data is not balanced.</p>

<p>If you have approximately equal amount of data for each class and they have same importance, accuracy is a good metric which can help you understand how much your classifier has performed well without regarding to the number of labels, binary or multi-class classificaitons. </p>

<p><strong>Binary classification</strong><br />
There is no difference between 0 or 1 because you yourself choose to assign each class to a label but it is common to set the rare and maybe, in some occasions, the desired output to 1.</p>

<p>In cases which data is distributed badly and it is so much skewed it recommended to use <code>F1</code> score. For <em>last, imagine a 75/25 ratio for either</em> I personally prefer to use <code>F1</code> because it is unbalanced.</p>

<p>Also in cases where one class is much important than the other class and maybe the data is skewed, it is better to use <code>recall</code>, although you have to change the cost function to emphasize the importance of one class over the other. As an example, suppose that you want to know whether a patient has a bad illness or not. In such cases you try to predict correctly all of those who has that illness because if you tell that someone has an illness but you're wrong, it will have less harm than saying a patient is healthy. So in these cases <code>recall</code> is so much better than <code>precision</code>. </p>

<p><strong>Multi-Class classification</strong><br /></p>

<blockquote>
  <p>Majority of samples are dominated by one class (very common in my experience)</p>
</blockquote>

<p>In this case first of all, if the data is so much skewed, I suggest you using anomaly detection. But if you have more than <em>few</em> data, you can again use <code>F1</code> score. </p>

<blockquote>
  <p>Classes have roughly equally the same amount</p>
</blockquote>

<p>In this case you can use accuracy if the classes have the same importance. If they don't, again use accuracy but change the cost function to emphasize on the important class.</p>

<blockquote>
  <p>Many class labels with few samples in each (imagine 1000 samples with 100 classes ranging from 2 - 20 samples in each)</p>
</blockquote>

<p>In this case I don't think you will have a good learning. Anyway, you have to use confusion matrix in this for tracking all classes. Although I guess your model will be over-fitted because of the amount of provided data.</p>

<p><hr />
Consider that in all cases you can use <em>confusion matrix</em>.
<hr />
Finally I suggest you taking a look at <a href=""https://datascience.stackexchange.com/a/26855/28175"">here</a> and <a href=""http://www.damienfrancois.be/blog/files/modelperfcheatsheet.pdf"" rel=""nofollow noreferrer"">here</a>.</p>
","1","2","28175","12588"
"26862","<p>The labels of data are not mutually exclusive so you can't say this is a one vs. all problem, because more than one entry may be one in the output vector. Moreover, if in the seen there should be an apple or pear this can be considered as an exhaustive problem which means one of them should happen for each input. </p>

<p>My opinion is that for this problem you don't have to make a new cost function. For the mutually exclusive part, as you have truly stated that, and for the second part of your vector, the well known <code>cross-entropy</code> cost function will perform fine. I guess the problem is something else. For problems with mutually exclusive classes, we use <a href=""https://datascience.stackexchange.com/q/25315/28175"">soft max layer</a> as the last layer for neural nets while for cases which classes are not mutually exclusive, you can use <code>sigmoid</code> as the activation function. In your case that you have combination of them I suggest you an alternative approach:</p>

<p>Change your mutually exclusive part to a binary output, means if the corresponding entry is less than half, you can understand that it is e.g. an apple otherwise, it is the other class. and for the rest, just keep the output vector as it is. Finally use <code>sigmoid</code> activation function as the last layer if you are using neural nets.</p>
","1","2","28175","12588"
"26863","<p>Actually it is not completely clear which deep neural nets you are referring to but I guess you are referring to Dense, aka fully connected, networks. It depends on your data but for simplification, based on the answer <a href=""https://datascience.stackexchange.com/a/26642/28175"">here</a>, neurons of the first hidden layer try to find lines for separating the data, and the neurons of the subsequent layers try to consider them simultaneously, I suggest reading the hole answer. As you increase the number of neurons in the first layer, you try to increase the number of lines for separation, The last shape of the link, and as you try to increase the number of e.g. the second layer, the number of convex shapes will increase. If you increase them too much you will over-fit your data. </p>

<p>Increasing the number of filters, neurons or kernels, in convolutional nets has another meaning. If you increase the number of filters, it means that you try to find more features that can represent your data better. In conv layers you actually try to find features of data. Although you are somehow doing same procedure in a dense net but because the neurons are less connected and they get updated similarly in conv nets, the features learned by conv nets are completely different from dense layers. for more information I recommend you taking a look at <a href=""https://datascience.stackexchange.com/a/26291/28175"">here</a>.</p>
","1","2","28175","12588"
"26889","<p>Both cases will work. The point that you have to consider is that you shouldn't use <code>Softmax</code> activation in the last layer because your classes have intersection and they are not mutually exclusive. You should employ <code>Sigmoid</code> activation function and there should be two of them which each of them can be one or zero as the output vector. Consider that in this case you should not compare each activation to the other. Each one shows the probability of existence of the corresponding object, cat or dog.</p>

<p>For your case I suggest you to provide images which don't have either and in some cases have both. The former is more important. Because if you don't provide negative labels, not existing, the net always try to make something out of nothing. Means the net always try to flag similar patters as cats or dogs. </p>
","2","2","28175","12588"
"26892","<p>You can take a look at <a href=""http://hiddendomains.sourceforge.net/"" rel=""nofollow noreferrer"">here</a>. As you can read from there:</p>

<blockquote>
  <ol>
  <li>Download and unpack hiddenDomains</li>
  </ol>
  
  <p>If you haven't already done this, download the latest version of hiddenDomains from the Sourceforge <a href=""https://sourceforge.net/projects/hiddendomains/"" rel=""nofollow noreferrer"">website</a></p>
  
  <p>Now unpack it</p>
</blockquote>

<pre><code>shell$ tar -xzvf hiddenDomains.VERSION.NUMBER.tar.gz
</code></pre>

<blockquote>
  <p>Where VERSION and NUMBER represent the release that you downloaded.</p>
  
  <ol start=""2"">
  <li>Install R and required R packages (if necessary)
  Download and install R if you don't already have it.</li>
  </ol>
  
  <p>If you don't already have them, you will need to install two hidden Markov model libraries:</p>
  
  <p><a href=""http://cran.r-project.org/web/packages/depmixS4/index.html"" rel=""nofollow noreferrer"">depmixS4</a><br />
  <a href=""http://cran.r-project.org/web/packages/HiddenMarkov/index.html"" rel=""nofollow noreferrer"">HiddenMarkov</a><br />
  You can do this by starting R on the command line:</p>
</blockquote>

<pre><code>shell$ R
</code></pre>

<blockquote>
  <p>Or, if you prefer RStudio, you can use that.</p>
  
  <p>Regardless of how you started R, type the following commands to install the packages.</p>
</blockquote>

<pre><code>&gt; install.packages(""depmixS4"")
&gt; install.packages(""HiddenMarkov"")
</code></pre>

<p><a href=""https://stackoverflow.com/questions/23678786/rstudio-hiddenmarkov-package-and-dthmm-function"">Here</a> and <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjPv8q-q-nYAhXHLFAKHXU1BsQQFggnMAA&amp;url=https%3A%2F%2Fcran.r-project.org%2Fweb%2Fpackages%2FHMM%2FHMM.pdf&amp;usg=AOvVaw1ywdsUCA92akpljCkPV2N0"" rel=""nofollow noreferrer"">here</a> may also be helpful for knowing how to use it.</p>
","0","2","28175","12588"
"26893","<p>Based on the solution <a href=""https://stackoverflow.com/questions/43833081/attributeerror-module-object-has-no-attribute-computation"">here</a> you have different choices:</p>

<p>The simplest solution is to revert back to <code>Pandas 0.19.2</code>. For this purpose use the following command in your command line or terminal.</p>

<pre><code>conda install pandas=0.19.2
</code></pre>
","1","2","28175","12588"
"26900","<p>Based on my experience you can almost do everything that can be done using pandas in your sql. I've not seen recent versions of pandas but I remember that sql is even better because using pandas you are restricted to the size of memory. If memory fills out you may crash, something that does not happen using sql commands. You can save your pandas data frame in a <code>csv</code> file and manipulate that <code>csv</code> file using your sql. <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html"" rel=""nofollow noreferrer"">This link</a> and also <a href=""https://stackoverflow.com/questions/16923281/pandas-writing-dataframe-to-csv-file"">here</a> may help you. Also for importing your <code>csv</code> file to your sql, you have not specified what sql you have but <a href=""https://stackoverflow.com/questions/15242757/import-csv-file-into-sql-server"">this link</a> may help you. Other sqls also provide this behavior. </p>
","1","2","28175","12588"
"26916","<p>You can read the popular paper <a href=""http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf"" rel=""noreferrer"">Understanding Neural Networks Through Deep Visualization</a> which discusses visualization of convolutional nets. <a href=""https://github.com/yosinski/deep-visualization-toolbox"" rel=""noreferrer"">Its implementation</a> not only  displays each layer but also depicts the activations, weights, deconvolutions and many other things that are deeply discussed in the paper. It's code is in <code>caffe'</code>. The interesting part is that you can replace the pre-trained model with your own.</p>
","5","2","28175","12588"
"26951","<p>Actually in your case I guess the pure images are not that important. The features that you extract from them are important because if your feature space is constructed base on intensity of images at different picture elements, pixels, then you will need so many coefficients. As an easy solution, use <code>MNIST</code> digits and use <a href=""https://www.researchgate.net/publication/40902683_A_Survey_of_Shape_Feature_Extraction_Techniques"" rel=""nofollow noreferrer"">shape features</a> to extract features from the images of numbers. You can use plausible number of features and then use <code>PCA</code> for the data that is in the new feature space that you have just constructed. In this case smaller number of coefficients will be needed if the features are fine.</p>
","0","2","28175","12588"
"26957","<p>First of all, you have to shuffle your data because it seems that the model has learned a special pattern in the training data which has not occurred in the test data so much. After that, suppose that you get a validation curve like the current one. As you can see, Increasing the value of depth, does not change the learning. The two lines are parallel. In cases which each of the lines may have intersection, the upper line has negative slope and the lower one has positive slope, in the future on seen levels, you may want to increase the number of levels, not in this case.</p>

<p>Having same error, means that you are not over-fitting. but as you can see the amount of learning is not too much which means that you are having high bias problem, which means you have not learned the problem so well. In this case means that your current feature space maybe has high Bayes error which means that there are samples which have same features and different labels. Actually the distributions of different classes overlap.</p>

<p>There is something to argue about decision trees. If you have numerical features which are continuous, you may not have exactly same input patterns but they have overlap in their range.</p>
","0","2","28175","12588"
"26960","<p>In regression problems it is customary to normalize the output too, because the scale of output and input features may differ. After getting the result of the <code>SVR</code> model, you have to add the mean to the result and multiply that by the standard deviation, if you have done that during normalizing.</p>

<blockquote>
  <p>How can one explain the impact of output normalization for <code>svm</code> regression?</p>
</blockquote>

<p>If you normalize your data, you will have a cost function which is well behaved. means that you can find local parts more easily. The reason is that you have to construct the output using the input features in regression problems. It is difficult to make large values with small normalized features but with small numbers, making a normalized output is easier and can be learned faster. </p>
","3","2","28175","12588"
"26973","<p>What you need to do:</p>

<ol>
<li>Ensure that your output vector for training and test data is exactly what you need, continuous for each element of output vector. </li>
<li>Use what you said and familiar for the layers before the last layer.</li>
<li>For the last layer use a dense layer with n, number of outputs, outputs each having linear activation, y = x.</li>
</ol>
","1","2","28175","12588"
"27039","<p>I guess you are doing something wrong in your code. I guess its better to use <a href=""https://www.coursera.org/learn/deep-neural-network/lecture/htA0l/gradient-checking"" rel=""nofollow noreferrer""><code>gradient checking</code></a> approach for figuring out whether the whole code has any problem or not. </p>

<p>Based on the comments, if I want to show you exactly what happens, first take a look at <a href=""https://www.coursera.org/learn/neural-networks/lecture/zO1Is/a-simple-example-of-learning-6-min"" rel=""nofollow noreferrer"">here</a> which professor Hinton himself explains that what <code>MLPs</code> learn is like learning masks instead of learning the features of the inputs. For illustrating more you can take a look at <a href=""https://datascience.stackexchange.com/a/26642/28175"">here</a> which shows that MLPs can learn what and I hope that you can expand it to higher dimensions, for your case 784 dimensions. If you use <code>MLPs</code> alone, you will face to the problem of something which is like masking. What <code>CNNs</code> try to find is feature. They try to find features which can be better explained by finding hierarchical features. You can also take a look at <a href=""https://datascience.stackexchange.com/a/26291/28175"">here</a> which explains that using <code>CNNs</code> you try to find features, and after convolution layers, you use dense layers, <code>MLPs</code> which they try to classify the features. </p>

<p><code>MLPs</code> are good for classifying <code>MNIST</code> data set but they are weak for generalizing unseen data. If your code works fine after using <em>gradient checking</em> try to use different hyper parameters which I guess our friend has explained so much well for you.</p>
","2","2","28175","12588"
"27066","<p>If I've got the meaning of question, first convolution accepts inputs of size <code>224*224*3</code> means that height and width are both <code>224</code> and the size of depth or number of activation maps, channels here, is equal to three. The output of this layer will be activation maps with equal height and width as the input, <code>224</code>, because it is <em>same</em> convolution. The number of activation maps for this layer which is going to be passed to the next layer is equal to <code>64</code> because you have <code>64</code> filters in this layer. The point is that each filter is of size <code>3*3*3</code> to fit to the input. The output of each filter is an activation map of size 224*224*1. The output of filters come together and construct output of size <code>224*224*64</code> which means the input of the next layer will have <code>64</code> channels, actually depth here. Consequently, the filters of the second convolution layer is of size <code>3*3*224</code> to match the entire input of the previous layer. In other words, you will have 64 filters of size <code>3*3*64</code> and each will have an output of size <code>224*224*1</code>. Take a look at <a href=""https://www.coursera.org/learn/convolutional-neural-networks/lecture/MmYe2/classic-networks"" rel=""nofollow noreferrer"">here</a> which definitely can help you.
<hr />
The purpose of writers of <code>VGG</code> net was to make a network which was just deep enough to perform well on <code>ImageNet</code> data-set. <code>CNNs</code> have lots of hyper parameters which setting them is not based on well behaved math stuff. I mean there is no proof to show which hyper parameter is better than the others. They are found based on experience. They are gained practically. Writers of this paper tried to show that if you use same hyper parameters for convolution layers and just make the network deep and deeper, it was much deeper than <code>AlexNet</code>, you will gain good performance without caring about different set of hyper parameters. For understanding what <code>ConvNets</code> do, there is already an answer <a href=""https://datascience.stackexchange.com/a/26291/28175"">here</a> which may help you, it contains the interpretation of different layers in <code>CNNs</code>. They have reached to this setting of convolution layers and architecture by experience and there idea was too use just a deep net without complicated hyper parameters.</p>
","1","2","28175","12588"
"27077","<p>You are initializing your weights to zero. This will cause symmetric problem in updating weights. Use <code>Xavier</code> or <code>He</code> technique for initialization and train for some epochs.</p>
","2","2","28175","12588"
"27086","<p>You have imbalanced data set, so you should use <code>F1</code> score. Also you can use weight for rare classes, so that your cost function will be formed in a way that it cares about rare classes so much and tries to classify them correctly. You also can use confusion matrix for the details, but <code>F1</code> will suffice. And yes, use <code>F1</code> instead of precision or recall. You can also take a look at <a href=""https://datascience.stackexchange.com/a/26730/28175"">here</a>.</p>
","1","2","28175","12588"
"27097","<p>I usually try to do the following process, I don't know whether it has name or not:</p>

<ul>
<li>Try to find features which an expert can say what the label is without hesitating. </li>
<li>Plot correlation matrix for your sample to investigate whether each feature has correlation with others or not. I do this process whenever the input features are so much. Then try to reduce the correlated features or try to apply <code>PCA</code>. Consider the point that <code>PCA</code> alone does not care about the labels, so it may find principal components that your data would be so difficult to be separated but not necessarily.</li>
</ul>
","2","2","28175","12588"
"27115","<p>I have the following suggestion:</p>

<ul>
<li><p>The size of your data-set is small, you should increase the sample size. You also have to use data augmentation methods for improving the performance, translation, orientation and blurring are highly recommended. </p></li>
<li><p>For your task I guess the most popular approach is what which is discussed in <code>YOLO</code> paper, and it is highly recommended using that if you want to have object detection not just localization and recognition for good performance.</p></li>
<li><p>I also suggest you providing real data for your task. The images of first row are artificially made. You have to provide real data which your classifier is going to face. The distribution of your test time should be like validation and train time if you want to have good performance.</p></li>
</ul>
","2","2","28175","12588"
"27117","<p>You have to specify the bin size, if I've figured out the question. As stated <a href=""https://stackoverflow.com/a/12176344/5120235"">here</a>.</p>

<p><em>You can give a list with the bin boundaries.</em></p>

<pre><code>plt.hist(data, bins=[0, 10, 20, 30, 40, 50, 100])
</code></pre>

<p>If you just want them equally distributed, you can simply use range:</p>

<pre><code>plt.hist(data, bins=range(min(data), max(data) + binwidth, binwidth))
</code></pre>

<p>You can also take a look at <a href=""https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.hist.html"" rel=""nofollow noreferrer"">here</a> and <a href=""https://matplotlib.org/examples/statistics/histogram_demo_histtypes.html"" rel=""nofollow noreferrer"">here</a>.</p>
","1","2","28175","12588"
"27131","<p>It's just the expansion of one dimensional mean and standard deviation. Suppose that you are trying to estimate the weight of a person and you have two inputs, salary and height. For finding the weight, you have two inputs which are of different scales, so you try to find the mean and variance of each feature, salary and height, separately using the data samples. Suppose you have two data samples in a tuple like (salary, height). They are as follows:</p>

<blockquote>
  <p>(10000, 180)<br />
  (5000, 175)</p>
</blockquote>

<p>Although the first guy may seem so rich, the real point about the first and second data samples is that their scale is not the same. The approach is like this:</p>

<ol>
<li>Find the mean of first feature among data samples, salary here. <code>(10000 + 5000) / 2 = 7500</code></li>
<li>Find the mean of second feature among data samples, height here. <code>(180 + 175) / 2 = 177.5</code></li>
<li>Find the standard deviation of first feature, among data samples, which is <code>2500</code> for salary.</li>
<li>Find the standard deviation of second feature, among data samples, which is <code>2.5</code> for height. </li>
<li><p>Reduce the amount of mean from corresponding feature for each data sample and divide each with the standard deviation of corresponding feature.<br />
<code>(10000 - 7500) / 2500 = 1</code><br />
<code>(5000 - 7500) / 2500 = -1</code><br /></p>

<p><code>(180 - 177.5) / 2.5 = 1</code><br />
<code>(175 - 177.5) / 2.5 = -1</code><br /></p></li>
</ol>

<p>Data samples which have been normalized now are like as follows:</p>

<blockquote>
  <p>(1, 1)<br />
  (-1, -1)</p>
</blockquote>

<p>Whenever you normalize your data, your cost function would be so easier to learn. The weights don't have to struggle to reach to high values in situations where your features are not in same range.</p>
","2","2","28175","12588"
"27154","<p>I have a suggestion for you. Maybe not complete enough to be a complete solution but that is exactly what I've experienced. Suppose that you have writings in a paper and the paper is a part of a scene, or you have boys with nice mustaches which are beside a wide scene. In such cases which you have special information which are significant for classifying, the first one may help the classifier recognize book and written things and the second one can help the classifier find the gender of people in the scene, try to resize the images in a way that such significant things be recognizable to the human. If humans can understand them, you can hope that your classifier will be able too. Consider that you shouldn't resize the images to small pixels to avoid your net being with lots parameters. If you resize the images to too small ones, you may loose important information.</p>
","1","2","28175","12588"
"27198","<p>You have not specified that what neural network you are using but as comments, you should try to fit your data first. You have to try to find a model that learns your training data. For this purpose you don't have to increase the number of data, at least not at this stage. You should try to find a good model which suits your data. For this purpose you have to change the hyper-parameters of your neural network, e.g. number of layers or number of neurons in layers. You can take a look at <a href=""https://datascience.stackexchange.com/a/26642/28175"">here</a> and <a href=""https://datascience.stackexchange.com/a/26291/28175"">here</a> which the former can help you and the latter helps you understand the features learned by <code>CNNs</code> in case you are using them. </p>

<p>For using <code>F1</code> score in <code>Keras</code> I've not seen but you can implement it and pass it to compile method, take a look at <a href=""https://stackoverflow.com/q/45411902/5120235"">here</a>.</p>
","0","2","28175","12588"
"27220","<p>There is an easier way instead of using loops. <code>Scikit</code> provides <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html"" rel=""nofollow noreferrer"">cross_val_score</a>. </p>

<pre><code>from sklearn.cross_validation import KFold, cross_val_score
k_fold = KFold(len(y), n_folds=10, shuffle=True, random_state=0)
clf = &lt;any classifier&gt;
print cross_val_score(clf, X, y, cv=k_fold, n_jobs=1)
</code></pre>

<p>The topic also has been discussed <a href=""https://stackoverflow.com/questions/16379313/how-to-use-the-a-k-fold-cross-validation-in-scikit-with-naive-bayes-classifier-a/38711253#38711253"">here</a>.</p>

<p>You can also see <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"" rel=""nofollow noreferrer"">here</a> which has a code snippet which may help you:</p>

<pre><code>from sklearn.model_selection import KFold
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4])
kf = KFold(n_splits=2)
kf.get_n_splits(X)

print(kf)  

for train_index, test_index in kf.split(X):
   print(""TRAIN:"", train_index, ""TEST:"", test_index)
   X_train, X_test = X[train_index], X[test_index]
   y_train, y_test = y[train_index], y[test_index]
</code></pre>

<p>which <em>The first n_samples % n_splits folds have size n_samples // n_splits + 1, other folds have size n_samples // n_splits, where n_samples is the number of samples.</em>  </p>
","2","2","28175","12588"
"27237","<p>Actually, what I'm going to discuss is not an architecture and is like a module used in networks. It performs really well on character based data-sets like <code>MNIST</code> although there is other functionality for them. There is a paper called <em>Spatial Transformer Networks</em> written by <em>Max Jaderberg</em> et al. It tries to introduce an alternative for pooling layers. What it does is tries to find the canonical shape of its input by reducing transformations, like translation and rotation, or even diminishing the distortion of the inputs. It introduces a module which helps convolutional networks to <em>really</em> be spatial invariant. The work is amazing and the paper is easy to be read. Based on experience, using these modules may reduce the number of neurons and layers, because networks employing them won't try to learn extra stuff of the inputs, like applied transformations to the inputs. The reason is that the net will try to learn the <strong>canonical</strong> shape of inputs. </p>

<p><a href=""https://i.stack.imgur.com/b6KQh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b6KQh.png"" alt=""enter image description here""></a></p>

<p>(a) shows the arbitrary input to the network (b) shows what spatial transformer has done and finally (c) is the output of the spatial transformer which can be used through other layers of the networks. </p>

<p>One of the significant achievements of this module is that it tries to enhance distorted inputs.</p>

<p><a href=""https://i.stack.imgur.com/hqj67.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hqj67.png"" alt=""enter image description here""></a></p>

<p>Its performance can be seen <a href=""https://drive.google.com/file/d/0B1nQa_sA3W2iN3RQLXVFRkNXN0k/view"" rel=""nofollow noreferrer"">here</a>.</p>
","3","2","28175","12588"
"27250","<p>The reason is that you are printing the object of function. Instead call the function. Suppose the name of your function is <code>func</code>. You are doing something like </p>

<pre><code>print(func)
</code></pre>

<p>instead try to call it by adding parenthesis. </p>

<pre><code>print(func())
</code></pre>
","1","2","28175","12588"
"27275","<p>In <code>TensorFlow</code> there are different convolution layers. <code>Conv1d</code>, <code>Conv2d</code> and <code>Conv3d</code>. the first one is used for one dimensional signals like sounds, the second one is used for images, gray-scale or <code>RGB</code> images and both cases are considered to be two dimensional signals. The last one is used for three dimensional signals like video frames, images as two dimensional signals vary during time. In your case <code>Conv1d</code> is used as one dimensional signal and you can specify the number of filters in the arguments of the method. You can take a look at <a href=""https://datascience.stackexchange.com/a/26321/28175"">here</a> and <a href=""https://datascience.stackexchange.com/a/26384/28175"">here</a>.</p>
","2","2","28175","12588"
"27287","<p>I quote from <a href=""https://en.wikipedia.org/wiki/Energy_(signal_processing)"" rel=""nofollow noreferrer"">here</a> which our friend has provided.</p>

<blockquote>
  <p><em>Energy in this context is not, strictly speaking, the same as the conventional notion of energy in physics and the other sciences. The two concepts are, however, closely related, and it is possible to convert from one to the other:</em></p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/KV0IX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KV0IX.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p><em>where Z represents the magnitude, in appropriate units of measure, of the load driven by the signal.</em></p>
</blockquote>

<p>Also for figuring out the interpretation of energy in this context you can see <a href=""https://electronics.stackexchange.com/a/77679/111225"">here</a>.</p>

<p><hr />
After a while, I found a bit better explanation for energy as you can read the answers from <a href=""https://stackoverflow.com/q/4562801/5120235"">here</a>. </p>

<blockquote>
  <p><em>Energy is used to describe a measure of ""information"" when formulating an operation under a probability framework such as MAP (maximum a priori) estimation in conjunction with Markov Random Fields. Sometimes the energy can be a negative measure to be minimised and sometimes it is a positive measure to be maximized.</em></p>
</blockquote>

<p>or you can read:</p>

<blockquote>
  <p><em>Energy is a fairly loose term used to describe any user defined function (in the image domain).</em></p>
</blockquote>
","2","2","28175","12588"
"27323","<p>That's true. For understanding how many correct decision your classifier has made, confusion matrix can be used. The main diagonal illustrates that. It depicts how many data samples are correctly classified and how many are mislabeled to which class. you can take a look at <a href=""http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/"" rel=""nofollow noreferrer"">here</a>.</p>
","1","2","28175","12588"
"27335","<p>Whenever you have skewed dataset, it means that you know some classes better than some others. In such cases it means that the data is your knowledge and there are learning algorithms for such occasions.</p>

<p>Consider an important fact here. Suppose that you have feature vectors of conditions of a nuclear company and they describe whether the company is in danger of nuclear radiation or not. In such case it is clear that it does not happen a lot that you have infected companies so most of your data have label of healthy condition. You have so much knowledge about the healthy class but you don't know much about the infected class because you don't have much data consequently you don't know its distribution and you can not estimate it well. Whenever your data is skewed, it means that e.g. you have 1 million feature vectors of negative class and 5 feature vectors of positive class. Now suppose that you change the feature vectors. In such cases you have imbalanced data-set or you just have the data samples of some classes without some other, you can use <a href=""https://en.wikipedia.org/wiki/Anomaly_detection"" rel=""nofollow noreferrer"">anomaly detection</a>.</p>

<blockquote>
  <p>In data mining, anomaly detection (also outlier detection) is the identification of items, events or observations which do not conform to an expected pattern or other items in a dataset. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.</p>
</blockquote>
","2","2","28175","12588"
"27385","<p>At coursera, the homework of third week of convolutional networks by professor Andrew Ng, is about this. I recommend you to see that homework. It also implements the <code>YOLO</code> paper. I can't add the code here, but the architecture of the network is as follows:</p>

<pre><code>____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 608, 608, 3)   0                                            
____________________________________________________________________________________________________
conv2d_1 (Conv2D)                (None, 608, 608, 32)  864         input_1[0][0]                    
____________________________________________________________________________________________________
batch_normalization_1 (BatchNorm (None, 608, 608, 32)  128         conv2d_1[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_1 (LeakyReLU)        (None, 608, 608, 32)  0           batch_normalization_1[0][0]      
____________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)   (None, 304, 304, 32)  0           leaky_re_lu_1[0][0]              
____________________________________________________________________________________________________
conv2d_2 (Conv2D)                (None, 304, 304, 64)  18432       max_pooling2d_1[0][0]            
____________________________________________________________________________________________________
batch_normalization_2 (BatchNorm (None, 304, 304, 64)  256         conv2d_2[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_2 (LeakyReLU)        (None, 304, 304, 64)  0           batch_normalization_2[0][0]      
____________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)   (None, 152, 152, 64)  0           leaky_re_lu_2[0][0]              
____________________________________________________________________________________________________
conv2d_3 (Conv2D)                (None, 152, 152, 128) 73728       max_pooling2d_2[0][0]            
____________________________________________________________________________________________________
batch_normalization_3 (BatchNorm (None, 152, 152, 128) 512         conv2d_3[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_3 (LeakyReLU)        (None, 152, 152, 128) 0           batch_normalization_3[0][0]      
____________________________________________________________________________________________________
conv2d_4 (Conv2D)                (None, 152, 152, 64)  8192        leaky_re_lu_3[0][0]              
____________________________________________________________________________________________________
batch_normalization_4 (BatchNorm (None, 152, 152, 64)  256         conv2d_4[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_4 (LeakyReLU)        (None, 152, 152, 64)  0           batch_normalization_4[0][0]      
____________________________________________________________________________________________________
conv2d_5 (Conv2D)                (None, 152, 152, 128) 73728       leaky_re_lu_4[0][0]              
____________________________________________________________________________________________________
batch_normalization_5 (BatchNorm (None, 152, 152, 128) 512         conv2d_5[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_5 (LeakyReLU)        (None, 152, 152, 128) 0           batch_normalization_5[0][0]      
____________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)   (None, 76, 76, 128)   0           leaky_re_lu_5[0][0]              
____________________________________________________________________________________________________
conv2d_6 (Conv2D)                (None, 76, 76, 256)   294912      max_pooling2d_3[0][0]            
____________________________________________________________________________________________________
batch_normalization_6 (BatchNorm (None, 76, 76, 256)   1024        conv2d_6[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_6 (LeakyReLU)        (None, 76, 76, 256)   0           batch_normalization_6[0][0]      
____________________________________________________________________________________________________
conv2d_7 (Conv2D)                (None, 76, 76, 128)   32768       leaky_re_lu_6[0][0]              
____________________________________________________________________________________________________
batch_normalization_7 (BatchNorm (None, 76, 76, 128)   512         conv2d_7[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_7 (LeakyReLU)        (None, 76, 76, 128)   0           batch_normalization_7[0][0]      
____________________________________________________________________________________________________
conv2d_8 (Conv2D)                (None, 76, 76, 256)   294912      leaky_re_lu_7[0][0]              
____________________________________________________________________________________________________
batch_normalization_8 (BatchNorm (None, 76, 76, 256)   1024        conv2d_8[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_8 (LeakyReLU)        (None, 76, 76, 256)   0           batch_normalization_8[0][0]      
____________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)   (None, 38, 38, 256)   0           leaky_re_lu_8[0][0]              
____________________________________________________________________________________________________
conv2d_9 (Conv2D)                (None, 38, 38, 512)   1179648     max_pooling2d_4[0][0]            
____________________________________________________________________________________________________
batch_normalization_9 (BatchNorm (None, 38, 38, 512)   2048        conv2d_9[0][0]                   
____________________________________________________________________________________________________
leaky_re_lu_9 (LeakyReLU)        (None, 38, 38, 512)   0           batch_normalization_9[0][0]      
____________________________________________________________________________________________________
conv2d_10 (Conv2D)               (None, 38, 38, 256)   131072      leaky_re_lu_9[0][0]              
____________________________________________________________________________________________________
batch_normalization_10 (BatchNor (None, 38, 38, 256)   1024        conv2d_10[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_10 (LeakyReLU)       (None, 38, 38, 256)   0           batch_normalization_10[0][0]     
____________________________________________________________________________________________________
conv2d_11 (Conv2D)               (None, 38, 38, 512)   1179648     leaky_re_lu_10[0][0]             
____________________________________________________________________________________________________
batch_normalization_11 (BatchNor (None, 38, 38, 512)   2048        conv2d_11[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_11 (LeakyReLU)       (None, 38, 38, 512)   0           batch_normalization_11[0][0]     
____________________________________________________________________________________________________
conv2d_12 (Conv2D)               (None, 38, 38, 256)   131072      leaky_re_lu_11[0][0]             
____________________________________________________________________________________________________
batch_normalization_12 (BatchNor (None, 38, 38, 256)   1024        conv2d_12[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_12 (LeakyReLU)       (None, 38, 38, 256)   0           batch_normalization_12[0][0]     
____________________________________________________________________________________________________
conv2d_13 (Conv2D)               (None, 38, 38, 512)   1179648     leaky_re_lu_12[0][0]             
____________________________________________________________________________________________________
batch_normalization_13 (BatchNor (None, 38, 38, 512)   2048        conv2d_13[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_13 (LeakyReLU)       (None, 38, 38, 512)   0           batch_normalization_13[0][0]     
____________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)   (None, 19, 19, 512)   0           leaky_re_lu_13[0][0]             
____________________________________________________________________________________________________
conv2d_14 (Conv2D)               (None, 19, 19, 1024)  4718592     max_pooling2d_5[0][0]            
____________________________________________________________________________________________________
batch_normalization_14 (BatchNor (None, 19, 19, 1024)  4096        conv2d_14[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_14 (LeakyReLU)       (None, 19, 19, 1024)  0           batch_normalization_14[0][0]     
____________________________________________________________________________________________________
conv2d_15 (Conv2D)               (None, 19, 19, 512)   524288      leaky_re_lu_14[0][0]             
____________________________________________________________________________________________________
batch_normalization_15 (BatchNor (None, 19, 19, 512)   2048        conv2d_15[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_15 (LeakyReLU)       (None, 19, 19, 512)   0           batch_normalization_15[0][0]     
____________________________________________________________________________________________________
conv2d_16 (Conv2D)               (None, 19, 19, 1024)  4718592     leaky_re_lu_15[0][0]             
____________________________________________________________________________________________________
batch_normalization_16 (BatchNor (None, 19, 19, 1024)  4096        conv2d_16[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_16 (LeakyReLU)       (None, 19, 19, 1024)  0           batch_normalization_16[0][0]     
____________________________________________________________________________________________________
conv2d_17 (Conv2D)               (None, 19, 19, 512)   524288      leaky_re_lu_16[0][0]             
____________________________________________________________________________________________________
batch_normalization_17 (BatchNor (None, 19, 19, 512)   2048        conv2d_17[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_17 (LeakyReLU)       (None, 19, 19, 512)   0           batch_normalization_17[0][0]     
____________________________________________________________________________________________________
conv2d_18 (Conv2D)               (None, 19, 19, 1024)  4718592     leaky_re_lu_17[0][0]             
____________________________________________________________________________________________________
batch_normalization_18 (BatchNor (None, 19, 19, 1024)  4096        conv2d_18[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_18 (LeakyReLU)       (None, 19, 19, 1024)  0           batch_normalization_18[0][0]     
____________________________________________________________________________________________________
conv2d_19 (Conv2D)               (None, 19, 19, 1024)  9437184     leaky_re_lu_18[0][0]             
____________________________________________________________________________________________________
batch_normalization_19 (BatchNor (None, 19, 19, 1024)  4096        conv2d_19[0][0]                  
____________________________________________________________________________________________________
conv2d_21 (Conv2D)               (None, 38, 38, 64)    32768       leaky_re_lu_13[0][0]             
____________________________________________________________________________________________________
leaky_re_lu_19 (LeakyReLU)       (None, 19, 19, 1024)  0           batch_normalization_19[0][0]     
____________________________________________________________________________________________________
batch_normalization_21 (BatchNor (None, 38, 38, 64)    256         conv2d_21[0][0]                  
____________________________________________________________________________________________________
conv2d_20 (Conv2D)               (None, 19, 19, 1024)  9437184     leaky_re_lu_19[0][0]             
____________________________________________________________________________________________________
leaky_re_lu_21 (LeakyReLU)       (None, 38, 38, 64)    0           batch_normalization_21[0][0]     
____________________________________________________________________________________________________
batch_normalization_20 (BatchNor (None, 19, 19, 1024)  4096        conv2d_20[0][0]                  
____________________________________________________________________________________________________
space_to_depth_x2 (Lambda)       (None, 19, 19, 256)   0           leaky_re_lu_21[0][0]             
____________________________________________________________________________________________________
leaky_re_lu_20 (LeakyReLU)       (None, 19, 19, 1024)  0           batch_normalization_20[0][0]     
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 19, 19, 1280)  0           space_to_depth_x2[0][0]          
                                                                   leaky_re_lu_20[0][0]             
____________________________________________________________________________________________________
conv2d_22 (Conv2D)               (None, 19, 19, 1024)  11796480    concatenate_1[0][0]              
____________________________________________________________________________________________________
batch_normalization_22 (BatchNor (None, 19, 19, 1024)  4096        conv2d_22[0][0]                  
____________________________________________________________________________________________________
leaky_re_lu_22 (LeakyReLU)       (None, 19, 19, 1024)  0           batch_normalization_22[0][0]     
____________________________________________________________________________________________________
conv2d_23 (Conv2D)               (None, 19, 19, 425)   435625      leaky_re_lu_22[0][0]             
====================================================================================================
Total params: 50,983,561
Trainable params: 50,962,889
Non-trainable params: 20,672
</code></pre>

<p>Like you, I can't see any flatten layer.</p>
","0","2","28175","12588"
"27425","<p>Whenever you train the network using batch means that you have chosen to train using batch gradient descent. There are three variants for gradient descent algorithm:</p>

<ul>
<li>Gradient Descent</li>
<li>Stochastic Gradient Descent</li>
<li>Batch Gradient Descent </li>
</ul>

<p>The first one passes the whole data through the network and finds the error rate for all of them and finds the gradients with respect to all the data samples and updates the weights after passing the whole data-set. That means for each epoch, passing the whole data-set through the network, one update occurs. This update is accurate toward descending gradient. </p>

<p>The second one, updates the weights after passing each data which means if your data sample has one thousand samples, one thousand updates will happen whilst the previous method updates the weights one time per the whole data-sample. This method is not accurate but is so much faster than the previous one.</p>

<p>The last one tries to find a trade-off between the above approaches. You specify a batch size and you will update the weights after passing the data samples in each batch, means the gradients are calculated after passing each batch. Suppose you have one thousand data sample and you have specified a batch size with one hundred data sample. You will have 10 weight update for each epoch. This method is more accurate than the second approach and is more faster than the first approach. </p>

<blockquote>
  <p>Do I back propagate after each batch has been presented to network or after each image?</p>
</blockquote>

<p>Your method is the last one. Consequently, after passing the entire batch, you would update the weights.</p>

<p><hr />
Based on the comments of one of our friends, the above approaches are named as follows, respectively:</p>

<ul>
<li>[Batch] Gradient Descent (batch size = all training samples)</li>
<li>True SGD (batch size = 1 - weights update for each training sample)</li>
<li>Mini-batch SGD (batch size = m out of n training samples). </li>
</ul>
","12","2","28175","12588"
"27434","<p>It is not dangerous if you have enough data. If you have enough data, you can somehow estimate the distribution of the phenomenon in hand. If you find the distribution of your sample and its parameters, it means that you know everything about your phenomenon under study. If you are familiar with statistics and probability, you may know that whenever you have enough data, you can estimate the expected value of the random variable using the mean of data samples and standard deviation of the sample will be equal to the standard deviation of the random variable, again if you have enough data. If you have enough data, it means that there will be no difference between the value of mean and expected value also standard deviation of the sample and standard deviation of the population. So if you extract enough data, these values may be so close together. Moreover, the reason we are using standardization of data is that we want to have features with same scale. This is the main reason, consequently there is no need to find the exact value of mean and standard deviation. You may see among machine learning and deep learning practitioners that they may not do this operation because it's a bit time consuming. They just usually divide each feature by the greatest value of the corresponding feature among data samples.</p>
","1","2","28175","12588"
"27451","<p>For this purpose you have to have a data-set that can be interpreted by the human. I mean an expert should label the data samples without hesitating. Then you can make a typical neural network and train it. In your case you have 10 classes and all 60 thousand images are labeled by an expert. Consequently, you are able to train a network. Your task is supervised and you need labeled data-set.</p>
","1","2","28175","12588"
"27452","<p>Consider that you are doing vector operation, change your cost function to the following:</p>

<pre><code>(1 / m) * sum(((-y) .* (log(h)) - ((1 - y) .* log((1-h)))));
</code></pre>

<p>and your gradient to the following:</p>

<pre><code>grad = (1./m) * (x' * (h - y))
</code></pre>

<p>Although the latter is just for precedence reassuring. 
<hr />
Based on the discussion in the chat, although the code calculates the cost in a wrong way, the reason the cost does not decrease is that the data is not linearly separable. Logistic regression is a simple algorithm which classifies successfully linearly separable data. Take a look at <a href=""https://datascience.stackexchange.com/q/21896/28175"">here</a>. </p>
","0","2","28175","12588"
"27480","<p>I guess you need anomaly detection algorithm. It is like fraud detection for finding abnormal behaviors. </p>

<blockquote>
  <p>In data mining, anomaly detection (also outlier detection) is the identification of items, events or observations which do not conform to an expected pattern or other items in a dataset. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.</p>
</blockquote>

<p>I highly recommend you taking a look at <a href=""https://www.coursera.org/learn/machine-learning/lecture/Mwrni/developing-and-evaluating-an-anomaly-detection-system"" rel=""nofollow noreferrer"">here</a>.</p>
","3","2","28175","12588"
"27562","<p>Yes, it may. In machine-learning there is an approach called early stop. In that approach you plot the error rate on training and validation data. The horizontal axis is the <code>number of epochs</code> and the vertical axis is the <code>error rate</code>. You should stop training when the error rate of validation data is minimum. Consequently if you increase the number of epochs, you will have an over-fitted model. </p>

<p>In deep-learning era, it is not so much customary to have early stop. There are different reasons for that but one of them is that deep-learning approaches need so much data and plotting the mentioned graph would be so much wavy because these approach use stochastic-gradient-like optimizations. In deep-learning again you may have an over-fitted model if you train so much on the training data. To deal with this problem, another approaches are used for avoiding the problem. Adding noise to different parts of models, like drop out or somehow batch normalization with a moderated batch size, help these learning algorithms not to over-fit even after so many epochs. </p>

<p>In general too many epochs may cause your model to over-fit the training data. It means that your model does not <em>learn</em> the data, it memorizes the data. You have to find the accuracy of validation data for each epoch or maybe iteration to investigate whether it over-fits or not.</p>
","14","2","28175","12588"
"27582","<p>Based on the answer <a href=""https://stackoverflow.com/a/48551629/5120235"">here</a>, <em>Since you are doing a classification task, you should be using the metric R-squared (co-effecient of determination) instead of accuracy score (accuracy score is used for classification purposes).</em> You should use something like <code>score</code> for evaluation because your task is regression.</p>
","1","2","28175","12588"
"27617","<p>Definitely you should normalize your data. You normalize the data for the following aims:</p>

<ul>
<li><p>For having different features in same scale, which is for accelerating learning process. </p></li>
<li><p>For caring different features fairly without caring the scale.</p></li>
</ul>

<p>After training, your learning algorithm has learnt to deal with the data in scaled form, so you have to normalize your test data with the normalizing parameters used for training data.</p>
","10","2","28175","12588"
"27666","<p>If you use sigmoid-like activation functions, like sigmoid and tanh, after some epochs of training, the linear part of each neuron will have values that are very big or very small. This means that the linear part will have a big output value regardless of its sign. Consequently, the input of sigmoid-like functions in each neuron which adds non-linearity will be far from the center of these functions.</p>
<p><a href=""https://i.stack.imgur.com/vvy9I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vvy9I.png"" alt=""enter image description here"" /></a></p>
<p>In those locations, the gradient/derivative value is very small. Consequently, after numerous iterations, the weights get updated so slowly because the value of the gradient is very small. This is why we use the <code>ReLU activation function</code> for which its gradient doesn't have this problem. Saturating means that after some epochs that learning happens relatively fast, the value of the linear part will be far from the center of the sigmoid and it somehow saturates, and it takes too much time to update the weights because the value of gradient is small. You can take a look at <a href=""https://datascience.stackexchange.com/q/23493/28175"">here</a> as a solution for this problem.</p>
<p>If I want to explain the math part, suppose that you are using sigmoid as the activation function. If <span class=""math-container"">$\sigma$</span> represents sigmoid, its gradient is <span class=""math-container"">$\sigma (1-\sigma$</span>). Now suppose that your linear part, the input of sigmoid is a positive number which is too large, then sigmoid which is:</p>
<blockquote>
<p><span class=""math-container"">$$ \frac{1}{1+e^{-x}} $$</span></p>
</blockquote>
<p>will have a value near to one but smaller than that. On the other hand <span class=""math-container"">$\sigma$</span> will be so close to zero, multiplying <span class=""math-container"">$\sigma$</span> and <span class=""math-container"">$1 - \sigma$</span> will result in a small value, which means the value of the gradient is very small. If the value of the linear part is very small, then <span class=""math-container"">$\sigma$</span> will be close to zero and <span class=""math-container"">$1- \sigma$</span> will be close to 1 but smaller than that. Again, multiplying these will lead to a small value as the gradient.</p>
","3","2","28175","12588"
"27695","<p>You can also take a look at <a href=""https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a"" rel=""noreferrer"">here</a> for the labels in the imagenet. I guess you are right, there is no label for <em>human</em> in the data-set but there is something to notice. There are labels in imagenet like cowboy or some specific hats and other related things to human like shirt and t-shirt. You can take a look at <a href=""https://datascience.stackexchange.com/a/26291/28175"">here</a> and also <a href=""http://yosinski.com/deepvis"" rel=""noreferrer"">here</a>. In the latter link Yosinski et al, has tried to show that the popular AlexNet has learned to recognize human faces although there is no label as human face in the imagenet data-set. In their paper, they have investigated that Convolutional neural networks may try to learn things that are distributed among layers or maybe not and they may not have special label in the training data. As an example, the face of cats and humans can be referred to. Moreover, as you can see <a href=""http://image-net.org/downloads/attributes/README"" rel=""noreferrer"">here</a> maybe the aim was <em>attribute learning in large-scale datasets</em>, as quoted in the last line of the page, as the reference.</p>
","7","2","28175","12588"
"27729","<p>Maybe reading papers is a bit hard, because they usually don't explain everything. I suggest you watching <a href=""https://coursera.org/specializations/deep-learning"" rel=""nofollow noreferrer"">deep learning specialization</a> by professor Andrew Ng, which is brief and easy to figure out. If you insist on reading papers, <a href=""https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner&#39;s-Guide-To-Understanding-Convolutional-Neural-Networks/"" rel=""nofollow noreferrer"">this</a> three sequential articles may help you. Choosing neural networks highly depend on your task. For most computer vision tasks convolutional neural nets are used. You can also take a look at <a href=""https://datascience.stackexchange.com/tags/deep-learning/info"">here</a> which contains different resources and discussions and many other useful stuff.</p>
","1","2","28175","12588"
"27752","<blockquote>
  <p>predictions = classifier.predict(x_test)</p>
</blockquote>

<p>You have not provided the shape of your x_test but based on the documentation of the predict function that you should provide an array-like item, you are inputting an array-like input. Each output already shows the probability of each corresponding input. It seems that because the low values of predictions, they are smaller than <code>0.5</code>, the predicted labels for your test data are all zero.</p>
","2","2","28175","12588"
"27780","<p>There is a paper called <em>Spatial Transformer Networks</em> written by <em>Max Jaderberg et al</em>. What it does is trying to find the canonical shape of its input by reducing transformations, like translation and rotation, or even diminishing the distortion of the inputs. It introduces a module which helps convolutional network to be spatial invariant. One of the significant achievements of this module is that it tries to enhance distorted inputs. Take a look at <a href=""https://drive.google.com/file/d/0B1nQa_sA3W2iN3RQLXVFRkNXN0k/view"" rel=""nofollow noreferrer"">here</a>.</p>

<p><hr />
I edit the answer because of the request of one of our friends. First I quote something from the popular book written by Pr. <em>Gonzalez</em>, I hope nothing goes wrong with copyright. Then I suggest my recommendation. </p>

<p><em>Figure 2.40</em>, the one that I've attached, <em>shows an example of the steps in Fig. 2.39. In this case, the transform used was the Fourier transform, which we mention briefly later in this section and discuss in detail in Chapter 4. Figure 2.40(a) is an image corrupted by sinusoidal interference and Fig. 2.40(b) is the magnitude of its Fourier transform, which is the output of the first stage in Fig. 2.39. As you will learn in Chapter 4, sinusoidal interference in the spatial domain appears as bright bursts of intensity in the transform domain. In this case, the bursts are in a circular pattern that can be seen in Fig. 2.40(b). Figure 2.40(c) shows a mask
image (called a filter) with white and black representing 1 and 0, respectively.
For this example, the operation in the second box of Fig. 2.39 is to multiply the mask by the transform, thus eliminating the bursts responsible for the interference. Figure 2.40(d) shows the final result, obtained by computing the inverse of the modified transform. The interference is no longer visible, and important detail is quite clear. In fact, you can even see the fiducial marks (faint crosses) that are used for image alignment.</em></p>

<p><a href=""https://i.stack.imgur.com/MA3SZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MA3SZ.png"" alt=""enter image description here""></a></p>

<p>Okay! After those quotes, I refer to my suggestion. In the <em>ST</em> network, the authors have claimed that their differentiable module can learn different kinds of transformations other than affine transformation. The point about that is that we usually apply two kinds of transformations. One transformation is applied to the intensity of the image, which is popular by the means of <em>filters</em>. The other one is called image warping where we change the position of intensities and the intensity values do not change unless they locate between the discrete grids of image entries also called pixels, <em>picture element</em>. Spatial transformers are good for the second task but they also can be used for the first task. There are studies about using <em>CNNs</em> for evaluating the images in their frequency domain and not in the spatial domain. You can employ these differentiable modules in those nets. </p>

<p>Finally about your specific task, what I'm seeing in your pictures, your noise has a same behaviour. I guess if that is true for all cases, your task is not learning and can be solved using usual image processing techniques.</p>
","1","2","28175","12588"
"27783","<p>Actually there are so many ways for doing so depending on your point of view. You can use deep learning and machine learning approaches or old fashioned vision approaches. For instance, you can use <code>Segnet</code> neural net if you want to use learning approaches. But because you have limited computation power, vision approaches is more common. Take a look at <a href=""https://www.superdatascience.com/opencv-face-detection/"" rel=""nofollow noreferrer"">here</a>.</p>
","0","2","28175","12588"
"27808","<p>Actually you are in the right path but in the question you are wrong in the second paragraph. </p>

<p>What you should do is as follows:</p>

<ul>
<li>change the output layer to 4 classes, dogs, cats, dogs and cats, none.</li>
<li>If you have so many data, don't freeze the convolutional layers and train them all but if not, freeze them and fine tune the last layers, maybe just the dense layers.</li>
</ul>
","1","2","28175","12588"
"27809","<p>Suppose that you want to know the name of countries with more than ten <em>Ginger</em>:</p>

<pre><code># read your data using pd.read_csv and specify appropriate inputs based on your excel file in a typical variable named csvFile and transpose it
csvFile = csvFile[csvFile['Ginger'] &gt; 10]
</code></pre>

<p>Also if you want to see the countries with zero <em>Ginger</em>:</p>

<pre><code>csvFile = csvFile[csvFile['Ginger'] == 0]
</code></pre>
","1","2","28175","12588"
"27892","<p>The reason is that in your model, you have specified the input size to vary, as the parameter of your function specifies. You have to change your code as follows:</p>

<pre><code>def neural_network_model(input_size, output_size):
network = input_data(shape=[None, input_size, 1], name='input')

network = fully_connected(network, 128, activation='relu')
network = dropout(network, 0.8)

network = fully_connected(network, 256, activation='relu')
network = dropout(network, 0.8)

network = fully_connected(network, 512, activation='relu')
network = dropout(network, 0.8)

network = fully_connected(network, 256, activation='relu')
network = dropout(network, 0.8)

network = fully_connected(network, 128, activation='relu')
network = dropout(network, 0.8)

network = fully_connected(network, output_size, activation='softmax')
network = regression(network, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')
model = tflearn.DNN(network, tensorboard_dir='log')

return model
</code></pre>

<p>You have hard coded the output size, which is two, you have to change it to be a variable and during calling, you have to specify the size of your new output, which is four.</p>
","0","2","28175","12588"
"27896","<p>According to the comment of the <a href=""https://keras.io/preprocessing/image/"" rel=""nofollow noreferrer"">documentation</a> in the code:</p>

<pre><code># compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
datagen.fit(x_train)
</code></pre>

<p>It does the normalization, reducing mean and dividing by standard deviation, and more things like <code>PCA</code>. So it seems that you don't need to do normalization. That method does that, and normalizing features is required for accelerating training process and caring about all features with different scales the same.</p>
","1","2","28175","12588"
"27921","<p>For binary classification you can use <code>softmax</code> as the output layer but you should consider that if you do so, your last layer have to have two neurons, each corresponds to one specific class. Moreover, you have to use <code>categorical_crossentropy</code> as the loss function. Change the following code:</p>

<pre><code>model.add(Dense(1, activation='softmax'))
model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(lr=0.01),metrics=['accuracy'])
</code></pre>

<p>to:</p>

<pre><code>model.add(Dense(2, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.SGD(lr=0.01),metrics=['accuracy'])
</code></pre>
","0","2","28175","12588"
"27928","<p>You can use the following code snippet:</p>

<pre><code>from matplotlib import cm
cmap = cm.get_cmap('gnuplot')
scatter = pd.scatter_matrix(YOUR_TRAINING_DATA, c = YOUR_LABELS_OF_TRAINING, marker = 'o', s = 40, hist_kwds = {'bins':15}, figsize = (12, 12), cmap = cmap)
</code></pre>

<p>It plots the scatter plot of each feature separately and together. It is like the correlation matrix. You can take a look at <a href=""https://datascience.stackexchange.com/a/25271/28175"">here</a>.</p>
","0","2","28175","12588"
"27962","<p>When you have 32 feature maps with height and width equal to 100 and the depth of each equal to one it means that you have 32 planes, a common jargon among vision people, with 100 by 100 entries. You can set the height and width of the next layer and they can be arbitrary. You can also set the number of feature maps but the depth of each feature map would be equal to the number of feature maps of the previous layer. So it should be 16 * 9 * 9 * 32 if you set height and width equal to 9 and the number of feature maps to 16. As you can see in 16 * 9 * 9 * 32, 32 is located at the end, this is called <em>channels last</em>. You can not set the depth, because it should be equal to the number of channels, features maps, of the previous layer. </p>

<p>16 * 9 * 9 * 32 means that you have 16 feature maps of dimension 9 * 9 * 32, so the output of each feature map would be the member-wise product of all the outputs of the feature maps of the previous layer and each of 9 * 9 * 32 kernels. Consequently the result would be 16 planes. </p>

<p>I highly recommend you taking a look at <a href=""https://www.coursera.org/learn/convolutional-neural-networks/lecture/ctQZz/convolutions-over-volume"" rel=""nofollow noreferrer"">here</a>.</p>
","4","2","28175","12588"
"27967","<p>Based on what I've seen and experienced, the best way is to store and retrieve your data from your drive account. Actually your question is a bit unclear but first I say, try to use the following command to see the current files in your directory, although I guess each 12 hours they all would be deleted automatically. </p>

<pre><code>!ls
</code></pre>

<p>Anyway I recommend the following instructions:</p>

<p>Use the following code for having permission to access to your drive account:</p>

<pre><code>!pip install -U -q PyDrive

import tensorflow as tf
import timeit

config = tf.ConfigProto()
config.gpu_options.allow_growth = True

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
</code></pre>

<p>Use the following code to get the id of contents in your drive:</p>

<pre><code>file_list = drive.ListFile({'q': ""'root' in parents and trashed=false""}).GetList()
for file1 in file_list:
  print('title: %s, id: %s' % (file1['title'], file1['id']))
</code></pre>

<p>Put the id of the desired file, e.g. a typical text file, in the content of the following dictionary with id key:</p>

<pre><code>downloaded = drive.CreateFile({'id': 'the id of typical text file'})
file = downloaded.GetContentString()
print('Downloaded content ""{}""'.format(len(file)))
</code></pre>

<p>Till now you have copied the text file, then you have to write it in your Colab disk using the following code:</p>

<pre><code>text_file = open(""your desired name.txt"", ""w"")
text_file.write(file)    
text_file.close()
</code></pre>

<h1>Create &amp; upload a file.</h1>

<pre><code>uploaded = drive.CreateFile({'title': 'filename.csv'})
uploaded.SetContentFile('filename.csv')
uploaded.Upload()
print('Uploaded file with ID {}'.format(uploaded.get('id')))
</code></pre>

<h1>Downloading from Colab without Uploading to drive</h1>

<pre><code>from google.colab import files
files.download('your typical h5 file or what ever.h5')
</code></pre>

<p>For more information about transferring different data formats there are more explanations in the notebook provided with <a href=""https://colab.research.google.com/"" rel=""nofollow noreferrer""><em>Colab</em></a>.</p>
","4","2","28175","12588"
"28029","<p>Basically I guess <code>TensorFlow</code> does not support decision trees. I quote from <a href=""https://datascience.stackexchange.com/a/12921/28175"">here</a>, </p>

<blockquote>
  <p><em>This is a big oversimplification, but there are essentially two types of machine learning libraries available today, <code>Deep learning</code>
  (CNN,RNN, fully connected nets, linear models) and Everything else
  (SVM, GBMs, Random Forests, Naive Bayes, K-NN, etc). The reason for
  this is that deep learning is much more computationally intensive than
  other more traditional training methods, and therefore requires
  intense specialization of the library (e.g., using a GPU and
  distributed capabilities). If you're using Python and are looking for
  a package with the greatest breadth of algorithms, try scikit-learn.
  In reality, if you want to use deep learning and more traditional
  methods you'll need to use more than one library. There is no
  ""complete"" package.</em></p>
</blockquote>

<p>You can see from <a href=""https://stackoverflow.com/a/43031536/5120235"">here</a> that there are other learning algorithms implemented in <code>TensorFlow</code> which are not deep models. </p>

<p>You can take a look at <a href=""https://terrytangyuan.github.io/2016/08/06/tensorflow-not-just-deep-learning/"" rel=""noreferrer"">here</a> for tracking algorithms implemented in <code>TensorFlow</code>.</p>
","10","2","28175","12588"
"28103","<p>Use the following code:</p>

<pre><code>import keras
import numpy as np



a = np.array([[1, 1], [0, 1], [1, 0], [0, 0]])
b = np.array([[0], [1], [1], [0]])

model = keras.Sequential()
model.add(keras.layers.Dense(2, activation = 'sigmoid', input_shape = (2, )))
model.add(keras.layers.Dense(1, activation = 'sigmoid'))

model.compile(optimizer = keras.optimizers.rmsprop(), metrics = ['accuracy'], loss = 'binary_crossentropy')
model.fit(x = a, y = b, epochs = 750)
</code></pre>

<p>If you don't get <code>100%</code> accuracy increase the number of epochs. Also take a look at <a href=""https://datascience.stackexchange.com/a/26642/28175"">here</a> which may help you how increasing the size of each layer affects the learning process.</p>
","0","2","28175","12588"
"28199","<p>Biased in the context that you are speaking means, that your model overfits the training data and can not generalize well. It means your model performs very well on your training data, but can not do well on cross-validation and test data. It is customary to say that biased learners memorize the training data which is really true. Biased learners don't <em>learn</em> the data, they <em>fit</em> the data. For understanding the other usages of <em>bias</em> take a look at this <a href=""https://datascience.stackexchange.com/q/18123/28175"">question</a>.</p>

<p>There is something that may be worth mentioning. You may have heard people saying that your model has a high-bias problem. It just means that your model can not learn the training data, whilst the biased learners overfits the training data, means fits the training data. The latter can not generalize well because it has fitted the training data, memorized it, the former can not generalize because it has not learnt even the training data so it has not learnt so much and can not generalize.</p>
","2","2","28175","12588"
"28254","<p>Finding an appropriate architecture is somehow practical. Those hyper-parameters you are talking may be different. Try to use a base architecture and then train your model. If it does not learn your model try to change the hyper parameters. It is an iterative operation to find a good model. There are a lot of debates, but there is not exactly consensus for making good models in <code>CNNs</code>. You can think of if your model does not learn your data, it needs more features to be learned by the <code>CNN</code>. The behavior of the layers are somehow as follows:</p>

<h2>Max Pooling</h2>

<p>It is used for adding spacial invariance to the inputs of its layer. It is also used for decreasing the input size. By increasing its size, the two mentioned behavior would increase.</p>

<h2>Convoluional Layers</h2>

<p>These layers are used for extracting features to reduce the cost function in order to learn data. If you increase the size of these layers they behavior would be more vast, they would find features of a larger region.</p>

<p>Take a look at <a href=""https://datascience.stackexchange.com/a/26291/28175"">here</a> and <a href=""https://datascience.stackexchange.com/a/26642/28175"">here</a>.</p>
","1","2","28175","12588"
"28283","<p>The easiest way is to replace your labels. The other way is to set importance of the more important class to a higher value so the cost function moves toward direction to take much care for your desired label. You can set the <em>class_weight</em>. Take a look at <a href=""https://stackoverflow.com/questions/30972029/how-does-the-class-weight-parameter-in-scikit-learn-work"">here</a> and <a href=""http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html"" rel=""nofollow noreferrer"">here</a>.</p>
","0","2","28175","12588"
"28308","<p>Suppose that you want to have <code>k-means</code> algorithm, in the formulation of average, you have to take the average of each cluster, and then reassign the centers. If you have categorical data, how do you want to take mean? Changing categorical data to numeric data is for translating situations which don't have numerical features to be suited to be used for such algorithms.</p>
","0","2","28175","12588"
"28312","<blockquote>
  <p>Would it potentially have a big impact to use higher resolution images?</p>
</blockquote>

<p>Yes, if you increase the input size to your convolutional neural network, the size of each activation map for each layer increases, so you will have more computation. Also if you use same architecture, the number of neurons and consequently the number of parameters, in dense layers increases. </p>

<blockquote>
  <p>Does it make sense and is it possible to use a pre-trained network on low resolution and re-training the last layer/classifier with higher resolutions?</p>
</blockquote>

<p>The answer is no. When you train a network with a special size of input, you reserve variables to hold the weights and middle variables. If you increase the size of input, dense layers will have different size, so their number of wights should vary too.</p>

<p>To wrap up, for networks with classification tasks, it is appropriate to pass the network small size of images. For other tasks like edge detection where the information of edges can be destroyed by resizing, you have to be careful. In those cases you have to find an appropriate size of the image in order to keep the important information. The small size of the inputs is for reducing number of operations and number of parameters.</p>
","5","2","28175","12588"
"28369","<p>Actually I guess you are making mistake about the second part. The point is that in <code>CNN</code>s, convolution operation is done over volume. Suppose the input image is in three channels and the next layer has 5 kernels, consequently the next layer will have five feature maps but the convolution operation consists of convolution over volume which has this property: each kernel will have its width and height, moreover, a depth. its depth is equal to the number of feature maps, here channels of the image, of the previous layer. Take a look at <a href=""https://datascience.stackexchange.com/a/27962/28175"">here</a>.</p>
","4","2","28175","12588"
"28371","<p>Labeling data is not always an easy task. There are occasion that the data in hand does not have label and you need to make a model using them. You have to find the similarities and differences in your input data. Clustering approaches try to find these similarities and differences to find similar data. Also they are used as a pre-processing before doing supervised classification. In cases that the input data does not have any label, employing clustering approaches can be a way to label the data and use them for training supervised models.</p>
","1","2","28175","12588"
"28373","<p>It depends, if you have the distribution of that feature, you can take the marginal distribution over that feature which its interpretation is to use the expected value of that feature. If you don't have the distribution you can take the mean of the sample in hand of those samples which have value for that feature and add the mean for those which don't have. Another solution is to separate the data of each class and find the mean of the feature of those data samples having the value and putting the mean for each entry of the corresponding class which does not have value in that entry.</p>
","1","2","28175","12588"
"28413","<p>It seems that <code>Intel</code> lets users use its AI <code>DevCloud</code> for free for thirty days I guess. <a href=""https://software.intel.com/ai-academy/tools/devcloud?cid=&amp;utm_content=Machine_Learning_Academic&amp;utm_medium=Banner_Ad&amp;utm_source=StackOverflow&amp;utm_campaign=AI_Student_ASMO_1H_18_MixedMedia"" rel=""nofollow noreferrer"">Here</a> is the instructions.</p>
","0","2","28175","12588"
"28415","<p>Based on the answer <a href=""https://stackoverflow.com/a/44453621/5120235"">here</a>, use the following code:</p>

<pre><code>old_stdout = sys.stdout
sys.stdout = mystdout = StringIO()
clf = SGDClassifier(**kwargs, verbose=1)
clf.fit(X_tr, y_tr)
sys.stdout = old_stdout
loss_history = mystdout.getvalue()
loss_list = []
for line in loss_history.split('\n'):
    if(len(line.split(""loss: "")) == 1):
        continue
    loss_list.append(float(line.split(""loss: "")[-1]))
plt.figure()
plt.plot(np.arange(len(loss_list)), loss_list)
plt.savefig(""warmstart_plots/pure_SGD:""+str(kwargs)+"".png"")
plt.xlabel(""Time in epochs"")
plt.ylabel(""Loss"")
plt.close()
</code></pre>

<p>Also take a look at <a href=""https://stackoverflow.com/q/38179687/5120235"">here</a></p>
","4","2","28175","12588"
"28586","<p>You need to make a dictionary of words. It means you have to make a dictionary which you assign each word a unique value. then you can use one-hot-encoding to represent each word uniquely. If this is what you need it will do what you want. But this has a big problem. When you think about cats and dogs, you may find similarities and differences between them. This is because you have more knowledge than the only representation of words in your brain. Consequently, you should use approaches to assign a unique number to each word, and put near concepts as neighbors. For the first part take a look at <a href=""https://www.coursera.org/learn/nlp-sequence-models/lecture/gw1Xw/language-model-and-sequence-generation"" rel=""nofollow noreferrer"">here</a> and the second part, take a look at <a href=""https://www.tensorflow.org/tutorials/word2vec"" rel=""nofollow noreferrer"">here</a>.</p>
","1","2","28175","12588"
"28786","<p>Actually the shapes are for simplification. If you want to know the correct behavior you have to take a look at the formulas of each <code>LSTM</code> cell. To answer your first question, There may be different answers. What the pictures are depicting belongs to tasks which are many to many, and for each input you need exactly one output. There are different tasks for sequences that can be defined:</p>

<ul>
<li>One to one</li>
<li>One to many</li>
<li>Many to one</li>
<li>Many to many</li>
</ul>

<p>You can take a look at <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow noreferrer"">here</a>. </p>

<p>For illustrating the formulas of each <code>LSTM</code> cell I've given the following picture from professor Andrew Ng course about deep learning:</p>

<p><a href=""https://i.stack.imgur.com/OuRuR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OuRuR.png"" alt=""enter image description here""></a></p>

<p>As you can see, each node in the <code>LSTM</code> cell can be connected indirectly to the output of the adjacent cells of the previous time-step. It's indirect because there are gates between them. Also consider that <code>LSTM</code> cell shares the weights for all the inputs of different time steps. Consequently each neuron in <code>LSTM</code> cell is dependent to the input of the current time-step and the output of the adjacent nodes of the previous time-steps. </p>

<p>About the third question, the input size will be equal to the number of features of the input for each time-step. The number of the outputs depends on your task as I referred to at first. Take a look at the first link.</p>
","1","2","28175","12588"
"28859","<p>Neural networks can be used for classification and regression tasks. They are also used for transcription and clustering. Each of them can have their own characteristics. For classification tasks you may have different classes. The trained network should flag the output with the greatest value among other outputs which corresponds to the appropriate input. In regression tasks, there are different situations. Suppose that you are given the input feature containing age, gender, height and other related things and you try to find the size of foot and size of length of hands simultaneously. I mean you have numerous inputs and numerous outputs. As an other example, in detection tasks, you are asked the place of the desired object. If you want to do that using neural networks, the output of your system consists of the center of the detected item, the height and the width of the detected item, which <em>is</em> a regression task. </p>

<p>The interpretation of outputs for regression tasks is like estimating a real value, this is why linear activation functions are used as the last layers' activation. For classification tasks it is like finding the probability. The probability that your network finds depends on the activation function used in the last layer. If your classes are mutually exclusive, and each input should have just one label, for <code>OCR</code> tasks each image of number refers to just a single number so classes are mutually exclusive, the activation should be <code>Softmax</code> and it represents the probability of belonging to each class. If the input may contain different classes, e.g. an image containing both cat and dog, you can use <code>Sigmoid</code> activation which each output represents the chance of existing of each class. You can also take a look at <a href=""https://datascience.stackexchange.com/q/25315/28175"">here</a>.</p>
","0","2","28175","12588"
"28882","<p>To answer the last question, suppose that you have a binary classification problem. It is customary to label the class as positive if the output of the <code>Sigmoid</code> is more than <code>0.5</code> and negative if it's less than <code>0.5</code>. For increasing recall rate you can change this threshold to a value less than <code>0.5</code>, e.g. <code>0.2</code>. For tasks which you may want a better precision you can increase the threshold to bigger value than <code>0.5</code>. </p>

<p>About the first part of your question, it highly depends on your data and its feature space. There are problems which the data is linearly separable in higher dimensions which means you can easily employ just a single neuron for classifying the data by a single hyper-plane. If it has happened that you have such a good accuracy you can not say anything unless you try to find the value of cross validation error. By interpreting the difference between the value of training data, and cross-validation or maybe test data, you can figure out whether your classifier performs well or not.</p>
","9","2","28175","12588"
"28886","<p>Suppose that you have a learning problem and it's just for fitting a function which depends on only one feature, and the function to be predicted is a quadratic shape. you can have a good performance by just having the input feature if you use linear <code>SVM</code> but it will have errors. adding extra features, polynomial features as the input may be useful but it adds more complexity to the classifier and it causes more computational overheads. By the way that's true. You can have better estimate of functions by adding the high order polynomials of the features in hand. I don't know if you are familiar with normal equation or not but what that does is to add high order polynomials to better fit the function which makes the current data. </p>
","0","2","28175","12588"
"28939","<p>These are my recommendations: </p>

<ul>
<li>Try to train your model more, with different learning rates. Your current learning rate is so much small. At first steps put it to a big value then reduce it smoothly. </li>
<li>Try to change the architecture. If your task does not have the same scale as <em>image-net</em> surely you would need a network with simpler network architecture, consequently the number of parameters diminishes. </li>
<li>Changing the distribution of data is dangerous, but you can augment your entire data-set by some extent to let your network see different and more examples.</li>
<li>For those classes that have not been learnt you can set the class-weight to a bigger number to construct a cost function which cares more about those classes. </li>
<li>Because your data-set is not balanced, use other metrics to evaluate your model, like <code>F1</code> score. </li>
</ul>
","1","2","28175","12588"
"28974","<blockquote>
  <p>I want to ask you that how to predict the probabilities of each class in test images.</p>
</blockquote>

<p>At line 27 in the <em>train.py</em> you have the following code:</p>

<pre><code>correct_prediction = tf.equal(y_pred_cls, tf.argmax(y, axis=1))
</code></pre>

<p>It tries to find whether the predicted values are the same as the real ones. You can run y_pred_cls to see the probability of each class for your desired input.</p>

<blockquote>
  <p>I want to use the code to predict the probabilities of new data's labels, how to save and load the model we have trained which used the train data.</p>
</blockquote>

<p>for saving your model and its weights you can take a look at <a href=""https://stackoverflow.com/q/33759623/5120235"">here</a>. As you can see from there, you have to make a saver object:</p>

<pre><code>import tensorflow as tf

#Prepare to feed input, i.e. feed_dict and placeholders
w1 = tf.placeholder(""float"", name=""w1"")
w2 = tf.placeholder(""float"", name=""w2"")
b1= tf.Variable(2.0,name=""bias"")
feed_dict ={w1:4,w2:8}

#Define a test operation that we will restore
w3 = tf.add(w1,w2)
w4 = tf.multiply(w3,b1,name=""op_to_restore"")
sess = tf.Session()
sess.run(tf.global_variables_initializer())

#Create a saver object which will save all the variables
saver = tf.train.Saver()

#Run the operation by feeding input
print sess.run(w4,feed_dict)
#Prints 24 which is sum of (w1+w2)*b1 

#Now, save the graph
saver.save(sess, 'my_test_model',global_step=1000)
</code></pre>

<p>And for loading that you have to restore it:</p>

<pre><code>import tensorflow as tf

sess=tf.Session()    
#First let's load meta graph and restore weights
saver = tf.train.import_meta_graph('my_test_model-1000.meta')
saver.restore(sess,tf.train.latest_checkpoint('./'))


# Access saved Variables directly
print(sess.run('bias:0'))
# This will print 2, which is the value of bias that we saved


# Now, let's access and create placeholders variables and
# create feed-dict to feed new data

graph = tf.get_default_graph()
w1 = graph.get_tensor_by_name(""w1:0"")
w2 = graph.get_tensor_by_name(""w2:0"")
feed_dict ={w1:13.0,w2:17.0}

#Now, access the op that you want to run. 
op_to_restore = graph.get_tensor_by_name(""op_to_restore:0"")

print sess.run(op_to_restore,feed_dict)
#This will print 60 which is calculated 
</code></pre>

<p><hr />
Edit: Actually the code is a bit strange, anyway. The following part:</p>

<pre><code>tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)
</code></pre>

<p>can help you. It outputs the probabilities for each class.</p>
","3","2","28175","12588"
"29219","<p>What I'm going to refer to is introducing some papers which are about this context. The papers have available data-set and there code can be accessed easily. Actually there are a lot of works in this context, but I suggeest you reading the following papers which are relevant to your question. In English, the nature of the language is in a way that you make a word using combination of letters and usually it is not needed to connect the letters to construct words. There are other languages that letters have to be connected to make words. In <code>OCR</code> problems of those languages it is a commen practice to consider the <em>connected components</em>. The papers that I'm going to refer to have a good data-set that can be used for both supervised and unsupervised methods. </p>

<ul>
<li><a href=""https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9445/94450Z/Sub-word-image-clustering-in-Farsi-printed-books/10.1117/12.2181404.short"" rel=""nofollow noreferrer"">Sub-word image clustering in Farsi printed books</a></li>
<li><a href=""https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9402/94020C/Clustering-of-Farsi-sub-word-images-for-whole-book-recognition/10.1117/12.2075931.short"" rel=""nofollow noreferrer"">Clustering of Farsi sub-word images for whole-book recognition</a></li>
<li><a href=""http://www.sid.ir/En/Journal/ViewPaper.aspx?ID=352817"" rel=""nofollow noreferrer"">Sub-word Image Clustering in Old Printed Documents Using Template Matching</a></li>
<li><a href=""https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9402/94020H/A-comparison-of-1D-and-2D-LSTM-architectures-for-the/10.1117/12.2075930.short"" rel=""nofollow noreferrer"">A comparison of 1D and 2D LSTM architectures for the recognition of handwritten Arabic</a></li>
</ul>

<p>The last paper is a great paper and is somehow the result of the previous ones.</p>
","1","2","28175","12588"
"29243","<p>The quote that you have brought is speaking about something else. Zero means that most of your linear part of neurons have a negative value and after passing through the <code>ReLU</code> non-linearity, they become zero. That's the explanation of that part. About your question, not really. I don't know if you've ever visualized a convolutional network or not but you've done that you may have seen that there may be layers that get activated for special colors even in deep layers. The reason is that the convolutional layer just tries to do cross-correlation operation on the input which has three channels. Next layers of convolution also do this considering e.g. the input has 64 channels. To wrap up, there may be other deep-layers that use the information of colors, Although these operations really do not care about the colors, they care about the local patterns that may happen in the input. The reason for learning colors is that similar colors have similar patterns in their RGB channels. These patterns are what convolutional networks try to learn. Take a look at the tool box that have been discussed <a href=""https://datascience.stackexchange.com/a/26291/28175"">here</a>.</p>
","1","2","28175","12588"
"29247","<p>I guess it has been a mistake. Take a look at <a href=""https://www.coursera.org/learn/convolutional-neural-networks/lecture/MmYe2/classic-networks"" rel=""noreferrer"">here</a>.</p>

<blockquote>
  <p>The other author's were Ilya Sutskever and Geoffrey Hinton. So, AlexNet input starts with 227 by 227 by 3 images. And if you read the paper, the paper refers to 224 by 224 by 3 images. But if you look at the numbers, I think that the numbers make sense only of actually 227 by 227. </p>
</blockquote>
","10","2","28175","12588"
"29477","<blockquote>
  <p>What part the optimization alg. (Grad. Descent) plays in generalization of the learning algorithm?</p>
</blockquote>

<p>Actually for generalizing you have to find a model that does not overfits the training data. For doing so, there are numerous approaches, like L1/L2 regularization which adds noise to the cost function and somehow weights to prohibit the network from having large weights which may lead to overfitting. Take a look at <a href=""https://datascience.stackexchange.com/q/23287/28175"">here</a>. Other techniques are drop-out which adds noise to the activations to help the network not to depend on special nodes. These techniques add some noise to different parts of the network which lead to a cost function with a high error value. After finding the error back prop techniques try to set the parameters/weights to go downhill of the cost function. These techniques help the algorithm not to overfit which means the constructed model will be able to generalize, although you have to test it using cross-validation data and test data. Consequently, optimization techniques by themselves always try to reduce the amount of error and they somehow always lead to overfitting because they try to reduce the cost and this is regularization techniques and other variants that have to be used to construct a cost function which its optimum points do not lead to overfitting. This <a href=""https://datascience.stackexchange.com/a/24537/28175"">link</a> also may help you.</p>

<blockquote>
  <p>Or the influence of the data [4] plays a more important role? Although [6] sheds the light on the role of the model parameters from a neuroscience point of view in terms of the effect of neurons in generalization, the question on how to learn such models remains unsolved.</p>
</blockquote>

<p>Data is the most important part of the learning process. Your data have to be representative enough. Deep learning problems are considered those problems which may have better results if their training data increases. If the model should be able to generalize, it is vital to use data from the real distribution. This is just for training. You can use some techniques in order not to overfit the data using your training data-set. It is customary to add small noises to the input to let the model generalize well. Transforming data by translation, rotation and even distorting image inputs are examples of adding noise to the input to avoid overfitting. Although changing the data is dangerous because it may change the distribution of the real population. So, you can do something with your input data to avoid overfitting and let your model to generalize well.</p>

<blockquote>
  <p>So how can we really better understand the generalization considering different factors as mentioned above?</p>
</blockquote>

<p>Generalization techniques are used to make the network generalize well. The customary approaches are:</p>

<ul>
<li>Drop Out</li>
<li>L1/L2 Regularization</li>
<li>Early Stop</li>
<li>Adding Noise and data-augmentation</li>
</ul>

<p>It should be considered that adding noise to the input should be without changing the distribution. The ratio of signal to noise also should not be small because the information may be lost compeletely. Depending on your problem each of them make work well but I guess there is no consensus which one is the best but in deep-learning the first one is so helpful. </p>

<blockquote>
  <p>What is the connection between optimization and generalization (as defined in [5])?</p>
</blockquote>

<p>What I'm saying is based on my experience. Optimization itself always lead to overfitting. You have to use generalization techniques to avoid that. To help you figure out the problem, suppose that deep-learning algorithms are able to learn all functions. If you provide them with relatively small number of data, they will memorize a hypothesis, and they do not learn the problem. </p>
","3","2","28175","12588"
"29490","<p>I recommend you uploading a zip file containing your images to your drive and downloading the content from drive to Colab. Then you will be able to extract them. The code for uploading is <a href=""https://datascience.stackexchange.com/a/27967/28175"">here</a>.</p>
","0","2","28175","12588"
"29510","<p>If you use <code>max-pooling</code> layers, they may be insensetive to small shifts but not that much. If you want your network to be able to be invariant to transformations, such as translations and shifts or other types of customary transformations, you have two solutions, at least as far as I know:</p>

<ul>
<li>Increasing the size of data-set</li>
<li>Using spatial transformers </li>
</ul>

<p>Take a look at <a href=""https://datascience.stackexchange.com/a/27237/28175"">
What is the state-of-the art ANN architecture for MNIST</a> and <a href=""https://datascience.stackexchange.com/a/26291/28175"">Why do convolutional neural networks work</a>.</p>

<p>Thanks to one of our friends, another way is to use transfer learning after data-augmentation. </p>
","8","2","28175","12588"
"29522","<p>First of all, gradient descent cannot find the global optimum. If your function has just one extremum, it can find it but if it has lots of them there is no guarantee that it finds the best one. If you are familiar with derivative and slope of functions, the Normal equation, tries to find the point which all the derivative is equal to zero for all directions, variables. It is the result of the derivatives.</p>

<p>You are somehow right, mathematically we know the optimal solution of all ML problems but in practice, normal equations do not work. The reason is that if you increase the number of features, you will have a matrix which gets larger and larger. Most problems are not linearly separable. Consequently, you have to add high order polynomials and the point is that you don't know which polynomial to use. If you use just the second order polynomial you will get a matrix that won't be located in your computer's memory because of being so much large. suppose that you have <code>100</code> features. If you just add the combination of multiplication of two variables, compute how many entries your matrix will have. And if you do that you can be sure that it does not add too much complexity to your model, because you've not added complex higher order polynomials whilst you need them. ML algorithms like deep-nets try to learn so many complicated functions using the smaller number of entries.</p>

<blockquote>
  <p>How/why does this equation exactly work? Could it be compared to algorithms such as gradient descent? If so, how? Is it possible to use this for more complex activation functions such as hyperbolic tangent?</p>
</blockquote>

<p>It is the result of taking the multivariable derivative, gradient. Yes the goal of them is to find the local minima, but they have a different approach. Gradient descent tries to go downhill whilst normal equation tries to find the location by finding where the derivative is zero for all features.</p>

<p>I didn't figure out the last question to help you address it.</p>
","1","2","28175","12588"
"29588","<p>Actually, I guess it highly depends on the real data-set and its distribution. I guess the paper has referred to that is that on occasions that the distribution of each class varies, your model won't work well because of changing the distribution of each class. In cases like a disease prediction where the number of each class varies for different places, a model that is trained in the U.S won't work in African countries at all. The reason is that the distribution of classes has been changed. So in such cases that usually the negative and positive classes are not balanced in practice, balancing them will cause the problem of distribution changes. On these occasions, people usually use the real data-set which is not balanced and use <code>F1</code> score for evaluation. </p>
","2","2","28175","12588"
"29591","<p>If I want to quote from <a href=""https://en.wikipedia.org/wiki/Big_data"" rel=""nofollow noreferrer"">Wikipedia</a>, <em>Big data is data sets that are so voluminous and complex that traditional data-processing application software are inadequate to deal with them. Big data challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy and data source. There are five concepts associated with big data: volume, variety, velocity and, the recently added, veracity and value.</em></p>

<p><em>Big data can be described by the following characteristics:</em></p>

<ul>
<li>Volume</li>
</ul>

<p><em>The quantity of generated and stored data. The size of the data determines the value and potential insight, and whether it can be considered big data or not.</em></p>

<ul>
<li>Variety</li>
</ul>

<p><em>The type and nature of the data. This helps people who analyze it to effectively use the resulting insight. Big data draws from text, images, audio, video; plus it completes missing pieces through data fusion.</em></p>

<ul>
<li>Velocity</li>
</ul>

<p><em>In this context, the speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development. Big data is often available in real-time.</em></p>

<ul>
<li>Variability</li>
</ul>

<p><em>Inconsistency of the data set can hamper processes to handle and manage it.</em></p>

<ul>
<li>Veracity</li>
</ul>

<p><em>The data quality of captured data can vary greatly, affecting the accurate analysis.</em></p>

<hr />

<p>To me, big data is highly connected to the deep-learning era. The reason is that during past decades, people could make good descriptions and models of data using machine-learning and data-mining but because everyday new data is coming out, social networks increase rapidly and digital gadgets' popularity is increasing among different nations, the demand for processing data and converting them to information and knowledge is increasing. If we want to use previous techniques to gather information from raw data, it will take too much time, if possible, to reach to appropriate results. In big data and deep-learning era, we need more complicated algorithms and more powerful hardware to deal with difficulties. </p>

<p>You can also take a look at <a href=""https://www.sas.com/en_us/insights/big-data/what-is-big-data.html"" rel=""nofollow noreferrer"">here</a> and <a href=""http://searchcloudcomputing.techtarget.com/definition/big-data-Big-Data"" rel=""nofollow noreferrer"">here</a> which have relatively different perspective. <em>Big data is a term that describes the large volume of data – both structured and unstructured – that inundates a business on a day-to-day basis. But it’s not the amount of data that’s important. It’s what organizations do with the data that matters. Big data can be analyzed for insights that lead to better decisions and strategic business moves.</em></p>
","3","2","28175","12588"
"29810","<p>I have the following solutions:</p>

<ul>
<li>If you have abundant data you can shuffle them and make validation and training data. After that, your neural network should exploit generalization techniques not to overfit the training data. By doing so, you may have relatively acceptable performance which works in noisy situations. </li>
<li>The other technique is evaluating Bayes error. This does not have any relation to the neural nets. It just tries to investigate in the feature space of the problem what percentage of your data is misleading, having same input patterns with contradictory labels.</li>
<li>Another approach can be using an existing model for validating the data-set.</li>
</ul>
","0","2","28175","12588"
"29812","<p>It is a bit technical and depends where you use your <code>JavaScript</code> code. It is used in both backend and frontend apps. My opinion is that using that in frontend apps can help your ML algorithms run on distributed devices. Take a look at <a href=""http://cs.stanford.edu/people/karpathy/convnetjs/"" rel=""nofollow noreferrer"">here</a>. You can use this language to run ML codes on hosts' computers.</p>
","0","2","28175","12588"
"29833","<p>Polynomial regression can have multiple entries in the normal equation and it is not easy to say which polynomials you have to use in advance. Moreover, if you have lots of features you cannot handle memory errors most of the time. Nowadays people use <code>MLP</code>s and use batch normalization among layers for learning better. Those that you are referring to are a bit old algorithms but the former one is the logical mathematical solution for learning problems and the latter one is a beginning point for deep neural networks. I recommend taking a look at <a href=""https://datascience.stackexchange.com/q/21896/28175"">here</a> and <a href=""https://datascience.stackexchange.com/q/18802/28175"">here</a>.</p>
","1","2","28175","12588"
"29847","<blockquote>
  <p>Is there libraries to analyze sequence with python</p>
</blockquote>

<p>You can take a look at <a href=""http://biopython.org/DIST/docs/tutorial/Tutorial.html"" rel=""nofollow noreferrer"">here</a>. You can also use <code>TensorFlow</code> if your task is sequence classification, but based on comments you have referred that your task is unsupervised. Actually, <code>LSTMs</code> can be used for unsupervised tasks too depending on what you want. Take a look at <a href=""https://scholar.google.com/scholar?q=lstm+for+clustering&amp;hl=en&amp;as_sdt=0&amp;as_vis=1&amp;oi=scholart&amp;sa=X&amp;ved=0ahUKEwiiuNmBhZ7aAhXRIlAKHV4xBlMQgQMIJDAA"" rel=""nofollow noreferrer"">here</a>.</p>

<blockquote>
  <p>And is it right way to use Hidden Markov Models to cluster sequences?</p>
</blockquote>

<p>Markov hidden models are those that your current state does not depend on all previous states. If you your task has longterm dependencies, you can use <code>LSTM</code> networks. If your data does not have longterm dependencies you can use simple <code>RNNs</code>. </p>
","1","2","28175","12588"
"29957","<p>Take a look at professor Andrew Ng's course about deep learning. Its homework is written in <code>Keras</code> and <code>TensorFlow</code>. It is a very good point to begin.
<hr />
I guess it is not fair to mention non-free lectures because the owners of other non-free lectures may get disappointed but take a look at <a href=""https://www.udemy.com/complete-guide-to-tensorflow-for-deep-learning-with-python/"" rel=""nofollow noreferrer"">complete guide to TensorFlow for deep learning with python</a>, which is very useful.
<hr />
I have also found <a href=""http://web.stanford.edu/class/cs20si/syllabus.html"" rel=""nofollow noreferrer"">these</a> lectures from Stanford university very useful. Furthermore, you can take a look at the official paper of google research team about <a href=""https://arxiv.org/pdf/1603.04467"" rel=""nofollow noreferrer"">TensorFlow</a> where you can read <em>TensorFlow is an interface for expressing machine learning
algorithms, and an implementation for executing such algorithms.
A computation expressed using TensorFlow can be
executed with little or no change on a wide variety of heterogeneous
systems, ranging from mobile devices such as phones
and tablets up to large-scale distributed systems of hundreds
of machines and thousands of computational devices such as
GPU cards. The system is flexible and can be used to express
a wide variety of algorithms, including training and inference
algorithms for deep neural network models, and it has been
used for conducting research and for deploying machine learning
systems into production across more than a dozen areas of
computer science and other fields, including speech recognition,
computer vision, robotics, information retrieval, natural
language processing, geographic information extraction, and
computational drug discovery. This paper describes the TensorFlow
interface and an implementation of that interface that
we have built at Google. The TensorFlow API and a reference
implementation were released as an open-source package under
the Apache 2.0 license in November, 2015 and are available at
www.tensorflow.org.</em>.</p>
","1","2","28175","12588"
"30033","<p>You can have that but I recommend something else for your case. Suppose that you have 10 classes of digits that they are mutually exclusive. Consequently, your output vector will always be on-hot encoded. In a simple case, your output vector should contain ten entries for each class. To make your model distinguish between those inputs which are digits and those which are not, do the following:</p>

<p>Increase the size of output to eleven. Provide data-set that contains eleven-entries labels. the first entry corresponds to the existence of a digit. If it is a digit the first entry should be one and next 10 entries should be activated for the according to the value of the digit. So, your labels will not be one-hot encoded. If the input is not a digit the first entry of the output should be zero and other entries are don't cares. </p>
","1","2","28175","12588"
"30086","<p>Keras is a high-level API that can be used on top of <a href=""https://github.com/tensorflow/tensorflow"" rel=""nofollow noreferrer""><code>TensorFlow</code></a>, <a href=""https://github.com/Microsoft/cntk"" rel=""nofollow noreferrer""><code>CNTK</code></a> and <a href=""https://github.com/Theano/Theano"" rel=""nofollow noreferrer""><code>Theano</code></a>. You can use each of the low-level APIs but the problem of those is that you can get complicated if you design very deep nets whilst dealing with <code>Keras</code> is much easier. Consequently, <a href=""https://keras.io/"" rel=""nofollow noreferrer""><code>Keras</code></a> is designed for accelerating deep nets' designing. <code>Keras</code> is opensource like the underlying libraries it comes for and I guess its project is not for <code>Google</code>. <code>TensorFlow</code> itself has a high-level API, namely <a href=""http://tflearn.org/"" rel=""nofollow noreferrer""><code>TFLearn</code></a>. I cannot say which is <a href=""https://datascience.stackexchange.com/a/25318/28175"">better</a> but the point is that try to master one of them perfectly. </p>

<p>It seems that <code>plaidML</code> Keras backend is also available which enables training on <strong>AMD</strong> graphics.</p>
","1","2","28175","12588"
"30131","<p>As you can read from <a href=""https://stackoverflow.com/a/34162641/5120235"">here</a></p>
<blockquote>
<p><code>plt.subplots()</code> is a function that returns a tuple containing a figure
and axes object(s). Thus when using <code>fig, ax = plt.subplots()</code> you
unpack this tuple into the variables <code>fig</code> and <code>ax</code>. Having <code>fig</code> is useful
if you want to change figure-level attributes or save the figure as an
image file later (e.g. with <code>fig.savefig('yourfilename.png')</code>. You
certainly don't have to use the returned figure object but many people
do use it later so it's common to see. Also, all axes objects (the
objects that have plotting methods), have a parent figure object.</p>
</blockquote>
<p>The parameter <code>bins</code> is used to set the number of ranges to be used to accumulate the data in those ranges. I don't know where else you have question.</p>
","3","2","28175","12588"
"30134","<blockquote>
  <p>I now want to analyze the factors which are most contributing to the medical condition</p>
</blockquote>

<p>If this is your purpose I suggest you plotting covariance matrix for input features. By doing so you will be able to see the correlation of your features together. That may help you diminish redundant correlated features <a href=""https://datascience.stackexchange.com/a/25271/28175"">1</a>.  Moreover, you can plot the correlation of your outcomes and each feature to see their relationship. That helps you figure out which features are correlated or uncorrelated to the outcomes.</p>

<p><code>PCA</code> helps you find major components are needed to describe the phenomena <a href=""https://datascience.stackexchange.com/a/26719/28175"">2</a>. Neural nets are useful for making a model that can predict for labeled data-sets. For your case, I've not seen any paper but there might be.</p>
","2","2","28175","12588"
"30246","<p>Depending on the data structure you are keeping the values there might be different solutions.</p>

<p>If you are using <code>Numpy</code> arrays, you can employ <code>np.insert</code> method which is referred <a href=""https://stackoverflow.com/a/39848762/5120235"">here</a>:</p>

<pre><code>import numpy as np
a = np.arrray([(122.0, 1.0, -47.0), (123.0, 1.0, -47.0), (125.0, 1.0, -44.0)]))
np.insert(a, 2, np.nan, axis=0)
array([[ 122.,    1.,  -47.],
       [ 123.,    1.,  -47.],
       [  nan,   nan,   nan],
       [ 125.,    1.,  -44.]])
</code></pre>

<p>If you are using <code>Pandas</code> you can use instance method <code>replace</code> on the objects of the <code>DataFrames</code> as referred <a href=""https://stackoverflow.com/a/34794112/5120235"">here</a>:</p>

<pre><code>In [106]:
df.replace('N/A',np.NaN)

Out[106]:
    x    y
0  10   12
1  50   11
2  18  NaN
3  32   13
4  47   15
5  20  NaN
</code></pre>

<p>In the code above, the first argument can be your arbitrary input which you want to change.</p>
","1","2","28175","12588"
"30354","<p>Actually, there are many linear and non-linear machine learning algorithms. Selecting a right algorithm highly depends on your data-set and the nature of your data. for instance, in machine-learning era, it was customary to estimate functions by assigning a typical model to the problem and reducing the error by predicting the appropriate coefficients using a cost function for regression tasks. In such cases, you should have bias-variance trade-off. That means you should not fit the data nor miss it. you should find a good estimate, a good model that has generalization capability. For doing all of these you have to choose features which help you describe the problem better for making a model. In deep-learning, we usually do not have this trade-off. If you increase the size of your training set, you can almost be sure that you can have better results. In machine-learning, you can always be sure that by making complex non-linear models, you overfit your data while using complex deep-learning models does not necessarily mean that if you employ generalization techniques which avoid overfitting. For choosing a right model, it is customary to use a simple linear model and make it complicated step by step. Problems which their inputs have numerous features, you can not see and visualize the data-set to check whether they are linearly separable in that space or not. Consequently, beginning with a simple model and making it complex step by step is a logical solution. For measuring performance there are different solutions that all depend on your goal. Take a look at <a href=""https://datascience.stackexchange.com/a/26855/28175"">here</a>.</p>
","3","2","28175","12588"
"30360","<p>Whenever you have features that they have different scale and it is significant for some features, you should standardize your feature. Take a look at <a href=""https://stats.stackexchange.com/q/207108/179078"">here</a>.</p>
","1","2","28175","12588"
"30440","<p>Actually, if you change the input the input size, nothing goes wrong with the convolutional layers but the outputs of these layers increases and that will cause to the increased number of inputs to the dense layers <a href=""https://datascience.stackexchange.com/a/28312/28175"">1</a>. Consequently, you will have to have extra weights and you have to train them. That is why it is better to have a fixed size input. But there are other solutions that can be expanded to your task. Take a look at the contents of the third week of <a href=""https://www.coursera.org/learn/convolutional-neural-networks"" rel=""nofollow noreferrer"">this</a> course. As you will see, you have to input patches of the image to the network but what is done there is for detection tasks, you have to expand it to your task, but maybe resizing all inputs to a predefined size is simpler.</p>
","2","2","28175","12588"
"30450","<p>Although <code>CNN</code> stands for convolutional neural networks, what they do is named cross-correlation in mathematics and not convolution. Take a look at <a href=""https://www.coursera.org/learn/convolutional-neural-networks/lecture/wfUhx/strided-convolutions"" rel=""nofollow noreferrer"">here</a>.</p>

<blockquote>
  <p>Now, before moving on there is a technical comment I want to make about cross-correlation versus convolutions and just for the facts what you have to do to implement convolutional neural networks. If you reading different math textbook or signal processing textbook, there is one other possible inconsistency in the notation which is that, if you look at the typical math textbook, the way that the convolution is defined before doing the element Y's product and summing, there's actually one other step ...</p>
</blockquote>
","1","2","28175","12588"
"30616","<p>Because your inputs have numerous features, it is not clear whether your data-set is linearly separable using hyperplanes as a hypothesis or not due to the fact that you can not visualize your data and see the outcomes. First, try to add one simple neuron which has a linear calculation and a nonlinearity activation. The cost function of a single neuron is convex and it shows you whether your data is linearly separable or not in that feature space. If you have high error rate then you have to add neurons. For figureing out how you should add extra neurons and layers take a look at <a href=""https://datascience.stackexchange.com/a/26642/28175"">here</a> which may help you.</p>
","1","2","28175","12588"
"30618","<p>The basic intuition is that you should not have the same learning rate for different dimensions. For instance, you can have a high slope in one direction but not for another. Consequently, you should not have the same speed for the two directions. Momentum adds acceleration. Suppose gradient is your instant velocity and the average is your average velocity. Momentum is actually viscosity or somehow friction. Suppose that you are near your optimal points, your gradients become zero and you have low average which means your speed changes slowly. They have both alpha term but what is going to be used is the running average, just a kind of average which is simple to be calculated. Take a look at <a href=""https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop"" rel=""nofollow noreferrer"">here</a> and <a href=""https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum"" rel=""nofollow noreferrer"">here</a> for making an analogy.</p>
","3","2","28175","12588"
"30650","<p>One of the good approaches for dealing with such problems is using landmark detection. In this case, your problem will look like a combination of classification and regression task using deep neural networks. Take a look at <a href=""https://www.coursera.org/learn/convolutional-neural-networks/lecture/OkD3X/landmark-detection"" rel=""nofollow noreferrer"">here</a> and <a href=""https://pdfs.semanticscholar.org/46b2/c6727f5ccf06f53020844aa0157237ac5a89.pdf"" rel=""nofollow noreferrer"">here</a>. You should specify points which have the same identity among all data-set. Then your network will try to find the position of each point. Also take a look at <a href=""http://www.robots.ox.ac.uk/~vgg/software/via/"" rel=""nofollow noreferrer"">here</a>.</p>
","0","2","28175","12588"
"30699","<p>Actually, your task is supervised. <a href=""https://arxiv.org/abs/1511.00561"" rel=""nofollow noreferrer""><code>Segnet</code></a> can be good architecture for your purpose which one of its implementations can be accessed <a href=""https://github.com/alexgkendall/caffe-segnet"" rel=""nofollow noreferrer"">here</a>. <em>SegNet learns to predict pixel-wise class labels from supervised learning. Therefore we require a dataset of input images with corresponding ground truth labels. Label images must be single channel, with each pixel labelled with its class <a href=""http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html"" rel=""nofollow noreferrer"">...</a></em>.</p>

<p>Also, take a look at <a href=""https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf"" rel=""nofollow noreferrer""><strong>Fully Convolutional Networks</strong></a> which are well suited for your task.</p>

<hr />

<p>Based on the edits in the question, I add extra information. There are numerous methods that can be applied for this task. Basically the easiest one is to use a <em>background</em> label and classify those classes that you don't know as background by employing the mentioned architectures. By doing so you will have labels which can have overlap for background class which is a probable downside of this approach but its advantage is that in cases where your trained labels are frequently used in the inputs, you can have a relatively light version of architecture which recognizes the unknown classes. </p>
","2","2","28175","12588"
"30871","<p>Based on my experience, not just for <em>ImageNet</em>, if you have enough data it's better to train your network from scratch. There are numerous reasons that I can explain why.</p>

<ul>
<li><p>First of all, I don't know whether you have had this experience or not but I've trained complicated <code>CNNs</code> neworks with over 25 million parameters. After reaching <code>95%</code> accuracy, after convergence, I changed the learning rate to a bit bigger number to find another probable local minimum. I've not found any answer till today but whenever I did so, my accuracy decreased signifincantly all the time and it never imporoved even after more than thosands of epochs. </p></li>
<li><p>The other problem is that whenver you use transfer learning, your training data should have two options. First of all, the distribution of the training data which your pre-trianed model has used should be like the data that you are going to face during test time or at least don't vary too much. Second, the number of training data for transfer learning should be in a way that you don't overfit the model. If you have a few number of labeled training data, and your typical pre-trained model, like <code>ResNet</code> has millions of parameters, you have to be aware of overfitting by choosing appropriate metrics for evaluation and good testing data which represent the real distribution of the real population.</p></li>
<li><p>Next problem is that you can not remove layers with confidence to reduce the number of parameters. Basically the number of layers, as you yourself know, is a hyper-parameter that there is no consensus on how to be chosen. If you remove the convolutional layers from the first layers, again based on experience, you won't have good learning because of the nature of the architecture which finds low level features. Furthermore, if you remove first layers you will have problem for your dense layers, because the number of trainable parameters changes in this way. Densely connected layers and deep convolutional layers can be good points for reduction but it may take time to find how many layers and neurons to be diminished inorder not to overfit.</p></li>
</ul>

<p>If you don't have enough data and there is already a pre-trained model, you can do something that can help you. I divide my answer to two parts for this situation:</p>

<h3>The pre-trained model does not have common labels for most of its classes</h3>

<p>If this is the case, you can forget all the fully connected layers and replace new  ones. Basically what they do is just classifying the features that the network has found to reduce the error rate. About the convolutional layers you have to consider two major points:</p>

<ol>
<li><p>Pooling layers try to sum up the information in a local neighbourhood and make a higher representation of the inputs. Suppose that the inputs of a pooling layer have nose, eyes, eybrows and so on. Your pooling layers somehow attempt to check whether they exist in a neighbourhood or not. Consequently, convolutional layers after pooling layers usually keep information which may be irrelevant to your task. There is a downside for this interpretation. The information may be distributed among different activation maps as Jason Yosinski <em>et al.</em> have investigated this nature at <a href=""http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf"" rel=""noreferrer"">Understanding neural networks through deep visualization</a> where you can read <em>One of the most interesting conclusions so far has been that representations on <strong>some</strong> layers seem to be surprisingly local. Instead of finding distributed representations on all layers, we see, for example, detectors for text, flowers, fruit, and faces on conv4 and conv5. These conclusions can be drawn either from the live visualization or the optimized images (or, best, by using both in concert) and suggest several directions for
future research</em> and <em>These visualizations suggest that further study into the exact nature of learned representations—whether they are local to a single channel or distributed across several ...</em>. A partial solution can be keeping the first layers which find low level features that are usually shared among different data distributions and removing deeper convolutional layers which find higher abstractions. </p></li>
<li><p>As stated, the main problem of convolutional layers is that the information they find may be distributed among different activation maps. Consequently, you can not be sure by removing a layer you can have better performance or not. </p></li>
</ol>

<h3>The pre-trained model has some common classes with your labels</h3>

<p>If this is the case you can visualize the activations using the techniques described <a href=""http://yosinski.com/deepvis"" rel=""noreferrer"">here</a>. They have shown that although human face is not a label in <code>ImageNet</code>, some of internal activation maps get activated by face. This observation can be seen for other labels too. For instance, netwroks for deciding a scene contains a car or not are usually sensetive to roads and trees. The image below shows that which parts of ouputs get activated for which parts of images. This can help you when you don't have enough data and you have to use transfer learning. </p>

<p><a href=""https://i.stack.imgur.com/cUBJb.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/cUBJb.jpg"" alt=""enter image description here""></a></p>

<p>Based on the answer <a href=""https://stats.stackexchange.com/a/267242/179078"">here</a> <em>The standard classification setting is an input distribution $p(X)$ and a label distribution $p(Y|X)$. Domain adaptation: when $p(X)$ changes between training and test. Transfer learning: when $p(Y|X)$ changes between training and test. In other words, in DA the input distribution changes but the labels remain the same; in TL, the input distributions stay the same, but the labels change.</em> Consequently, domain adaption problems also can be considered for the mentioned solutions.</p>
","11","2","28175","12588"
"30873","<p>I guess you have made a file in your default document folder. A probable location for your notebook is where it starts if you have not changed the default path or maybe the location in which your files is located. Try to rerun your jupyter from your terminal or command prompt. It will open the contents of the folder you are looking for your notebook.</p>
","1","2","28175","12588"
"30875","<p>Actually this is called <code>text line extraction</code>. What I'm going to tell you is inspired from the lectures of <a href=""https://scholar.google.com/citations?user=i9zCOJEAAAAJ&amp;hl=en&amp;oi=ao"" rel=""nofollow noreferrer"">this</a> scientist. For finding digits you don't need to design a network. You should extract them and then feed them to a network in turn.</p>

<p>First of all you have to read your image.</p>

<pre><code>import cv2
import numpy as np
from matplotlib import pyplot as plt
%matplotlib inline
img = cv2.imread('./doc1.png')
plt.imshow(img)
</code></pre>

<p><a href=""https://i.stack.imgur.com/Rj7Wl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Rj7Wl.png"" alt=""enter image description here""></a></p>

<p>Then you have to make your image as a binary array.</p>

<pre><code>img.shape
</code></pre>

<blockquote>
  <p>(2360, 1649, 3)</p>
</blockquote>

<pre><code>img = cv2.imread('./doc1.png',0)
img.shape
</code></pre>

<blockquote>
  <p>(2360, 1649)</p>
</blockquote>

<pre><code>plt.imshow(img)
</code></pre>

<p><a href=""https://i.stack.imgur.com/rvdzP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rvdzP.png"" alt=""enter image description here""></a></p>

<pre><code>plt.imshow(img,cmap = 'gray')
</code></pre>

<p><a href=""https://i.stack.imgur.com/8fKmW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8fKmW.png"" alt=""enter image description here""></a></p>

<p>The following code shows some of the lines of the document:</p>

<pre><code>plt.imshow(img[900:1020,500:900],cmap = 'gray')
</code></pre>

<p><a href=""https://i.stack.imgur.com/ppaLK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ppaLK.png"" alt=""enter image description here""></a></p>

<pre><code>bimg=cv2.cvtColor(img[900:1020,500:900],cv2.COLOR_GRAY2RGB)
bimg.shape
</code></pre>

<blockquote>
  <p>(120, 400, 3)</p>
</blockquote>

<p>Next, you have to find the lines in the image, then you have to find the characters.</p>

<p><a href=""https://i.stack.imgur.com/ZYrag.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZYrag.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/upjyh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/upjyh.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/IIWk4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IIWk4.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Ss47c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ss47c.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/6IF1c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6IF1c.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/BBD8e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BBD8e.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/kw09f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kw09f.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Fmj9K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fmj9K.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/JBL3y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JBL3y.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/WHdbv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WHdbv.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/dKhmC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dKhmC.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/SPcHV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SPcHV.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/GlfYH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GlfYH.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/LRlk6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LRlk6.png"" alt=""enter image description here""></a></p>

<p>Basically the code was completely clear to me. If you don't understand it, let me know.</p>
","2","2","28175","12588"
"30885","<pre><code>#x = tf.reshape(x, shape=[-1, 299, 299, 3])
</code></pre>

<p>change the above line to the following code snippet.</p>

<pre><code>dim = x.get_shape().as_list()
x = tf.reshape(x, shape = [-1, *dim[1:]])
</code></pre>

<p>The same behavior applies for <code>z</code> and <code>Z1</code>.</p>
","0","2","28175","12588"
"30954","<p>Based on the answer <a href=""https://datascience.stackexchange.com/a/17708/28175"">here</a> and the blog post <a href=""https://harmdevries89.wordpress.com/2015/03/27/tied-biases-vs-untied-biases/"" rel=""nofollow noreferrer"">here</a> there are two variants for using bias in convolutional layers. <strong><em>Tied biases</strong> if you use one bias per convolutional filter/kernel  and <strong>untied biases</strong> if you use one bias per kernel and output location.</em> </p>
","1","2","28175","12588"
"30965","<p>Basically fine-tuning or transfer learning is used for situations where you don't have so much data or computation time or maybe computation capacity to train a whole network from scratch. In this way, you usually replace the last layers of a pre-trained network with new ones. The first layers have already found the needed good features and last layers try to classify, you replace them because you have your own labels. Take a look at <a href=""https://datascience.stackexchange.com/a/30871/28175"">here</a> for considering how it should be applied and <a href=""https://www.coursera.org/learn/convolutional-neural-networks/lecture/4THzO/transfer-learning"" rel=""nofollow noreferrer"">here</a> for more details. You can keep weights, also called freezing, or let the gradient-based algorithms change them, but usually freezing first layers and training last layers is faster.</p>
","2","2","28175","12588"
"30994","<p>Whenever you have a convex cost function you are allowed to initialize your weights to zeros. The cost function of logistic regression and linear regression have convex cost function if you use MSE for, also <a href=""https://stats.stackexchange.com/a/90556/179078"">RSS</a>, linear regression and cross-entropy for logistic regression. The main idea is that for convex cost function you'll have just a single optimal point and it does not matter where you start, the starting point just changes the number of epochs to reach to that optimal point whilst for neural networks the cost function does not have just one optimal point. Take a look at <a href=""https://datascience.stackexchange.com/q/18802/28175"">here</a>. About random initialization, you have to consider that you are not allowed to choose random weights which are too small or too big although the former was a more significant problem. If you choose random small weights you may have vanishing gradient problem which may lead to a network that does not learn. Consequently, you have to use standard initialization methods like <code>He</code> or <code>Glorot</code>, take a look at <a href=""https://datascience.stackexchange.com/a/10930/28175"">here</a> and <a href=""https://scholar.google.com/scholar?cluster=17889055433985220047&amp;hl=en&amp;as_sdt=0,22"" rel=""noreferrer"">Understanding the difficulty of training deep feedforward neural networks</a>. </p>

<p>Also, take a look at the following question.</p>

<ul>
<li><a href=""https://datascience.stackexchange.com/a/18145/28175"">How should the bias be initialized and regularized</a></li>
<li><a href=""https://datascience.stackexchange.com/q/13061/28175"">When to use (He or Glorot) normal initialization over uniform init? And what are its effects with Batch Normalization</a></li>
</ul>
","7","2","28175","12588"
"31252","<p>For increasng your accuracy the simplest thing to do in tensorflow is using <code>Dropout</code> technique. Try to use <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dropout"" rel=""nofollow noreferrer""><code>tf.nn.dropout</code></a>. between your hidden layers. Do not use it for your first and last layers. For applying that, you can take a look at <a href=""https://stackoverflow.com/questions/40879504/how-to-apply-drop-out-in-tensorflow-to-improve-the-accuracy-of-neural-network"">How to apply Drop Out in Tensorflow to improve the accuracy of neural network</a>.</p>
","2","2","28175","12588"
"31253","<p>You have to use <code>F1</code> score. A simple solution for that is to use <em>confusion matrix</em>. The way you can find <code>F1</code> score for each class is simple. your true labels for each class can be considered as true predictions and the rest which are classified wrongly as the other classes should be added to specify the number of false predictions. For each class, you can find the <code>F1</code> score. For more details take a look at <a href=""https://stackoverflow.com/q/37615544/5120235"">F1-score per class for multi-class classification</a>. You can take a look at <a href=""https://github.com/keras-team/keras/issues/6507"" rel=""nofollow noreferrer"">this</a> implementation.</p>
","2","2","28175","12588"
"31295","<blockquote>
  <p>Also please explain what this array - array.mean() do?</p>
</blockquote>

<p>Basically, it is doing memberwise subtraction operation after <a href=""https://docs.scipy.org/doc/numpy-1.14.0/user/basics.broadcasting.html"" rel=""noreferrer"">broadcasting</a>. <code>np.mean</code> function finds the mean in your array and its result will be a scalar, a single number. Your array is a <code>numpy</code> array and the result of the latter term is a single value as mentioned. Consequently, the single value gets extended to the shape of the former term. then a memberwise subtraction will be performed for each entry of the array and the result will have the same shape as the former term.</p>

<blockquote>
  <p>Can you please explain what does this normalize function do?</p>
</blockquote>

<p>Normalizing data is done for accelerating optimization. If you have features with different scales, it will take too much time for your optimizer function to find optimal points. Suppose you have age feature which can change between 0 to 150 (!) and salary which can be changed from 0 to whatever, like 500,000,000 $. your optimization algorithm used in your ML model will take too much time, if possible, to find appropriate weights for each feature. Moreover, if you don't scale your data, your ML algorithm may take too much care to features with large scales.</p>
","7","2","28175","12588"
"31299","<p>The idea of convolutional layers is that we need same weights to be applied to different regions of the input. It lets you identify same patterns that occur in different regions of the input. You can consider each convolution operation which is carried out by each window as a single neuron which just transforms a local region with a non-linear transformation. The linear and non-linear operations that are done by these so-called local neurons help the convolutional layers learn non-linear and complex features. Another aspect of this operation is that it reduces the number of weights significantly. The final interpretation is that if you don't use non-linear activations, you would have linear local features extracted from the input. Those linear features can easily be found with fewer number of parameters.</p>
","1","2","28175","12588"
"31394","<p>My solution is like your first recommendation, but with slight changes.</p>

<ol>
<li>Construct your convolutional layers and stack them till the flatten-layer. This network should be fed with the image data.</li>
<li>Flat your activation maps</li>
<li>Construct a fully connected network with the desired number of neurons and layers. </li>
<li>Concatenate the outputs of the flattened layer of the convolutional net and the fully connected net. </li>
<li>add some dense layers and connect them to the last layer which represents your classes.</li>
</ol>

<p>You can use customary cost functions for this architecture.</p>
","14","2","28175","12588"
"31464","<p>If you use logistic regression and the <code>cross-entropy</code> cost function, it's shape is convex and there will be a single minimum. But during optimization, you may find weights that are near to optimal point and not exactly on the optimal point. This means that you can have multiple classifies that reduce the error and maybe set it to zero for the training data but with different weights which are slightly different. This can lead to different decision boundaries. This approach is based on based on <a href=""https://stats.stackexchange.com/a/29326/179078"">statistical methods</a>. As it is illustrated in the following shape, you can have different decision boundaries with slight changes in the weights and all of them have zero error on the training examples.</p>

<p><a href=""https://i.stack.imgur.com/fAEnA.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fAEnA.png"" alt=""logistic regression decision boundaries""></a></p>

<p>What <code>SVM</code> does is an attemption to find a decision boundary that reduces the risk of error on the test data. It tries to find a decision boundary that has the same distance from the boundary points of both classes. Consequently, both classes will have a same space for the empty space which there is no data there. <code>SVM</code> is <a href=""https://stats.stackexchange.com/a/95348/179078"">geometrically motivated rather than statistically</a>. </p>

<p><a href=""https://i.stack.imgur.com/K5Flr.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/K5Flr.png"" alt=""enter image description here""></a> </p>

<blockquote>
  <p>None kernelized SVMs are nothing more than linear separators. Therefore, is the only difference between an SVM and logistic regression the criterium to choose the boundary?</p>
</blockquote>

<p>They are linear separators and if you find out that your decision boundary can be a hyperplane, it's better to use an <code>SVM</code> for diminishing the risk of error on test data.</p>

<blockquote>
  <p>Apparently SVM chooses the maximum margin classifier and logistic regression the one that minimizes the cross-entropy loss.</p>
</blockquote>

<p>Yes, as stated <code>SVM</code> is based on geometrical properties of the data whilst <code>logistic regression</code> is based on statistical approaches.</p>

<blockquote>
  <p>In this case, are there situations where SVM would perform better than logistic regression, or vice-versa?</p>
</blockquote>

<p>Ostensibly, their results are not very different, but they are. <code>SVM</code>s are better for generalization <a href=""http://tka4.org/materials/lib/Articles-Books/Speech%20Recognition/from%20Nickolas/SVM%20and%20Generalization.pdf"" rel=""noreferrer"">1</a>, <a href=""https://stats.stackexchange.com/q/259788/179078"">2</a>.</p>
","11","2","28175","12588"
"31643","<p>I quote the answers from <a href=""https://www.quora.com/What-is-a-bilinear-tensor-layer-in-contrast-to-a-standard-linear-neural-network-layer-or-how-can-I-imagine-it"" rel=""nofollow noreferrer"">What is a bilinear tensor layer (in contrast to a standard linear neural network layer) or how can I imagine it?</a>.</p>
<p><em>A bilinear function is a function of two inputs <span class=""math-container"">$x$</span> and <span class=""math-container"">$y$</span> that is linear in each input separately. Simple bilinear functions on vectors are the dot product or the element-wise product.</em></p>
<p><em>Let <span class=""math-container"">$M$</span> be a matrix. The function <span class=""math-container"">$f(x,y)=x^TMy=\sum_iM_{ij}x_iy_j$</span> is bilinear in <span class=""math-container"">$x$</span> and <span class=""math-container"">$y$</span>. In fact, any scalar bilinear function on two vectors takes this form. Note that a bilinear function is a linear combination of <span class=""math-container"">$x_iy_j$</span> whereas a linear function such as <span class=""math-container"">$g(x,y)=Ax+By$</span> can only have <span class=""math-container"">$x_i$</span> or <span class=""math-container"">$y_i$</span>. For neural nets, that means a bilinear function allows for richer interactions between inputs.</em></p>
<p><em>Now what if you want a bilinear function that outputs a vector? Well, you simply define a matrix <span class=""math-container"">$M_k$</span> for each coordinate of the output and you end up with a stack of matrices. That stack of matrices is called a tensor (3-mode tensor to be exact). You can imagine the bilinear tensor product with two vectors as <span class=""math-container"">$x^⊤M_ky$</span> computed on each “slice” of the tensor.</em></p>
<p>Bilinear Models <em>consists of two feature extractors whose outputs
are multiplied using an outer product at each location of the image
and pooled to obtain an image descriptor. <a href=""http://www.ics.uci.edu/%7Eskong2/img/CVgourp_20160120.pdf"" rel=""nofollow noreferrer"">1</a></em></p>
<p><a href=""https://i.stack.imgur.com/pfvFk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pfvFk.png"" alt=""enter image description here"" /></a></p>
<p>Its advantage is that <em>it can model pairwise feature interactions in a translationally invariant manner, which is particularly useful for fine-grained categorization. It also allows end-to-end training using image labels only, and achieves state-of-the-art performance on fine-grained classification.</em></p>
","7","2","28175","12588"
"31879","<p>You should normalize your input data for different purposes. As you can read from <a href=""https://datascience.stackexchange.com/a/31295/28175"">here</a>, <em>Normalizing data is done for accelerating optimization. If you have features with different scales, it will take too much time for your optimizer function to find optimal points. Suppose you have age feature which can change between 0 to 150 (!) and salary which can be changed from 0 to whatever, like 500,000,000 $. your optimization algorithm used in your ML model will take too much time, if possible, to find appropriate weights for each feature. Moreover, if you don't scale your data, your ML algorithm may take too much care to features with large scales</em>. </p>

<p>You have not explicitly specified your task, if its classification, you may need to turn the labels into one-hot encoded versions or if possible you may need to use <a href=""https://en.wikipedia.org/wiki/Word2vec"" rel=""nofollow noreferrer"">word embeding</a> for adding logical distance to different classes. If your task is a regression one, you may need to <a href=""https://datascience.stackexchange.com/a/26960/28175"">normalize your outputs</a>. <em>In regression problems it is customary to normalize the output too, because the scale of output and input features may differ.</em></p>
","1","2","28175","12588"
"32048","<blockquote>
  <p>What does it mean that classes are mutually exclusive but soft-labels are accepted?</p>
</blockquote>

<p>As it can be seen from <a href=""https://stackoverflow.com/a/34243720/5120235"">here</a>, <em><code>tf.nn.softmax</code> produces just the result of applying the softmax function to an input tensor. The softmax ""squishes"" the inputs so that sum(input) = 1; it's a way of normalizing. The shape of the output of a softmax is the same as the input - it just normalizes the values. The outputs of softmax can be interpreted as probabilities. In contrast, <code>tf.nn.softmax_cross_entropy_with_logits</code> computes the cross-entropy of the result after applying the softmax function (but it does it all together in a more mathematically careful way).</em> It's mathematically careful due to the fact that $y_i$ in $log(y_i)$ can be zero. As you can read from <a href=""https://datascience.stackexchange.com/a/9311/28175"">here</a>, <em>A randomly-initialized softmax layer is extremely unlikely to output an exact 0 in any class. But it is possible, so worth allowing for it. First, don't evaluate log(yi) for any y′i=0, because the negative classes always contribute 0 to the error. Second, in the practical code you can limit the value to something like $log( max( y_predict, 1e-15 ) )$ for numerical stability - in many cases, it is not required, but this is sensible defensive programming.</em> I encourage you to take a look at the answers to <a href=""https://stackoverflow.com/q/37312421/5120235"">this</a> question.</p>

<blockquote>
  <p>NOTE: While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.</p>
</blockquote>

<p>The first sentence means that your classifier may not be able to classify the labels exactly as they are, one-hot-encoded. What it does is to find the chance that how likely it is that the input belongs to each class. And this won't make a problem if it does not have mutually exclusive output vector as the algorithm implies. It just needs a vector that the sum of its entries is equal to one. <em>If they are not, the computation of the gradient will be incorrect.</em> I guess this line is added to announce that the outputs of this differentiable component will not be one-hot-encoded and it's because of the nature of these nets. The first layers of convolutional networks are like bases vectors and each instance of classes share these bases. All inputs are made up of these bases. </p>

<blockquote>
  <p>I'm wondering if, in a multi-class exclusive case where the only constraint on the labels is that they have to be a valid probability distribution, labels = [0.5 0.5] should be a valid instance label. This label means that the annotator nor the net can tell if this ground-truth instance belongs to class_0 or class_1...</p>
</blockquote>

<p>Basically, for multi-label classification, your input may have different labels. Consequently, your classes won't be mutually exclusive anymore. Moreover, in those cases, we don't use softmax as the last layer. You have to have sigmoid for each output and your cross-entropy cost function will be slightly different <a href=""https://datascience.stackexchange.com/q/9302/28175"">1</a>. Consequently, the output of each entry should be a valid probability, that is why sigmoid is used, and the label vectors for such tasks is a not one-hot-encoded anymore. different classes have different entries and based on the existence of instances of each class, the corresponding entry should be one. </p>
","-2","2","28175","12588"
"32050","<p>You have a high Bayes error rate and it means that you almost can not learn anything. You have to add extra features and investigate whether your data has a small Bayes error or not. Currently, it is worse than a disaster. This large Bayes error illustrates that you have patterns, input vectors, that have completely same components as each feature but different labels. Take a look at <a href=""https://datascience.stackexchange.com/a/25413/28175"">here</a>.</p>
","1","2","28175","12588"
"32190","<p>It depends on the type of input pattern but to make a decision, I suggest not to. There are different reasons for that. First of all, you are damaging your input signal. I don't know whether you are familiar with the information theory or not but the signal to noise ratio will be too small and if you do so, you will be left with a signal which is far from your real signal. </p>

<p>Moreover, it is also not good to add dropout in the convolutional layers because they are feature extractors and they are significant features for classification problems. If you miss them, means that you are losing information more than usual. Consider the point that your input to the network is already resized to a smaller shape than its original shape, for instance, the input shape of typical <code>CNNs</code> is <code>224 * 224</code> while the original shape may be ten times bigger or even more  for each direction.</p>

<p>You may have seen that in the Lenet-5 the authors have used a data-augmentation technique that changes the colors of the inputs with different distributions. The point there is that they have not changed the locality of the input signal. Moreover, the signal to noise ratio also is not a too small number due to the fact that they have not set the input features to zeros. They just have changed them slightly.</p>

<p>Finally, the last layer should not employ drop-out. Because the output size has to have specified characteristics, sum to one. Dense layers due to having a large number of weights, and consequently a large number of activations, are good points for exploiting drop-out.</p>
","3","2","28175","12588"
"32191","<p>I've not seen any paper about that but based on what I've faced till now, normalizing data intuitively is just for assigning same importance to different features which their raw values do not have a same range. Take a look at <a href=""https://datascience.stackexchange.com/a/27617/28175"">here</a>. Also, you can take a look at <a href=""https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling"" rel=""nofollow noreferrer"">here</a> that professor says that you just need to employ a technique and it's not really important which technique. Also, take a look at <a href=""https://www.coursera.org/learn/machine-learning/supplement/CTA0D/gradient-descent-in-practice-i-feature-scaling"" rel=""nofollow noreferrer"">here</a>.</p>
","0","2","28175","12588"
"32207","<blockquote>
  <p>Maintaining aspect ratio is important or not.</p>
</blockquote>

<p>Yes, it's really important in most cases. As you can read from <a href=""https://digital-photography-school.com/aspect-ratio-what-it-is-and-why-it-matters/"" rel=""nofollow noreferrer"">here</a>, <em>Why does aspect ratio matter? It’s all to do with the relationship of the main subject to the sides of the frame, and the amount of empty space you end up with around the subject. An awareness of the characteristics of the aspect ratio of your particular camera can help you compose better images. It also helps you recognise when cropping to a different aspect ratio will improve the composition of your image.</em> </p>

<p>In deep learning tasks, it depends how you want to feed data to your network. It's better to train your network with real data. Consequently if you are going to face data with standard aspect ratio, you have too keep it during training.</p>

<blockquote>
  <p>If yes (224,224) is a good choice or should I set it to higher resolution.</p>
</blockquote>

<p>Depending on your task <code>224</code> may be good or not. In typical classification networks, it is an acceptable size, like the inputs of <em>AlexNet</em> or <em>VGG</em> and such customary nets. For localization tasks, not really. For instance, the input height and width of YOLO is more than that <a href=""https://datascience.stackexchange.com/a/27385/28175"">1</a>.</p>
","1","2","28175","12588"
"32283","<p>Actually, it is not clear what you mean by deactivating but if it means the output of neuron would be zero, it is not correct due to having bias term, also known as intercept. Furthermore, we usually use normalisation for features which are of different scales. Your boolean values do not have a large range. You don't need to scale them. If I want to be more precise, you may need depending on the other features' range, because they may change slightly among different input patterns and vary less than let say 1e-5 for different samples, but most of the time, booleans are not needed to be scaled. </p>
","1","2","28175","12588"
"32342","<p>Basically what you want to do is showing the full numpy array in jupyter notebook. Use the following code:</p>

<pre><code>import numpy
numpy.set_printoptions(threshold=numpy.nan)
</code></pre>

<p>Then run your code and see the entire activation map.</p>
","1","2","28175","12588"
"32472","<p>The convolution operation used in <code>CNNs</code> are batch operations. The customary inputs for these networks, at least for <code>Keras</code> and <code>Tensorflow</code>, are channels last and channels first. The first dimension specifies the number of data in your batch, the second and third can be height and width if you choose <em>channels-last</em> and the last one will be the number of channels of your input, for instance the number of channels for grey-scaled images is one whilst for coloured images is three. If you use channels first, the first dimension would be the number of data in your batch, the second would be the number of channels and finally, the third and fourth dimensions would specify the number of height and width respectively.</p>

<p>For your case, it seems you are using <code>MNIST</code> data-set. What you need to do is just reshaping your input images to one of the standard mentioned manners. You can resize your input to <code>(1000,28,28, 1)</code>.</p>
","1","2","28175","12588"
"32823","<blockquote>
  <p>Replace the missing part of the features with zeros. I am not sure how this will affect the algorithm though.</p>
</blockquote>

<p>I don't suggest this. This input may have special meaning as the input in the real data. A simple an most common behavior is to use the expected value for that entry if you have the distribution of that specific feature which is missed. If you don't know the distribution which is a common issue, you can find the mean of that specific entry among your training data and replace it although there are numerous ways to deal with missing data, that's a simple approach due to regression task. </p>

<blockquote>
  <p>Use a decision tree or random forest? Do these have a more natural way of dealing with a variable number of features?</p>
</blockquote>

<p>It is also a common practice but due to having regression task, it may have so many branches. In data mining, it's a common approach.</p>
","1","2","28175","12588"
"32826","<p>I guess based on the answer <a href=""https://stackoverflow.com/a/23575424/5120235"">here</a> and the point referred <a href=""https://github.com/keras-team/keras/issues/5475"" rel=""nofollow noreferrer"">here</a> your problem is not from <code>Keras</code>. Where you can read <em>... we see that PIL is reading in blocks of the file and that it expects that the blocks are going to be of a certain size. It turns out that you can ask PIL to be tolerant of files that are truncated (missing some file from the block) by changing a setting.</em></p>

<p><em>Somewhere before your code block, simply add the following:</em></p>

<pre><code>from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
</code></pre>

<p><em>...and you should be good.</em></p>

<p><em>EDIT: It looks like this helps for the version of PIL bundled with Pillow (""pip install pillow""), but may not work for default installations of PIL.</em></p>
","0","2","28175","12588"
"32828","<blockquote>
  <p>... each layer of a neural network is responsible for recognizing one feature of the input data. For example, if we build a neural network that classifies cars, buses, vans and bicycles, a layer will be responsible to recognize the tires, another one will responsible for recognizing the size of the vehicle.</p>
</blockquote>

<p>There are numerous kinds of neural networks and there are different differentiable components that are used inside them to achieve end-to-end learning. <a href=""https://datascience.stackexchange.com/a/26291/28175"">Convolutional layers</a> are responsible for finding the features that are essential for reducing the error. These features are shared among different nodes and are not necessarily meaningful to us. They have to be used simultaneously due to being shared among different weights. What you are trying to say is that in convolutional networks, the first layers attempt to find low-level features like vertical and horizontal edges whilst deeper convolutional layers try to find high-level features which are more abstract. But the point here is that they attempt to <em>recognize</em> and the way they do that is by using weights and activations which are shared in different activation maps. Consequently, one neuron is not necessarily responsible for faces or for cars. There may be different neurons and filters which are responsible for them. Take a look at the link which has illustrated that. </p>

<p>The other point is about <a href=""https://datascience.stackexchange.com/a/26642/28175"">fully connected layers</a> which are responsible for classification. What they do is finding decision boundaries to classify inputs or estimating a function for regression tasks. They are not feature extractors in a way convolutional layers do. As it is illustrated in the cited link, what they try to do is to separate the input space to make it possible for generalization in the current feature space. </p>

<blockquote>
  <p>The question is, why is this true? i.e. each layer appears to perform similar to the others and there is no special design for each one. Is there any way to assign each layer a specific feature or it is done implicitly?</p>
</blockquote>

<p>It was not true as mentioned. Although each layer seems similar to other layers, because of the cost function, their corresponding weights are set to decrease the cost function. Consequently, for fully connected layers all the featrues are feed to all the neurons in each layer and the training process decides how to use them. Moreover, the convolutional layers try to find similar patterns, kernels, in the input pattern.</p>
","2","2","28175","12588"
"32830","<p>Maybe you are making a mistake, put your code here. But without seeing your code, these are possible points:</p>

<ul>
<li>Vanishing problem, I don't think you this problem due to having a very shallow network. You can change your activation function to <code>relu</code> for avoiding that.</li>
<li>Covariat shift, What it means is that similar to input features which have to be normalized, the inputs of the deeper layers have to be normalized to. The normalization process is in a way that the inputs to each layer should have a special distribution that does not change. You can use batch normalisation for avoiding that problem. </li>
<li><p>Bug in coding, you may have fed the activations of each layer to the next layer in a wrong way or you may have updated the weights not simultaneously If you have not used vectorization. There are also numerous different reasons which may lead to bugs in your code. </p></li>
<li><p>The number of neurons may not be enough for each layer, try to increase them. For understanding the meaning of increasing the layers and the number of neurons in each layer, you can take a look at <a href=""https://datascience.stackexchange.com/a/26642/28175"">here</a>.</p></li>
</ul>
","2","2","28175","12588"
"32839","<p>I want to know whether there is a way to limit the output of a regression deep model. Suppose that I want my model outputs values which are in a specified range and penalizes the outputs which are not inside the range while training. I've not seen any paper but I have two solutions for that right now but I don't know if there is any standard way or not. </p>

<ol>
<li>My first suggested solution is to use an activation function like the following. It is linear in the specified range and has high slope out of the specified range. Although the gradient will oscillate if I use simple gradient descent, I guess it will perform well if I use second-order optimization algorithms like <code>Adam</code>. Any suggestions for this?</li>
</ol>

<p><a href=""https://i.stack.imgur.com/vmKql.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vmKql.png"" alt=""enter image description here""></a></p>

<ol start=""2"">
<li>My second solution is inspired by <code>L1/L2</code> regularization. I find the output of the model. If it exceeds the absolute value of the range, suppose it's symmetric, I'll add a constant big value which is far from the real output. Here We can replace it with a big constant or maybe multiply it by a constant. The second one has this property that it will be differentiable with a slope. </li>
</ol>

<p>Does anyone have suggestions which are applicable or even these are ok or any other thing?</p>

<p><hr />
In regression tasks, it is customary to use linear activation function as the non-linearity of the final layer in order to estimate a function which outputs real value(s). The reason sigmoid function is used is that its output is limited to the range <code>0 to 1</code> which is a good range for specifying probability. Moreover, sigmoid is used more for classification tasks where the classes are mutually exclusive in the input o.w. softmax activation function is used. <code>Tanh</code> is used due to having zero mean which accelerates the training process. These are usually not applied in classification tasks these days because their differentiation saturates in their limits. <code>Relu</code> is a customary choice because its slope does not saturate. 
Sigmoid-shape activation functions cannot be used as the last layer of the regression tasks. I want to use a kind of activation function like linear as the <strong>last layer</strong>'s activation function in order to have outputs in a special range. While training I want to train the model in a way that it never tries to output a value which is outside a specified range. For doing so, I've put two suggestions that I have tried. I don't know whether there is a paper about that or not. Moreover, about the provided answer, sigmoid does not penalize the outputs, I mean does not increase them in a way that the error value increases, which are outside the range of the domain of the function. What it does is saturating the output to zero or one. What I want to do is to make a huge difference between the output of the network and the real output of the input in order to cause a high error. Consequently, the network will try to find weights which do not yield results out of the range.</p>
","2","1","28175","12588"
"32857","<p>A simple solution is to do the following:</p>

<pre><code>temp = tf.get_collection(""my_important_ops"")
print(temp.get_shape().as_list())
</code></pre>

<p>Then you can evaluate the outputs. </p>
","0","2","28175","12588"
"33274","<p>You have to add higher order polynomial terms to your dataset yourself. Accordingly, you should add new columns to your dataset which consists of higher order polynomials with the desired combination of the existing features. It means that if you have already a <code>CSV</code> file which contains two columns for different samples, you have to add your desired extra feature columns which are the polynomial combination of the existing features. Then use simple regression.</p>
","4","2","28175","12588"
"33292","<p>At least as far as I know reducing the size of images is always lossy and you can not retrieve the original image due to the fact that you are throwing away information. Although there are solutions for dimensionality reduction like <code>PCA</code>, they are linear approaches. It depends on your task that what you want to do but there is a solution if you have a classification problem. Try to use a spatial transformer module before the input of the predefined network and send the output of that module to your predefined network. For your grid sampler, you can set the size of the grid to $48 * 48$. For more details take a look at <a href=""https://kevinzakka.github.io/2017/01/18/stn-part2/"" rel=""nofollow noreferrer"">here</a>.</p>

<p><em>Concretely, the grid generator first creates a normalized meshgrid of the same size as the input image U of shape $(H, W)$, that is, a set of indices $(x^t,y^t)$ that cover the whole input feature map (the subscript $t$ here stands for target coordinates in the output feature map). Then, since we’re applying an affine transformation to this grid and would like to use translations, we proceed by adding a row of ones to our coordinate vector to obtain its homogeneous equivalent. This is the little trick we also talked about last week. Finally, we reshape our 6 parameter $θ$ to a $2x3$ matrix and perform the following multiplication which results in our desired parametrised sampling grid... since bilinear interpolation is differentiable, it is perfectly suitable for the task at hand. Armed with the input feature map and our parametrised sampling grid, we proceed with bilinear sampling and obtain our output feature map $V$ of shape $(H’, W’, C’)$. Note that this implies that we can perform downsampling and upsampling by specifying the shape of our sampling grid. (take that pooling!) We definitely aren’t restricted to bilinear sampling, and there are other sampling kernels we can use, but the important takeaway is that it must be differentiable to allow the loss gradients to flow all the way back to our localisation network.</em></p>
","1","2","28175","12588"
"33851","<p>Yes, you can use one-dimensional convolutions but you have to consider the fact that the size of your inputs should be the same. In <code>Tensorflow</code> you can exploit <code>tf.nn.conv1d</code> for your purpose. <code>RNN</code> networks like <code>LSTMs</code> and <code>GRUs</code> are also other architectures that can be used. If your input has a patter that may happen different times in a single signal try to use <code>conv1d</code>. If you want to use <code>RNNs</code> you have to find which task you have such as a one-to-one or one-to-many-relation. For convolutional architecture, take a look at <a href=""https://datascience.stackexchange.com/a/26321/28175"">here</a> and <a href=""https://datascience.stackexchange.com/a/27275/28175"">here</a>, and take a look at [here] for <code>RNNs</code>.</p>
","0","2","28175","12588"
"33918","<p>First, consider normalising your data, inputs and outputs. If it does not do the desired behaviour, you can have different options.</p>

<p>A simple solution is to add some higher order polynomials to the current features for each input pattern and use the simple linear regression. The fact about that is this that you can not be sure what features you would need due to the fact that you can not see your data. Consequently, if you don't find the acceptable answers you want you can use the current features the higher order polynomials that you are going to add to train an MLP.</p>
","1","2","28175","12588"
"34102","<p>About equalizing the number of samples for each class, it is not desirable at all. The distribution of each class should be real. There are numerous reasons for that, but the main point is that the distribution of your train and validation data should be like the distribution of your test data. Take a look at <a href=""https://datascience.stackexchange.com/a/25747/28175"">here</a>.</p>
","0","2","28175","12588"
"34139","<p>As you can see <a href=""https://www.coursera.org/learn/nlp-sequence-models/lecture/ehs0S/deep-rnns"" rel=""nofollow noreferrer"">here</a>, derivatives will be propagated by the chain rule although they are stacked. Actually, there will be two main paths. The first one will be backpropagation through time and the next one will be the backpropagation from the output of each unrolled cell which can directly be connected to the output or can be connected to the stacked unrolled cells. Also, take a look at <a href=""https://www.coursera.org/learn/nlp-sequence-models/lecture/bc7ED/backpropagation-through-time"" rel=""nofollow noreferrer"">here</a>.</p>
","2","2","28175","12588"
"34152","<blockquote>
  <p>How is data predicted from activation functions? (considering that it returns constants on weighted input sum). </p>
</blockquote>

<p>You should consider the fact that the label of your input data is going to be predicted by the network. Moreover, the outputs of the network usually represent the probability of belonging to each class. The label is not predicted by the activation function, it is predicted by the network. Take a network as a mapper which can map the inputs to outputs. The activations are used to add non-linearity in order to approximate complicated non-linear mappings.</p>

<blockquote>
  <p>What's the point of multiple hidden layers?</p>
</blockquote>

<p>As you can read <a href=""https://datascience.stackexchange.com/a/26642/28175"">here</a>, the purpose of adding those is that you can learn different complicated regions. Another interpretation can be this that by adding more layers you can learn more complicated mappings which have non-linear behaviour.</p>

<p>About the global minimum, because you can not see the cost function which is the error based on the weights, due to the fact that it is not possible to visualise it because it has so many dimensions, you should use gradient-descent based algorithms.</p>
","3","2","28175","12588"
"34156","<blockquote>
  <p>Is there a better way to optimize linear regression than gradient descent?</p>
</blockquote>

<p>If by <em>better</em> you mean finding the better separator, no you can't due to the fact that the cost function for linear regression is convex which means there is just one optimal point. </p>

<p>If you want to optimize using different algorithms, there are different kinds of solutions. Gradient-based algorithms like <code>Adam</code> and <code>RMSProp</code> are of those. You also can use normal equation.</p>
","4","2","28175","12588"
"34162","<p>What you are attempting to do is so much like the Bayes decision theory; you can find the math behind it <a href=""https://en.wikipedia.org/wiki/Bayes%27_theorem"" rel=""nofollow noreferrer"">here</a>. <em>In probability theory and statistics, Bayes’ theorem (alternatively Bayes’ law or Bayes' rule, also written as Bayes’s theorem) describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using Bayes’ theorem, a person’s age can be used to more accurately assess the probability that they have cancer, compared to the assessment of the probability of cancer made without knowledge of the person's age. One of the many applications of Bayes' theorem is Bayesian inference, a particular approach to statistical inference. When applied, the probabilities involved in Bayes' theorem may have different probability interpretations. With the Bayesian probability interpretation the theorem expresses how a subjective degree of belief should rationally change to account for availability of related evidence. Bayesian inference is fundamental to Bayesian statistics.</em> </p>

<p>Take a look at <a href=""https://cedar.buffalo.edu/~srihari/CSE555/Chap2.Part1.pdf"" rel=""nofollow noreferrer"">here</a> and <a href=""https://www.byclb.com/TR/Tutorials/neural_networks/ch4_1.htm"" rel=""nofollow noreferrer"">here</a> too.</p>
","0","2","28175","12588"
"34191","<p>Based on your accuracies the $12 \%$ difference is introducing high variance problem which means you are overfitting. Due to the fact that the number of parameters is too many for <code>VGG16</code> and you have a moderate-size dataset which is smaller than <code>ImageNet</code> overfitting is obvious. Try to decrease the number of parameters in the bottlenecks of your model, the connections among fully connected networks and convolutional layers and fully connected layers. Moreover, try to use <code>AdamOptimizer</code> which better. Also try to train for more epochs.</p>
","0","2","28175","12588"
"34261","<p>Change $w1=[9,9,1,64]$ to $w1=[9,9,3,64]$. The first two numbers specify the size of the filter, the third specifies the number of input channels and the last one specifies the number of output filters.</p>

<blockquote>
  <p>Is it possible to use <code>conv2d</code> function for RGB images in <code>Tensorflow</code>?</p>
</blockquote>

<p>Yes, <code>conv2d</code> is used for one channel, grey level, images and three channel images, coloured images.</p>
","1","2","28175","12588"
"34343","<p>Bias simply means how much the output does not depend on the inputs. It is exactly equivalent to intercept term which means by dropping all the inputs what the outputs will be. 
We usually set bias terms to zeros. You have cost function and it is the error rate based on the weights and biases. Consequently, you try to change these parameters to reduce the error.</p>
","1","2","28175","12588"
"35687","<p>In each sampling, your data is going to be different from the previous sampling. For each sampling, you are going to find the best line which describes your sample with the least error value. Consequently, for each sample you are going to find a model which may be different due to the fact that it reduces the cost for each sample.</p>
","1","2","28175","12588"
"35700","<p>I guess you are making mistake about the filters. After applying filters to the input of each layer, the output will be used as the input of the next layer. The first layer's filters try to find the edges in the image and their output show whether those edges exist in a specified position or not. Next layer filters try to find patterns in the outputs of the previous layer which shows the existence of edges. Due to the point that each filter is a window and specifies a receptive field on the input, it finds patterns in the input which are more abstract and more complicated than the previous layers' activations.</p>
","1","2","28175","12588"
"35820","<p>Based on your comments and your overall architecture, take a look at your classification layers, dense layers. You just have one neuron which is just a linear separator of the extracted features. Moreover, you are dropping the extracted features and feed them to that linear separator with rate $0.5$ which means you are destroying the signal. Try not to use dropout first in order to find a model that learns your data well or even memorise it to find a model that is appropriate for your task and also add at least one more dense layer after the flatten layer and before the last layer with maybe $10$ neurons to classify non-linear data points. It seems that your task is classification, so keep using binary-cross-entropy.</p>
","0","2","28175","12588"
"35825","<p>In <code>GANs</code> you have to train some parameters, freeze them and train some other which this operation may occur multiple times. You can do the following sequence of operations.</p>

<p>Specify all generator related variables inside their corresponding variable scope and after that access them using <a href=""https://www.tensorflow.org/api_docs/python/tf/get_collection"" rel=""nofollow noreferrer""><code>tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='my_scope')</code></a>. After that, during training, you can pass these variables as the trainable parameters of your optimiser by setting the <code>var_list</code> of the <code>minimize</code> method. You can also take a look at <a href=""https://stackoverflow.com/q/35298326/5120235"">here</a>.</p>

<p>If you want to get all the trainable variables, you can get all of them inside of a list using <a href=""https://stackoverflow.com/a/43801016/5120235""><code>tf.trainable_variables</code></a> method. </p>

<p>Maybe it worth looking <a href=""https://stackoverflow.com/q/40736859/5120235"">here</a> for other aspects for freezing variables.</p>

<p>You can also take a look at <a href=""https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/12_Adversarial_Noise_MNIST.ipynb"" rel=""nofollow noreferrer""><code>Hvass-Labs</code></a>'s implementation of adversarial networks. Take a look at <a href=""https://github.com/nlintz/TensorFlow-Tutorials/blob/master/11_gan.py"" rel=""nofollow noreferrer"">here</a> too.</p>
","0","2","28175","12588"
"35833","<p>Based on the official <a href=""https://www.tensorflow.org/versions/r1.2/get_started/mnist/beginners"" rel=""nofollow noreferrer"">documentation</a> after loading the data,</p>

<pre><code>from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)
</code></pre>

<p>You can use the following line,</p>

<pre><code>mnist.train.images.shape
</code></pre>

<p>to see the shape of images, as you can see it is a rank-2 matrix which means images are unrolled which one dimension specifies the number of examples and another specifies the number of pixels __features.</p>
","1","2","28175","12588"
"35839","<p>Your question text is not very clear but I try to give you what you need. Max-pooling layers give the <code>CNNs</code> spatial invariance ability <em>through the layers</em> due to the fact that they check the existence of something or not. If you stack them through multiple layers, your network will have a rough ability to be spatial invariance but this is not that much. Moreover, professor Hinton has stated somewhere that using pooling layers is a mistake and it's a disaster that they work; I didn't quote the exact words. If you want a network to be spatial invariance you should use <a href=""https://datascience.stackexchange.com/a/29510/28175"">spatial transformers</a> which are differentiable modules and can be used in <code>CNNs</code> without any supervision. Take a look at <a href=""https://towardsdatascience.com/convnets-series-spatial-transformer-networks-cff47565ae81"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Finally, to answer your question, pooling layers somehow remove spatial information that is why <a href=""https://www.analyticsvidhya.com/blog/2018/04/essentials-of-deep-learning-getting-to-know-capsulenets/"" rel=""nofollow noreferrer"">capsnets</a> are introduced. </p>

<p>About the fully connected layers, There are two main points. First the inputs, second, the outputs. In convolutional layers, the inputs of each layer is limited to the region which its is employed and for that input there is a single output. Consequently for a specific region, there is a single output which is responsible for that point. In MLPs, all the inputs go to a single neuron and for all the inputs there is just a single value. I guess this is why they don't keep the spatial information and are just used for classification tasks. Actually, they just try to classify the extracted features by the <code>CNNs</code>.</p>
","1","2","28175","12588"
"35855","<p>What you are trying to do is called <em>object localisation</em>. If you have multiple of that object in the scene it is called <em>object detection</em>. There are numerous things that you can try that I refer to some of them. </p>

<ol>
<li><a href=""https://arxiv.org/abs/1506.02640"" rel=""nofollow noreferrer""><em>YOLO</em></a> paper which may be the perfect thing for your task which has the state of the art performance these days.</li>
<li>There is another algorithm which is old and is not efficient but is a simple way of detecting objects. It is not that much accurate. It is called <a href=""https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/"" rel=""nofollow noreferrer""><em>Sliding Window</em></a>. </li>
<li>The previous algorithm is time-consuming due to the fact that it calculates repetitive calculations. In <a href=""https://arxiv.org/abs/1312.6229"" rel=""nofollow noreferrer"">OverFeat: Integrated Recognition, Localization and ...</a> paper they have tried to overcome that mistake.</li>
</ol>

<p>There are other approaches too because object detection is an important task in computer vision and every day someone makes another approach. You can see the citation of the mentioned papers which means there are abundant studies about the problem.</p>
","1","2","28175","12588"
"36159","<p>It is not important that you don't have data of different classes which are unbalanced. What is important is that your data should have a real distribution. The distribution of your training data should be the same as your test environment. As you can read <a href=""https://datascience.stackexchange.com/a/25747/28175"">here</a>, your data should be suited well for the task it is going to be used. Consequently, if the distribution of your samples is real, there will be no problem. Consider the point that for unbalanced data-sets we should use appropriate evaluation metrics like <code>F1</code> score. </p>

<p>Finally, if this is the real distribution of your data, I highly recommend you not to change the real distribution even if you want to augment your data. You should not change the relative ratio of different classes.</p>
","0","2","28175","12588"
"36192","<p>You as the designer of the network specify each class in the training example. You set e.g. a car class to label $0$ and another class to $1$. During training your classifier tries to map the inputs to the corresponding class which are those numbers where you have specified in the dataset. This phase is called encoding. After outputting the label, you should decode the label which is a trivial task!</p>
","2","2","28175","12588"
"36193","<p>This highly depends on your test data. Suppose you have trained your data which all contains some kind of food. If you give it a hand at test time, it will try to find the most similarity it has with the current labels and outputs the one which is more similar than the others. The point is that your test environment is really important. If you are going to test your app near the sea, your training data set should contain data on that condition. If you are going to use it during the night, your dataset should be taken during the night. In your case, you have a simple task, for the current dataset add an entry with a zero which means you are trying to add a new class. After that, try to add new pictures that are not images of food and add one in the new entry for these images. But consider this important point. The number of images in each class should have the same distribution as the test data.</p>
","2","2","28175","12588"
"36195","<p>You can use <code>h5py</code> files but due to the fact that they store the dimensions, they take even more disk to be stored and while fetching you have to supply enough memory which is not possible in your case, so you have to read them chunk by chunk. There is a simpler approach. One of <code>pandas</code>'s inputs is <code>chunk</code>. try to <a href=""https://stackoverflow.com/a/25962187/5120235"">use it</a>.</p>
","2","2","28175","12588"
"36216","<p>It highly depends on your task, your data and your network. Basically, <code>PCA</code> is a linear transformation of the current features. Suppose your data are images or a kind of data that locality is important. If you use <code>PCA</code> you are throwing away those locality information. Consequently, it is clear that people usually do not use them in convolutional networks. For sequential tasks, again it highly depends on your agent whether is <em>online</em> or not. If it is online, you don't have the entire signal from the beginning. Even if you have that for offline tasks, by doing such diminishing transformations you are again throwing away sequential information, I have to say I've not seen the use of them. I guess their main use is in tasks where your problem can be solved using simple <code>MLPs</code> which you don't keep sequential or local information. In those tasks due to the fact that you can employ <code>PCA</code> which leads to the reduction of highly correlated features, the number of parameters of your training model can be reduced significantly.</p>
","1","2","28175","12588"
"36254","<p>Besides to what our friend has referred to, I want to add something which is somehow relevant. It is customary to convolve the inputs in the convolution layer, then they would be passed to the non-linearity and after that max-pooling is usually employed. I have experienced an alternative solution which can reduce the number of calculations for the activation function. Try to use filters, pass the outputs to the max-pooling layer, finally add the non-linearity, activation function, after exploiting max-pooling layer. It reduces the number of calculations without any changes to the outputs of the previous sequence of layers.</p>
","1","2","28175","12588"
"36304","<p>I guess the reason is clear. We usually split things into specified parts which are not contradictory. A special thing can be small and medium, as one group, and large, as the other group. But it cannot be small and large at the same time. The point is that you have a sequence in your data. If there was no such thing you could have different combinations of attribute values. Suppose you have a set of attribute values for a fruit. It can be apple, pineapple and watermelon. Due to the fact that there is no ordinal, you can have all possible combination for binary splits; in the previous case, you can not because your binary split somehow violates the logical sequence. </p>
","1","2","28175","12588"
"36330","<p>You want to solve <span class=""math-container"">$X \times \theta = Y$</span>. Actually, you have to find the parameters <span class=""math-container"">$\theta$</span>. To find it, you should multiply both sides by <span class=""math-container"">$X^{-1}$</span> but it may have not an inverse. Consequently, you multiply each side by <span class=""math-container"">$X^t$</span> due to the fact that <span class=""math-container"">$X^tX$</span> has the inverse. After that, you multiply each side by <span class=""math-container"">$(X^tX)^{-1}$</span> which leads to a closed form equation for finding <span class=""math-container"">$\theta$</span>. It should be mentioned that <span class=""math-container"">$X^tX$</span> does not always have an inverse. If you want to be sure it has it, you have to exploit data samples which are more than the number of features and they should not be linearly dependent otherwise the constructed matrix will not inverse and you will not be able to use it.</p>
","0","2","28175","12588"
"36356","<p>Suppose bias as a threshold. Using threshold, your activation function moves across the $x$ axis which may get complicated. Consequently, people usually use the bias term and always centre the activation function which is the step function at zero. There is nothing wrong in both cases. </p>
","3","2","28175","12588"
"36361","<p>In each layer of a <code>CNN</code> you have a number of filters. Suppose for the first convolutional layer you have $C_{out} = 10$ filters. The customary dimension for each of these filters is $(C_{input}, H, W)$ where $C_{input}$ represents the number of channels of the inputs of the current convolutional layer. $H$ and $W$ represents the height and width of the window of the filter. Consider this point that you are convolving each filter with the inputs. An input is a volume with $(C_{input}, H^{'}, W^{'})$ dimension. What you do as convolution is convolving these volumes, each filter and the input. The height and the width of the input $H^{'}$ and $W^{'}$ are the same or bigger than the height and width of each filter, but the depth or the number of the channels of each filter has to be the same as the input channel. After convolving each filter you will have a two-dimensional activation map. By stacking the result of each filter you will have $C_{output}$ number of output feature maps which is equal to the number of filters.</p>
","1","2","28175","12588"
"36464","<p>There are two terms which relate to the number of examples while learning. <em>Epoch</em> and <em>iteration</em>. During each epoch, you feed all the examples in your training set and update the network. You can feed the data simultaneously, not for large-scale tasks, or batch by batch. Each turn you pass each batch is called iteration. Consequently, each epoch may have multiple iterations. The reason we have to pass the data multiple times is that we don't know the height of each local optimum along each axis, feature. Consequently, we use a learning rate to limit our steps to small ones and take multiple steps to get close to the desired point.</p>

<p>Due to the request of one of our friends I add the explicit definitions:</p>

<ul>
<li>Epoch: It means how many times the entire dataset has been passed through the network. </li>
<li>Iteration: It means for each epoch, how many times you have passed the chunks of the dataset. These chunks of data are called batch and the number of times you pass them through the network is called the number of iterations. </li>
</ul>
","3","2","28175","12588"
"36465","<p>If you use a convex loss function you always have one optimum point, and you will always be able to find it. </p>

<blockquote>
  <p>I have done some calculations and I get correct results but not for all learning rates I tried to specify.</p>
</blockquote>

<p>The reason is that you may have selected a large learning rate which may lead to overshoot the optimum. It also may be so small that you don't reach to the desired point and it all depends on your data. If you use the simple <em>gradient descent</em> algorithm, it is an incident that can be seen a lot. You can use other optimisation techniques like <code>Adam</code> which is much more better and you rarely see these problems.</p>
","1","2","28175","12588"
"36601","","0","5","28175","12588"
"36602","It has been a typing mistake. ","0","4","28175","12588"
"37120","<p>I don't know exactly how your data is but <code>y = data.temp</code> may be a Series containing the string values which should be cast to float values. Try to change it to the following alternative.</p>

<blockquote>
  <p>y = data.temp.astype(float)</p>
</blockquote>
","0","2","28175","12588"
"37239","<p>There are multiple points that I try to explain them. </p>

<p>First, each filter for convolutional networks for images is a 3d volume. Consequently, whenever it is said we have $n$ filters, means we have $n$ volumes of those 3d filters. </p>

<p>Second, you can consider each convolutional layer as an MLP which is applied to small regions of the input. These are applied to different regions of the input to investigate whether a typical pattern is in that region or not. These patterns are going to be learned by means of cost functions. You can easily consider that for each filter which is a volume, you are concretely doing a summation over weighted inputs, exactly as MLPs. </p>

<p>Third, as the result, your trained filters will decide to choose the information of which channels depending on the task using the cost function. They may be in a single plane or they may be among multiple of them.</p>
","1","2","28175","12588"
"37244","<blockquote>
  <p>I am currently training a neural network and I cannot decide which to use to implement my Early Stopping criteria: validation loss or a metrics like accuracy/f1score/auc/whatever calculated on the validation set.</p>
</blockquote>

<p>If you are training a deep network, I highly recommend you not to use early stop. In deep learning, it is not very customary. Instead, you can employ other techniques like drop out for generalizing well. If you insist on that, choosing criterion depends on your task. If you have unbalanced data, you have to employ <code>F1</code> score and evaluate it on your cross-validation data. If you have balanced data, try to use accuracy on your cross-validation data. Other techniques highly depend on your task.</p>

<p>I highly encourage you to find a model which fits your data very well and employ drop out after that. This is the most customary thing people use for deep models.</p>
","-1","2","28175","12588"
"37427","<p>You don't need to train your model each time form the beginning. After training, you can store you model and next time you can load it, train it from that point or use it for your recall phase. You can find good tips <a href=""https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model"">here</a> and <a href=""https://www.tensorflow.org/guide/saved_model"" rel=""nofollow noreferrer"">here</a> which have discussed storing and loading models in <code>Tensorflow</code>.</p>
","0","2","28175","12588"
"37524","<p>It is an easy task. Make some training data, two inputs and one output. this is a regression task. You can train a simple <code>MLP</code> or you can employ <code>RNNs</code> such as <code>LSTM</code> for this task. Take a look at <a href=""https://gist.github.com/siemanko/b18ce332bde37e156034e5d3f60f8a23"" rel=""nofollow noreferrer"">here</a>.</p>
","1","2","28175","12588"
"37535","<p>First, you have to have an object detector for recognizing different objects. After that, you have to post-process the outcomes of your detector to count the numbers. Basically, you cannot recognize all objects due to the fact that the number of labels that a detector finds is limited. I highly recommend you using built in models in the libraries you mentioned due to this reason that training detector models are very time consuming and need appropriate hardwre. If you want to learn something, take a look at papers that already exist and try to implement them. For this case there are numerous studies that I separate them in this way. </p>

<ol>
<li>Pixel-wise classification based techniques</li>
<li>YOLO-based techniques</li>
</ol>

<p>This is a general classification that there may not be any consensus on that but is something that shows you the mainstream directions. </p>

<p>I guess the ImageNet data-set contains the label <em>clock</em>. Try to use them alongside another label after implementing the model you want. Don't train your model using all the labels which takes to much time.</p>
","1","2","28175","12588"
"37711","<p>By doing so, the joint distribution can be found easily by just multiplying the probability of each feature whilst in the real world they may not be independent and you have to find the correct joint distribution. It is naive due to this simplification.</p>
","7","2","28175","12588"
"37749","<p>With respect to the presented answers, I want to add an extra explanation. Basically, what ML approaches do is approximating a mapping from inputs to outputs. This function usually should be <a href=""https://math.stackexchange.com/a/248/487106"">well-behaved</a> <a href=""https://classroom.udacity.com/courses/ud262/lessons/3625438937/concepts/6405791900923"" rel=""nofollow noreferrer"">1</a> otherwise you should have so much data to enable your model to learn it in the current feature space. To be more specific, you should find the distribution of your training data in your current feature space. It helps you investigate how much the distribution of your different labels overlap, for classification tasks. By doing that, you'll be able to figure out the best performance your best ML approach can have. The distribution of your data in your current feature space can show you the <a href=""https://stats.stackexchange.com/a/302910/179078"">Bayes error</a> of your model.</p>

<p>If you find out that the current Bayes error is a large value, then you can be sure that your data cannot be learned in the current feature space and you have to change the current features.</p>
","3","2","28175","12588"
"37961","<p>I have two suggestions. </p>

<ol>
<li>Change your optimiser to <code>AdamOptimizer</code>.</li>
<li>Change the number of neurons in the hidden layers. They are too many for this task. Two layers with 25 neurons for each layer will suffice.
<hr >
Omit <code>output=tf.nn.softmax(output)</code>. Due to calculating that in the <code>softmax_cross_entropy_with_logits</code>. As you can read in the document of the function <code>softmax_cross_entropy_with_logits</code>: </li>
</ol>

<blockquote>
  <p><strong>WARNING:</strong> This op expects unscaled logits, since it performs a <code>softmax</code>
  on <code>logits</code> internally for efficiency.  Do not call this op with the
  output of <code>softmax</code>, as it will produce incorrect results.</p>
</blockquote>
","2","2","28175","12588"
"37963","<p>There are numerous things that you can do. I suggest two things that are very plausible. </p>

<ol>
<li>Try to use <a href=""https://datascience.stackexchange.com/a/26719/28175""><code>PCA</code></a>. Although it is linear, you have the flexibility to reduce the number of features and investigate how much information you are losing. </li>
<li>Try to find the correlation between each feature and the output. If they are uncorrelated, you might think of ignoring the feature. But beware of cross-features. Eg: It maybe the case that your underlying data is like:</li>
</ol>

<pre><code>F1 F2 L
A  X  1
A  Y  0
B  X  0
B  Y  1
</code></pre>

<p>Now, F1 and F2 individually are uncorrelated with the data, but together they determine the label completely.</p>

<p>Although you have too many features, it can be done automatically.</p>
","0","2","28175","12588"
"37964","<p>I guess the best way to understand it is to read its paper called <a href=""https://www.ncbi.nlm.nih.gov/pubmed/12850008"" rel=""nofollow noreferrer"">A generalized feedforward neural network architecture for classification and regression</a>.</p>

<blockquote>
  <p>This article presents a new generalized feedforward neural network (GFNN) architecture for pattern classification and regression. The GFNN architecture uses as the basic computing unit a generalized shunting neuron (GSN) model, which includes as special cases the perceptron and the shunting inhibitory neuron. GSNs are capable of forming complex, nonlinear decision boundaries. This allows the GFNN architecture to easily learn some complex pattern classification problems. In this article the GFNNs are applied to several benchmark classification problems, and their performance is compared to the performances of SIANNs and multilayer perceptrons. Experimental results show that a single GSN can outperform both the SIANN and MLP networks.</p>
</blockquote>

<p>I have to add this point that the paper is so much old. People usually use <code>Relu</code> nonlinearity these days. Also take a look at <a href=""https://ieeexplore.ieee.org/document/1223906/"" rel=""nofollow noreferrer"">here</a>.</p>
","1","2","28175","12588"
"37966","<p>I don't know exactly where you have the problem but according to comments, take a look at the following line.</p>

<pre><code>_, c = sess.run([optimizer, cost], feed_dict={x_img: X, y_label: Y})
</code></pre>

<p><code>feed_dict</code> is used for passing data to your network. As you can see, <code>X</code> is the training data. You can replace it with the test data. You should also change the <code>y_labels</code> to the labels of the test data.</p>
","1","2","28175","12588"
"37991","<p>Well, let's say in this way. Although there are numerous learning approaches, each is useful for a particular situation. It is <em>possible</em> that for a problem you have multiple choices. Each of learning approaches has a special application domain and that is why people usually know where to use decision trees and where to choose neural networks, e.g. in situations that all of your inputs are real-valued numbers, attempting to use decision trees is not a wise choice. I try to explain the main things that an ML practitioner usually considers. </p>

<p><strong>Number of Available Features</strong></p>

<p>The number of features is important due to not being able to visualise them easily in cases there are so many features. This may lead to not being able to recognize whether the data is linearly separable or not. So many features do not imply that the dataset is not linearly separable. Consequently, say you want to use neural nets for modelling the problem. You shouldn't begin with a complicated network with so many layers and neurons. You have to begin with a single neuron, equivalent to logistic regression for classification tasks, to figure out whether your data is linearly separable or not. If you observe that you don't have good performance, you can add extra neurons and layers. How? Take a look at <a href=""https://datascience.stackexchange.com/a/26642/28175"">How to set the number of neurons and layers in neural networks</a>. </p>

<p><strong>Feature Space</strong></p>

<p>About choosing ML approaches, it is simple to consider the limitations of each algorithm with respect to the feature space. E.g. decision trees are not very good for problems with many features which some of them are numerical features. They may get really big. <code>SVMs</code> are not very good for non-linear problems with so many features due to the fact that you have to specify the kernel size. For different regions in the feature space, a single kernel size may not be valuable. To generalize, problems that have very large input space are usually handled using neural networks. If the problem has so many features but they are e.g. binary features or categorical features with a small number of choices, then the problem has smaller feature space, input space, and other ML approaches can be considered.</p>

<p><strong>Size of Dataset</strong></p>

<p>Depending on your problem, feature types and feature ranges, the input space may be very small or very big. Consequently, the number of possible data may differ depending upon the input space. For large input space, as mentioned, neural nets are very powerful for mappings. </p>

<p><strong>Distributions and Bayes Error</strong></p>

<p>For different tasks, e.g. classification, you have to do statistical analysis for knowing your available dataset better. You have to investigate if there are input patterns that there are same but their labels differ. If so, why? Is that for expert error or not. The current feature space is not valid for understanding the problem. After addressing these questions you can employ Bayes error to investigate the best approach will have what accuracy on your data.</p>
","4","2","28175","12588"
"37998","<p>Basically, you <em>can</em> have multiple convolutional modules in one layer. It is called <a href=""http://colah.github.io/posts/2014-12-Groups-Convolution/"" rel=""nofollow noreferrer"">grouping</a> and was introduced in <code>AlexNet</code>. The inputs are the same in this case and the outputs of all convolutional modules should be concatenated after passing the input.</p>

<p>I quote from the link the benefit of grouping in conv nets.</p>

<blockquote>
  <p>Group convolutions provide elegant language for talking about lots of situations involving probability ... Group convolutions naturally extend convolutional neural networks, with everything fitting together extremely nicely. Since convolutional neural networks are one of the most powerful tools in machine learning right now, that’s pretty interesting...</p>
</blockquote>

<p>About stacking, the provided answers will suffice.</p>
","2","2","28175","12588"
"38000","<p>Inductive bias means all assumptions your learning approach assumes for generalising to unseen data. What we do in machine learning is induction which means we don't have rules. By seeing data, we make rules. For example, in linear regression you may consider that the output is linear with respect to inputs, or it is polynomial. Another example can be this that in women gesture recognition, you don't care about moustaches because you know they do not have that. You can also take a look at <a href=""https://stackoverflow.com/a/50729693/5120235"">here</a>.</p>
","1","2","28175","12588"
"38053","<p>Yes, there are open source examples. Take a look at <a href=""https://github.com/nlintz/TensorFlow-Tutorials/blob/master/06_autoencoder.py"" rel=""nofollow noreferrer"">here</a> and <a href=""http://github.com/nlintz/TensorFlow-Tutorials/blob/master/06_autoencoder.ipynb"" rel=""nofollow noreferrer"">here</a>.
About your second question, yes. There are numerous studies. For instance, take a look at <a href=""https://www.ijcai.org/Proceedings/15/Papers/578.pdf"" rel=""nofollow noreferrer"">Supervised Representation Learning: Transfer Learning with Deep Autoencoders</a>.</p>
","2","2","28175","12588"
"38132","<p>In spatial transformer networks, basically, the concept of localisation network is to learn to apply a transformation to find the canonical form of the input. Imagine the output of the network $\theta$ as an activation which is passed to another layer. The point is that the sampling sequence of operations is differentiable. $\theta$ is just an output which specifies how the sampling should be performed. The sampling operation that is usually used is bilinear interpolation which although is not differentiable at all points due to the <code>floor</code> and <code>ceiling</code> functions, it <em>can</em> <a href=""https://stackoverflow.com/a/40758135/5120235"">backpropagate</a> the error and is differentiable in most of its inputs. Consider the $\theta$ just as activation which is passed to the bilinear sampler for changing the input of the next network. bilinear sampling is considered to be differentiable. </p>

<p>To understand it better, consider the following figure which illustrates the process inside a spatial transformer easier than the one in the original paper. </p>

<p><a href=""https://i.stack.imgur.com/rCuwC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rCuwC.png"" alt=""enter image description here""></a></p>

<p>As it is clear, the output of the localisation network which is $\theta$ will be passed to the sampling grid. Sampling gird will be multiplied to the $\theta$ to find appropriate regions in the original image. Consider that you don't multiply $\theta$ to the original image. The reason is that if you multiply by the original image, there will be multiple choices for a single pixel while if you multiply the output of the localisation network by the sampling grid, for each entry there is just a single choice. Next, the sampled grid and the original image will be used in the interpolation to find the transformed image. As it is clear, $\theta$ is like the other activations.</p>
","1","2","28175","12588"
"38210","<p>I prefer not to add drop out in <code>LSTM</code> cells for one specific and clear reason. <code>LSTMs</code> are good for long terms but an important thing about them is that they are not very well at memorising multiple things simultaneously. The logic of drop out is for adding noise to the neurons in order not to be dependent on any specific neuron. By adding drop out for <code>LSTM</code> cells, there is a chance for forgetting something that should not be forgotten. Consequently, like <code>CNNs</code> I always prefer to use drop out in dense layers after the <code>LSTM</code> layers.</p>
","14","2","28175","12588"
"38214","<p>The main purpose of batch normalisation is not for dealing with overfitting but if you have small batches while training it can have regularization effect. In the paper that it was introduced for dealing with covariat shift, it was mentioned that it should be used before activation function. Consequently, you can use it both in convolutional layers and dense layers, after employing weights and before activation functions. But I've seen people using that after activation function. It can also be used there.</p>
","1","2","28175","12588"
"38259","<p>As you have mentioned, the output of linear regression is a real value while logistic regression's represents classes(classification). Their main difference is this.</p>

<p>The loss function of linear regression is convex which means you always can find the optimal point using customary optimisations while if you use that for logistic regression, you may get stuck in a non-global minimum which is not optimal. Consequently, people take logarithm of that loss and call it cross entropy. For simple logistic regression tasks, it is convex.</p>

<p>Another difference that has to be cared about is the non-linearity which is usually applied for different tasks. For logistic regression, it is customary that people use non-linearites like <code>Tanh</code> or <code>Sigmoid</code> after the linear part, inner product of weights and inputs, for specifing the similarity to class <code>1</code> or <code>0</code> for typical binary classifications. For linear regression, people usually use <code>Linear</code> activation function. There is a point here. The idea of using the linear activation is not due to the need for employing a linear function. It is used because its output is not <strong>limited</strong>. Consequently, you can use other functions that are <code>one-to-one</code> and are not limited. Consider <span class=""math-container"">$y = x^3$</span> as an example.</p>
","6","2","28175","12588"
"38615","<p>In <code>CNNs</code>, the convolutional layers are used to extract features in the input in order to reduce the cost function. These extracted convolutional features should be classified using dense layers. Consequently, the use of dense layers is for classifying convolutional extracted features. Based on the complexity of the high level features, extracted features in deep convolutional layers, you can have one or more layers. </p>

<p>Take a look at <a href=""https://datascience.stackexchange.com/a/26642/28175"">
How to set the number of neurons and layers in neural networks
</a> to investigate how to set the number of neurons in dense layers.</p>

<blockquote>
  <p>Can I use only one fully connected layer in CNN?</p>
</blockquote>

<p>Yes, you can. For instance, you can use only one layer for <code>MNIST</code> data set and get an acceptable learning. </p>

<blockquote>
  <p>How to determine the dimension of the fully connected layer output?</p>
</blockquote>

<p>The dimension of each fully connected layer is equal to the number of neurons in that layer. For instance, suppose you have a weight matrix <span class=""math-container"">$W$</span> which is <span class=""math-container"">$10\times20$</span>. the latter number represents the number of output neurons. Consequently, the output dimension belongs to <span class=""math-container"">$\mathbb{R}^{20}$</span>.</p>
","5","2","28175","12588"
"38617","<p>Based on our discussion, omit <code>reverse = True</code> and use list instead of sorted and print it. I guess you will see what you want.</p>
","0","2","28175","12588"
"38648","<blockquote>
  <p>Don't we lose image details in doing so?</p>
</blockquote>

<p>We do lose information by resizing the images. You have to consider an important fact about scaling down images. If you want to find an exact position of an object in the input image, you usually need the real size image, with real aspect ratio. In papers like YOLO which attempts to find exact positions, it uses a down-scale value which is near <span class=""math-container"">$4$</span> I guess. This is acceptable because it will introduce at least <span class=""math-container"">$\pm2$</span> pixel errors. In classification tasks, you can usually see images with <span class=""math-container"">$224\times224$</span> dimensions. It is an acceptable dimension that keeps the main structure of the images. You should be aware that it does not keep the aspect ratio but it is not a big problem due to resizing images while test time.</p>

<blockquote>
  <p>What would be the consequences if we scale down the image to some other higher resolution such as 512*512 or 1024*1024 or some other.</p>
</blockquote>

<p>Something like the former case is already done in <code>YOLO</code> and other precise object localisation and annotation tasks. Its consequence is that the number of computation and training parameters increase significantly. Increasing the number of training parameters has a prominent side effect. If you have a large number of parameters, you have to increase the number of training examples, otherwise, the chance of overfitting will be high, albeit you use regularisation techniques.</p>

<blockquote>
  <p>Can we feed the network without 1:1 square images?</p>
</blockquote>

<p>Yes, you can. As I've already mentioned, depending on your task you can preserve the aspect ratio or not. In this case, you have to be aware of your convolutional operations if you set them to <em>VALID</em> operations. Because the dimension with the smaller number of entries will be finished sooner. Consequently, you have to choose the windows size and the type of convolutions wisely.</p>
","1","2","28175","12588"
"38686","<p>As you've mentioned your task is classification and due to using images, it is better using convolutional neural networks. First I have a suggestion, try to find an appropriate size with the same dimensions for all the images and feed them to your network. you can keep aspect ratio or not depending on the environment you are going to test your model. You can also take a look at <a href=""https://datascience.stackexchange.com/a/38648/28175"">Why do we scale down images before feeding them to the network</a>.</p>

<blockquote>
  <p>Which will be better: to put whole images into training set or to divide each image into 4 parts (cutting in midline horizontally and vertically)?</p>
</blockquote>

<p>I guess you attempt to do data augmentation. If so, it depends. If you do that, you may have images that do not contain the flower or they may contain a part of a flower which is common among different classes. Consequently, it may increase the Bayes error. If you are sure you do not have these problems, you can use it albeit I do not think so.</p>

<blockquote>
  <p>Will it help if I rotate/tilt these images and put them also in training set?</p>
</blockquote>

<p>Yes, it is better than the previous technique. You have to be aware that this is dangerous in some cases. Basically, you should train your network using the training set which is given <em>i.i.d</em>ly from the real distribution that your test data has. Suppose that while testing your model, your test data all are given from flowers which are vertically placed in the scene. In such cases, if you train your network with just rotated versions, you may not have a good test result.</p>

<blockquote>
  <p>Will it help if I blur these images and put them also in training set?</p>
</blockquote>

<p>Again like the previous answer, it depends on your test data. If it is something that happens while testing, it is legitimate.</p>

<p><hr/>
I didn't notice to the images. Based on the comment of our friend, I update the answer. Your classes can be one these things. They can be exhaustive or not and they can be mutually exclusive or not. if the former is satisfied, it means that the input should at least belong to one class. The latter means that if you have mutually exclusive classes, the inputs should contain only one class. If they are not mutually exclusive, you can have multiple classes in a single input.</p>

<p>To add an update for the answers above you should consider that data augmentation with reduction can be difficult because it needs an expert to label the inputs by hand which is time consuming.</p>
","1","2","28175","12588"
"38688","<p>okey, I will try to explain them as easy as possible. </p>

<blockquote>
  <p>Regardless of the sequence length, the learned model always has the same input size, because it is specified in terms of transition from one state to another state, rather then specified in terms of variable-length history of states.</p>
</blockquote>

<p>I will use a simple example for simplification but it does not lead to losing generalisation. Suppose that your task is to add to binary numbers. Numbers are stored bitwise in memory and usually for applying any arithmetic operations, they should have a same size. What you need for adding two numbers is to learn how to add zeros and ones and when to output <em>carray</em> to the next step if you use sequential learning approaches, like <code>RNNs</code>. In this case, it is not really important that the two numbers do not have the same shape due to the fact that they both can be resized to the biggest size by adding zeros to their most significant bits. After resizing the inputs to have the same shape you have two options, to use <code>MLPs</code> or <code>RNNs</code>. If you use <code>MLPs</code> what the network learns is entirely different from what an <code>RNN</code> learns. The former does not learn the transition of <em>carray</em> at least as the way <code>RNN</code> learns. Another difference is that your <code>MLP</code> always will be restricted to the size which it was trained while the <code>RNN</code> model will be able to add two numbers with even more bits. To explain it why, for <code>MLPs</code>, all inputs which are connected to the hidden layers or maybe output layers, have weight. Consequently, increasing the number of inputs will lead to more weights which are not trained yet. You are not allowed to input a signal which its size is not equal to the input size of the <code>MLPs</code>. On the contrary, <code>RNNs</code> are exploited in a different way.</p>

<p>First, you should know <code>RNNs</code> better. Try to think of hidden layers of an<code>RNN</code>. They are like usual <code>MLPs</code>. Their difference is that for each node, the inputs come from the previous <strong>step's</strong> outputs and the current time's inputs. Bear in mind that the outcomes of the previous time step are not coming from previous neurons. The reason and I guess the <em>main</em> answer to your question is that each <code>RNN</code> is repeated for each input in time <span class=""math-container"">$t$</span>. It means you have just an <code>MLP</code> which is used for all time step. Suppose you are at the middle of the calculations. Two inputs are <span class=""math-container"">$1$</span> and <span class=""math-container"">$1$</span>. You <code>RNN</code> takes them and the carray value and outputs <span class=""math-container"">$1$</span> as the result of time step <span class=""math-container"">$t$</span> and outputs <span class=""math-container"">$1$</span> to the next step as <em>carray</em>. For the next time step the same <code>RNN</code> is again used and takes the inputs alongside the carry which is coming from the previous step's outputs and outputs the corresponding outputs and carries.</p>

<p>Due to the nature of <code>RNNs</code> which just take the inputs of time step <span class=""math-container"">$t$</span>, they are capable of dealing with signals with different lengths. The reason is that The <code>RNN</code> is used for each time step. This behaviour is usually called unfolding the network.</p>
","3","2","28175","12588"
"38877","<p>I guess you have not figured out the concept of dropout very well. First, the reason we apply it is that we add some noise to the architecture in order not be dependant on any special node. The reason is that it was observed that while training a network, after overfitting, the weights for some of neurons increases and cause the network to be dependant on them. By exploiting dropout, we are not dependant on any node anymore due to it is possible to drop it while training. </p>

<p>Now, answers to your question. First, you have to bear this point in mind that the probability shows the chance of dropping a node in a layer. Consequently, chance 0.5 does not mean you, for instance, will have those two nodes. It just means after employing dropout, the chance of dropping for each node is half. Dropout is used for layers. It is customary to use it in fully connected layers. You set the hyper-parameter and it is the chance of keeping the nodes in the layer. While testing, you don't drop any node. We don't multiply neurons to the probability. The probability specifies the chance of existence of that node.</p>

<p><hr/>
Okey-doke! I update the answer. As you can read in the paper, </p>

<blockquote>
  <p>At test time, it is not feasible to explicitly average the predictions from exponentially
  many thinned models. However, a very simple approximate averaging method works well in
  practice. The idea is to use a single neural net at test time without dropout. The weights
  of this network are scaled-down versions of the trained weights. If a unit is retained with
  probability p during training, the outgoing weights of that unit are multiplied by p at test
  time as shown in Figure 2. This ensures that for any hidden unit the expected output (under
  the distribution used to drop units at training time) is the same as the actual output at
  test time. By doing this scaling, 2n networks with shared weights can be combined into
  a single neural network to be used at test time. We found that training a network with
  dropout and using this approximate averaging method at test time leads to signicantly
  lower generalization error on a wide variety of classication problems compared to training
  with other regularization methods.</p>
</blockquote>

<p>I guess the easiest way to understand it is to watch <a href=""https://www.coursera.org/lecture/deep-neural-network/dropout-regularization-eM33A"" rel=""nofollow noreferrer"">this</a> video. As you can see there are different implementations for that but the reason it is multiplied is that <em>for any hidden unit the expected output (under the distribution used to drop units at training time) is the same as the actual output at test time.</em> To be concise, it is done in order not to change the distribution that the outputs of the layer have.</p>
","3","2","28175","12588"
"38908","<p>Using max-pooling is not a good idea on its own. The reason is that by employing that, you ignore <span class=""math-container"">$75$</span>% of the information each time. If your input is a signal which is quite a small pattern, as the number you have referred, it is better not to use max-pool that much. Although the use of max-pooling adds relative spatial invariance to the objects in the input and is useful for classification tasks, the main reason it is employed nowadays is to reduce the number of parameters to train. For instance, for images which belong to <span class=""math-container"">$R^{224\times224}$</span>, it is wise to use them for some layers due to lessening the number of training parameters significantly. For images with smaller sizes, the input signal on its own has a smaller number of entries, features, and by employing max-pooling, you are chiefly discarding information which may be necessary and prominent for generalisation. You are actually shrugging them off!</p>
","2","2","28175","12588"
"38965","<p>I can mention two main reasons.</p>

<ol>
<li>Complexity of dataset</li>
<li>high Bayes error</li>
</ol>

<p>The former means your dataset is highly non-linear and the latter simly means you may have same input with contradictory outputs in your dataset.</p>
","2","2","28175","12588"
"38971","<p>You may not be very familiar with deep learning. Each kind of network is used for a special kind of task, you cannot just stack <code>LSTMs</code>, <code>GRUs</code>, dense layers and other stuff without supervision. If you have a task that your patterns are local and they may be in multiple locations in an input pattern, you can employ convolutional layers for feature extraction and you can employ dense layers for classifying those extracted features. If you want to classify data which there is a kind of sequence in it, temporal data, you can employ <code>LSTMs</code> and <code>GRUs</code> and you can stack them for better accuracy and you can use their output and feed them to other networks based on your need. <code>MLPs</code> are good for learning non-linear mappings. </p>

<p>Dropout is used for avoiding overfitting.</p>
","2","2","28175","12588"
"39275","<blockquote>
  <p>What is the meaning of: Non-trainable params: 454? Should this ideally be 0? If so, how can this be made 0.</p>
</blockquote>

<p>I guess it is due to using <code>batch normalisation</code>. Try to ascertain by investigating the math of that. If you see the paper, you can easily find out that the bias terms and <span class=""math-container"">$\beta$</span>, if I remember, both get added. Consequently, one can be ignored and won't be trained because it is not needed.</p>

<blockquote>
  <p>Dense layer should be added at the end (in the part you mentioned in your comment) or can it be added in previous layers also?</p>
</blockquote>

<p>Not really, dense layers should be employed after conv layers. What they do is classifying the extracted features obtained by conv layers. About conv layers, they are employed for reducing the number of parameters and finding local patterns.</p>

<p>There is no consensus on how to change the number of filters in convolutional layers, at least as far as I know. But there is a point here. In the following lines of your code, you've employed a kind of pooling layer just before dense layer. If the number of activations coming from conv layer is many, you can use it but consider that by doing so, you ignore important features. I suggest you not doing that, especially, for the last conv layer. Also, try to increase the number of neurons in a dense layer or add extra layers for better accuracy.</p>

<pre><code>conv2d_4 (Conv2D)            (None, 16, 16, 64)        73792     
_________________________________________________________________
global_average_pooling2d_1 ( (None, 64)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 13)                845       
=================================================================
</code></pre>

<p>About using batch norm, it is used due to a kind of problem which is called <em>Covariat Shift</em>. It simply tries to keep the distribution of the outputs of different layers in order to facilitate the learning process. </p>

<p>Based on your questions, I highly recommend you watching professor Andrew Ng's course about ConvNets in Coursera.</p>
","1","2","28175","12588"
"39287","<p>If your task is a kind of classification that the labels are mutually exclusive, each input just has one label, you have to use <code>Softmax</code>. If the inputs of your classification task have multiple labels for an input, your classes are not mutually exclusive and you can use <code>Sigmoid</code> for each output. For the former case, you should choose the output entry with the maximum value as the output. For the latter case, for each class, you have an activation value which belongs to the last sigmoid. If each activation is more than <code>0.5</code> you can say that entry exists in the input.</p>
","8","2","28175","12588"
"39589","<blockquote>
  <p>What is the best way to do that? Should I just add a new class label that includes pictures of anything but food?</p>
</blockquote>

<p>Adding a new class which can be called <em>none</em> is a thing that is usual for such tasks.</p>

<blockquote>
  <p>Is there a best way to chose which pictures I should use?</p>
</blockquote>

<p>Yes, you should pick the images of the <em>none</em> type from the real distribution that your classifier is going to face while test time. It means all the classes should have a same distribution while training and testing. It should be satisfied even for the none class. For instance, suppose that your none type class can be a human hand or a table or something like that. You should not put an elephant image for none type because you are not going to have an elephant in your kitchen.</p>
","1","2","28175","12588"
"39781","<p>Let's say that in this way. The easiest way to ascertain the relation among different features and the outputs is to use covariance matrix. You can even visualise the data for each class. Take a look at the following image.</p>

<p><a href=""https://i.stack.imgur.com/LqDh0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LqDh0.png"" alt=""enter image description here""></a></p>

<p>Suppose that the vertical axis is the output and the horizontal axis is for one of the features. As you can see, having knowledge about the features informs us of the changes in the output. Now, consider the following illustration.</p>

<p><a href=""https://i.stack.imgur.com/Fzhh0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fzhh0.png"" alt=""enter image description here""></a></p>

<p>In this figure, you can see that considering this typical feature does not inform you of the changes in the output. </p>

<p>Another approach can be using <code>PCA</code> which finds the appropriate features itself. What it does is finding a linear combination of important features which are more relevant to the output.</p>
","1","2","28175","12588"
"39797","<p>Consider <code>RNN</code> networks as a simple <code>MLP</code> which for each time span t, takes the inputs of that time and the outputs of the previous step. Actually each time, you unroll the network. Consequently, the number of cells does not have any relation to the input size or the length of the time series. Take a look at <a href=""https://datascience.stackexchange.com/a/38688/28175"">here</a> for a better understanding. </p>
","1","2","28175","12588"
"39801","<p>You've not provided the dataset but I will try to answer based on your descriptions. </p>

<blockquote>
  <p>how many (multiple of 200?) rows do I choose for train/test sets?</p>
</blockquote>

<p>Actually, the train/validation/test splits do not really depend on the length of the time series. It is more dependent on the number of examples, time series here. <span class=""math-container"">$300k / 200 = 1.5k$</span> which means the size of your dataset is small and you can use the customary percentages for your splits. Something like 60/20/20 for train/validation/test. </p>

<blockquote>
  <p>Is my batch size 200?</p>
</blockquote>

<p><span class=""math-container"">$200$</span> is not your batch size, it is the length of your signal. In <code>LSTM</code> networks, you usually deal with temporal data. For each sample, you have 12 features for each time step. <span class=""math-container"">$200$</span> means each sample has <span class=""math-container"">$200$</span> steps. In vectorise implementations of <code>LSTMs</code> it is customary to define batch size. That is, you stack <span class=""math-container"">$m = batch size$</span> samples for acceleration in training, it has details that I skip. </p>

<blockquote>
  <p>What is the basis for choosing number of neurons?</p>
</blockquote>

<p>Take a look at <a href=""https://datascience.stackexchange.com/a/26642/28175"">here</a>. Although the details of <code>MLPs</code> are described, they can be generalised to <code>LSTMs</code> too. Consider <code>RNNs</code> like simple <code>MLPs</code> which take the inputs of the current time step and the outputs of the previous step.</p>
","1","2","28175","12588"
"39818","<p>If your data is stored in a typical variable named <em>var</em>, do the following:</p>

<pre><code>[np.round(item) for item in var[0]]
</code></pre>

<p>Because <em>numpy</em> is vectorised, you can simply use the following code which its result is as above:</p>

<pre><code>np.round(var[0])
</code></pre>

<p>which will get you the rounded array. Consider that you can specify the number of decimal points using <code>decimals</code> in the argument list of <code>round</code> function.</p>
","0","2","28175","12588"
"40276","<p>It is essential for all input patterns to have the same number of features. The reason is that each input feature is connected to specified neurons and they have specified trained weights. A typical solution is to resize your input by reserving its aspect ratio. You should consider that if you want to have good accuracy, you have to have such an operation while training the network too. Even in convolutional networks, it is essential to have the same input size. Although the convolutional layers won't bother you, the connection of fully connected layers and flattened layers used after convolutional layers will have a problem if you don't use correct input shape; the dimensions won't match. </p>

<p>About recent studies, I have not seen yet, but a typical solution can be employing <code>PCA</code> and using let's say its top-10 features as the input of a fully connected network, although for input patterns with a huge difference in the number of input dimensions I guess it is not logical.</p>
","1","2","28175","12588"
"40289","<p>Try to use batch version for training. You have to implement a function to manage your data which exist in the memory. For each batch, you can load the data into your memory as much as possible, high <code>RAM</code> size can help. Then you can pass a batch to your network, the size of graphical memory matters here, using the current data which is in memory now. By doing that you reduce the number of disk calls. When it happens to load data from disk, you fill the memory as much as possible, the disk speed matters here; consequently, high speed <code>SSDs</code> are valuable for facilitating the reading operation.</p>
","1","2","28175","12588"
"40311","<p>There are two interpretations of this formula that I explain one of them. </p>

<p><span class=""math-container"">\begin{equation} Xw = y \end{equation}</span></p>

<p><span class=""math-container"">\begin{equation} X^tXw = X^ty \end{equation}</span></p>

<p>The above is for making sure that you make a square matrix that it has an inverse. It is possible that <span class=""math-container"">$X^tX$</span> does not have any inverse but its chance for linear regression problems is not that much. The reason is that you have a matrix <span class=""math-container"">$X$</span> which belongs to <span class=""math-container"">$R^{m \times n}$</span> which <span class=""math-container"">$m$</span> represents the number of samples and <span class=""math-container"">$n$</span> represents the number of features. Usually, the number of samples is much more than the number of features. Next, </p>

<p><span class=""math-container"">\begin{equation}(X^tX)^{-1}(X^tX)w = (X^tX)^{-1}X^ty\end{equation}</span></p>

<p><span class=""math-container"">\begin{equation}w = (X^tX)^{-1}X^ty\end{equation}</span></p>

<p>Consequently, you have found a closed form for the <span class=""math-container"">$w$</span> linear regression problem which can be generalised to non-linear regression.</p>

<p>Be aware that <span class=""math-container"">$(X^tX)^{-1}X^t$</span> is called the pseudo-inverse of the matrix <span class=""math-container"">$X$</span>. The reason is that <span class=""math-container"">$X$</span> is not a square matrix and it does not have inverse but by the mentioned formula you can find its pseudo-inverse. Just multiply it by <span class=""math-container"">$X$</span> and you will get <em>Identity</em> matrix.</p>

<p>There is another interpretation of this. You can find <a href=""https://datascience.stackexchange.com/a/29523/28175"">here</a>.</p>
","0","2","28175","12588"
"40334","<p>The reason is that by adding more layers, you've added more trainable parameter to your model. You have to train it more. You should consider that <code>MNIST</code> data set is a very easy-to-learn dataset. You can have to layers with much less number of neurons in each layer. Try <span class=""math-container"">$10$</span> neurons for each to facilitate the learning process. You can reach to <span class=""math-container"">$100%$</span> accuracy.</p>
","2","2","28175","12588"
"40465","<p>At least, as far as I know, you can't. The reason is clear. In neural networks, you attempt to find appropriate weights to diminish a typical cost function. You have to find appropriate weights for a specified number of predefined weights. When you specify an input shape, the rest of the network weights will depend on the weights of input. You can't change the input size of a network. In other words, you can't feed your network with different input sizes for convolutional networks. A typical solution for dealing with such situations is to resize the input.</p>
","0","2","28175","12588"
"40883","<p>For imbalanced datasets you can employ <code>F1</code> score. It considers both rare and common classes. You can take a look at <a href=""https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/"" rel=""nofollow noreferrer"">this</a> article if you are not familiar with that.</p>
","0","2","28175","12588"
"41062","<p>I guess you are trying to use pandas, if so. Don't because you can't do that for what you want. I guess your operation needs all data to be loaded to the memory which is not possible. Try to use dask or a kind of sql. I'm not sure whether you want to do operations such as group by which needs all data to be loaded simultaneously or not. If you can do extra coding and your operations don't imply to load all data, such as finding the minimum of a column, you can use generators and specify the <em>chunk_size</em> of the <code>read_csv</code> method in pandas. But you have to do extra coding. You can also take a look at <a href=""https://stackoverflow.com/q/14262433/5120235"">here</a>. You can also take a look at <a href=""https://datascience.stackexchange.com/a/40881/28175"">here</a> to figure out why sql and dask are more better for large operations.</p>
","1","2","28175","12588"
"41098","<p>Try this:</p>

<pre><code>imgs = tf.convert_to_tensor(images[0])
</code></pre>

<p>If all of your images are of the same shape. Otherwise you have to resize them to a same dimension then call the code.</p>
","0","2","28175","12588"
"41172","<p>Generally speaking, it highly depends on your data. If you have images of numbers for each image, it may not be that much bad but for images of cats or dogs, you completely put your information away by resizing to that size.</p>

<p>To answer your question, yes. The reason is that it leads to high Bayes error. It simply means that you as an expert can not say what they are. Consequently, it is not possible for the network to learn them. You can easily see the images and figure out that there is not anything to be learned. For instance, in that case, what is the difference between the sky and sea? Can a <span class=""math-container"">$64\times64$</span> image represent them? Can you as an expert find it out without any previous knowledge?</p>
","0","2","28175","12588"
"41232","<p>It depends on the number of data you have. If you have enough data you can train the entire network, if you don't have, the other one is better. The reason is somehow clear and is due to the nature of the convolutional networks. The first layers learn simple lines and edges and such primitive structures. The next layers try to somehow connect the previous patterns. If you have so much data, you can ignore the pretrained model by updating the pretrained weights. If you don't have enough data, you can freeze the first layers due to the mentioned reason. Deep layers, more specifically, the fully connected layers are for classification. You should let them be trained even if you don't have much data.</p>
","0","2","28175","12588"
"41384","<p>Save your content in the current directory and execute <code>!dir</code> to see the content. Then follow the following code snippet:</p>

<pre><code>from google.colab import files
files.download('your typical text file or what ever.txt')
</code></pre>

<p>For more information take a look at <a href=""https://datascience.stackexchange.com/a/27967/28175"">here</a>.</p>
","3","2","28175","12588"
"41389","<blockquote>
  <p>how we end up with a model which uses a leak at the first layers and then a conventional RELU at the end?</p>
</blockquote>

<p>What matters is to add a non-linearity to the outputs of a neuron. Consequently, by employing each function that adds non-linearity, the network will work due to the derivative that can backpropagate the differentiation of the error term. If you use Relu or leaky Relu, the update terms will change but your network will work fine. The reason that leaky version is used in the mentioned papers is due to avoiding dying relu problem which can happen a lot for regression problems. </p>

<p>About your second question, consider that it usually suffice to use leaky relu in the first layers and due to them, the chance of deep neurons to be stuck at zero is not very much as the results of those papers show. You can use leaky version all over the network but by experience, relu is very fast to be trained!</p>
","0","2","28175","12588"
"41621","<p>They can be employed wherever you can find meaningful adjacent patterns in the input. As an example, you can take a look at <a href=""http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"" rel=""nofollow noreferrer"">here</a> for its use in NLP:</p>

<p><a href=""https://i.stack.imgur.com/s6xHm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s6xHm.png"" alt=""enter image description here""></a></p>

<p>You can also read the well-known paper <a href=""https://www.aclweb.org/anthology/D14-1181"" rel=""nofollow noreferrer"">Convolutional Neural Networks for Sentence Classification</a> which is about sentence classification.</p>
","5","2","28175","12588"
"41677","<p>I don't know what kind of image you have, 16 channels?! oh boy :)
Anyway, if they are images, the first one is better. The reason is that in the second approach you are somehow unrolling the input signal. By doing so, you are removing the information about the locality. You are removing the information of near adjacent inputs. Convolutional neural nets attempt to find these kinds of features. As an example, consider the <code>MNIST</code> dataset. You can learn using CNN and MLP but the former is used due to the fact that CNNs care about patterns which are somehow replicated in different parts of inputs. 
If they are not images and you are aware that adjacent pixels or inputs are related, again you should exploit CNNs. Consider that the convolutional layers are CNNs are for extracting appropriate features. The classification task is done using dense layers in CNNs.</p>

<p>About efficiency, consider two points. Graphic cards are called SIMD computers. It stands for single instruction multiple data. Matrix operations are done using GPUs very efficiently as the name of Graphic cards implies. Consequently, Dense layers are very fast in GPUs compared to CPUs. The other point is about parallel programming. Each filter in convolutional layers are independent; consequently, they can be applied using paralyzed instructions. Again appropriate GPUs are very good at this too. I know I've said two things but there are actually three things to be kept in mind. Forget all of the above mentioned points! The most important thing to consider is the memory and the bus. There are situations that you don't have a graphic card with more than 6 Gig of memory. In those situations, I really prefer the last generations of CPUs instead of GPUS. The reason is that you have to deal with the limitations of memory. </p>

<p>In your case consider that if you use dense layers exactly after inputs, the number of parameters will be science fictional although You are supposed to use CNNs.</p>
","1","2","28175","12588"
"41876","<p>By increasing batch size your steps can be more accurate because your sampling will be closer to the real population. If you increase the size of batch, your batch normalisation can have better results. The reason is exactly like the input layer. The samples will be closer to the population for inner activations.</p>
","7","2","28175","12588"
"42097","<pre><code>for time_range in range (len(data['time'])):
start = [datetime.strptime(t,'%H:%M:%S') for t in time_range]
</code></pre>

<p><code>time_range</code> is an integer due to using range(int). There is nothing to iterate using an int. It is not a container. </p>
","1","2","28175","12588"
"42459","<p>You can simply use gaussian as the nonlinearity and use it in Tensorflow. I guess it is the easiest solution.</p>
","0","2","28175","12588"
"42563","<blockquote>
  <p>Does the network recognize A as an A as good as before?</p>
</blockquote>

<p>Yes, it can. The reason is that if you try to illustrate the confusion matrix you can see that the FP, FN, TP and TN can be kept at a good level. What you have done is actually changing the distribution of your input data. Suppose that you train a network and it should recognize cats and dogs. You can train it just using white cats or with cats of different colours. The point here is that what you've done is not telling that <span class=""math-container"">$A$</span> and <span class=""math-container"">$B$</span> are of the same class. What you have done is classifying the first two letters as the same entity. Consider that what your network learns depends on the kind of network you use. Avoid using just Dense nets. Having image inputs, Dense nets try to find something like a mask while convolutional layers find appropriate features and those features which are better for classification would then passed to the dense layers.</p>

<blockquote>
  <p>Does the network recognize B as an A too?</p>
</blockquote>

<p>Again yes! But it depends on the distribution of your data. It can recognize <span class=""math-container"">$B$</span>s, because you have trained it in that way. But should consider that the performance depends on the data you have. How your data is obtained to be good for generalisation. </p>
","1","2","28175","12588"
"42566","<p>The point in using batch training is that you can't take a step using all data due to the size of the training data which is really big. Consequently, you may want to use batch optimisation techniques which take steps which are near to the best step. The reason is that in each batch, the distribution of data points is similar to the whole training data or at least it is not very different. If you have a small size for your batch, your steps may oscillate and may not be very perfect but for bigger batches they are better. </p>

<p>Batch techniques also facilitate the number of steps that you can take. They also can be helpful for not being stuck in local minimum points.</p>
","0","2","28175","12588"
"42569","<p>Summing up to one or not both can have their special meaning that I'll try to explain them. If you have classes that are not mutually exclusive, say you have dog and cat classes and they both can exist in an image. In such cases, you should use <code>sigmoid</code> nonlinearity as the output of each class and interpret each one separately. Each one that has a value greater than half can explain the existence of the corresponding label. On the contrary, your inputs may be mutually exclusive which means in each input you may just have a cat or a dog. In this case, you should employ <code>softmax</code> nonlinearity and yes, it sums up to one. The winner would be the one with the highest value. </p>
","1","2","28175","12588"
"42572","<p>The first thing you should notice is that you've almost ruined your input signal. Take a look at a <span class=""math-container"">$28\times28$</span> image of a face? what can you see? is there any difference between a teenager and a middle-aged person? The point is that the network should be trained using data that does not have high Bayes error which means you as an expert can distinguish between inputs and label them correctly. Increase the size of your inputs. By doing so, if you use the current regime, you may have lots of trainable parameters between dense layers and convolutional layers. Consequently, try to employ more convolutional layers and some pooling layers among them. Also, try to add more dense layers with more neurons in each.</p>
","1","2","28175","12588"
"42658","<p>Try to use dropout after your dense layers not after maxpooling layers. Whatever comes before dense layers can be considered as the inputs of a classification layer. So keep them otherwise it somehow means you are loosing appropriate information. You should also be aware that you should not use dropout after the last layer.</p>

<p>Also you can add another dense layer, two hidden dense layers, for classification. It seems your data is not easy to learn.</p>
","4","2","28175","12588"
"43118","<p>There are many solutions for this task. I suggest one of them. As you know, words have relation and if you choose to give each word a special code, you can't have this relation. Consequently, first try to use an embedding network in order to assign each word a code. Then assign each article a label. Next, for each article, you have a sequence of words, codes, which are now embedded. You can employ <code>LSTM</code> networks for classification. </p>

<p>If you are not very familiar with the concepts I referred to, you may want to look for <code>Word2Vec</code>.</p>
","2","2","28175","12588"
"43120","<p>Actually, with more GPUs you distribute the calculations and run them parallel. As an example, you can take the group concept used in AlexNet. Although, after employing that it was observed that it can have <a href=""https://blog.yani.io/filter-group-tutorial/"" rel=""nofollow noreferrer"">other</a> properties but one of the main purposes of using SLI is due to the fact that you can distribute the group convolutions among multiple GPUs which can facilitate the convolution operations. Each update is done in the corresponding GPU.</p>
","0","2","28175","12588"
"43134","<p>The main reason for that may <a href=""https://datascience.stackexchange.com/q/20075/28175"">be</a> <em>typically use Euclidean metric; Manhattan may be appropriate if different dimensions are not comparable.</em>. While using regression-based methods you may have noticed that you usually have features of real values. You usually normalise the features and feed them to your model. The act of normalising features somehow means your features are comparable. In cases where you have categorical features, you may want to use decision trees, but I've never seen people have interest in Manhattan distance but based on answers [<a href=""https://datascience.stackexchange.com/a/20231/28175"">2</a>, <a href=""https://datascience.stackexchange.com/a/20234/28175"">3</a>] there are some use cases for Manhattan too. You can also consider that they are <a href=""https://math.stackexchange.com/a/1168765/487106"">comparable</a>.</p>
","-2","2","28175","12588"
"43202","<p>Basically, what you have mentioned in your question is like that you have just one input feature and one output, and your task is regression. The point here is that you have to investigate your data first and find out whether it can be learnt or not. I will explain this later but before that, I have a suggestion about your consideration about separating models. Do not do that if you are not sure each training split has a same distribution as the real data distribution. It seems that you want to do ensemble learning in a wrong way. The main reason that you have not found a good precision is that if you change the distribution of your data, you will have poor performance on the test data. In other words, your model cannot generalise well due to poor training. On the other hand, changing the size of the training data does not reduce the size of your model if they are drawn iid and they are large enough to be learnt.  </p>

<p>First, you have to find the Bayes error of your training data. Because you have just one input feature, you may have high Bayes error, which in your case means your inputs and outputs do not satisfy the functionality condition, for each specific input you should just have one output. You have to investigate whether for each input there is just a single output or not. If you find out that you have a large Bayes error, you can be sure that you cannot have a good performance. On the contrary, if you see that you have low Bayes error, then you can employ linear and non-linear models. </p>

<p>My solution is that, try to solve your problem using a two-layer neural network, find the correct architecture that can generalise well first, then, after finding a good model check the size of the network. Another solution can be using non-linear <code>SVM</code> if you are sure that your data is non-linear, although I guess a two-layer network can solve your problem if you don't have high Bayes error. </p>
","0","2","28175","12588"
"43315","<p>If you have a model that has <span class=""math-container"">$100%$</span> accuracy on unseen test data there are some situations that I will explain. </p>

<ol>
<li>If you have a situation that does not change over time and there are no exceptions in that, the mentioned accuracy is very satisfactory and you can say with confidence that your model has learnt what it should do. As an example, you can implement an <code>LSTM</code> network which can be able to find the sum of binary numbers. In this case, due to the fact that you always know the some of typical <span class=""math-container"">$10 + 01$</span> values is always <span class=""math-container"">$100$</span>, and it is a fact that does not change, <span class=""math-container"">$100$</span>% is acceptable.</li>
<li>There are cases where the situations of the current time are different from those of tomorrow's. It means that the behaviour of the nature that you are going to model it is not a function but a distribution. This means for the current feature space you may have different outcomes. This is the case where in the current feature space the distribution of different classes, i.e. in classification tasks, overlap. This means that it is impossible that you have <span class=""math-container"">$100%$</span> accuracy because the nature you are going to model is not a function. If I want to explain it again there are two situations. first, the nature of the distribution is time variant or invariant. The former case is something that is affected by the previous outcomes. The latter case can have contradictory outcomes with the same input features due to the overlap of distributions. </li>
</ol>
","0","2","28175","12588"
"43789","<p>We usually say a <em>model</em> is overfitting, not a layer or a neuron. Basically, there is not a standard way to set the number of convolutional layers. Iff you see your model is not learning well enough, increase the number of filters. Iff it is overfitting, decrease the number of filters or use the common approaches like <em>drop out</em> to overcome the problem. There are standard ways for interpreting the number of neurons is dense layers. Take a look at <a href=""https://datascience.stackexchange.com/a/26642/28175"">How to set the number of neurons and layers in neural networks</a>.</p>

<p>If by <em>activation normalisation</em> you mean <em>batch normalisation</em>, there is an answer. Batch normalisation is a technique for avoiding covariat shift. It does not allow the distribution of activations to change. It is needed for very deep networks and it does help. If you have a very deep network, use it and you will see that it is very helpful most of the time. Simple networks may be able not to have them. These are usual facts but you may be able not to use them in dense layers, something like AlexNet, and still have a good result.</p>
","1","2","28175","12588"
"44337","<p>It is not necessarily cross features. Actually, it is a subset of a general concept, namely, <em>kernel trick</em>. Using kernel trick you will be able to find decision boundaries which are more complex than usual hyperplanes which can be found using the linear combination of current feature space. Suppose you have a single feature <span class=""math-container"">$x$</span>. By employing that you will be able to make a line as a separator of two classes. If you add <span class=""math-container"">$x^2$</span> alongside <span class=""math-container"">$x$</span> you will be able to have a parabola decision boundary rather than a simple line, which enables you to have a complex decision boundary in cases which is needed. Moreover, because you have both <span class=""math-container"">$x$</span> and <span class=""math-container"">$x^2$</span> you can set the coefficient of each to zero which means you have a kind of flexibility to decide to have a simple line or a parabola. </p>
","1","2","28175","12588"
"44734","<p>There is a famous paper about tracking what a CNN network has learnt. You can visualise to see which parts are more engaged in the classification using <em>DeConvNet</em>. In this paper, it was officially observed that first layers attempt to find simple lines and edges while deeper layers try to put the previous things together to make abstract concepts, like mouth, eye and such meaningful things. As an example take a look at the following image: </p>

<p><a href=""https://i.stack.imgur.com/ztQuS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ztQuS.png"" alt=""enter image description here""></a></p>

<p>I guess there are implementations of this paper that you can replace your pre-trained model with the one already exists and see what exactly is learnt by your network.</p>

<blockquote>
  <p>Isn't this image a different vector?</p>
</blockquote>

<p>They are different vectors. ML and DL models are for generalisation which means they should be good at test time.</p>
","1","2","28175","12588"
"45779","<p>The main reason is that in deep learning the number of training parameters are so many and there is a fact that for each parameter you need at least <span class=""math-container"">$5$</span> to <span class=""math-container"">$10$</span> data to have a good prediction. The reason is a bit complicated to explain but it is related to <code>pack learning</code> and if you insist to know why, I can tell you that in the error term for the test data, you have an overfit term which grows with the number of sample size if your training model is a kind of hypothesis that increases when the number of data increases. In hypothesis with the growth of <span class=""math-container"">$O(2^n)$</span> it is impossible to make the generalisation error same as training error, such as <strong>1NN</strong> on the contrary, hypothesis with the growth <span class=""math-container"">$O(n^c)$</span> which are limited to polynomials can have an overfit which can be diminished by increasing the size of training data. Consequently, if you increase the size of your data you can have better generalisation error. Deep learning models obey the second growth manner. The more data you have, the better generalisation you have.</p>
","0","2","28175","12588"
"46194","<p>A decision tree is a non-linear classifier. If your dataset contains consistent samples, namely you don't have the same input features and contradictory labels, decision trees can classify the data entirely and overfit it. To clarify more the <span class=""math-container"">$VC$</span> dimension for decision trees is <span class=""math-container"">$2^d$</span> which <span class=""math-container"">$d$</span> is for the number of the binary features. Consequently, the Hypothesis space contains <span class=""math-container"">$2^{2^d}$</span> different possibilities which can be dealt with using decision trees. One important point is the number of possible leaves that a decision tree can have. Suppose you have <span class=""math-container"">$m$</span> samples with <span class=""math-container"">$d$</span> binary features. Depending on the quantity of each, the number of possibilities is <span class=""math-container"">$min(2^d, m)$</span>. Decision trees can overfit the training data-set no matter whether they are linearly separable or not, and that is why people use approaches like <code>ID3</code> or <code>C4.5</code> for pruning the tree or setting a threshold for the height and length of the trees in order not to overfit the data. </p>
","2","2","28175","12588"
"46205","<p>Using mutual information namely the correlation of each feature and the output is not something that can be helpful very much. The reason is that the correlation coefficient is just capable of finding whether they are linearly dependent or not. Other than Gaussian distribution, as far as I know, it is not even capable of finding out whether the inputs are independent or not which means if you see the coefficient is equal to zero, if you don't know the distribution you cannot conclude that they are independent but you can say they are not linearly dependent. In real-world applications, it scarcely happens that features just have linear dependencies with the output. Consequently, approaches like this are not very helpful because you may have two features that can have a better relationship with the output but none of them has a good separation with the output.</p>

<p>For cases that you have a lot of features, you can decide to choose other feature selection and extraction methods. I guess for your case wrapper method may suit better. In the wrapper method, you don't have any criterion and you just search through all possibilities of features using heuristic-based methods in order to find the best sub-features which have the best cross-validation accuracy.</p>
","0","2","28175","12588"
"46307","<p>In trivial update rules like gradient descent, the learning rate is important and it somehow specifies the speed you go downhill. In popular papers like <em>Adam</em> optimisation technique, and in non-paperised(!) popular solution namely <em>RMSProp</em> the authors cared that the slope of different features may vary differently and in a direction you may need to go faster due to its slope. Consequently, They decided to set the learning rate and update each parameter based on its own slope and this learning rate is somehow affected by the slope of each direction independently to the other dimensions. The motivation is this. As far as I know, you just need to set the learning rate for your optimisation and it will be adapted by itself.</p>
","1","2","28175","12588"
"46313","<p>Based on the comments I'll try to answer. I guess you don't have the corresponding labeles. What you can do as a solution is that you can use k-means algorithm as the easiest start point to and specify the hyper parameter, k, to two. Then you can find two clusters and you yourself can evaluate the results. As another approach, you can increase the size of k and again evaluate your answers. You can also use Gaussian Mixture Models for finding better non-convex clusters which have better results. The point is that you have to evaluate the results as an expert and label them manually. This task accelerates the labeling process. After that you can employ a simple <code>MLP</code> for finding a descriminative model.</p>
","2","2","28175","12588"
"46314","<p>Convolutional networks can be used for regression tasks too. The difference corresponds to the output layers of the dense networks. In classification tasks you use sigmoid or softmax depending on your task. In regression tasks you can simply use linear activation function as the non-linearity of the last layers. Consequently, if you have the ages as the outputs of your network and if they are in a same scale as the inputs you can simply use them as they are right now. </p>
","0","2","28175","12588"
"46352","<p>Do the following:</p>

<pre><code>print ''.join(params)
</code></pre>

<p>You have added an extra space. Omit it.</p>

<p>Edit:</p>

<p>It seems I've forgotten to mention that you have to split the <code>'\x00'</code> sequence from your string. Consequently, you have to use the following code:</p>

<pre><code>print """".join(params).split('\x00')
</code></pre>
","1","2","28175","12588"
"46564","<p>The first reason is the number of parameters. The former case that you've mentioned, for each neuron there should be corresponding entries that would increase the number of training parameters. The other reason is that by employing simple feed-forward neurons you are somehow discarding the temporal information of your data which means you are discarding the sequence information in your data. This is somehow like the spatial data which is obtained by convolutional layers in <code>CNNs</code>.</p>
","1","2","28175","12588"
"46662","<p>I guess expanding basis is somehow looking at the problem using <code>linear algebra</code> perspective. For explaining it using a trivial example, in linear algebra, for the <code>3D</code> space, there are three bases namely, <span class=""math-container"">$i$</span>, <span class=""math-container"">$j$</span> and <span class=""math-container"">$k$</span>. It means for referring to any vector in <code>3D</code> space you just need a linear combination of these three vectors to construct any desired vectors. Each basis for each space has a number of properties like being orthogonal or not or maybe being linearly independent and some other important properties. By adding extra features, you increase the number of features if they are linearly independent.</p>

<p>In machine learning, there are situations that comparing each feature with the output directly may not lead to a good decision boundary while using some features alongside each other can make decision boundaries which can lead to a better decision boundary. All of that means that if you change the input space to space with more number of features which the features are extracted using the current feature space, you can find a space that the data can be separated better. Finding a feature space such with properties can be done using feature extraction methods. <em>Kernel methods</em> are some of them.</p>

<p><strong>Kernel trick</strong> is a kind of mathematical approach for reducing the difficulties of transforming the data in the current feature space to the appropriate feature space. Suppose you have one million rows of raw data and the feature space belongs to <span class=""math-container"">$R^{3}$</span>. Suppose you find out that a feature space that can have a good  generalisation belongs to the space which belongs to <span class=""math-container"">$R^{95}$</span>. Trying to transform the data in hand to the desired feature space is a time-consuming task if possible. Instead of transforming the data directly, the kernel trick tries to calculate the outcome of the transformed data without transforming the data into the new space. </p>
","0","2","28175","12588"
"46682","<p>They are tools for different purposes. <code>Softmax</code> is used in cases that you have labels which are mutually exclusive, they should be contradictory, and exhaustive, one of the labels should always be one while the other is used for cases that there may be multiple labels in the input pattern.</p>

<p>Consider that softmax is just used to face the outputs of a network as probabilities This means that it is a simple function that maps <span class=""math-container"">$R^{n}$</span> space to <span class=""math-container"">$R^{n}$</span> which means <code>softmax</code> has n inputs and n outputs. </p>
","0","2","28175","12588"
"46686","<p>Based on the documentation <a href=""https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.to_sql.html"" rel=""noreferrer"">0.22</a> and <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html"" rel=""noreferrer"">0.24.1</a>, the flavor does not exist in the argument list of the <code>to_sql</code> method. You're probably running the <code>0.24.1</code> version which does not need <code>flavor</code> argument.</p>
","6","2","28175","12588"
"47153","<p>The answer to the question may be no. The reason is simply due to numerous optimisation algorithms that are available, but choosing one highly depends on the context and the time you have for optimising. For instance, <em>Genetic</em> algorithm is a well-known optimisation approach which does not have any gradient descent inside it. There are also other approaches like backtracking in some contexts. They all can be used which do not leverage gradient descent step by step. </p>

<p>On the other hand, for tasks like regression, you can find close-form for solving the problem to find extremums, but the point is that depending on the feature space and the number of inputs you may choose the close-form equation or the gradient descent to decrease the number of calculations. </p>

<p>While there are so many optimisation algorithms, in neural networks gradient descent based approaches are used more due to multiple reasons. First of all, they are very fast. In deep learning, you have to provide so many data that they cannot be loaded to the memory simultaneously. Consequently, you have to apply batch gradient methods for optimisation. It's a bit statistic stuff but you can consider that each sample you bring to your network can have a roughly similar distribution to the real data and can be representative enough to find a gradient which can be close to the real gradient of the cost function which should be constructed using all data in hand. </p>

<p>Second, The complexity of finding extremums using matrices and their inverse is <span class=""math-container"">$O(n^3)$</span> for a simple regression task which the parameters can be found using <span class=""math-container"">$w = (X^tX)^{-1}(X^ty)$</span>. It turns out that simple gradient-based methods can have better performance. It should also be mentioned that in the former case, you have to bring the data simultaneously to the memory which is not possible for occasions where you deal with big data tasks.</p>

<p>Third, there are optimisation problems that do not necessarily have a close-form solution. <a href=""https://stackoverflow.com/a/37997390/5120235"">Logistic regression</a> is one of them.</p>
","3","2","28175","12588"
"47338","<p>Yes, you can. Basically, for regression tasks, it is customary to use the linear function as the non-linearity due to the fact that it's differentiable and it does not limit the output. This means you can make any output using your inputs. People do not use <strong>tanh</strong> or <strong>sigmoid</strong> as the activation function of the last layers for regression tasks due to the fact that they are limited and cannot generate all numbers which are needed. In your task, you can use <strong>ReLU</strong> as the non-linearity. The concept of non-linearities in hidden layers is to add non-linear boundaries and for the last layer in regression tasks, it should make all possible choices. In your case, <strong>ReLU</strong> is the best.</p>
","4","2","28175","12588"
"47365","<p>The title of your question and the question itself are somehow different but I'll try to answer the question, <em>the meaning of decreasing loss without changes in accuracy.</em></p>

<p>The reason is simply due to using probabilities. For instance, for classification task if you have an output <span class=""math-container"">$0.7$</span> for input and the last layer is a softmax, then you classify the input as that class which has <span class=""math-container"">$0.7$</span>. Imagine you train more and that output changes to something like <span class=""math-container"">$0.95$</span>. Consequently, the accuracy does not change because you already classify it as what it really is but the loss lessens. </p>

<p>To answer the question which is in the body of your post, there can be numerous reasons that I'll try to refer to them. </p>

<p>One of the possibilities is that your data of different classes have overlap in the current feature space. This may lead to high Bayes error. For instance, suppose you have two same inputs and the label of them are contradictory. In this situation, your performance cannot be improved. To check it whether you've got this problem or not, take a look at the histogram of your data. </p>

<p>Another problem can be the weakness of <code>LSTM</code> networks which cannot memorise numerous things. <code>LSTM</code> models are very good at things like considering the gender of a subject or the plural or singular form of subjects but in cases where they should consider many things simultaneously, they have difficulties.</p>

<p>Another reason can be the incorrect way of using dropout. At first, do not use it and let your network overfits the training data to find a good model. After fitting your data, try to use dropout. </p>

<p>You can also test <code>Stacked LSTMs</code> which are powerful models.</p>
","2","2","28175","12588"
"47481","<p>There are numerous topics that you've mentioned but I will suggest those which I've read and are helpful.</p>

<p>For hidden markov models and markov processes I suggest reading <strong>Pattern Classification</strong> by <em>Richard O. Duda</em>. You can also take a look at <strong>Pattern Recognition and Machine Learning</strong> by <em>Christopher Bishop</em>. For better understanding Markov processes and their behaviour, you can also take the course stochastic process which is also known as <em>random process</em>. I suggest taking that course before reading any book, because you may need some help if you're not very familiar to the concepts.</p>

<p>For <code>LSTMs</code>, I highly suggest taking a look at the fifth course of deep learning on coursera by pr. <em>Andrew Ng</em>. If you do their homework you can realise how the inner operations exactly work and you'll have a very deep understanding of time series and what their nature is. After that, you can take a look at the <em>Deep Learning</em> book by <em>Ian Goodfellow</em>.</p>
","2","2","28175","12588"
"47762","<p>There are two points that have to be considered. </p>

<ol>
<li>Take care of the output of your network. If that's a Real number and can take any value, you have to use linear activation as the output.</li>
<li>The inner activations highly depend on your task and the size of the network that you use. What I'm going to tell you is based on experience. If you don't have a very deep network, <span class=""math-container"">$tanh$</span> and <span class=""math-container"">$ReLU$</span> may not differ very much in convergence time. If you're using very deep networks, don't use <span class=""math-container"">$tahn$</span> at all. <span class=""math-container"">$ReLU$</span> is also not recommended in some <a href=""https://arxiv.org/abs/1502.01852"" rel=""nofollow noreferrer"">contexts</a>. You can employ <span class=""math-container"">$PReLU$</span> in very deep networks. It does not add too many parameters to learn. You can also use <span class=""math-container"">$leaky-ReLU$</span> in order to avoid dying ReLU problem which may occur. </li>
</ol>

<p>Finally, about the other nonlinearity that you've referred; try not to use <span class=""math-container"">$Sigmoid$</span> due to that fact that it's expected value is not equal to zero but half. It's a bit statistical stuff, but you can consider it's roughly hard for a network to learn shifted weights. </p>
","3","2","28175","12588"
"48002","<p>Well, let's say it depends on the distribution of your data. In approaches like <em>PCA</em> the approach does not care about the labels of the data in hand. This is why <em>PCA</em> may lead to data which are sometimes difficult to be separated or vice versa. <em>PCA</em> just cares about which direction leads to more variance and take that direction as a new basis. Not caring about the labels is why you cannot say it may lead to a better space for classification or not. You have to employ that and after that, investigate whether it's helpful or not. Approaches like <em>LDA</em> or other variants of that take care of the labels but they are linear classifiers which are not strong at least in a current feature space where you've not done any feature engineering.</p>
","1","2","28175","12588"
"48011","<p>Your data size is not that much huge, but there are some debates whenever you deal with big data <a href=""https://stackoverflow.com/a/23028862/5120235"">What is the best way to store data in Python</a> and <a href=""https://towardsdatascience.com/optimized-i-o-operations-in-python-194f856210e0"" rel=""nofollow noreferrer"">Optimized I/O operations in Python</a>. They all depend on the way the serialisation occurs and the policies which are taken in different layers. For instance, security, valid transactions and such things. I guess the latter link can help you dealing with large data.</p>
","1","2","28175","12588"
"48168","<p>The point is that if your training data does not have the same input features with different labels which leads to <span class=""math-container"">$0$</span> Bayes error, the decision tree can learn it entirely and that can lead to overfitting also known as high variance. This is why people usually use pruning using cross-validation for avoiding the trees to get overfitted to the training data. </p>

<p>Decision trees are powerful classifiers. Algorithms such as Bagging try to use powerful classifiers in order to achieve ensemble learning for finding a classifier that does not have high variance. One way can be ignoring some features and using the others, Random Forest, in order to find the best features which can generalize well. The other can be using choosing random training data for training each decision tree and after that put it that again inside the training data, bootstrapping.</p>

<p>The reason that decision trees can overfit is due to their VC. Although it is not infinite, unlike <em>1-NN</em>, it is very large which leads to overfitting. It simply means you have to provide multiple numerous data in order not to overfit. For understanding VC dimension of decision trees, take a look at <a href=""https://datascience.stackexchange.com/a/46194/28175"">Are decision tree algorithms linear or nonlinear</a>.</p>
","1","2","28175","12588"
"48230","<p>It is because of the kind of convolution you've used. It is a <code>valid</code> convolution. If you want the output to be <span class=""math-container"">$128$</span>, set the convolution to be <code>same</code>. Consider that this is also applicable to the deep layers too. They also can have either of these convolutions.</p>
","2","2","28175","12588"
"48530","<p>At first, I have to mention that <span class=""math-container"">$5k$</span> cannot be considered as a large dataset for training a deep neural network. Anyway, about the question. In general, yes you can, but you have to be aware of some points. Data augmentation can be helpful or it can damage your entire predictions. The reason for each is that whenever you utilize data augmentation, you are somehow changing and manipulating the distribution of your data in hand. Because neural networks are considered to deal with random processes which their behaviour is iid, your data samples should not be dependent on each other. There is also another perspective. Your training data should have the same distribution as your test data. If you are sure that while testing what you are going to face can be sharpened images, so go ahead and carry out that, but if you're sure that your sensors are placed in a noisy situation, something you can encounter in self-driving cars, you can be pretty much sure that your raw data is blurry, and it is almost impossible to have sharp images due to velocity unless you take a preprocessing step and after that you feed it to your network.  </p>
","1","2","28175","12588"
"48554","<p>It's not an error, it's simply notifying you that the code is written using past versions of Tensorlfow and some of the arguments of special methods were going to be deprecated in the future releases of the library. It's ok to be used, but you can also check what version is used and make a virtual environment and install the specified version of the library you want. </p>

<p>You can run your cell twice in order not to see the <em>warning</em> again.</p>
","0","2","28175","12588"
"49550","<p>In Euclidian space where the axes are represented by <span class=""math-container"">$i, j, k$</span> vectors, three-dimensional space, the distance can be obtained by connecting the two points and finding the length of the connection. This space is used whenever the basis, each of directions, are independent. In other words, whenever it is needed to find the true distance, Euclidian distance can be employed if the features or variables, axes indeed, are independent. On the contrary, whenever the variables are correlated, the Euclidian distance cannot be employed, because the axes are not independent anymore. In such situations which is not rare, <em>Mahalanobis</em> can be utilised. Its form is like Gaussian distance. </p>
","0","2","28175","12588"
"49672","<p>Based on your code, your training accuracy is increasing and the loss is decreasing. On the contrary, the accuracy of your test data is decreasing which means you are overfitting your training data. I don't know exactly what kind of data you have, but it seems your data does not have any meaningful spatial information. Meaningful spatial one-dimensional signals are like audio signals. It is not correct to use structured data for convolutional networks because different columns usually do not have spatial information. Consequently, I suggest using solely dense layers in your network. Keep going and use drop-out in order not to overfit. </p>

<p><em>acc</em> is for training accuracy and <em>val_acc</em> is for validation accuracy. accurayc should be increased. loss is for the cost function and should be decreased which means you are decreasing the error.</p>
","2","2","28175","12588"
"49679","<p><code>CNNs</code> are applicable wherever the input signal contains spatial information. For instance, you can think of the following list:</p>

<ul>
<li>1-D signal: Recorded voices are examples of one-dimensional inputs where there are relations between adjacent entries of inputs. This means adjacent entries have patterns which are valuable for different tasks, such as classification. You can employ 1-D convolutional layers for these input signals.</li>
<li>2-D signal: Images are an example of this kind, albeit they may have different channels, like RGB. It's clear that adjacent pixels are roughly like each other, and other than that, the adjacent pixels share patterns which may be repeated over the entire image multiple times. Consequently, 2-D convolutional layers can be employed for these signals. </li>
<li>3-D signal: Video frames are examples of this kind. Other than the similarity you can find inside each frame, the different frames that exist one after another in the time axis can have similar meaningful patterns which are replicated. You can employ 3-D convolutional layers for these signal. </li>
</ul>

<p>About structured data where you can find them as the rows of relational databases which each column belongs to a specified feature, it does not have any meaning to use convolutional layers. The reason is clear. They do not have any spatial information. The adjacent rows should not share a common concept otherwise they would be redundant. Moreover, they are not spatially related to each other. For structured data, people utilise dense layers. </p>
","1","2","28175","12588"
"50961","<p>There are multifarious studies about object detection. The method you are going to use depends on your data. For instance, if you have centre of the objects alongside their height and width, you can simply use <em>YOLO</em> method which is one of the pioneer approaches for detection. As another example, If you have data which is classified pixel-wise, you can employ <em>RCNN</em> methods or <em>Fully Convolutional</em> methods like <em>FCN</em> itself or <em>SegNet</em>. There are open source implementations with appropriate trained parameters for these models on GitHub.</p>

<p>Apart from the methods, you have to consider the restrictions they may exist for your conditions. For instance, there may be situations where your task is online and you have limited computational power. In such cases, I don't think you can employ very deep architectures like <em>SegNet</em>. </p>
","0","2","28175","12588"
"51427","<p>You can employ the linear regression algorithm even for categorical data. The point is that whether your data is learnable or not. For instance, take a look at your data, and see whether an expert can really find the output by taking a look at the input vector. If it's possible, your task can be learnt using linear regression method.</p>

<p>About linearity, the point is that linear regression can also learn nonlinear mappings. You just have to provide enough higher order polynomials of the current feature space you have which is not an easy task. For instance, you can expand your current feature space by adding the square of each feature to the current feature space. You will observe that it may have better performance than the usual case if your mapping is not linear, but you may still have error. Consequently, you have to supply more polynomial features, but you do not know which to use. </p>

<p>An alternative to linear regression which does not need to add extra features is <em>multi layer neural networks</em> (MLP). You can simply use them which can learn nonlinear mappings. You can take a look at the official page of <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"" rel=""nofollow noreferrer"">SKlearn</a> for applying them. Furthermore, you can take a look at <a href=""https://scikit-learn.org/stable/modules/neural_networks_supervised.html"" rel=""nofollow noreferrer"">here</a> for applying them. </p>
","1","2","28175","12588"
"51435","<p>The large size of your data is acceptable for deep learning and big data projects. Your system is also acceptable, though it is not powerful. If you have enough hard disk to store them all, it will suffice which means you can train your network. The elapsed time for each epoch depends on multiple aspects. For instance, some elements which are important are, the batch size and your vectorized implementation, the bottle-neck between the disk and RAM, the bottle between RAM and GPU, the size of the model, the size of training data, the memory size of your GPU alongside the size of your RAM, the size of each data, the load which is imposed to your GPU by your OS, and so forth. The easiest way is to code your network and try it yourself. </p>

<p>As I've mentioned, by the current settings you can train your network, but you may not have very fast computation. However, you can use some techniques to faciliate your training phase as much as possible. For instance, you have two main bottle-necks. The first bottleneck, which exists between disk and RAM, can be dealt with using <strong>generators</strong>. Namely, you can employ generators to decrease the number of disk calls. The other bottle-neck, between RAM and GPU, can be handled using vectorized implementation of your neural network. After loading your network, you can find the appropriate batch size to use all available GPU memory. </p>

<p>I also want to point out that the current GPU you have may have space limitations. This can incur difficulties when your network is very large. In such cases, you won't be able to load your entire network to your GPU. </p>
","1","2","28175","12588"
"51469","<p>Decrease the number of hidden layers; you can omit the dense layer with <span class=""math-container"">$50$</span> neurons. Furthermore, train your network more. You should also provide more data. It is not much at the moment. </p>

<p>Your current architecture is very deep for such a relatively easy task. Consequently, it needs more train time. You can just decrease the size of the current model by diminishing the number of hidden layers and neurons. For instance, use the following setting to see how you can train very fast and have a good accuracy. </p>

<pre><code>model = Sequential()
model.add(Dense(20, activation = 'relu', input_dim = 1))
model.add(Dense(20, activation = 'relu'))
model.add(Dense(1))
</code></pre>
","6","2","28175","12588"
"51474","<p><strong>Conv1D</strong> is used for input signals which are similar to the voice. By employing them you can find patterns across the signal. For instance, you have a voice signal and you have a convolutional layer. Each convolution traverses the voice to find meaningful patterns by employing a cost function.</p>

<p><strong>Conv2D</strong> is used for images. This use case is very popular. The convolution method used for this layer is so called <em>convolution over volume</em>. This means you have a two-dimensional image which contains multiple channels, RGB as an example. In this case, each convolutional filter should be a three-dimensional filter to be convolved, cross-correlated actually, with the image to find appropriate patterns across the image.</p>

<p><strong>Conv3D</strong> is usually used for videos where you have a frame for each time span. These layers usually have more parameters to be learnt than the previous layers. The reason we call them <em><span class=""math-container"">$3D$</span></em> is that other than images for each frame, there is another axis called <em>time</em> containing discrete values, and each of them corresponds to a particular frame.</p>
","11","2","28175","12588"
"51476","<p>What I remember is that they give you more insight in future lectures, but the main reason for that is the maximum likelihood with which you try to increase the chance of making the current data set by setting the parameters. It is a good choice for setting the parameters, but its weakness is that it may overfit to your training data. You may want to take a look at <a href=""https://en.wikipedia.org/wiki/Maximum_likelihood_estimation"" rel=""nofollow noreferrer"">Maximum Likelihood Estimation</a>.</p>

<p>The cost function which is discussed there is called minimum square errors. It is found by maximum likelihood. This means that you want to increase the chance of making your training set. In other words, you want to increase the chance of <span class=""math-container"">$P(D|\theta)$</span> where <span class=""math-container"">$D$</span> can be considered as your training set. Due to the fact that your data should be iid, you can write the previous probability as <span class=""math-container"">$\pi p(x_i|\theta)$</span>. You then apply some simplifications and you finally find that cost function. You can take a look at <a href=""https://www.jessicayung.com/mse-as-maximum-likelihood/"" rel=""nofollow noreferrer"">MSE as Maximum Likelihood</a> for exact justification.</p>
","0","2","28175","12588"
"51551","<p>I have plotted what you are referring in the following picture. As you can see, by employing a coefficient as the input of the <em>tanh</em> function, you are limiting the range of changes of the function with respect to <span class=""math-container"">$x$</span> axis. This has a negative effect. The reason is that although you are making the slope sharper for a very small region in the domain, you are making the differentiation of the other points in the domain more close to zero. The vanishing problem occurs due to the fact that the outputs of neurons go far from the zero and they will be biased to each of the two directions. After that, the differentiation value is so much small and due to begin smaller than one and bigger than zero, it gets even smaller after being multiplied by the other differentiations which are like itself.</p>

<p><a href=""https://i.stack.imgur.com/EE3FQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EE3FQ.png"" alt=""enter image description here""></a></p>

<p>Another nice question can be this that you have a coefficient value smaller than one. I've illustrated that In the following picture. In this figure, you are changing the function in a way that you have a differentiation which is larger than before in more points of the domain, but again it is smaller than one. This is not valuable for deep networks.</p>

<p><a href=""https://i.stack.imgur.com/JISCS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JISCS.png"" alt=""enter image description here""></a></p>

<p>As I have mentioned, in both cases which you employ coefficient, the derivative would be smaller than one and is not valuable for deep networks.</p>
","3","2","28175","12588"
"51552","<p>In high dimensional spaces like the one-dimensional space, you have to employ a very simple network, maybe just a single neuron and investigate whether your data is linearly separable in that feature space which can not be visualized. If you observe that you do not have a good performance, you can increase the size or the layers of your network step by step. You can generalise what I've just referred to other networks like RNNs. </p>

<p>What I've referred means that your data may be in a way that is separable though it's in 1000 dimensional space and you just need one neuron to classify it.</p>
","1","2","28175","12588"
"52638","<p>You may want to use something like the following snippet:</p>

<pre><code>with h5py.File(""some_path.h5"") as f:
   f[""data1""] = some_data

import h5py #added
with h5py.File('../images.h5', 'w') as f:    
    h5_file = tables.open_file(""images.h5"", mode=""w"")
    # do what ever you want to do, it will be closed by itself.
</code></pre>
","1","2","28175","12588"
"54193","<p>There are numerous ways to serialise data. For instance, JSON, XML, and CSV are possible approaches for serialisation. Another way is YAML. It is human-readable which means it contains readable text which is written using usual words. This stands against binary serialisation. Whenever you open the file, you will not be able to see what is written, because it is serialised in binary format and not as a human-readable format. As you may find it <a href=""https://en.wikipedia.org/wiki/YAML"" rel=""nofollow noreferrer"">here</a>, it is used for configuration files mostly. In ML context depending on your task, you may face different file format. For instance, one of the widely used formats for supervised ML tasks is CSV though you may find others use their favorite encodings. You may also want to make your own serialisation too which is possible among in-company operations is rare. </p>
","4","2","28175","12588"
"54282","<p>The reason is that your data is in a way that the algorithm does not make any mistake on it in the current feature space. it is an easy problem that the algorithm does not need to ignore wrong points due to the easy data which is provided. If it occurs that your data is hard to be classified, the margin then will try to ignore some data points which may lead to narrow margins.</p>

<p>It is worth mentioning that even though it performs the same, it won't be as a simple perceptron, at least in most cases. Consider that SVM somehow considers the geometrical position of data points while a simple perceptron always tries to reduce the cost function. You can take a look at the pictures which are provided <a href=""https://datascience.stackexchange.com/a/31464/28175"">here</a>.</p>
","0","2","28175","12588"
"54378","<p>You have already computed that, but you've not bound the output to a variable, also called <em>name</em> in python. Try the following snippet:</p>

<pre><code>result = np.linalg.norm(v1,ord=2,axis=1,keepdims=True)
print(result)
</code></pre>

<p><hr />
Based on the edit, I update the answer. As you may find <a href=""https://stackoverflow.com/q/21030391/5120235"">answers</a> to your question, a typical way to find what you need is something like the following function:</p>

<pre><code>def normalize(v):
    norm = np.linalg.norm(v)
    if norm == 0: 
       return v
    return v / norm
</code></pre>

<p>Equivalently, there is a function called <code>normalize</code> in <code>sklearn.preprocessing</code> which can be employed for your task. </p>
","7","2","28175","12588"
"54894","<p>The answer to your question is yes, and it is already implemented due to popular approaches which are introduced in academic papers. For instance, <em>Adam</em> optimisaiton approach is an optimiser which assumes it is not valid to multiply each dimension, feature, with a specified value. It uses the idea of momentum and geometric average and a bunch of other methods to employ the idea that you should not go down the hill with the same speed for different dimensions. What it says is that you may be in a location which each dimension has different slope, and due to this fact you should not go downhill with the same learning rate. In that approach, although you specify the same learning rate for the optimiser, due to using momentum, it changes in practice for different dimensions. At least as far as I know, the idea of different learning rates for each dimension was introduced by Pr. Hinton with his approache, namely <em>RMSProp</em>.</p>
","4","2","28175","12588"
"55487","<p>If I've figured it out correctly, the answer is no. The point is that your saved model solely contains the network architecture and the parameters it has. What you want relates to the recall phase where you have to provide input to get output. This means that you need input data to be fed to your network in order to get output. What you want can be done using another approach. First, load your network and feed your data to your model. After that, get the outputs and store the inputs and outputs alongside each other using <code>Numpy</code> save method or maybe .h5 format.</p>

<hr />

<p>The flow for achieving the <code>y_pred</code> can be like the following sequence of actions:</p>

<ol>
<li>Load your model.</li>
<li>Feed your data to your model and get <code>y_pred</code>.</li>
<li>define a <code>Numpy</code> array of inputs and a <code>Numpy</code> array of outputs.</li>
<li>Store inputs, real outputs and <code>y_preds</code> using the <a href=""https://stackoverflow.com/a/28440249/5120235"">methods</a> which are available. </li>
<li>Later, when you want to make your confusion matrix, you can load your inputs and outputs, and the real outputs to make your matrix.</li>
</ol>
","2","2","28175","12588"
"55516","<p>I guess the other answer is sufficient for the question. I just want to add this point that the algorithm uses different anchor boxed due to this fact that the centre of distinct objects may reside on the same pixel, though the real algorithm uses more than two anchor boxes. For instance, you can clearly see the image that he has used in his slide. The centre of the two objects is on the same pixel. You should also consider that the anchor boxes for each class differ, and is unique for each.</p>
","1","2","28175","12588"
"55666","<p>I don't remember exactly what the book has mentioned, but I guess the difference between the two is due to having one or multiple features. I guess it is already mentioned in the book. They are the same. One is for multi-dimensional input, and the other is for one-dimensional. One sigma is iterating over the features and the other is iterating over the examples. You can put <span class=""math-container"">$k$</span> to one to achieve the simpler formula.</p>
","0","2","28175","12588"
"61204","<p>In statistics, there are two concepts, <em>population</em> and <em>sample</em>. We say, for instance, that a random phenomenon can be described using a distribution. For instance, the grades students achieve at school have Normal distribution. This is satisfied for other random natures. There is a point, whenever you study a phenomenon, you have a limited size of data. This data, sample, is taken from a larger group, population. if you pick each data independently and all the data that you've picked are in their routine condition, these are called iid condition which stands for independent and identical distribution, your data which is going to be called <em>sample</em> will be <em>like</em> your population. This means the statistics of the population, expected value and other descriptive things, can roughly be found using a bunch of formulas. This means by studying some samples of the population, you can find out how the entire population works. This is clear that if you have more data which is more representative, you can have better approximations. In my answer, I've avoided referring to formulas for finding population statistics out of sample statistics, but if you study them, you will see that but increasing the size of sample, usually noted by <span class=""math-container"">$n$</span>, you can have better approximations to describe the entire population by just having a very small subsets of that. </p>

<p>I refer to my first words about distribution. Depending on the phenomenon you are attempting to deal with, it has a distribution which that specifies the appropriate formulas to go from samples to population.</p>
","0","2","28175","12588"
"74974","<p>Finally, I find time to answer this question whose answer was found in a well-known online course provided by Pr. Boyd for <em>convex optimisation</em>. In that course, he refers to applications of optimisation. One of its applications is penalty function approximation. As a brief answer, just define your penalty for the parameters you want and add it to the cost function, it will be like multi-objective optimisation which should be optimised using scalarization. For simplicity, consider what we have in l1/l2 regularisation methods for avoiding overfitting. Simply, add this to the current cost function. But, consider your cost function to be smooth. For instance, I've added the example he has provided.</p>

<p><a href=""https://i.stack.imgur.com/UEUvp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UEUvp.png"" alt=""enter image description here""></a></p>

<p>As you can see, the log-barrier limits the range of errors for a particular variable, and values more/less than one will be penalised significantly while inputs in the range <span class=""math-container"">$(-1, 1)$</span> won't. </p>

<p>As you can see, there are other methods too. In today's usual neural networks, people employe l2 norm for tasks like classification to penalise mislabeled items..</p>
","0","2","28175","12588"
"79847","<p>Solving optimisation problems is difficult, and finding a closed-form solution that finds the optimal point for the cost function is complicated. Consequently, optimisation problems are solved using iterative steps. This means people choose solutions which are guaranteed to decrease the cost or objective function with each step. This idea is somehow used in neural networks.</p>
","0","2","28175","12588"
"82371","<p>Pr. Hinton in his popular course on Coursera refers to <a href=""https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a"" rel=""nofollow noreferrer"">the following fact</a>:</p>
<blockquote>
<p>Rprop doesn’t really work when we have very large datasets and need to perform mini-batch weights updates. Why it doesn’t work with mini-batches? Well, people have tried it, but found it hard to make it work. The reason it doesn’t work is that it violates <strong>the central idea behind stochastic gradient descent</strong>, which is when we have small enough learning rate, it averages the gradients over successive mini-batches. Consider the weight, that gets the gradient 0.1 on nine mini-batches, and the gradient of -0.9 on tenths mini-batch. What we’d like is to those gradients to roughly cancel each other out, so that the stay approximately the same. But it’s not what happens with rprop. With rprop, we increment the weight 9 times and decrement only once, so the weight grows much larger.</p>
</blockquote>
<p>As you can see, the central idea behind SGD is that the successive gradients in mini-batches should be averaged. Does any one have any valid formal source for this? is there any justification? I've not encountered to any proof till now.</p>
","2","1","28175","12588"
"82421","<p>Recently, deeplearning.ai has released its <a href=""https://www.coursera.org/specializations/generative-adversarial-networks-gans?utm_source=deeplearningai&amp;utm_medium=institutions&amp;utm_campaign=EmailBatchAdGANs"" rel=""nofollow noreferrer"">GAN specialization on Coursera</a>. It may help you.</p>
","1","2","28175","12588"
"84546","<p>Is there any specific reason that we observe in CNNs, the fully connected layers usually have the same sizes? You can verify this for many CNNs.</p>
<p>I'm aware that if, for instance, we have a vector of two components and we transform it into a three-dimensional vector space, the basis of that vector will contain still two components at most. Consequently, if we have, for instance, a layer with 100 neurons and the subsequent layer has 120 neurons, our outputs can still be described with at most 100 neurons, although they are transformed from 100-space to 120-space. The point is that this is <strong>linear</strong> algebra. In fully connected layers, we have nonlinearity other than linear calculations.</p>
","0","1","28175","12588"
"84766","<p>You can simply <a href=""https://math.stackexchange.com/a/1172480/487106"">approximate</a> <span class=""math-container"">$f(x)=|x|$</span> by <span class=""math-container"">$f(x)=\sqrt{x^2+c}$</span> where <span class=""math-container"">$c&gt;0$</span>.
You can also utilize <a href=""https://en.wikipedia.org/wiki/Subderivative"" rel=""nofollow noreferrer"">subderivative</a> method.</p>
","3","2","28175","12588"
"85474","<p>Well, after years, now I really know why we shuffle data! The idea is very simple, but I do not know why we really did not consider it.</p>
<p>For making the cost function, we are explicitly considering that the samples are i.i.d. For instance, in binary cross-entropy, you can easily see that we have a summation. That summation has been a product at first, and after taking the logarithm, it has been changed to sum. Actually, in the formulation of that cost function, we have discarded the joint probability, because it is difficult to compute. With i.i.d assumption, we have the current cost function.
Now suppose our task is learning with different mini-batches and these mini-batches are not identical.</p>
","0","2","28175","12588"
"85947","<p>If I want to tell you, both are based on a same concept, and that is <strong>weight sharing</strong>. It is better to think about them in this way.</p>
<p>In CNNs, we try to find similar patterns throughout the input which can be image, text, or other things. This can be done by sliding a same filter that its parameters do not change while scanning the image to find similar patterns. Due to the fact that an image can have multiple patterns, we employ this idea with multiple filters.</p>
<p>On the other hand, In RNNs, the idea is that we want to find a similar patterns throughout the sequence. For instance, wherever you see a cat in a sentence, it is still a cat. Consequently, it should not matter where you see it. RNNs also employ the idea of weight sharing. They use a network that faces each input in the sequence separately to see whether a similar pattern is observed or not. However, there is a difference between CNNs and RNNs. The input of RNNs is a sequence, and the order <em>matters</em>. Consequently, at each time step, the RNN encounters the input of that time step, and it also receives some information from the past.</p>
<p>You can see that these intuitions are different than yours.</p>
","1","2","28175","12588"
"85990","<p>Actually, it is not possible that leaky ReLU suffers from a dying problem unless the slope of the negative inputs is so small. Try to use something like 0.2 for the slope of the negative part.</p>
<p>On the other hand, I believe your problem is not dying ReLU, because if it happens, almost all of your activations will output zero. My answer is basically telling you that your outputs may be so small and they change so slowly.</p>
","1","2","28175","12588"
"86682","<p>There are different reasons. First of all, there is a general rule. Try to initialize your weights to be near zero, but avoid setting them to values that are too small. If you normalize your inputs and initialize your weights in this way, your cost function will somehow be a rounded cost function and it will be elongated.</p>
<p>One way is to sample from the uniform and another way is to sample from a Gaussian. Uniform picks values in a range with the same probability, while the Gaussian chooses values near mean, zero, with more probability. Consequently, the Gaussian is better.</p>
<p>One of the drawbacks of the usual Gaussian is that large values can be selected. In large networks, there are many parameters. So we may pick many weights with large values. large weight values are not good. They will lead to overfitting and will slow down the training process. So, people try to resample if a large value appears. To do so, they specify a threshold for the numbers. This approach is called the truncated method which can be applied to Gaussian distribution.</p>
","0","2","28175","12588"
"86724","<p>There are different approaches for your case, but I believe you can employ approaches that are based on the self-attention mechanism. There numerous studies that show they perform better than the latent outcomes that can be achieved with SVD. For instance, you can see the paper of <a href=""https://github.com/seongjunyun/SAIN"" rel=""nofollow noreferrer"">SAIN</a> which its code is also open source. In the paper, you can see that they have shown their approach has better performance than SVD and usual self-attention mechanisim. By the way, it is based on self-attention.</p>
<p>If I want to summarize the method, it uses the contents of the products and the items and finds the interactions between the words, and it outputs the score that a user may assign to a product. It has good performance.</p>
<p>Due to the fact that you have the contents of mails, this approach may help you.</p>
","0","2","28175","12588"
"87488","<p><strong>It can be shown that any function can be approximated using multi-layer networks that are fully connected and have nonlinear activations.</strong> In your case, if you add one more fully connected layer other than the current ones, you can achieve a better outcome. People usually add two hidden fully connected layers after convolutional layers and before the output layer. The reason is that convolutional layers try to extract features in a differentiable manner, and fully connected layers try to classify the features. Consequently, adding more layers to the dense section can empower your network's ability to classify the extracted features better.</p>
","0","2","28175","12588"
"87512","<p>Yes, you can employ different methods. You can employ deep learning models, but you should not train them from scratch. You should employ transfer learning. Due to the fact that your dataset is small, you should utilize a deep learning model that is already trained. Next, you should replace the last layer with another layer that has the same number of neurons as your classes. The connections of this newly added layer should have random weight at first. Finally, you will freeze all the weights except for the newly added layer. In this case, your model will have a nice capacity to learn your data, and it will not overfit it. You may want to see the following links:</p>
<ul>
<li><a href=""https://www.coursera.org/lecture/convolutional-neural-networks/transfer-learning-4THzO"" rel=""nofollow noreferrer"">Pr. Ng's video</a></li>
<li><a href=""https://www.coursera.org/lecture/deep-learning-reinforcement-learning/transfer-learning-part-1-txvGx"" rel=""nofollow noreferrer"">Another helpful video</a></li>
</ul>
<p>You can also utilize SVM with soft-margin to have good generalization.</p>
<p>About the number of samples, it cannot be said in advance. Moreover, for different tasks, it may be different. By the way, someone who looks at your data can easily figure out you have small dataset.</p>
","1","2","28175","12588"
"88055","<p>Oh, well, let's say it depends on your task and model. For instance, in autoencoders, there are two main choices for upsampling. You can employ transposed convolution or rescaling. The better choice is the latter case due to the fact that the former can lead to checker-board artifacts. About zero padding, it is usually done in almost all CNNs if you use the same convolution which means the height and width of the output activation maps should be equal to the size of their input counterparts. Zero padding is usually done when you want to extract features from inputs with conv layers. The main reason we use zero padding is that the boundary locations in the inputs of the conv layers affect more entries in the output activation map.</p>
","0","2","28175","12588"
"88898","<blockquote>
<p>Doesn't ConvNets allow parameters to be shared, detecting the similar features of different images?</p>
</blockquote>
<p>Your final statement holds true for convolutional layers which are in early layers, but final layers detect more abstract features, e.g. a full object. Consequently, the last layers of a CNN that is trained with images of flowers will not be helpful for a dataset that is full of cars. Their high-level features differ. In transfer learning, there are different scenarios depending on the size of the datasets and the similarity of the datasets.</p>
<p>By the way, the last layers in conv nets see a wider range, and first layers access to limited regions simultaneously.</p>
","0","2","28175","12588"
"89627","<p>Well, yes there are. One of the applications of GANs is image inpainting which is widely used in Photoshop-like applications. You can omit an obstacle that prohibits you to see the entire image and use GANs to see, generate, the original object without any other disturbing objects that may not let you see the desired object entirely.</p>
","0","2","28175","12588"
"92537","<p>After taking a look at your code, it seems that you've not employed any kind of regularization. You may want to use dropout. Moreover, in convolutional autoencoders, in the decoder part, there is a <a href=""https://distill.pub/2016/deconv-checkerboard/"" rel=""nofollow noreferrer"">well-known artifact called checkerboard</a>. I don't know how this can be a problem for your task since you're using one-dimensional convolution in the decoder. By the way, I guess using dropout will suffice. Try to use both in decoder and encoder.</p>
","2","2","28175","12588"
"25749","<p>This question already has so many answers but I guess it will be very useful for machine-learning practitioners to use so many <code>CSV</code> files that are <a href=""http://iranopendata.org/en/datasets/"" rel=""nofollow noreferrer"">here</a>.</p>
","2","2","28175","12588"
"26464","<p>Although you need book, I recommend the following courses respectively for understanding statistics which are used for machine learning and other tasks in data science. They are free.</p>

<ul>
<li><a href=""https://www.udacity.com/course/intro-to-statistics--st101"" rel=""nofollow noreferrer"">Learn Statistics - Intro to Statistics Course</a></li>
<li><a href=""https://www.udacity.com/course/intro-to-descriptive-statistics--ud827"" rel=""nofollow noreferrer"">Intro to Descriptive Statistics</a></li>
<li><a href=""https://www.udacity.com/course/intro-to-inferential-statistics--ud201"" rel=""nofollow noreferrer"">Inferential Statistics: Learn Statistical Analysis</a></li>
</ul>

<p>If I want to recommend a book, I would recommend the following book which is free under <em>CC license</em>. It has nice examples and is so much practical; moreover, there are lots of codes in it which help you feel statistics in real world examples. </p>

<ul>
<li><p><a href=""http://greenteapress.com/thinkstats2/index.html"" rel=""nofollow noreferrer"">Think Python by Allen B. Downey</a></p></li>
<li><p><a href=""https://jakevdp.github.io/PythonDataScienceHandbook/"" rel=""nofollow noreferrer"">Python Data Science Handbook</a></p></li>
</ul>

<p>Also the following link may help: </p>

<ul>
<li><a href=""https://www.blog.google/topics/machine-learning/learn-google-ai-making-ml-education-available-everyone/amp/"" rel=""nofollow noreferrer"">From Google Itself Good And Concise</a></li>
</ul>
","10","2","28175","12588"
"25318","<p><em><a href=""http://tflearn.org/"" rel=""noreferrer"">TFlearn</a> is a modular and transparent deep learning library built on top of Tensorflow. It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations, while remaining fully transparent and compatible with it</em>. Even with TensorFlow, however, we face a choice of which “front-end” framework to use. Should we use straight TensorFlow, or TF Learn, or Keras, or the new TF-Slim library that Google released within TensorFlow.</p>

<p><em><a href=""https://keras.io/"" rel=""noreferrer"">Keras</a> is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.</em></p>

<p>Straight <code>TensorFlow</code> is really verbose while <code>Keras</code> and <code>TfLearn</code> both seem solid, but the <code>TfLearn</code> syntax seems a little cleaner. One drawback to Tflearn is the lack of easily integrated pre-trained models. </p>

<p>Actually there are so many answers for your question <a href=""https://www.reddit.com/r/MachineLearning/comments/50eokb/which_one_should_i_choose_keras_tensorlayer/"" rel=""noreferrer"">here</a> and <a href=""https://www.quora.com/session/Fran%C3%A7ois-Chollet/1"" rel=""noreferrer"">here</a> and I quote some of them here.</p>

<p><em>TensorFlow is currently the mainstream of deep learning framework, they are all the wrapper of TF. Whereas, Keras was released at the age of Theano, and therefore having a good support from Theano’s users. While TensorLayer and TFLearn are both released after TensorFlow. A good reason to choose Keras is that you could use TensorFlow backend without actually learning it. Plus Keras tends to wrap up the model deeply, so you don't necessarily need to consider the backend to be Theano or TF, which is a big advantage of Keras.</em> </p>

<p><em>It depend on what do you want to do, fast prototyping or something else?</em></p>

<p><em>Keras : Many people are using it, easy to find examples on github. Suitable for beginner. Capable of running on top of either TensorFlow or Theano. Tflearn : Why no one discuss it? It is also a famous library, transparent over TensorFlow. High running speed. TensorLayer: Just release (Sep 2016), transparent over TensorFlow. High running speed. Easy to extend, suitable for professional, its tutorial include all modularized implementation of Google TensorFlow Deep Learning tutorial. TF-Silm: Just release (Aug 2016) similar with Tflearn, but no RNN layer at the moment (Sep 2016).</em></p>

<p><em>The best deep learning framework is the one you know best.</em></p>
","16","2","28175","12588"
"42621","<p>It has been customary for the users of different <a href=""https://stats.stackexchange.com/q/1337/179078"">communities</a> to quote funny things about their fields. It may be fun to share your funny things about Machine Learning, Deep Learning, Data Science and the things that you face every day! </p>
","41","1","28175","12588"
"42622","<p>A: What is machine learning sir?
B: It is not machine learning! It is machine burning, man.</p>
<hr />
<p><a href=""https://i.stack.imgur.com/8wu39.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8wu39.png"" alt=""enter image description here"" /></a></p>
<p>by Davide Mazzini</p>
","4","2","28175","12588"
"40881","<p>I'll attempt to answer this question based on my own experience. In contrast to the other answers, I prefer <code>Sql</code> for deep learning and big-data-related things. There are numerous reasons for that. As it can be seen <a href=""http://matthewrocklin.com/blog/work/2017/01/12/dask-dataframes"" rel=""nofollow noreferrer"">here</a>, </p>

<blockquote>
  <p>Pandas provides an intuitive, powerful, and fast data analysis experience on tabular data. However, because Pandas uses only one thread of execution and requires all data to be in memory at once, it doesn’t scale well to datasets much beyond the gigabyte scale.</p>
</blockquote>

<p>Sql engines usually keep the keys or special columns in data-structures like <span class=""math-container"">$B ^+$</span> tree in order to facilitate CRUD operations. This data structure keeps the status of all data in the database. This is not pandas can do because it can't access all the data simultaneously. On the other hand, it can't do some operations even with its chunk parameter used in read_csv. As an example, you can't have direct batch operations for large datasets that your memory can't accommodate them. Any other tasks which depend on your entire dataset need extra coding. All of these can be handled in Sql without extra coding, just with a simple query. Simple Sql operations are just used without any fear about the memory. </p>

<p>Another difference is that CRUD operations in Sql can be applied distributed with different authorisation policies which are not possible in pandas. </p>

<p>It is not meant to say which is better, it all depends on your task. For large scale computation I prefer Sql and for small ones, I prefer pandas. </p>

<p>There are other things that are not in pandas which are really important for fast experience for data extraction that I'll refer to later. For now, just take a look at <a href=""https://datascience.stackexchange.com/a/17212/28175"">here</a>.</p>
","4","2","28175","12588"