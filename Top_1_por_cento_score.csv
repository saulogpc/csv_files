Post ID,Post Link,Post Body,TAGS,Post Score,Votos,Votos UP,Votos DOWN,User ID,User Reputation
"24051","{
  ""id"": 24051,
  ""title"": ""Micro Average vs Macro average Performance in a Multiclass classification setting""
}","<p>Micro- and macro-averages (for whatever metric) will compute slightly different things, and thus their interpretation differs. A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance (i.e you may have many more examples of one class than of other classes). </p>

<p>To illustrate why, take for example precision $Pr=\frac{TP}{(TP+FP)}$. Let's imagine you have a <em>One-vs-All</em> (there is only one correct class output per example) multi-class classification system with four classes and the following numbers when tested:</p>

<ul>
<li>Class A: 1 TP and 1 FP</li>
<li>Class B: 10 TP and 90 FP</li>
<li>Class C: 1 TP and 1 FP</li>
<li>Class D: 1 TP and 1 FP</li>
</ul>

<p>You can see easily that $Pr_A = Pr_C = Pr_D = 0.5$, whereas $Pr_B=0.1$.</p>

<ul>
<li>A macro-average will then compute: $Pr=\frac{0.5+0.1+0.5+0.5}{4}=0.4$</li>
<li>A micro-average will compute: $Pr=\frac{1+10+1+1}{2+100+2+2}=0.123$</li>
</ul>

<p>These are quite different values for precision. Intuitively, in the macro-average the ""good"" precision (0.5) of classes A, C and D is contributing to maintain a ""decent"" overall precision (0.4). While this is technically true (across classes, the average precision is 0.4), it is a bit misleading, since a large number of examples are not properly classified. These examples predominantly correspond to class B, so they only contribute 1/4 towards the average in spite of constituting 94.3% of your test data. The micro-average will adequately capture this class imbalance, and bring the overall precision average down to 0.123 (more in line with the precision of the dominating class B (0.1)).</p>

<p>For computational reasons, it may sometimes be more convenient to compute class averages and then macro-average them. If class imbalance is known to be an issue, there are several ways around it. One is to report not only the macro-average, but also its standard deviation (for 3 or more classes). Another is to compute a weighted macro-average, in which each class contribution to the average is weighted by the relative number of examples available for it. In the above scenario, we obtain:</p>

<p>$Pr_{macro-mean}={0.25·0.5+0.25·0.1+0.25·0.5+0.25·0.5}=0.4$
$Pr_{macro-stdev}=0.173$</p>

<p>$Pr_{macro-weighted}={0.0189·0.5+0.943·0.1+0.0189·0.5+0.0189·0.5}={0.009+0.094+0.009+0.009}=0.123$</p>

<p>The large standard deviation (0.173) already tells us that the 0.4 average does not stem from a uniform precision among classes, but it might be just easier to compute the weighted macro-average, which in essence is another way of computing the micro-average.</p>
","<multiclass-classification><evaluation>","352","354","353","1","40949","3629"
"12110","{
  ""id"": 12110,
  ""title"": ""What are deconvolutional layers?""
}","<p><em>Deconvolution layer</em> is a very unfortunate name and should rather be called a <a href=""https://github.com/tensorflow/tensorflow/issues/256#issuecomment-162257789"" rel=""noreferrer""><em>transposed convolutional layer</em></a>.</p>

<p>Visually, for a transposed convolution with stride one and no padding, we just pad the original input (blue entries) with zeroes (white entries) (Figure 1).</p>

<p><a href=""https://i.stack.imgur.com/YyCu2.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/YyCu2.gif"" alt=""Figure 1""></a></p>

<p>In case of stride two and padding, the transposed convolution would look like this (Figure 2): </p>

<p><a href=""https://i.stack.imgur.com/f2RiP.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/f2RiP.gif"" alt=""Figure 2""></a></p>

<p>All credits for the great visualisations go to</p>

<ul>
<li>Vincent Dumoulin, Francesco Visin - <a href=""https://arxiv.org/pdf/1603.07285v1.pdf"" rel=""noreferrer"">A guide to convolution arithmetic for deep learning</a></li>
</ul>

<p>You can find more visualisations of convolutional arithmetics <a href=""https://github.com/vdumoulin/conv_arithmetic"" rel=""noreferrer"">here</a>.</p>
","<neural-network><convnet><convolution>","279","279","279","0","14578","3070"
"12346","{
  ""id"": 12346,
  ""title"": ""What's the difference between fit and fit_transform in scikit-learn models?""
}","<p>To <a href=""https://en.wikipedia.org/wiki/Standard_score"" rel=""nofollow noreferrer"">center the data</a> (make it have zero mean and unit standard error), you subtract the mean and then divide the result by the standard deviation:</p>
<p><span class=""math-container"">$$x' = \frac{x-\mu}{\sigma}$$</span></p>
<p>You do that on the training set of the data. But then you have to apply the same transformation to your test set (e.g. in cross-validation), or to newly obtained examples before forecasting. But you have to use the exact same two parameters <span class=""math-container"">$\mu$</span> and <span class=""math-container"">$\sigma$</span> (values) that you used for centering the training set.</p>
<p>Hence, every scikit-learn's transform's <code>fit()</code> just calculates the parameters (e.g. <span class=""math-container"">$\mu$</span> and <span class=""math-container"">$\sigma$</span> in case of <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"" rel=""nofollow noreferrer"">StandardScaler</a>) and saves them as an internal object's state. Afterwards, you can call its <code>transform()</code> method to apply the transformation to any particular set of examples.</p>
<p><code>fit_transform()</code> joins these two steps and is used for the initial fitting of parameters on the training set <span class=""math-container"">$x$</span>, while also returning the transformed <span class=""math-container"">$x'$</span>. Internally, the transformer object just <a href=""https://github.com/scikit-learn/scikit-learn/blob/6c3e17989a7d80c34f124365f2c436a3fdcb1497/sklearn/base.py#L659-L690"" rel=""nofollow noreferrer"">calls first <code>fit()</code> and then <code>transform()</code></a> on the same data.</p>
","<python><scikit-learn>","218","218","218","0","15527","3142"
"37879","{
  ""id"": 37879,
  ""title"": ""Difference between isna() and isnull() in pandas""
}","<h3>Pandas <code>isna()</code> vs <code>isnull()</code>.</h3>

<p>I'm assuming you are referring to <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isna.html"" rel=""noreferrer""><code>pandas.DataFrame.isna()</code></a> vs <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isnull.html#pandas.DataFrame.isnull"" rel=""noreferrer""><code>pandas.DataFrame.isnull()</code></a>. Not to confuse with <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html"" rel=""noreferrer""><code>pandas.isnull()</code></a>, which in contrast to the two above isn't a method of the DataFrame class.</p>

<p>These two DataFrame methods do <strong>exactly</strong> the same thing! Even their docs are identical. You can even confirm this in pandas' <a href=""https://github.com/pandas-dev/pandas/blob/537b65cb0fd2aa318e089c5e38f09e81d1a3fe35/pandas/core/dtypes/missing.py#L109"" rel=""noreferrer"">code</a>.</p>

<h3>But why have two methods with different names do the same thing?</h3>

<p>This is because pandas' DataFrames are based on R's DataFrames. In R <code>na</code> and <code>null</code> are two separate things. Read <a href=""https://www.r-bloggers.com/r-na-vs-null/"" rel=""noreferrer"">this post</a> for more information.</p>

<p>However, in python, pandas is built on top of numpy, which has <strong>neither <code>na</code> nor <code>null</code></strong> values. Instead numpy has <strong><code>NaN</code></strong> values (which stands for ""Not a Number""). Consequently, pandas also uses <code>NaN</code> values.</p>

<h3>In short</h3>

<ul>
<li><p>To detect <code>NaN</code> values numpy uses <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.isnan.html"" rel=""noreferrer""><code>np.isnan()</code></a>.</p></li>
<li><p>To detect <code>NaN</code> values pandas uses either <code>.isna()</code> or <code>.isnull()</code>.<br>
The <code>NaN</code> values are inherited from the fact that pandas is built on top of numpy, while the two functions' names originate from R's DataFrames, whose structure and functionality pandas tried to mimic.</p></li>
</ul>
","<python><pandas><dataframe>","204","204","204","0","34269","7013"
"18722","{
  ""id"": 18722,
  ""title"": ""How to set class weights for imbalanced classes in Keras?""
}","<p>You could simply implement the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html"" rel=""noreferrer""><code>class_weight</code></a> from <code>sklearn</code>:</p>

<ol>
<li><p>Let's import the module first</p>

<pre><code>from sklearn.utils import class_weight
</code></pre></li>
<li><p>In order to calculate the class weight do the following</p>

<pre><code>class_weights = class_weight.compute_class_weight('balanced',
                                                 np.unique(y_train),
                                                 y_train)
</code></pre></li>
<li><p>Thirdly and lastly add it to the model fitting</p>

<pre><code>model.fit(X_train, y_train, class_weight=class_weights)
</code></pre></li>
</ol>

<p><strong>Attention</strong>: I edited this post and changed the variable name from <em>class_weight</em> to <em>class_weight<strong>s</em></strong> in order to not to overwrite the imported module. Adjust accordingly when copying code from the comments.</p>
","<deep-learning><classification><keras><weighted-data>","200","200","200","0","31846","2151"
"9447","{
  ""id"": 9447,
  ""title"": ""When to use One Hot Encoding vs LabelEncoder vs DictVectorizor?""
}","<p>There are some cases where <code>LabelEncoder</code> or <code>DictVectorizor</code> are useful, but these are quite limited in my opinion due to ordinality.</p>
<p><code>LabelEncoder</code> can turn <code>[dog,cat,dog,mouse,cat]</code> into <code>[1,2,1,3,2]</code>, but then the imposed ordinality means that the average of dog and mouse is cat. Still there are algorithms like decision trees and random forests that can work with categorical variables just fine and <code>LabelEncoder</code> can be used to store values using less disk space.</p>
<p>One-Hot-Encoding has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space.  The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality. In these cases, I typically employ one-hot-encoding followed by PCA for dimensionality reduction.  I find that the judicious combination of one-hot plus PCA can seldom be beat by other encoding schemes.  PCA finds the linear overlap, so will naturally tend to group similar features into the same feature.</p>
","<scikit-learn><categorical-data><feature-engineering>","194","194","194","0","9420","6528"
"13496","{
  ""id"": 13496,
  ""title"": ""How to set class weights for imbalanced classes in Keras?""
}","<p>If you are talking about the regular case, where your network produces only one output, then your assumption is correct. In order to force your algorithm to treat every instance of <strong>class 1</strong> as 50 instances of <strong>class 0</strong> you have to:</p>

<ol>
<li><p>Define a dictionary with your labels and their associated weights</p>

<pre><code>class_weight = {0: 1.,
                1: 50.,
                2: 2.}
</code></pre></li>
<li><p>Feed the dictionary as a parameter:</p>

<pre><code>model.fit(X_train, Y_train, nb_epoch=5, batch_size=32, class_weight=class_weight)
</code></pre></li>
</ol>

<p>EDIT:
""treat every instance of <strong>class 1</strong> as 50 instances of <strong>class 0</strong>"" means that in your loss function you assign higher value to these instances.
Hence, the loss becomes a weighted average, where the weight of each sample is specified by <strong>class_weight</strong> and its corresponding class. </p>

<p>From <a href=""https://keras.io/models/sequential/"" rel=""noreferrer"">Keras docs</a>: </p>

<blockquote>
  <p><strong>class_weight</strong>: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). </p>
</blockquote>
","<deep-learning><classification><keras><weighted-data>","187","187","187","0","23478","1986"
"24","{
  ""id"": 24,
  ""title"": ""K-Means clustering for mixed numeric and categorical data""
}","<p>The standard k-means algorithm isn't directly applicable to categorical data, for various reasons.  The sample space for categorical data is discrete, and doesn't have a natural origin.  A Euclidean distance function on such a space isn't really meaningful.  As someone put it, ""The fact a snake possesses neither wheels nor legs allows us to say nothing about the relative value of wheels and legs."" (from <a href=""http://www.daylight.com/meetings/mug04/Bradshaw/why_k-modes.html"">here</a>)</p>

<p>There's a variation of k-means known as k-modes, introduced in <a href=""http://www.cs.ust.hk/~qyang/Teaching/537/Papers/huang98extensions.pdf"">this paper</a> by Zhexue Huang, which is suitable for categorical data.   Note that the solutions you get are sensitive to initial conditions, as discussed <a href=""http://arxiv.org/ftp/cs/papers/0603/0603120.pdf"">here</a> (PDF), for instance.</p>

<p>Huang's paper (linked above) also has a section on ""k-prototypes"" which applies to data with a mix of categorical and numeric features.  It uses a distance measure which mixes the Hamming distance for categorical features and the Euclidean distance for numeric features.</p>

<p>A Google search for ""k-means mix of categorical data"" turns up quite a few more recent papers on various algorithms for k-means-like clustering with a mix of categorical and numeric data.  (I haven't yet read them, so I can't comment on their merits.)  </p>

<hr>

<p>Actually, what you suggest (converting categorical attributes to binary values, and then doing k-means as if these were numeric values) is another approach that has been tried before (predating k-modes).  (See Ralambondrainy, H. 1995. A conceptual version of the k-means algorithm. Pattern Recognition Letters, 16:1147–1157.)  But I believe the k-modes approach is preferred for the reasons I indicated above.</p>
","<data-mining><clustering><octave><k-means><categorical-data>","169","169","169","0","14","2942"
"5734","{
  ""id"": 5734,
  ""title"": ""What is the \""dying ReLU\"" problem in neural networks?""
}","<p>A ""dead"" ReLU always outputs the same value (zero as it happens, but that is not important) for any input. Probably this is arrived at by learning a large negative bias term for its weights.</p>

<p>In turn, that means that it takes no role in discriminating between inputs. For classification, you could visualise this as a decision plane <em>outside</em> of all possible input data.</p>

<p>Once a ReLU ends up in this state, it is unlikely to recover, because the function gradient at 0 is also 0, so gradient descent learning will not alter the weights. ""Leaky"" ReLUs with a small positive gradient for negative inputs (<code>y=0.01x</code> when x &lt; 0 say) are one attempt to address this issue and give a chance to recover.</p>

<p>The sigmoid and tanh neurons can suffer from similar problems as their values saturate, but there is always at least a small gradient allowing them to recover in the long term.</p>
","<machine-learning><neural-network><deep-learning>","167","167","167","0","836","25808"
"15136","{
  ""id"": 15136,
  ""title"": ""Train/Test/Validation Set Splitting in Sklearn""
}","<p>You could just use <code>sklearn.model_selection.train_test_split</code> twice. First to split to train, test and then split train again into validation and train. Something like this:</p>

<pre><code> X_train, X_test, y_train, y_test 
    = train_test_split(X, y, test_size=0.2, random_state=1)

 X_train, X_val, y_train, y_val 
    = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2
</code></pre>
","<machine-learning><scikit-learn><cross-validation>","167","167","167","0","26146","1982"
"9431","{
  ""id"": 9431,
  ""title"": ""What is the \""dying ReLU\"" problem in neural networks?""
}","<p>Let's review how the ReLU (Rectified Linear Unit) looks like :
<a href=""https://i.stack.imgur.com/8CGlM.png""><img src=""https://i.stack.imgur.com/8CGlM.png"" alt=""""></a></p>

<p>The input to the rectifier for some input $x_n$ is 
$$z_n=\sum_{i=0}^k w_i a^n_i$$ for weights $w_i$, and activations from the previous layer $a^n_i$ for that particular input $x_n$. The rectifier neuron function is $ReLU = max(0,z_n)$</p>

<p>Assuming a very simple error measure </p>

<p>$$error = ReLU - y$$</p>

<p>the rectifier has only 2 possible gradient values for the deltas of backpropagation algorithm:
$$\frac{\partial error}{\partial z_n} = \delta_n = \left\{
\begin{array}{c l}     
    1 &amp; z_n \geq 0\\
    0 &amp; z_n &lt; 0
\end{array}\right.$$
(if we use a proper error measure, then the 1 will become something else, but the 0 will stay the same)
and so for a certain weight $w_j$ :
$$\nabla error = \frac{\partial error}{\partial w_j}=\frac{\partial error}{\partial z_n} \times \frac{\partial z_n}{\partial w_j} = \delta_n \times a_j^n = \left\{
\begin{array}{c 1}
    a_j^n &amp; z_n \geq 0\\
    0 &amp; z_n &lt; 0
\end{array}\right.$$</p>

<p>One question that comes to mind is how actually ReLU works ""at all"" with the gradient $=$ 0 on the left side. What if, for the input $x_n$, the current weights put the ReLU on the left flat side while it optimally should be on the right side for this particular input ? The gradient is 0 and so the weight will not be updated, not even a tiny bit, so where is ""learning"" in this case? </p>

<p>The essence of the answer lies in the fact that Stochastic Gradient Descent will not only consider a single input $x_n$, but many of them, and the hope is that not all inputs will put the ReLU on the flat side, so the gradient will be non-zero for <strong>some</strong> inputs (it may be +ve or -ve though). If at least one input $x_*$ has our ReLU on the steep side, then the ReLU is still <strong>alive</strong>  because there's still learning going on and weights getting updated for this neuron. If all inputs put the ReLU on the flat side, there's no hope that the weights change at all and the neuron is <strong>dead</strong>.</p>

<p>A ReLU may be alive then die due to the gradient step for some <strong>input batch</strong> driving the weights to smaller values, making $z_n &lt; 0$ for all inputs. A large learning rate amplifies this problem.</p>

<p>As @Neil Slater mentioned, a fix is to modify the flat side to have a small gradient, so that it becomes $ReLU=max(0.1x,x)$ as below, which is called LeakyReLU.
<a href=""https://i.stack.imgur.com/LZk6i.png""><img src=""https://i.stack.imgur.com/LZk6i.png"" alt=""enter image description here""></a></p>
","<machine-learning><neural-network><deep-learning>","131","131","131","0","860","1751"
"695","{
  ""id"": 695,
  ""title"": ""Best python library for neural networks""
}","<p>UPDATE: the landscape has changed quite a bit since I answered this question in July '14, and some new players have entered the space.  In particular, I would recommend checking out:</p>

<ul>
<li><a href=""https://github.com/tensorflow/tensorflow"" rel=""noreferrer"">TensorFlow</a></li>
<li><a href=""https://github.com/mila-udem/blocks"" rel=""noreferrer"">Blocks</a></li>
<li><a href=""https://github.com/Lasagne/Lasagne"" rel=""noreferrer"">Lasagne</a></li>
<li><a href=""https://github.com/fchollet/keras"" rel=""noreferrer"">Keras</a></li>
<li><a href=""https://github.com/uaca/deepy"" rel=""noreferrer"">Deepy</a></li>
<li><a href=""https://github.com/dnouri/nolearn"" rel=""noreferrer"">Nolearn</a></li>
<li><a href=""https://github.com/itdxer/neupy"" rel=""noreferrer"">NeuPy</a></li>
</ul>

<p>They each have their strengths and weaknesses, so give them all a go and see which best suits your use case.  Although I would have recommended using PyLearn2 a year ago, the community is no longer active so I would recommend looking elsewhere.  My original response to the answer is included below but is largely irrelevant at this point.</p>

<hr>

<p><a href=""http://deeplearning.net/software/pylearn2/"" rel=""noreferrer"">PyLearn2</a> is generally considered the library of choice for neural networks and deep learning in python.  It's designed for easy scientific experimentation rather than ease of use, so the learning curve is rather steep, but if you take your time and follow the tutorials I think you'll be happy with the functionality it provides.  Everything from standard Multilayer Perceptrons to Restricted Boltzmann Machines to Convolutional Nets to Autoencoders is provided.  There's great GPU support and everything is built on top of Theano, so performance is typically quite good.  The source for PyLearn2 is available <a href=""https://github.com/lisa-lab/pylearn2"" rel=""noreferrer"">on github</a>.</p>

<p>Be aware that PyLearn2 has the opposite problem of PyBrain at the moment -- rather than being abandoned, PyLearn2 is under active development and is subject to frequent changes.</p>
","<machine-learning><python><neural-network>","130","132","131","1","684","1969"
"9408","{
  ""id"": 9408,
  ""title"": ""The cross-entropy error function in neural networks""
}","<p>One way to interpret cross-entropy is to see it as a (minus) log-likelihood for the data <span class=""math-container"">$y_i'$</span>, under a model <span class=""math-container"">$y_i$</span>.</p>

<p>Namely, suppose that you have some fixed model (a.k.a. ""hypothesis""), which predicts for <span class=""math-container"">$n$</span> classes <span class=""math-container"">$\{1,2,\dots, n\}$</span> their hypothetical occurrence probabilities <span class=""math-container"">$y_1, y_2,\dots, y_n$</span>. Suppose that you now observe (in reality) <span class=""math-container"">$k_1$</span> instances of class <span class=""math-container"">$1$</span>, <span class=""math-container"">$k_2$</span> instances of class <span class=""math-container"">$2$</span>, <span class=""math-container"">$k_n$</span> instances of class <span class=""math-container"">$n$</span>, etc. According to your model the likelihood of this happening is:
<span class=""math-container"">$$
P[data|model] := y_1^{k_1}y_2^{k_2}\dots y_n^{k_n}.
$$</span>
Taking the logarithm and changing the sign:
<span class=""math-container"">$$
-\log P[data|model] = -k_1\log y_1 -k_2\log y_2 - \dots -k_n\log y_n = -\sum_i k_i \log y_i
$$</span>
If you now divide the right-hand sum by the number of observations <span class=""math-container"">$N = k_1+k_2+\dots+k_n$</span>, and denote the empirical probabilities as <span class=""math-container"">$y_i'=k_i/N$</span>, you'll get the cross-entropy:
<span class=""math-container"">$$
-\frac{1}{N} \log P[data|model] = -\frac{1}{N}\sum_i k_i \log y_i = -\sum_i y_i'\log y_i =: H(y', y)
$$</span></p>

<p>Furthermore, the log-likelihood of a dataset given a model can be interpreted as a measure of ""encoding length"" - the number of bits you expect to spend to encode this information if your encoding scheme would be based on your hypothesis. </p>

<p>This follows from the observation that an independent event with probability <span class=""math-container"">$y_i$</span> requires at least <span class=""math-container"">$-\log_2 y_i$</span> bits to encode it (assuming efficient coding), and consequently the expression 
<span class=""math-container"">$$-\sum_i y_i'\log_2 y_i,$$</span>
is literally the expected length of the encoding, where the encoding lengths for the events are computed using the ""hypothesized"" distribution, while the expectation is taken over the actual one.</p>

<p>Finally, instead of saying ""measure of expected encoding length"" I really like to use the informal term ""measure of surprise"". If you need a lot of bits to encode an expected event from a distribution, the distribution is ""really surprising"" for you.</p>

<p>With those intuitions in mind, the answers to your questions can be seen as follows:</p>

<ul>
<li><p><strong>Question 1</strong>. Yes. It is a problem <em>whenever the corresponding <span class=""math-container"">$y_i'$</span> is nonzero at the same time</em>. It corresponds to the situation where your model believes that some class has zero probability of occurrence, and yet the class pops up in reality. As a result, the ""surprise"" of your model is infinitely great: your model did not account for that event and now needs infinitely many bits to encode it. That is why you get infinity as your cross-entropy. </p>

<p>To avoid this problem you need to make sure that your model does not make rash assumptions about something being impossible while it can happen. In reality, people tend to use sigmoid or ""softmax"" functions as their hypothesis models, which are conservative enough to leave at least some chance for every option.</p>

<p>If you use some other hypothesis model, it is up to you to <em>regularize</em> (aka ""smooth"") it so that it would not hypothesize zeros where it should not.</p></li>
<li><p><strong>Question 2</strong>. In this formula, one usually assumes <span class=""math-container"">$y_i'$</span> to be either <span class=""math-container"">$0$</span> or <span class=""math-container"">$1$</span>, while <span class=""math-container"">$y_i$</span> is the model's probability hypothesis for the corresponding input. If you look closely, you will see that it is simply a <span class=""math-container"">$-\log P[data|model]$</span> for binary data, an equivalent of the second equation in this answer.</p>

<p>Hence, strictly speaking, although it is still a log-likelihood, this is not syntactically equivalent to cross-entropy. What some people mean when referring to such an expression as <em>cross-entropy</em> is that it is, in fact, a <em>sum</em> over binary cross-entropies for individual points in the dataset:
<span class=""math-container"">$$
\sum_i H(y_i', y_i),
$$</span>
where <span class=""math-container"">$y_i'$</span> and <span class=""math-container"">$y_i$</span> have to be interpreted as the corresponding binary distributions <span class=""math-container"">$(y_i', 1-y_i')$</span> and <span class=""math-container"">$(y_i, 1-y_i)$</span>.</p></li>
</ul>
","<machine-learning><tensorflow>","121","121","121","0","14519","2021"
"36797","{
  ""id"": 36797,
  ""title"": ""How to draw Deep learning network architecture diagrams?""
}","<p>I recently found <a href=""http://alexlenail.me/NN-SVG/LeNet.html"" rel=""noreferrer"">this online tool</a> that produces publication-ready NN-architecture schematics. It is called <a href=""http://alexlenail.me/NN-SVG/LeNet.html"" rel=""noreferrer"">NN-SVG</a> and made by <a href=""http://alexlenail.me"" rel=""noreferrer"">Alex Lenail</a>.</p>

<p>You can easily export these to use in, say, <strong>LaTeX</strong> for example.</p>

<p>Here are a few examples:</p>

<p><em>AlexNet style</em>
<a href=""https://i.stack.imgur.com/Q0xOe.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Q0xOe.png"" alt=""AlexNet style""></a></p>

<hr>

<p><em>LeNet style</em>
<a href=""https://i.stack.imgur.com/K9lmg.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/K9lmg.png"" alt=""enter image description here""></a></p>

<hr>

<p>and the good old <em>Fully Connected</em> style
<a href=""https://i.stack.imgur.com/DlJ8J.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DlJ8J.png"" alt=""enter image description here""></a></p>
","<machine-learning><neural-network><deep-learning><svm><software-recommendation>","112","112","112","0","10430","1221"
"898","{
  ""id"": 898,
  ""title"": ""How to get correlation between two categorical variable and a categorical variable and continuous variable?""
}","<h2>Two Categorical Variables</h2>

<p>Checking if two categorical variables are independent can be done with Chi-Squared test of independence. </p>

<p>This is a typical <a href=""https://en.wikipedia.org/wiki/Chi-square_test"" rel=""noreferrer"">Chi-Square test</a>: if we assume that two variables are independent, then the values of the contingency table for these variables should be distributed uniformly. And then we check how far away from uniform the actual values are.</p>

<p>There also exists a <a href=""http://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V"" rel=""noreferrer"">Crammer's V</a> that is a measure of correlation that follows from this test</p>

<h3>Example</h3>

<p>Suppose we have two variables</p>

<ul>
<li>gender: male and female</li>
<li>city: Blois and Tours</li>
</ul>

<p>We observed the following data:</p>

<p><img src=""https://i.stack.imgur.com/zcCfV.png"" alt=""observed values""> </p>

<p>Are gender and city independent? Let's perform a Chi-Squred test. Null hypothesis: they are independent, Alternative hypothesis is that they are correlated in some way. </p>

<p>Under the Null hypothesis, we assume uniform distribution. So our expected values are the following</p>

<p><img src=""https://i.stack.imgur.com/H8bKJ.png"" alt=""expected value""></p>

<p>So we run the chi-squared test and the resulting p-value here can be seen as a measure of correlation between these two variables.</p>

<p>To compute Crammer's V we first find the normalizing factor chi-squared-max which is typically the size of the sample, divide the chi-square by it and take a square root </p>

<p><img src=""https://i.stack.imgur.com/v7HY6.png"" alt=""crammers v""></p>

<h3>R</h3>

<pre><code>tbl = matrix(data=c(55, 45, 20, 30), nrow=2, ncol=2, byrow=T)
dimnames(tbl) = list(City=c('B', 'T'), Gender=c('M', 'F'))

chi2 = chisq.test(tbl, correct=F)
c(chi2$statistic, chi2$p.value)
</code></pre>

<p>Here the p value is 0.08 - quite small, but still not enough to reject the hypothesis of independence. So we can say that the ""correlation"" here is 0.08</p>

<p>We also compute V: </p>

<pre><code>sqrt(chi2$statistic / sum(tbl))
</code></pre>

<p>And get 0.14 (the smaller v, the lower the correlation) </p>

<p>Consider another dataset </p>

<pre><code>    Gender
City  M  F
   B 51 49
   T 24 26
</code></pre>

<p>For this, it would give the following</p>

<pre><code>tbl = matrix(data=c(51, 49, 24, 26), nrow=2, ncol=2, byrow=T)
dimnames(tbl) = list(City=c('B', 'T'), Gender=c('M', 'F'))

chi2 = chisq.test(tbl, correct=F)
c(chi2$statistic, chi2$p.value)

sqrt(chi2$statistic / sum(tbl))
</code></pre>

<p>The p-value is 0.72 which is far closer to 1, and v is 0.03 - very close to 0</p>

<h2>Categorical vs Numerical Variables</h2>

<p>For this type we typically perform <a href=""http://en.wikipedia.org/wiki/F_test#One-way_ANOVA_example"" rel=""noreferrer"">One-way ANOVA test</a>: we calculate in-group variance and intra-group variance and then compare them.</p>

<h3>Example</h3>

<p>We want to study the relationship between absorbed fat from donuts vs the type of fat used to produce donuts (example is taken from <a href=""http://statland.org/R/R/R1way.htm"" rel=""noreferrer"">here</a>)</p>

<p><img src=""https://i.stack.imgur.com/LMOS3.png"" alt=""donuts""></p>

<p>Is there any dependence between the variables?
For that we conduct ANOVA test and see that the p-value is just 0.007 - there's no correlation between these variables. </p>

<h3>R</h3>

<pre><code>t1 = c(164, 172, 168, 177, 156, 195)
t2 = c(178, 191, 197, 182, 185, 177)
t3 = c(175, 193, 178, 171, 163, 176)
t4 = c(155, 166, 149, 164, 170, 168)

val = c(t1, t2, t3, t4)
fac = gl(n=4, k=6, labels=c('type1', 'type2', 'type3', 'type4'))

aov1 = aov(val ~ fac)
summary(aov1)
</code></pre>

<p>Output is </p>

<pre><code>            Df Sum Sq Mean Sq F value  Pr(&gt;F)   
fac          3   1636   545.5   5.406 0.00688 **
Residuals   20   2018   100.9                   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
</code></pre>

<p>So we can take the p-value as the measure of correlation here as well.</p>

<h2>References</h2>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Chi-square_test"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Chi-square_test</a></li>
<li><a href=""http://mlwiki.org/index.php/Chi-square_Test_of_Independence"" rel=""noreferrer"">http://mlwiki.org/index.php/Chi-square_Test_of_Independence</a></li>
<li><a href=""http://courses.statistics.com/software/R/R1way.htm"" rel=""noreferrer"">http://courses.statistics.com/software/R/R1way.htm</a></li>
<li><a href=""http://mlwiki.org/index.php/One-Way_ANOVA_F-Test"" rel=""noreferrer"">http://mlwiki.org/index.php/One-Way_ANOVA_F-Test</a></li>
<li><a href=""http://mlwiki.org/index.php/Cramer%27s_Coefficient"" rel=""noreferrer"">http://mlwiki.org/index.php/Cramer%27s_Coefficient</a></li>
</ul>
","<r><statistics><correlation>","110","110","110","0","816","2610"
"158","{
  ""id"": 158,
  ""title"": ""Publicly Available Datasets""
}","<p>There is, in fact, a very reasonable list of publicly-available datasets, supported by different enterprises/sources. </p>

<p>Some of them are below:</p>

<ul>
<li><a href=""http://aws.amazon.com/publicdatasets/"" rel=""noreferrer"">Public Datasets on Amazon WebServices</a>;</li>
<li><a href=""http://fimi.ua.ac.be/data/"" rel=""noreferrer"">Frequent Itemset Mining Implementation Repository</a>;</li>
<li><a href=""http://archive.ics.uci.edu/ml/datasets.html"" rel=""noreferrer"">UCI Machine Learning Repository</a>;</li>
<li><a href=""http://www.kdnuggets.com/datasets/index.html"" rel=""noreferrer"">KDnuggets</a> -- a big list of lots of public repositories.</li>
</ul>

<p>Now, two considerations on your question. First one, regarding policies of database sharing. From personal experience, there are some databases that can't be made publicly available, either for involving privacy restraints (as for some social network information) or for concerning government information (like health system databases).</p>

<p>Another point concerns the usage/application of the dataset. Although some bases can be reprocessed to suit the needs of the application, it would be great to have some <em>nice organization</em> of the datasets by purpose. The <em>taxonomy</em> should involve social graph analysis, itemset mining, classification, and lots of other research areas there may be.</p>
","<open-source><dataset>","106","106","106","0","84","3987"
"339","{
  ""id"": 339,
  ""title"": ""Python vs R for machine learning""
}","<p>Some real important differences to consider when you are choosing <strong>R</strong> or <strong>Python</strong> over one another:</p>

<ul>
<li><strong>Machine Learning</strong> has 2 phases. Model Building and Prediction phase. Typically, model building is performed as a batch process and <strong>predictions are done realtime</strong>. The model building process is a compute intensive process while the prediction happens in a jiffy. Therefore, performance of an algorithm in Python or R doesn't really affect the turn-around time of the user. Python 1, R 1.</li>
<li><strong>Production:</strong> The real difference between Python and R comes in being production ready. Python, as such is a full fledged programming language and many organisations use it in their production systems. R is a statistical programming software favoured by many academia and due to the rise in data science and availability of libraries and being open source, the industry has started using R. Many of these organisations have their production systems either in Java, C++, C#, Python etc. So, ideally they would like to have the <strong>prediction system</strong> in the same language to reduce the latency and maintenance issues.
Python 2, R 1.</li>
<li><strong>Libraries:</strong> Both the languages have enormous and reliable libraries. R has over 5000 libraries catering to many domains while Python has some incredible packages like <strong>Pandas, NumPy, SciPy, Scikit Learn, Matplotlib</strong>. Python 3, R 2.</li>
<li><strong>Development:</strong> Both the language are interpreted languages. Many say that python is easy to learn, it's almost like reading english (to put it on a lighter note) but R requires more initial studying effort. Also, both of them have good IDEs (Spyder etc for Python and RStudio for R). Python 4, R 2.</li>
<li><strong>Speed:</strong> R software initially had problems with large computations (say, like nxn matrix multiplications). But, this issue is addressed with the introduction of R by Revolution Analytics. They have re-written computation intensive operations in C which is blazingly fast. Python being a high level language is relatively slow. Python 4, R 3.</li>
<li><strong>Visualizations:</strong> In data science, we frequently tend to plot data to showcase patterns to users. Therefore, visualisations become an important criteria in choosing a software and R completely kills Python in this regard. Thanks to Hadley Wickham for an incredible ggplot2 package. R wins hands down. Python 4, R 4.</li>
<li><strong>Dealing with Big Data:</strong> One of the constraints of R is it stores the data in system memory (RAM). So, RAM capacity becomes a constraint when you are handling Big Data. Python does well, but I would say, as both R and Python have HDFS connectors, leveraging Hadoop infrastructure would give substantial performance improvement. So, Python 5, R 5.</li>
</ul>

<p>So, both the languages are equally good. Therefore, depending upon your domain and the place you work, you have to smartly choose the right language. The technology world usually prefers using a single language. Business users (marketing analytics, retail analytics) usually go with statistical programming languages like R, since they frequently do quick prototyping and build visualisations (which is faster done in R than Python).</p>
","<machine-learning><r><python>","105","109","107","2","514","674"
"14585","{
  ""id"": 14585,
  ""title"": ""When to use GRU over LSTM?""
}","<p>GRU is related to LSTM as both are utilizing different way if gating information to prevent vanishing gradient problem. Here are some pin-points about GRU vs LSTM-</p>

<ul>
<li>The GRU controls the flow of information like the LSTM unit, but without having to use a <strong><em>memory unit</em></strong>. It just exposes the full hidden content without any control.</li>
<li>GRU is relatively new,  and from my perspective, the performance is on par with LSTM,  but computationally <strong><em>more efficient</em></strong> (<em>less complex structure as pointed out</em>).  So we are seeing it being used more and more.</li>
</ul>

<p>For a detailed description, you can explore this Research Paper - <a href=""https://arxiv.org/pdf/1412.3555v1.pdf"" rel=""noreferrer"">Arxiv.org</a>. The paper explains all this brilliantly.</p>

<p>Plus, you can also explore these blogs for a better idea-</p>

<ul>
<li><a href=""http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/"" rel=""noreferrer"">WildML</a>  </li>
<li><a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""noreferrer"">Colah - Github</a></li>
</ul>

<p>Hope it helps! </p>
","<neural-network><deep-learning><lstm><gru>","104","104","104","0","10902","1799"
"11703","{
  ""id"": 11703,
  ""title"": ""Backprop Through Max-Pooling Layers?""
}","<p>There is no gradient with respect to non maximum values, since changing them slightly does not affect the output. Further the max is locally linear with slope 1, with respect to the input that actually achieves the max. Thus, the gradient from the next layer is passed back to only that neuron which achieved the max. All other neurons get zero gradient.</p>
<p>So in your example, <span class=""math-container"">$\delta_i^l$</span> would be a vector of all zeros, except that the <span class=""math-container"">$i^{*^{th}}$</span> location will get a values <span class=""math-container"">$\left\{\delta_j^{l+1}\right\}$</span> where <span class=""math-container"">$i^* = argmax_{i} (z_i^l)$</span></p>
","<neural-network><backpropagation>","97","97","97","0","18637","1088"
"996","{
  ""id"": 996,
  ""title"": ""SVM using scikit learn runs endlessly and never completes execution""
}","<p>Kernelized SVMs require the computation of a distance function between each point in the dataset, which is the dominating cost of $\mathcal{O}(n_\text{features} \times n_\text{observations}^2)$. The storage of the distances is a burden on memory, so they're recomputed on the fly. Thankfully, only the points nearest the decision boundary are needed most of the time. Frequently computed distances are stored in a cache. If the cache is getting thrashed then the running time blows up to $\mathcal{O}(n_\text{features} \times n_\text{observations}^3)$.</p>

<p>You can increase this cache by invoking SVR as</p>

<pre><code>model = SVR(cache_size=7000)
</code></pre>

<p>In general, this is not going to work. But all is not lost. You can subsample the data and use the rest as a validation set, or you can pick a different model. Above the 200,000 observation range, it's wise to choose linear learners.</p>

<p>Kernel SVM can be approximated, by approximating the kernel matrix and feeding it to a linear SVM. This allows you to trade off between accuracy and performance in linear time.</p>

<p>A popular means of achieving this is to use 100 or so cluster centers found by kmeans/kmeans++ as the basis of your kernel function. The new derived features are then fed into a linear model. This works very well in practice. Tools like <a href=""https://code.google.com/p/sofia-ml/"">sophia-ml</a> and <a href=""https://github.com/JohnLangford/vowpal_wabbit/wiki"">vowpal wabbit</a> are how Google, Yahoo and Microsoft do this. Input/output becomes the dominating cost for simple linear learners.</p>

<p>In the abundance of data, nonparametric models perform roughly the same for most problems. The exceptions being structured inputs, like text, images, time series, audio.</p>

<h2>Further reading</h2>

<ul>
<li><a href=""http://fastml.com/the-secret-of-the-big-guys/"">How to implement this.</a></li>
<li><a href=""http://fastml.com/go-non-linear-with-vowpal-wabbit/"">How to train an ngram neural network with dropout that scales linearly</a></li>
<li><a href=""http://peekaboo-vision.blogspot.co.uk/2012/12/kernel-approximations-for-efficient.html"">Kernel Approximations</a></li>
<li><a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.9009&amp;rep=rep1&amp;type=pdf"">A formal paper on using kmeans to approximate kernel machines</a></li>
</ul>
","<python><svm><scikit-learn>","93","93","93","0","2997","1031"
"37","{
  ""id"": 37,
  ""title"": ""How big is big data?""
}","<p>To me (coming from a relational database background), ""Big Data"" is not primarily about the data size (which is the bulk of what the other answers are so far).</p>

<p>""Big Data"" and ""Bad Data"" are closely related. Relational Databases require 'pristine data'. If the data is in the database, it is accurate, clean, and 100% reliable. Relational Databases require ""Great Data""  and a huge amount of time, money, and accountability is put on to making sure the data is well prepared before loading it in to the database. If the data is in the database, it is 'gospel', and it defines the system understanding of reality.</p>

<p>""Big Data"" tackles this problem from the other direction. The data is poorly defined, much of it may be inaccurate, and much of it may in fact be missing. The structure and layout of the data is linear as opposed to relational.</p>

<p>Big Data has to have enough volume so that the amount of bad data, or missing data becomes statistically insignificant. When the errors in your data are common enough to cancel each other out, when the missing data is proportionally small enough to be negligible and when your data access requirements and algorithms are functional even with incomplete and inaccurate data, then you have ""Big Data"".</p>

<p>""Big Data"" is not really about the volume, it is about the characteristics of the data.</p>
","<bigdata><scalability><efficiency><performance>","92","94","93","1","9","1102"
"20301","{
  ""id"": 20301,
  ""title"": ""Cross-entropy loss explanation""
}","<p>The <a href=""https://en.wikipedia.org/wiki/Cross_entropy"" rel=""noreferrer"">cross entropy</a> formula takes in two distributions, <span class=""math-container"">$p(x)$</span>, the true distribution, and <span class=""math-container"">$q(x)$</span>, the estimated distribution, defined over the discrete variable <span class=""math-container"">$x$</span> and is given by </p>

<h2><span class=""math-container"">$$H(p,q) = -\sum_{\forall x} p(x) \log(q(x))$$</span></h2>

<p>For a neural network, the calculation is independent of the following:</p>

<ul>
<li><p>What kind of layer was used.</p></li>
<li><p>What kind of activation was used - although many activations will not be compatible with the calculation because their outputs are not interpretable as probabilities (i.e., their outputs are negative, greater than 1, or do not sum to 1). Softmax is often used for multiclass classification because it guarantees a well-behaved probability distribution function.</p></li>
</ul>

<p>For a neural network, you will usually see the equation written in a form where <span class=""math-container"">$\mathbf{y}$</span> is the ground truth vector and <span class=""math-container"">$\mathbf{\hat{y}}$</span> (or some other value taken direct from the last layer output) is the estimate. For a single example, it would look like this:</p>

<h2><span class=""math-container"">$$L = - \mathbf{y} \cdot \log(\mathbf{\hat{y}})$$</span></h2>

<p>where <span class=""math-container"">$\cdot$</span> is the inner product.</p>

<p>Your example ground truth <span class=""math-container"">$\mathbf{y}$</span> gives all probability to the first value, and the other values are zero, so we can ignore them, and just use the matching term from your estimates <span class=""math-container"">$\mathbf{\hat{y}}$</span></p>

<p><span class=""math-container"">$L = -(1\times log(0.1) + 0 \times \log(0.5) + ...)$</span></p>

<p><span class=""math-container"">$L = - log(0.1) \approx 2.303$</span></p>

<p>An important point from comments</p>

<blockquote>
  <p>That means, the loss would be same no matter if the predictions are <span class=""math-container"">$[0.1, 0.5, 0.1, 0.1, 0.2]$</span> or <span class=""math-container"">$[0.1, 0.6, 0.1, 0.1, 0.1]$</span>? </p>
</blockquote>

<p>Yes, this is a key feature of multiclass logloss, it rewards/penalises probabilities of correct classes only. The value is independent of how the remaining probability is split between incorrect classes.</p>

<p>You will often see this equation averaged over all examples as a <em>cost</em> function. It is not always strictly adhered to in descriptions, but usually a <em>loss</em> function is lower level and describes how a single instance or component determines an error value, whilst a <em>cost</em> function is higher level and describes how a complete system is evaluated for optimisation. A cost function based on multiclass log loss for data set of size <span class=""math-container"">$N$</span> might look like this:</p>

<h2><span class=""math-container"">$$J = - \frac{1}{N}\left(\sum_{i=1}^{N} \mathbf{y_i} \cdot \log(\mathbf{\hat{y}_i})\right)$$</span></h2>

<p>Many implementations will require your ground truth values to be one-hot encoded (with a single true class), because that allows for some extra optimisation. However, in principle the cross entropy loss can be calculated - and optimised - when this is not the case.</p>
","<machine-learning><neural-network><deep-learning><softmax>","90","90","90","0","836","25808"
"27879","{
  ""id"": 27879,
  ""title"": ""Training an RNN with examples of different lengths in Keras""
}","<blockquote>
  <p>This suggests that all the training examples have a fixed sequence length, namely <code>timesteps</code>.</p>
</blockquote>

<p>That is not quite correct, since that dimension can be <code>None</code>, i.e. variable length. Within a single <em>batch</em>, you must have the same number of timesteps (this is typically where you see 0-padding and masking). But between batches there is no such restriction. During inference, you can have any length.</p>

<p>Example code that creates random time-length batches of training data.</p>

<pre class=""lang-py prettyprint-override""><code>from keras.models import Sequential
from keras.layers import LSTM, Dense, TimeDistributed
from keras.utils import to_categorical
import numpy as np

model = Sequential()

model.add(LSTM(32, return_sequences=True, input_shape=(None, 5)))
model.add(LSTM(8, return_sequences=True))
model.add(TimeDistributed(Dense(2, activation='sigmoid')))

print(model.summary(90))

model.compile(loss='categorical_crossentropy',
              optimizer='adam')

def train_generator():
    while True:
        sequence_length = np.random.randint(10, 100)
        x_train = np.random.random((1000, sequence_length, 5))
        # y_train will depend on past 5 timesteps of x
        y_train = x_train[:, :, 0]
        for i in range(1, 5):
            y_train[:, i:] += x_train[:, :-i, i]
        y_train = to_categorical(y_train &gt; 2.5)
        yield x_train, y_train

model.fit_generator(train_generator(), steps_per_epoch=30, epochs=10, verbose=1)
</code></pre>

<p>And this is what it prints. Note the output shapes are <code>(None, None, x)</code> indicating variable batch size and variable timestep size.</p>

<pre><code>__________________________________________________________________________________________
Layer (type)                            Output Shape                        Param #
==========================================================================================
lstm_1 (LSTM)                           (None, None, 32)                    4864
__________________________________________________________________________________________
lstm_2 (LSTM)                           (None, None, 8)                     1312
__________________________________________________________________________________________
time_distributed_1 (TimeDistributed)    (None, None, 2)                     18
==========================================================================================
Total params: 6,194
Trainable params: 6,194
Non-trainable params: 0
__________________________________________________________________________________________
Epoch 1/10
30/30 [==============================] - 6s 201ms/step - loss: 0.6913
Epoch 2/10
30/30 [==============================] - 4s 137ms/step - loss: 0.6738
...
Epoch 9/10
30/30 [==============================] - 4s 136ms/step - loss: 0.1643
Epoch 10/10
30/30 [==============================] - 4s 142ms/step - loss: 0.1441
</code></pre>
","<python><keras><rnn><training>","90","92","91","1","33202","1732"
"34366","{
  ""id"": 34366,
  ""title"": ""Why do people prefer Pandas to SQL?""
}","<p>The real first question is why are people more productive with DataFrame abstractions than pure SQL abstractions.</p>
<p>TLDR; SQL is not geared around the (human) development and debugging process, DataFrames are.</p>
<p>The main reason is that DataFrame abstractions allow you to construct SQL statements whilst avoiding verbose and illegible nesting. The pattern of writing nested routines, commenting them out to check them, and then uncommenting them is replaced by single lines of transformation. You can naturally run things line by line in a repl (even in Spark) and view the results.</p>
<p>Consider the example, of adding a new transformed (string mangled column) to a table, then grouping by it and doing some aggregations. The SQL gets pretty ugly. Pandas can solve this but is missing some things when it comes to truly big data or in particular partitions (perhaps improved recently).</p>
<p>DataFrames should be viewed as a high-level API to SQL routines, even if with pandas they are not at all rendered to some SQL planner.</p>
<hr />
<p>You can probably have many technical discussions around this, but I'm considering the user perspective below.</p>
<p>One simple reason why you may see a lot more questions around Pandas data manipulation as opposed to SQL is that to use SQL, by definition, means using a database, and a lot of use-cases these days quite simply require bits of data for 'one-and-done' tasks (from .csv, web api, etc.). In these cases loading, storing, manipulating and extracting from a database is not viable.</p>
<p>However, considering cases where the use-case may justify using either Pandas or SQL, you're certainly not wrong.  If you want to do many, repetitive data manipulation tasks and persist the outputs, I'd always recommend trying to go via SQL first.  From what I've seen the reason why many users, even in these cases, don't go via SQL is two-fold.</p>
<p>Firstly, the major advantage pandas has over SQL is that it's part of the wider Python universe, which means in one fell swoop I can load, clean, manipulate, and visualize my data (I can even execute SQL through Pandas...).  The other is, quite simply, that all too many users don't know the extent of SQL's capabilities.  Every beginner learns the 'extraction syntax' of SQL (SELECT, FROM, WHERE, etc.) as a means to get your data from a DB to the next place.  Some may pick up some of the more advance grouping and iteration syntax.  But after that there tends to be a pretty significant gulf in knowledge, until you get to the experts (DBA, Data Engineers, etc.).</p>
<p>tl;dr: It's often down to the use-case, convenience, or a gap in knowledge around the extent of SQL's capabilities.</p>
","<python><pandas><sql>","87","89","88","1","46537","456"
"26335","{
  ""id"": 26335,
  ""title"": ""Convert a list of lists into a Pandas Dataframe""
}","<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = [['New York Yankees', 'Acevedo Juan', 900000, 'Pitcher'], 
        ['New York Yankees', 'Anderson Jason', 300000, 'Pitcher'], 
        ['New York Yankees', 'Clemens Roger', 10100000, 'Pitcher'], 
        ['New York Yankees', 'Contreras Jose', 5500000, 'Pitcher']]

df = pd.DataFrame.from_records(data)
</code></pre>
","<pandas>","83","83","83","0","381","10223"
"414","{
  ""id"": 414,
  ""title"": ""Choosing a learning rate""
}","<ul>
<li><p><strong>Is the learning rate related to the shape of the error gradient, as
it dictates the rate of descent?</strong></p>

<ul>
<li>In plain SGD, the answer is no. A global learning rate is used which is indifferent to the error gradient. However, the intuition you are getting at has inspired various modifications of the SGD update rule.</li>
</ul></li>
<li><p><strong>If so, how do you use this information to inform your decision about a value?</strong></p>

<ul>
<li><p>Adagrad is the most widely known of these and scales a global learning rate <em>η</em> on each dimension based on l2 norm of the history of the error gradient <em>gt</em> on each dimension:</p>

<p><img src=""https://i.stack.imgur.com/aP96K.png"" alt=""enter image description here""></p></li>
<li><p><a href=""http://arxiv.org/pdf/1212.5701v1.pdf"">Adadelta</a> is another such training algorithm which uses both the error gradient history like adagrad and the weight update history and has the advantage of <em>not having to set a learning rate at all</em>.</p></li>
</ul></li>
<li><p><strong>If it's not what sort of values should I choose, and how should I choose them?</strong></p>

<ul>
<li>Setting learning rates for plain SGD in neural nets is usually a
process of starting with a sane value such as 0.01 and then doing cross-validation
to find an optimal value. Typical values range over a few orders of
magnitude from 0.0001 up to 1.</li>
</ul></li>
<li><p><strong>It seems like you would want small values to avoid overshooting, but
how do you choose one such that you don't get stuck in local minima
or take too long to descend? Does it make sense to have a constant learning rate, or should I use some metric to alter its value as I get nearer a minimum in the gradient?</strong></p>

<ul>
<li>Usually, the value that's best is near the highest stable learning
rate and learning rate decay/annealing (either linear or
exponentially) is used over the course of training. The reason behind this is that early on there is a clear learning signal so aggressive updates encourage exploration while later on the smaller learning rates allow for more delicate exploitation of local error surface.</li>
</ul></li>
</ul>
","<machine-learning><neural-network><deep-learning><optimization><hyperparameter>","80","80","80","0","548","4069"
"807","{
  ""id"": 807,
  ""title"": ""Advantages of AUC vs standard accuracy""
}","<p>Really great question, and one that I find that most people don't really understand on an intuitive level. <code>AUC</code> is in fact often preferred over accuracy for binary classification for a number of different reasons. First though, let's talk about exactly what <code>AUC</code> is. Honestly, for being one of the most widely used efficacy metrics, it's surprisingly obtuse to figure out exactly how <code>AUC</code> works.</p>

<p><code>AUC</code> stands for <code>Area Under the Curve</code>, which curve you ask? Well, that would be the <code>ROC</code> curve. <code>ROC</code> stands for <a href=""http://en.wikipedia.org/wiki/Receiver_operating_characteristic"" rel=""noreferrer"">Receiver Operating Characteristic</a>, which is actually slightly non-intuitive. The implicit goal of <code>AUC</code> is to deal with situations where you have a very skewed sample distribution, and don't want to overfit to a single class.</p>

<p>A great example is in spam detection. Generally, spam datasets are STRONGLY biased towards ham, or not-spam. If your data set is 90% ham, you can get a pretty damn good accuracy by just saying that every single email is ham, which is obviously something that indicates a non-ideal classifier. Let's start with a couple of metrics that are a little more useful for us, specifically the true positive rate (<code>TPR</code>) and the false positive rate (<code>FPR</code>):</p>

<p><img src=""https://i.stack.imgur.com/hNxTl.png"" alt=""ROC axes""></p>

<p>Now in this graph, <code>TPR</code> is specifically the ratio of true positive to all positives, and <code>FPR</code> is the ratio of false positives to all negatives. (Keep in mind, this is only for binary classification.) On a graph like this, it should be pretty straightforward to figure out that a prediction of all 0's or all 1's will result in the points of <code>(0,0)</code> and <code>(1,1)</code> respectively. If you draw a line through these lines you get something like this:</p>

<p><img src=""https://i.stack.imgur.com/B1WT1.png"" alt=""Kind of like a triangle""></p>

<p>Which looks basically like a diagonal line (it is), and by some easy geometry, you can see that the <code>AUC</code> of such a model would be <code>0.5</code> (height and base are both 1). Similarly, if you predict a random assortment of 0's and 1's, let's say 90% 1's, you could get the point <code>(0.9, 0.9)</code>, which again falls along that diagonal line.</p>

<p>Now comes the interesting part. What if we weren't only predicting 0's and 1's? What if instead, we wanted to say that, theoretically we were going to set a cutoff, above which every result was a 1, and below which every result were a 0. This would mean that at the extremes you get the original situation where you have all 0's and all 1's (at a cutoff of 0 and 1 respectively), but also a series of intermediate states that fall within the <code>1x1</code> graph that contains your <code>ROC</code>. In practice you get something like this:
<img src=""https://i.stack.imgur.com/13McM.png"" alt=""Courtesy of Wikipedia""></p>

<p>So basically, what you're actually getting when you do an <code>AUC</code> over accuracy is something that will strongly discourage people going for models that are representative, but not discriminative, as this will only actually select for models that achieve false positive and true positive rates that are significantly above random chance, which is not guaranteed for accuracy.</p>
","<machine-learning><accuracy>","80","80","80","0","548","4069"
"24096","{
  ""id"": 24096,
  ""title"": ""How to clone Python working environment on another machine?""
}","<p>First of all this is a Python/Anaconda question and should probably be asked in a different stack exchange subsite.</p>

<hr>

<p>As for the question itself - you can export your Anaconda environment using:</p>

<pre><code>conda env export &gt; environment.yml
</code></pre>

<p>And recreate it using:</p>

<pre><code>conda env create -f environment.yml
</code></pre>

<p>Please note that as others suggested - you should use virtual environments which allows you to create a certain environment that is separated from that of your machine and manage it more easily.</p>

<p>To create a virtual environment in Anaconda you can use:</p>

<pre><code>conda create -n yourenvname python=x.x anaconda
</code></pre>

<p>which you activate using:</p>

<pre><code>source activate yourenvname
</code></pre>
","<python><anaconda>","74","74","74","0","28328","901"
"40235","{
  ""id"": 40235,
  ""title"": ""How to draw Deep learning network architecture diagrams?""
}","<p>I wrote some latex code to draw Deep networks for one of my reports. You can find it here: <a href=""https://github.com/HarisIqbal88/PlotNeuralNet"" rel=""noreferrer"">https://github.com/HarisIqbal88/PlotNeuralNet</a></p>

<p>With this, you can draw networks like these:
<a href=""https://i.stack.imgur.com/TZgNQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TZgNQ.png"" alt=""enter image description here""></a></p>
","<machine-learning><neural-network><deep-learning><svm><software-recommendation>","74","76","75","1","61447","749"
"5229","{
  ""id"": 5229,
  ""title"": ""strings as features in decision tree/random forest""
}","<p>In most of the well-established machine learning systems, categorical variables are handled naturally. For example in R you would use factors, in WEKA you would use nominal variables. This is not the case in scikit-learn. The decision trees implemented in scikit-learn uses only numerical features and these features are interpreted always as <strong>continuous numeric variables</strong>. </p>

<p>Thus, simply replacing the strings with a hash code should be avoided, because being considered as a continuous numerical feature any coding you will use will induce an order which simply does not exist in your data. </p>

<p>One example is to code ['red','green','blue'] with [1,2,3], would produce weird things like 'red' is lower than 'blue', and if you average a 'red' and a 'blue' you will get a 'green'. Another more subtle example might happen when you code ['low', 'medium', 'high'] with [1,2,3]. In the latter case it might happen to have an ordering which makes sense, however, some subtle inconsistencies might happen when 'medium' in not in the middle of 'low' and 'high'.</p>

<p>Finally, the answer to your question lies in coding the categorical feature into <strong>multiple binary features</strong>. For example, you might code ['red','green','blue'] with 3 columns, one for each category, having 1 when the category match and 0 otherwise. This is called <strong>one-hot-encoding</strong>, binary encoding, one-of-k-encoding or whatever. You can check documentation here for <a href=""http://scikit-learn.org/stable/modules/preprocessing.html"" rel=""noreferrer"">encoding categorical features</a> and <a href=""http://scikit-learn.org/stable/modules/feature_extraction.html#dict-feature-extraction"" rel=""noreferrer"">feature extraction - hashing and dicts</a>. Obviously one-hot-encoding will expand your space requirements and sometimes it hurts the performance as well. </p>
","<machine-learning><python><scikit-learn><random-forest><decision-trees>","72","72","72","0","108","4044"
"201","{
  ""id"": 201,
  ""title"": ""Publicly Available Datasets""
}","<p>Update:</p>

<p><strong>Kaggle.com</strong>, a home of modern data science &amp; machine learning enthusiasts:), opened <strong><a href=""https://www.kaggle.com/datasets"" rel=""noreferrer"">it's own repository of the data sets</a></strong>.</p>

<hr>

<p>In addition to the listed sources.</p>

<p>Some social network data sets:</p>

<ul>
<li><a href=""http://snap.stanford.edu/data/"" rel=""noreferrer"">Stanford University large network dataset collection (SNAP)</a></li>
<li><a href=""http://blog.infochimps.com/2008/12/29/massive-scrape-of-twitters-friend-graph/"" rel=""noreferrer"">A huge twitter dataset that includes followers</a> + <a href=""http://www.infochimps.com/collections/twitter-census"" rel=""noreferrer"">large collection of twitter datasets here</a></li>
<li><a href=""http://mtg.upf.edu/node/1671"" rel=""noreferrer"">LastFM data set</a></li>
</ul>

<p>There are plenty of sources listed at Stats SE:</p>

<ul>
<li><a href=""https://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/"">Locating freely available data samples</a></li>
<li><a href=""https://stats.stackexchange.com/questions/12670/data-apis-feeds-available-as-packages-in-r"">Data APIs/feeds available as packages in R</a></li>
<li><a href=""https://stats.stackexchange.com/questions/973/free-data-set-for-very-high-dimensional-classification"">Free data set for very high dimensional classification</a></li>
</ul>
","<open-source><dataset>","72","72","72","0","97","5014"
"11933","{
  ""id"": 11933,
  ""title"": ""ValueError: Input contains NaN, infinity or a value too large for dtype('float32')""
}","<p>With <code>np.isnan(X)</code> you get a boolean mask back with True for positions containing <code>NaN</code>s.</p>

<p>With <code>np.where(np.isnan(X))</code> you get back a tuple with i, j coordinates of <code>NaN</code>s.</p>

<p>Finally, with <code>np.nan_to_num(X)</code> you ""replace nan with zero and inf with finite numbers"".</p>

<p>Alternatively, you can use: </p>

<ul>
<li><a href=""https://scikit-learn.org/stable/modules/impute.html#impute"" rel=""noreferrer"">sklearn.impute.SimpleImputer</a> for mean / median imputation of missing values, or</li>
<li>pandas' <code>pd.DataFrame(X).fillna()</code>, if you need something other than filling it with zeros.</li>
</ul>
","<python><random-forest><pandas>","71","71","71","0","10574","826"
"13514","{
  ""id"": 13514,
  ""title"": ""Why do internet companies prefer Java/Python for data scientist job?""
}","<p>So you can integrate with the rest of the code base. It seems your company uses a mix of Java and python. What are you going to do if a little corner of the site needs machine learning; pass the data around with a database, or a cache, drop to R, and so on? Why not just do it all in the same language? It's faster, cleaner, and easier to maintain.</p>

<p>Know any online companies that run solely on R? Neither do I...</p>

<p>All that said Java is the last language I'd do data science in.</p>
","<beginner><tools><career><reference-request>","69","69","69","0","381","10223"
"12533","{
  ""id"": 12533,
  ""title"": ""Does batch_size in Keras have any effects in results' quality?""
}","<p>After one and a half years, I come back to my answer because my previous answer was wrong. </p>

<p>Batch size impacts learning significantly. What happens when you put a batch through your network is that you average the gradients. The concept is that if your batch size is big enough, this will provide a stable enough estimate of what the gradient of the full dataset would be. By taking samples from your dataset, you estimate the gradient while reducing computational cost significantly. The lower you go, the less accurate your esttimate will be, however in some cases these noisy gradients can actually help escape local minima. When it is too low, your network weights can just jump around if your data is noisy and it might be unable to learn or it converges very slowly, thus negatively impacting total computation time.</p>

<p>Another advantage of batching is for GPU computation, GPUs are very good at parallelizing the calculations that happen in neural networks if part of the computation is the same (for example, repeated matrix multiplication over the same weight matrix of your network). This means that a batch size of 16 will take less than twice the amount of a batch size of 8.</p>

<p>In the case that you do need bigger batch sizes but it will not fit on your GPU, you can feed a small batch, save the gradient estimates and feed one or more batches, and then do a weight update. This way you get a more stable gradient because you increased your virtual batch size.</p>
","<deep-learning><keras>","69","71","70","1","14904","8748"
"16084","{
  ""id"": 16084,
  ""title"": ""What is the difference between \""equivariant to translation\"" and \""invariant to translation\""""
}","<p>Equivariance and invariance  are sometimes used interchangeably in common speech. They have ancient roots in maths and physics. As pointed out by <a href=""https://datascience.stackexchange.com/users/6622/xian"">@Xi'an</a>, you can find previous uses (anterior to Convolutional Neural Networks) in the statistical literature, for instance on the notions of the <a href=""https://en.wikipedia.org/wiki/Invariant_estimator"" rel=""noreferrer"">invariant estimator</a> and especially the <a href=""https://en.wikipedia.org/wiki/Invariant_estimator#Pitman_estimator"" rel=""noreferrer"">Pitman estimator</a>.</p>

<p>However, I would like to mention that <strong>it would be better if both terms keep separate meaning</strong>, as the prefix <a href=""https://en.wiktionary.org/wiki/in-#Latin"" rel=""noreferrer"">""<em>in-</em>""</a> in <em>invariant</em> is privative (meaning ""no variance"" at all), while <a href=""https://en.wiktionary.org/wiki/equi-"" rel=""noreferrer"">""<em>equi-</em>""</a> in <em>equivariant</em> refers to ""varying in a similar or equivalent proportion"". <strong>In other words, one <em>in-</em> does not vary, the other <em>equi-</em> does</strong>.</p>

<p>Let us start from simple image features, and suppose that image <span class=""math-container"">$I$</span> has a unique maximum <span class=""math-container"">$m$</span> at spatial pixel location <span class=""math-container"">$(x_m,y_m)$</span>, which is here the main classification feature. <strong>In other words: an image and all its translations are ""the same""</strong>.
An interesting property of classifiers is their ability to classify in the same manner some distorted versions <span class=""math-container"">$I'$</span> of <span class=""math-container"">$I$</span>, for instance translations by all vectors  <span class=""math-container"">$(u,v)$</span>. </p>

<p>The maximum value  <span class=""math-container"">$m'$</span> of <span class=""math-container"">$I'$</span> is <em>invariant</em>: <span class=""math-container"">$m'=m$</span>: the value is the same. While its location will be at <span class=""math-container"">$(x'_m,y'_m)=(x_m-u,y_m-v)$</span>, and is <strong>equivariant</strong>, meaning that <em>is varies ""equally"" with the distortion</em>. </p>

<p>The precise formulations given (in mathematical terms) for equivariance depend on the class of objects and transformations one considers: translation, rotation, scale, shear, shift, etc. So I prefer here to focus on the notion that is most often used in practice (I accept the blame from a theoretical stand-point).</p>

<p>Here, translations by vectors  <span class=""math-container"">$(u,v)$</span> of the image (or some more generic actions) can be equipped with a structure of composition, like that  of a group <span class=""math-container"">$G$</span> (here the group of translations). One specific <span class=""math-container"">$g$</span> denotes a specific element of the translation group (<a href=""https://en.wikipedia.org/wiki/Translational_symmetry"" rel=""noreferrer"">translational symmetry</a>). A function or feature <span class=""math-container"">$f$</span> is invariant under the group of actions <span class=""math-container"">$G$</span> if for all images in a class, and for any <span class=""math-container"">$g$</span>, 
<span class=""math-container"">$$f(g(I)) = f(I)\,.$$</span> </p>

<p>In other words: if you change the image by action <span class=""math-container"">$g$</span>, the values for feature or function <span class=""math-container"">$f$</span> are the same.</p>

<p>It becomes equivariant if there exists another mathematical <em>structure or action</em>  (often a group again) <span class=""math-container"">$G'$</span> that reflects 
 the
transformations  (from <span class=""math-container"">$G$</span>) in <span class=""math-container"">$I$</span>  <em>in a meaningful way</em>. In other words, such that for each <span class=""math-container"">$g$</span>, you have some (unique?) <span class=""math-container"">$g' \in G'$</span> such that </p>

<p><span class=""math-container"">$$f(g(I)) = g'(f(I))\,.$$</span> </p>

<p>In the above example on the group of translations, <span class=""math-container"">$g$</span> and <span class=""math-container"">$g'$</span> are the same (and hence <span class=""math-container"">$G'=G$</span>): an integer translation of the image reflects as the exact same translation of the maximum location. This is sometimes refered to as ""same-equivariance"".</p>

<p>Another common definition is:</p>

<p><span class=""math-container"">$$f(g(I)) = g(f(I))\,.$$</span> </p>

<p>I however used potentially different <span class=""math-container"">$G$</span> and <span class=""math-container"">$G'$</span> because sometimes <span class=""math-container"">$f(.)$</span> and <span class=""math-container"">$g(.)$</span> do not lie in the same domain. This happens for instance in multivariate statistics (see e.g. <a href=""http://dx.doi.org/10.1080/10485250903431710"" rel=""noreferrer"">Equivariance and invariance properties of multivariate quantile and related functions, and the role of standardisation</a>).
But here, the uniqueness of the mapping between <span class=""math-container"">$g$</span> and <span class=""math-container"">$g'$</span> allows to get back to the original transformation <span class=""math-container"">$g$</span>. </p>

<p>Often, people use the term invariance because the equivariance concept is unknown, or everybody else uses invariance, and equivariance would seem more pedantic. </p>

<p>For the record, other related notions (esp. in maths and physics) are termed <em>covariance</em>, <em>contravariance</em>, differential <em>invariance</em>. </p>

<p>In addition, translation-invariance, as least approximate, or in envelope, has been a quest for several signal and image processing tools. Notably, multi-rate (filter-banks) and multi-scale (wavelets or pyramids) transformations have  been design in the past 25 years, for instance under the hood of shift-invariant, cycle-spinning, stationary, complex, dual-tree wavelet transforms (for a review on 2D wavelets, <a href=""https://arxiv.org/abs/1101.5320"" rel=""noreferrer"">A panorama on multiscale geometric representations</a>). The wavelets can absorb a few discrete scale variations. All theses (approximate) invariances  often come with the price of redundancy in the number of transformed coefficients.
But they are more likely to yield shift-invariant, or shift-equivariant features.</p>
","<neural-network><deep-learning><convolution>","67","67","67","0","12527","1287"
"10273","{
  ""id"": 10273,
  ""title"": ""When should I use Gini Impurity as opposed to Information Gain (Entropy)?""
}","<p>Gini impurity and Information Gain Entropy are pretty much the same. And people do use the values interchangeably. Below are the formulae of both:</p>

<ol>
<li>$\textit{Gini}: \mathit{Gini}(E) = 1 - \sum_{j=1}^{c}p_j^2$</li>
<li>$\textit{Entropy}: H(E) = -\sum_{j=1}^{c}p_j\log p_j$</li>
</ol>

<p>Given a choice, I would use the Gini impurity, as it doesn't require me to compute logarithmic functions, which are computationally intensive. The closed form of it's solution can also be found.</p>

<blockquote>
  <p>Which metric is better to use in different scenarios while using
  decision trees ?</p>
</blockquote>

<p>The Gini impurity, for reasons stated above.</p>

<p>So, <strong>they are pretty much same when it comes to CART analytics.</strong></p>

<p><a href=""https://github.com/rasbt/python-machine-learning-book/blob/master/faq/decision-tree-binary.md"" rel=""noreferrer"">Helpful reference for computational comparison of the two methods</a></p>
","<machine-learning><decision-trees><information-theory>","67","67","67","0","11097","7746"
"848","{
  ""id"": 848,
  ""title"": ""Clustering geo location coordinates (lat,long pairs)""
}","<p>K-means is not the most appropriate algorithm here.</p>
<p>The reason is that k-means is designed to <strong>minimize variance</strong>. This is, of course, appearling from a statistical and signal procssing point of view, but your data is not &quot;linear&quot;.</p>
<p>Since your data is in latitude, longitude format, you should use an algorithm that can handle <em>arbitrary</em> distance functions, in particular geodetic distance functions. Hierarchical clustering, PAM, CLARA, and DBSCAN are popular examples of this.</p>
<p><a href=""https://www.youtube.com/watch?v=QsGOoWdqaT8"" rel=""nofollow noreferrer"">This</a> recommends OPTICS clustering.</p>
<p>The problems of k-means are easy to see when you consider points close to the +-180 degrees wrap-around. Even if you hacked k-means to use Haversine distance, in the update step when it recomputes the <em>mean</em> the result will be badly screwed. <strong>Worst case is, k-means will never converge!</strong></p>
","<machine-learning><python><clustering><k-means><geospatial>","66","66","66","0","924","7449"
"18589","{
  ""id"": 18589,
  ""title"": ""What is the difference between LeakyReLU and PReLU?""
}","<p>Straight from <a href=""https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"" rel=""noreferrer"">wikipedia</a>:</p>

<p><a href=""https://i.stack.imgur.com/BpZ0e.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/BpZ0e.png"" alt=""enter image description here""></a></p>

<ul>
<li><p><strong>Leaky ReLU</strong>s allow a small, non-zero gradient when the unit is not active.</p></li>
<li><p><strong>Parametric ReLU</strong>s take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural network parameters.</p></li>
</ul>
","<neural-network>","66","66","66","0","30928","1049"
"16818","{
  ""id"": 16818,
  ""title"": ""Why mini batch size is better than one single \""batch\"" with all training data?""
}","<p>The key advantage of using minibatch as opposed to the full dataset goes back to the fundamental idea of stochastic gradient descent<a href=""https://arxiv.org/abs/1804.07612"" rel=""noreferrer"">1</a>.</p>
<p>In batch gradient descent, you compute the gradient over the entire dataset, averaging over potentially a vast amount of information. It takes lots of memory to do that. But the real handicap is the batch gradient trajectory land you in a bad spot (saddle point).</p>
<p>In pure SGD, on the other hand, you update your parameters by adding (minus sign) the gradient computed on a <em>single</em> instance of the dataset. Since it's based on one random data point, it's very noisy and may go off in a direction far from the batch gradient. However, the noisiness is exactly what you want in non-convex optimization, because it helps you escape from saddle points or local minima(Theorem 6 in [2]). The disadvantage is it's terribly inefficient and you need to loop over the entire dataset many times to find a good solution.</p>
<p>The minibatch methodology is a compromise that injects enough noise to each gradient update, while achieving a relative speedy convergence.</p>
<p><a href=""https://arxiv.org/abs/1804.07612"" rel=""noreferrer"">1</a> Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010 (pp. 177-186). Physica-Verlag HD.</p>
<p>[2] Ge, R., Huang, F., Jin, C., &amp; Yuan, Y. (2015, June). Escaping From Saddle Points-Online Stochastic Gradient for Tensor Decomposition. In COLT (pp. 797-842).</p>
<h2>EDIT :</h2>
<p>I just saw this comment on Yann LeCun's facebook, which gives a fresh perspective on this question (sorry don't know how to link to fb.)</p>
<blockquote>
<p>Training with large minibatches is bad for your health.
More importantly, it's bad for your test error.
Friends dont let friends use minibatches larger than 32.
Let's face it: the <em>only</em> people have switched to minibatch sizes larger than one since 2012 is because GPUs are inefficient for batch sizes smaller than 32. That's a terrible reason. It just means our hardware sucks.</p>
</blockquote>
<p>He cited this paper which has just been posted on arXiv few days ago (Apr 2018), which is worth reading,</p>
<p>Dominic Masters, Carlo Luschi, <a href=""https://arxiv.org/abs/1804.07612"" rel=""noreferrer"">Revisiting Small Batch Training for Deep Neural Networks</a>, arXiv:1804.07612v1</p>
<p>From the abstract,</p>
<blockquote>
<p>While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance ...</p>
<p>The best performance has been consistently obtained for mini-batch sizes between m=2 and m=32, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.</p>
</blockquote>
","<machine-learning><deep-learning>","66","66","66","0","27250","1250"
"36451","{
  ""id"": 36451,
  ""title"": ""What is the difference between Gradient Descent and Stochastic Gradient Descent?""
}","<p>For a quick simple explanation:</p>
<p>In both gradient descent (GD) and stochastic gradient descent (SGD), you update a set of parameters in an iterative manner to minimize an error function.</p>
<p>While in GD, you have to run through ALL the samples in your training set to do a single update for a parameter in a particular iteration, in SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent.</p>
<p>Thus, if the number of training samples are large, in fact very large, then using gradient descent may take too long because in every iteration when you are updating the values of the parameters, you are running through the complete training set. On the other hand, using SGD will be faster because you use only one training sample and it starts improving itself right away from the first sample.</p>
<p>SGD often converges much faster compared to GD but the error function is not as well minimized as in the case of GD. Often in most cases, the close approximation that you get in SGD for the parameter values are enough because they reach the optimal values and keep oscillating there.</p>
<p>If you need an example of this with a practical case, check Andrew NG's notes here where he clearly shows you the steps involved in both the cases. <a href=""https://web.archive.org/web/20180618211933/http://cs229.stanford.edu/notes/cs229-notes1.pdf"" rel=""noreferrer"">cs229-notes</a></p>
<p>Source: <a href=""https://www.quora.com/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent"" rel=""noreferrer"">Quora Thread</a></p>
","<machine-learning><neural-network><deep-learning><gradient-descent>","65","65","65","0","44083","953"
"10192","{
  ""id"": 10192,
  ""title"": ""Why do cost functions use the square error?""
}","<p>Your loss function would not work because it incentivizes setting <span class=""math-container"">$\theta_1$</span> to any finite value and <span class=""math-container"">$\theta_0$</span> to <span class=""math-container"">$-\infty$</span>.</p>

<p>Let's call <span class=""math-container"">$r(x,y)=\frac{1}{m}\sum_{i=1}^m {h_\theta\left(x^{(i)}\right)} -y$</span> the <em>residual</em> for <span class=""math-container"">$h$</span>.</p>

<p>Your goal is to make <span class=""math-container"">$r$</span> <strong>as close to zero</strong> as possible, <strong>not just minimize it</strong>. A high negative value is just as bad as a high positive value.</p>

<p><strong>EDIT:</strong> You can counter this by artificially limiting the parameter space <span class=""math-container"">$\mathbf{\Theta} $</span>(e.g. you want <span class=""math-container"">$|\theta_0| &lt; 10$</span>). In this case, the optimal parameters would lie on certain points on the boundary of the parameter space. See <a href=""https://math.stackexchange.com/q/896388/12467"">https://math.stackexchange.com/q/896388/12467</a>. This is not what you want.</p>

<h2>Why do we use the square loss</h2>

<p>The squared error forces <span class=""math-container"">$h(x)$</span> and <span class=""math-container"">$y$</span> to match. It's minimized at <span class=""math-container"">$u=v$</span>, if possible, and is always <span class=""math-container"">$\ge 0$</span>, because it's a square of the real number <span class=""math-container"">$u-v$</span>.</p>

<p><span class=""math-container"">$|u-v|$</span> would also work for the above purpose, as would <span class=""math-container"">$(u-v)^{2n}$</span>, with <span class=""math-container"">$n$</span> some positive integer. The first of these is actually used (it's called the <span class=""math-container"">$\ell_1$</span> loss; you might also come across the <span class=""math-container"">$\ell_2$</span> loss, which is another name for squared error). </p>

<p>So, why is the squared loss better than these? This is a <em>deep</em> question related to the link between <em>Frequentist</em> and <em>Bayesian</em> inference. In short, the squared error relates to <strong>Gaussian Noise</strong>.</p>

<p>If your data does not fit all points exactly, i.e. <span class=""math-container"">$h(x)-y$</span> is not zero for some point no matter what <span class=""math-container"">$\theta$</span> you choose (as will always happen in practice), that might be because of <strong>noise</strong>. In any complex system there will be many small <strong>independent</strong> causes for the difference between your <em>model</em> <span class=""math-container"">$h$</span> and <em>reality</em> <span class=""math-container"">$y$</span>: measurement error, environmental factors etc. By the <a href=""https://en.wikipedia.org/wiki/Central_limit_theorem"" rel=""noreferrer""><strong>Central Limit Theorem</strong></a>(CLT), the total noise would be distributed <em>Normally</em>, i.e. according to the <strong>Gaussian distribution</strong>. We want to pick the best fit <span class=""math-container"">$\theta$</span> taking this noise distribution into account. Assume <span class=""math-container"">$R = h(X)-Y$</span>, the part of <span class=""math-container"">$\mathbf{y}$</span> that your model cannot explain, follows the Gaussian distribution <span class=""math-container"">$\mathcal{N}(\mu,\sigma)$</span>. We're using capitals because we're talking about random variables now.</p>

<p>The Gaussian distribution has two parameters, mean <span class=""math-container"">$\mu = \mathbb{E}[R] = \frac{1}{m} \sum_i h_\theta(X^{(i)})-Y^{(i))}$</span> and variance <span class=""math-container"">$\sigma^2 = E[R^2] = \frac{1}{m} \sum_i \left(h_\theta(X^{(i)})-Y^{(i))}\right)^2$</span>. See <a href=""https://math.stackexchange.com/questions/518281/how-to-derive-the-mean-and-variance-of-a-gaussian-random-variable"">here</a> to understand these terms better.</p>

<ul>
<li><p>Consider <span class=""math-container"">$\mu$</span>, it is the <em>systematic error</em> of our measurements. Use <span class=""math-container"">$h'(x) = h(x) - \mu$</span> to correct for systematic error, so that <span class=""math-container"">$\mu' = \mathbb{E}[R']=0$</span> (exercise for the reader). Nothing else to do here.</p></li>
<li><p><span class=""math-container"">$\sigma$</span> represents the <em>random error</em>, also called <em>noise</em>. Once we've taken care of the systematic noise component as in the previous point, the best predictor is obtained when <span class=""math-container"">$\sigma^2 = \frac{1}{m} \sum_i \left(h_\theta(X^{(i)})-Y^{(i))}\right)^2$</span> is minimized. Put another way, the best predictor is the one with the tightest distribution (smallest variance) around the predicted value, i.e. smallest variance. <strong>Minimizing the the least squared loss is the same thing as minimizing the variance!</strong> That explains why the least squared loss works for a wide range of problems. The underlying noise is very often Gaussian, because of the CLT, and minimizing the squared error turns out to be the <em>right</em> thing to do!</p></li>
</ul>

<p>To simultaneously take both the mean and variance into account, we include a <em>bias</em> term in our classifier (to handle systematic error <span class=""math-container"">$\mu$</span>), then minimize the square loss.</p>

<p>Followup questions:</p>

<ul>
<li><p><strong>Least squares loss = Gaussian error. Does every other loss function also correspond to some noise distribution?</strong> Yes. For example, the <span class=""math-container"">$\ell_1$</span> loss (minimizing absolute value instead of squared error) corresponds to the <a href=""https://en.wikipedia.org/wiki/Laplace_distribution"" rel=""noreferrer"">Laplace distribution</a> (Look at the formula for the PDF in the infobox -- it's just the Gaussian with <span class=""math-container"">$|x-\mu|$</span> instead of <span class=""math-container"">$(x-\mu)^2$</span>). A popular loss for probability distributions is the <a href=""https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"" rel=""noreferrer"">KL-divergence</a>.
-The Gaussian distribution is very well motivated because of the <strong>Central Limit Theorem</strong>, which we discussed earlier. When is the Laplace distribution the right noise model? There are some circumstances where it comes about naturally, but it's more commonly as a regularizer <a href=""https://math.stackexchange.com/q/1904767/12467"">to enforce <strong>sparsity</strong></a>: the <span class=""math-container"">$\ell_1$</span> loss is the <em>least convex</em> among all convex losses. </p>

<ul>
<li>As <a href=""https://datascience.stackexchange.com/users/14904/jan-van-der-vegt"">Jan</a> mentions in the comments, the minimizer of <em>squared</em> deviations is the mean and the minimizer of the sum of <strong>absolute</strong> deviations is the <strong>median</strong>. Why would we want to find the median of the residuals instead of the mean? Unlike the mean, the median isn't thrown off by one very large outlier. So, the <span class=""math-container"">$\ell_1$</span> loss is used for increased robustness. Sometimes a combination of the two is used.</li>
</ul></li>
<li><p><strong>Are there situations where we minimize both the Mean and Variance?</strong> Yes. Look up <a href=""https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff"" rel=""noreferrer"">Bias-Variance Trade-off</a>. Here, we are looking at a set of classifiers <span class=""math-container"">$h_\theta \in H$</span> and asking which among them is best. If we ask which <em>set</em> of classifiers is the best for a problem, minimizing both the bias and variance becomes important. It turns out that there is always a trade-off between them and we use <strong>regularization</strong> to achieve a compromise.</p></li>
</ul>

<h2>Regarding the <span class=""math-container"">$\frac{1}{2}$</span> term</h2>

<p>The 1/2 does not matter and actually, neither does the <span class=""math-container"">$m$</span> - they're both constants. The optimal value of <span class=""math-container"">$\theta$</span> would remain the same in both cases. </p>

<ul>
<li><p>The expression for the gradient becomes prettier with the <span class=""math-container"">$\frac{1}{2}$</span>, because the 2 from the square term cancels out.</p>

<ul>
<li>When writing code or algorithms, we're usually concerned more with the gradient, so it helps to keep it concise. You can check progress just by checking the norm of the gradient. The loss function itself is sometimes omitted from code because it is used only for validation of the final answer.</li>
</ul></li>
<li><p>The <span class=""math-container"">$m$</span> is useful if you solve this problem with gradient descent. Then your gradient becomes the average of <span class=""math-container"">$m$</span> terms instead of a sum, so its' scale does not change when you add more data points.</p>

<ul>
<li>I've run into this problem before: I test code with a small number of points and it works fine, but when you test it with the entire dataset there is loss of precision and sometimes over/under-flows, i.e. your gradient becomes <code>nan</code> or <code>inf</code>. To avoid that, just normalize w.r.t. number of data points.</li>
</ul></li>
<li><p>These aesthetic decisions are used here to maintain consistency with future equations where you'll add <strong>regularization</strong> terms. If you include the <span class=""math-container"">$m$</span>, the regularization parameter <span class=""math-container"">$\lambda$</span> will not depend on the dataset size <span class=""math-container"">$m$</span> and it will be more interpretable across problems.</p></li>
</ul>
","<machine-learning><linear-regression><loss-function>","65","65","65","0","13505","971"
"19617","{
  ""id"": 19617,
  ""title"": ""When to use GRU over LSTM?""
}","<p>*To complement already great answers above.</p>

<ul>
<li><p>From my experience, <strong>GRUs train faster</strong> and perform better than LSTMs on <strong>less training data</strong> if you are doing language modeling (not sure about other tasks). </p></li>
<li><p><strong>GRUs are simpler</strong> and thus easier to modify, for example adding new gates in case of additional input to the network. It's just less code in general.</p></li>
<li><p><strong>LSTMs</strong> should in theory <strong>remember longer sequences</strong> than GRUs and outperform them in tasks requiring modeling long-distance relations.  </p></li>
</ul>

<p>*Some additional papers that analyze GRUs and LSTMs.</p>

<ul>
<li><p>""Neural GPUs Learn Algorithms"" (Łukasz Kaiser, Ilya Sutskever, 2015)
<a href=""https://arxiv.org/abs/1511.08228"" rel=""noreferrer"">https://arxiv.org/abs/1511.08228</a></p></li>
<li><p>""Comparative Study of CNN and RNN for Natural Language Processing""
(Wenpeng Yin et al. 2017) <a href=""https://arxiv.org/abs/1702.01923"" rel=""noreferrer"">https://arxiv.org/abs/1702.01923</a></p></li>
</ul>
","<neural-network><deep-learning><lstm><gru>","64","64","64","0","22012","1847"
"31480","{
  ""id"": 31480,
  ""title"": ""How do you visualize neural network architectures?""
}","<p>I recently created a tool for drawing NN architectures and exporting SVG, called <a href=""http://alexlenail.me/NN-SVG/index.html"" rel=""noreferrer"">NN-SVG</a></p>

<p><a href=""https://i.stack.imgur.com/f96kw.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/f96kw.jpg"" alt=""enter image description here""></a></p>
","<machine-learning><neural-network><deep-learning><visualization>","64","64","64","0","50013","741"
"24453","{
  ""id"": 24453,
  ""title"": ""In supervised learning, why is it bad to have correlated features?""
}","<p>Correlated features in general don't improve models (although it depends on the specifics of the problem like the number of variables and the degree of correlation), but they affect specific models in different ways and to varying extents:</p>

<ol>
<li><p>For linear models (e.g., linear regression or logistic regression), <a href=""https://en.wikipedia.org/wiki/Multicollinearity"" rel=""noreferrer"">multicolinearity</a> can yield <a href=""https://en.wikipedia.org/wiki/Multicollinearity#Consequences_of_multicollinearity"" rel=""noreferrer"">solutions that are wildly varying and possibly numerically unstable</a>.</p></li>
<li><p>Random forests can be good at detecting interactions between different features, but highly correlated features can mask these interactions.</p></li>
</ol>

<p>More generally, this can be viewed as a special case of <a href=""https://en.wikipedia.org/wiki/Occam%27s_razor"" rel=""noreferrer"">Occam's razor</a>. A simpler model is preferable, and, in some sense, a model with fewer features is simpler. The concept of <a href=""https://en.wikipedia.org/wiki/Minimum_description_length"" rel=""noreferrer"">minimum description length</a> makes this more precise.</p>
","<machine-learning><correlation>","63","63","63","0","9518","1037"
"28011","{
  ""id"": 28011,
  ""title"": ""Data scientist vs machine learning engineer""
}","<p>Good question. Actually there is a lot of confusion on this subject, mainly because both are quite new jobs. But if we focus on the semantics, the real meaning of the jobs become clear.</p>

<p>Beforehand is better to compare apples with apples, talking about a single subject, the Data. Machine Learning and its sub-genre (Deep Learning, etc.) are just one aspect of the Data World, together with the statistic theories, the data acquisition (DAQ), the processing (which can be non-machine learning driven), the interpretation of the results, etc.</p>

<p>So, for my explanation, I will broad the Machine Learning Engineer role to the one of Data Engineer.</p>

<p>Science is about experiment, trials and fails, theory building, phenomenological understanding.
Engineering is about work on what science already knows, perfecting it and carry to the ""real world"".</p>

<p>Think about a proxy: what is the difference between a nuclear scientist and a nuclear engineer?</p>

<p>The nuclear scientist is the one which know the science behind the atom, the interaction between them, the one which wrote the recipe which allow to get energy from the atoms.</p>

<p>The nuclear engineer is the guy charged to take the recipe of the scientist, and carry it to the real world. So it's knowledge about the atomic physics is quite limited, but he also know about materials, buildings, economics, and whatever else useful to build a proper nuclear plant.</p>

<p>Coming back to the Data world, here another example: the guys which developed Convolutional Neural Networks (Yann LeCun) is a Data Scientist, the guy which deploy the model to recognize faces in pictures is a Machine Learning Engineer. The guy responsible of the whole process, from the data acquisition to the registration of the .JPG image, is a Data Engineer.</p>

<p>So, basically, 90% of the Data Scientist today are actually Data Engineers or Machine Learning Engineers, and 90% of the positions opened as Data Scientist actually need Engineers. An easy check: in the interview, you will be asked about how many ML models you deployed in production, not on how many papers on new methods you published.</p>

<p>Instead, when you see announces about ""Machine Learning Engineer"", that means that the recruiters are well aware of the difference, and they really need someone able to put some model in production.</p>
","<machine-learning>","62","62","62","0","29134","1684"
"9975","{
  ""id"": 9975,
  ""title"": ""What are deconvolutional layers?""
}","<p>I think one way to get a really basic level intuition behind convolution is that you are sliding K filters, which you can think of as K stencils, over the input image and produce K activations - each one representing a degree of match with a particular stencil. The inverse operation of that would be to take K activations and expand them into a preimage of the convolution operation. The intuitive explanation of the inverse operation is therefore, roughly, image reconstruction given the stencils (filters) and activations (the degree of the match for each stencil) and therefore at the basic intuitive level we want to blow up each activation by the stencil's mask and add them up.</p>

<p>Another way to approach understanding deconv would be to examine the deconvolution layer implementation in Caffe, see the following relevant bits of code:</p>

<pre><code>DeconvolutionLayer&lt;Dtype&gt;::Forward_gpu
ConvolutionLayer&lt;Dtype&gt;::Backward_gpu
CuDNNConvolutionLayer&lt;Dtype&gt;::Backward_gpu
BaseConvolutionLayer&lt;Dtype&gt;::backward_cpu_gemm
</code></pre>

<p>You can see that it's implemented in Caffe exactly as backprop for a regular forward convolutional layer (to me it was more obvious after i compared the implementation of backprop in cuDNN conv layer vs ConvolutionLayer::Backward_gpu implemented using GEMM). So if you work through how backpropagation is done for regular convolution you will understand what happens on a mechanical computation level. The way this computation works matches the intuition described in the first paragraph of this blurb.</p>

<blockquote>
  <p>However, I don't know how the learning of convolutional layers works. (I understand how simple MLPs learn with gradient descent, if that helps).</p>
</blockquote>

<p>To answer your other question inside your first question, there are two main differences between MLP backpropagation (fully connected layer) and convolutional nets:</p>

<p>1) the influence of weights is localized, so first figure out how to do backprop for, say a 3x3 filter convolved with a small 3x3 area of an input image, mapping to a single point in the result image.</p>

<p>2) the weights of convolutional filters are shared for spatial invariance. What this means in practice is that in the forward pass the same 3x3 filter with the same weights is dragged through the entire image with the same weights for forward computation to yield the output image (for that particular filter). What this means for backprop is that the backprop gradients for each point in the source image are summed over the entire range that we dragged that filter during the forward pass. Note that there are also different gradients of loss wrt x, w and bias since dLoss/dx needs to be backpropagated, and dLoss/dw is how we update the weights. w and bias are independent inputs in the computation DAG (there are no prior inputs), so there's no need to do backpropagation on those.</p>

<pre><code>(my notation here assumes that convolution is y = x*w+b where '*' is the convolution operation)
</code></pre>
","<neural-network><convnet><convolution>","62","62","62","0","15766","721"
"24112","{
  ""id"": 24112,
  ""title"": ""In softmax classifier, why use exp function to do normalization?""
}","<p>It is more than just numerical. A quick reminder of the softmax:
$$
P(y=j | x) = \frac{e^{x_j}}{\sum_{k=1}^K e^{x_k}}
$$</p>

<p>Where $x$ is an input vector with length equal to the number of classes $K$. The softmax function has 3 very nice properties: 1. it normalizes your data (outputs a proper probability distribution), 2. is differentiable, and 3. it uses the exp you mentioned. A few important points:</p>

<ol>
<li><p>The loss function is not directly related to softmax. You can use standard normalization and still use cross-entropy.</p></li>
<li><p>A ""hardmax"" function (i.e. argmax) is not differentiable. The softmax gives at least a minimal amount of probability to all elements in the output vector, and so is nicely differentiable, hence the term ""soft"" in softmax.</p></li>
<li><p>Now I get to your question. The $e$ in softmax is the natural exponential function. <strong>Before</strong> we normalize, we transform $x$ as in the graph of $e^x$:</p></li>
</ol>

<p><a href=""https://i.stack.imgur.com/Rs2tj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Rs2tj.png"" alt=""natural exponential function""></a></p>

<p>If $x$ is 0 then $y=1$, if $x$ is 1, then $y=2.7$, and if $x$ is 2, now $y=7$! A huge step! This is what's called a non-linear transformation of our unnormalized log scores. The interesting property of the exponential function combined with the normalization in the softmax is that high scores in $x$ become much more probable than low scores. </p>

<p><strong>An example</strong>. Say $K=4$, and your log score $x$ is vector $[2, 4, 2, 1]$. The simple argmax function outputs:</p>

<p>$$
[0, 1, 0, 0]
$$</p>

<p>The argmax is the goal, but it's not differentiable and we can't train our model with it :( A simple normalization, which is differentiable, outputs  the following probabilities:</p>

<p>$$
[0.2222, 0.4444, 0.2222, 0.1111]
$$</p>

<p>That's really far from the argmax! :( Whereas the softmax outputs:
$$
[0.1025, 0.7573, 0.1025, 0.0377]
$$</p>

<p>That's much closer to the argmax! Because we use the natural exponential, we hugely increase the probability of the biggest score and decrease the probability of the lower scores when compared with standard normalization. Hence the ""max"" in softmax.</p>
","<machine-learning><deep-learning>","60","60","60","0","41047","726"
"17445","{
  ""id"": 17445,
  ""title"": ""Train/Test/Validation Set Splitting in Sklearn""
}","<p>There is a great answer to this question over on <a href=""https://stackoverflow.com/a/38251213/1185293"">SO</a> that uses numpy and pandas. </p>

<p>The command (see the answer for the discussion):</p>

<pre><code>train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])
</code></pre>

<p>produces a 60%, 20%, 20% split for training, validation and test sets.</p>
","<machine-learning><scikit-learn><cross-validation>","59","59","59","0","13821","735"
"773","{
  ""id"": 773,
  ""title"": ""Cosine similarity versus dot product as distance metrics""
}","<p>Think geometrically. Cosine similarity only cares about angle difference, while dot product cares about angle and magnitude. If you normalize your data to have the same magnitude, the two are indistinguishable. Sometimes it is desirable to ignore the magnitude, hence cosine similarity is nice, but if magnitude plays a role, dot product would be better as a similarity measure. Note that neither of them is a ""distance metric"".</p>
","<classification>","58","58","58","0","154","696"
"2370","{
  ""id"": 2370,
  ""title"": ""Machine learning - features engineering from date/time data""
}","<p>I would start by graphing the time variable vs other variables and looking for trends.  </p>

<h2>For example</h2>

<p><img src=""https://i.stack.imgur.com/QGYUC.png"" alt=""enter image description here""></p>

<p>In this case there is a periodic weekly trend and a long term upwards trend.  So you would want to encode two time variables:</p>

<ul>
<li><code>day_of_week</code></li>
<li><code>absolute_time</code></li>
</ul>

<h2>In general</h2>

<p>There are several common time frames that trends occur over:</p>

<ul>
<li><code>absolute_time</code></li>
<li><code>day_of_year</code></li>
<li><code>day_of_week</code></li>
<li><code>month_of_year</code></li>
<li><code>hour_of_day</code></li>
<li><code>minute_of_hour</code></li>
</ul>

<p>Look for trends in all of these.  </p>

<h2>Weird trends</h2>

<p>Look for weird trends too.  For example you may see rare but persistent time based trends:</p>

<ul>
<li><code>is_easter</code></li>
<li><code>is_superbowl</code></li>
<li><code>is_national_emergency</code></li>
<li><code>etc.</code></li>
</ul>

<p>These often require that you cross reference your data against some external source that maps events to time.  </p>

<h2>Why graph?</h2>

<p>There are two reasons that I think graphing is so important.</p>

<ul>
<li><p><em>Weird trends</em><br>
While the general trends can be automated pretty easily (just add them
every time), weird trends will often require a human eye and knowledge
of the world to find.  This is one reason that graphing is so
important.</p></li>
<li><p><em>Data errors</em><br>
All too often data has serious errors in it.  For example, you may find that the dates were encoded in two formats and only one of them has been correctly loaded into your program.  There are a myriad of such problems and they are surprisingly common.  This is the other reason I think graphing is important, not just for time series, but for any data. </p></li>
</ul>
","<machine-learning><time-series><feature-selection>","56","56","56","0","4808","676"
"132","{
  ""id"": 132,
  ""title"": ""What is dimensionality reduction? What is the difference between feature selection and extraction?""
}","<p>Simply put: </p>

<ul>
<li>feature selection: you select a subset of the original feature set; while</li>
<li>feature extraction: you build a new set of features from the original feature set. </li>
</ul>

<p>Examples of feature extraction: extraction of contours in images, extraction of digrams from a text, extraction of phonemes from recording of spoken text, etc.</p>

<p>Feature extraction involves a transformation of the features, which often is not reversible because some information is lost in the process of dimensionality reduction.</p>
","<feature-selection><feature-extraction><dimensionality-reduction>","56","58","57","1","172","1456"
"51225","{
  ""id"": 51225,
  ""title"": ""What is the positional encoding in the transformer model?""
}","<p>For example, for word <span class=""math-container"">$w$</span> at position <span class=""math-container"">$pos \in [0, L-1]$</span> in the input sequence <span class=""math-container"">$\boldsymbol{w}=(w_0,\cdots, w_{L-1})$</span>, with 4-dimensional embedding <span class=""math-container"">$e_{w}$</span>, and <span class=""math-container"">$d_{model}=4$</span>, the operation would be
<span class=""math-container"">$$\begin{align*}e_{w}' &amp;= e_{w} + \left[sin\left(\frac{pos}{10000^{0}}\right), cos\left(\frac{pos}{10000^{0}}\right),sin\left(\frac{pos}{10000^{2/4}}\right),cos\left(\frac{pos}{10000^{2/4}}\right)\right]\\
&amp;=e_{w} + \left[sin\left(pos\right), cos\left(pos\right),sin\left(\frac{pos}{100}\right),cos\left(\frac{pos}{100}\right)\right]\\
\end{align*}$$</span></p>
<p>where the formula for positional encoding is as follows
<span class=""math-container"">$$\text{PE}(pos,2i)=sin\left(\frac{pos}{10000^{2i/d_{model}}}\right),$$</span>
<span class=""math-container"">$$\text{PE}(pos,2i+1)=cos\left(\frac{pos}{10000^{2i/d_{model}}}\right).$$</span>
with <span class=""math-container"">$d_{model}=512$</span> (thus <span class=""math-container"">$i \in [0, 255]$</span>) in the original paper.</p>
<p>This technique is used because there is <strong>no notion of word order</strong> (1st word, 2nd word, ..) in the proposed architecture. All words of input sequence are fed to the network with no special order or position; in contrast, in RNN architecture, <span class=""math-container"">$n$</span>-th word is fed at step <span class=""math-container"">$n$</span>, and in ConvNet, it is fed to specific input indices. Therefore, proposed model has no idea how the words are ordered. Consequently, a position-dependent signal is added to each word-embedding to help the model incorporate the order of words. Based on experiments, this addition not only avoids destroying the embedding information but also adds the vital position information.</p>
<p><a href=""https://kazemnejad.com/blog/transformer_architecture_positional_encoding/"" rel=""nofollow noreferrer"">This blog by Kazemnejad</a> explains that the specific choice of (<span class=""math-container"">$sin$</span>, <span class=""math-container"">$cos$</span>) pair helps the model in learning patterns that rely on relative positions. As an example, consider a pattern like</p>
<pre><code>if 'are' comes after 'they', then 'playing' is more likely than 'play'
</code></pre>
<p>which relies on relative position &quot;<span class=""math-container"">$pos(\text{are}) - pos(\text{they})$</span>&quot; being 1, independent of absolute positions <span class=""math-container"">$pos(\text{are})$</span> and <span class=""math-container"">$pos(\text{they})$</span>. To learn this pattern, any positional encoding should make it easy for the model to arrive at an encoding for &quot;they are&quot; that (a) is different from &quot;are they&quot; (considers relative position), and (b) is independent of where &quot;they are&quot; occurs in a given sequence (ignores absolute positions), which is what <span class=""math-container"">$\text{PE}$</span> manages to achieve.</p>
<p><a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">This article by Jay Alammar</a> explains the paper with excellent visualizations. The example on positional encoding calculates <span class=""math-container"">$\text{PE}(.)$</span> the same, with the only difference that it puts <span class=""math-container"">$sin$</span> in the first half of embedding dimensions (as opposed to even indices) and <span class=""math-container"">$cos$</span> in the second half (as opposed to odd indices). As pointed out by <a href=""https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model/51225#comment91915_51225"">ShaohuaLi</a>, this difference does not matter since vector operations would be invariant to the permutation of dimensions.</p>
","<nlp><encoding><attention-mechanism><transformer>","56","56","56","0","67328","7999"
"13362","{
  ""id"": 13362,
  ""title"": ""When to use (He or Glorot) normal initialization over uniform init? And what are its effects with Batch Normalization?""
}","<p>The normal vs uniform init seem to be rather unclear in fact.</p>

<p>If we refer solely on the <a href=""http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf"" rel=""noreferrer"">Glorot</a>'s and <a href=""http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf"" rel=""noreferrer"">He</a>'s initializations papers, they both use a similar theoritical analysis: they find a good variance for the distribution from which the initial parameters are drawn. This variance is adapted to the activation function used and is derived without explicitly considering the type of the distribution. As such, their theorical conclusions hold for any type of distribution of the determined variance. In fact, in the Glorot paper, a uniform distribution is used whereas in the He paper it is a gaussian one that is chosen. The only ""explaination"" given for this choice in the He paper is:</p>

<blockquote>
  <p>Recent deep CNNs are mostly initialized by random weights drawn from Gaussian distributions</p>
</blockquote>

<p>with a reference to <a href=""http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"" rel=""noreferrer"">AlexNet paper</a>. It was indeed released a little later than Glorot's initialization but however there is no justificaion in it of the use of a normal distribution.</p>

<p>In fact, in a <a href=""https://github.com/fchollet/keras/issues/52"" rel=""noreferrer"">discussion on Keras issues tracker</a>, they also seem to be a little confused and basically it could only be a matter of preference... (i.e. hypotetically Bengio would prefer uniform distribution whereas Hinton would prefer normal ones...) One the discussion, there is a small benchmark comparing Glorot initialization using a uniform and a gaussian distribution. In the end, it seems that the uniform wins but it is not really clear.</p>

<p>In the original <a href=""https://arxiv.org/pdf/1512.03385v1.pdf"" rel=""noreferrer"">ResNet paper</a>, it only says they used a gaussian He init for all the layers, I was not able to find where it is written that they used a uniform He init for the first layer. (maybe you could share a reference to this?)</p>

<p>As for the use of gaussian init with Batch Normalization, well, with BN the optimization process is less sensitive to initialization thus it is just a convention I would say.</p>
","<neural-network><deep-learning><normalization>","56","56","56","0","23283","899"
"712","{
  ""id"": 712,
  ""title"": ""Are Support Vector Machines still considered \""state of the art\"" in their niche?""
}","<p>SVM is a powerful classifier. It has some nice advantages (which I guess were responsible for its popularity)... These are:</p>

<ul>
<li>Efficiency: Only the support vectors play a role in determining the classification boundary. All other points from the training set needn't be stored in memory.</li>
<li>The so-called power of kernels: With appropriate kernels you can transform feature space into a higher dimension so that it becomes linearly separable. The notion of kernels work with arbitrary objects on which you can define some notion of similarity with the help of inner products... and hence SVMs can classify arbitrary objects such as trees, graphs etc.</li>
</ul>

<p>There are some significant disadvantages as well.</p>

<ul>
<li>Parameter sensitivity: The performance is highly sensitive to the choice of the regularization parameter C, which allows some variance in the model.</li>
<li>Extra parameter for the Gaussian kernel: The radius of the Gaussian kernel can have a significant impact on classifier accuracy. Typically a grid search has to be conducted to find optimal parameters. LibSVM has a support for grid search.</li>
</ul>

<p>SVMs generally belong to the class of ""Sparse Kernel Machines"". The sparse vectors in the case of SVM are the support vectors which are chosen from the maximum margin criterion. Other sparse vector machines such as the <strong>Relevance Vector Machine</strong> (RVM) perform better than SVM. The following figure shows a comparative performance of the two. In the figure, the x-axis shows one dimensional data from two classes y={0,1}. The mixture model is defined as P(x|y=0)=Unif(0,1) and P(x|y=1)=Unif(.5,1.5) (Unif denotes uniform distribution). 1000 points were sampled from this mixture and an SVM and an RVM were used to estimate the posterior. The problem of SVM is that the predicted values are far off from the true log odds.  </p>

<p><img src=""https://i.stack.imgur.com/zNYbt.png"" alt=""RVM vs. SVM""> </p>

<p>A very effective classifier, which is very popular nowadays, is the <strong>Random Forest</strong>. The main advantages are:</p>

<ul>
<li>Only one parameter to tune (i.e. the number of trees in the forest)</li>
<li>Not utterly parameter sensitive</li>
<li>Can easily be extended to multiple classes</li>
<li>Is based on probabilistic principles (maximizing mutual information gain with the help of decision trees)</li>
</ul>
","<data-mining><svm><state-of-the-art>","55","55","55","0","984","1496"
"27616","{
  ""id"": 27616,
  ""title"": ""Should we apply normalization to test data as well?""
}","<p>Yes you need to apply normalisation to test data, if your algorithm works with or needs normalised training data*. </p>

<p>That is because your model works on the representation given by its input vectors. The scale of those numbers is part of the representation. This is a bit like converting between feet and metres . . . a model or formula would work with just one type of unit normally.</p>

<p>Not only do you need normalisation, but you should apply the exact same scaling as for your training data. That means storing the scale and offset used with your training data, and using that again. A common beginner mistake is to separately normalise your train and test data.</p>

<p>In Python and SKLearn, you might normalise your input/X values using the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"" rel=""noreferrer"">Standard Scaler</a> like this:</p>

<pre><code>scaler = StandardScaler()
train_X = scaler.fit_transform( train_X )
test_X = scaler.transform( test_X )
</code></pre>

<p>Note how the conversion of <code>train_X</code> using a function which fits (figures out the params) then normalises. Whilst the <code>test_X</code> conversion just transforms, using the same params that it learned from the train data.</p>

<p>The tf-idf normalisation you are applying should work similarly, as it learns some parameters from the data set as a whole (frequency of words in all documents), as well as using ratios found in each document.</p>

<hr>

<p>* Some algorithms (such as those based on decision trees) do not need normalised inputs, and can cope with features that have different inherent scales.</p>
","<machine-learning><neural-network><deep-learning>","55","55","55","0","836","25808"
"41923","{
  ""id"": 41923,
  ""title"": ""Sparse_categorical_crossentropy vs categorical_crossentropy (keras, accuracy)""
}","<p>Use sparse categorical crossentropy when your classes are mutually exclusive (e.g. when each sample belongs exactly to one class) and categorical crossentropy when one sample can have multiple classes or labels are soft probabilities (like [0.5, 0.3, 0.2]).</p>

<p>Formula for categorical crossentropy (S - samples, C - classess, <span class=""math-container"">$s \in c $</span> - sample belongs to class c) is:</p>

<p><span class=""math-container"">$$ -\frac{1}{N} \sum_{s\in S} \sum_{c \in C} 1_{s\in c} log {p(s \in c)} $$</span> </p>

<p>For case when classes are exclusive, you don't need to sum over them - for each sample only non-zero value is just <span class=""math-container"">$-log p(s \in c)$</span> for true class c.</p>

<p>This allows to conserve time and memory. Consider case of 10000 classes when they are mutually exclusive - just 1 log instead of summing up 10000 for each sample, just one integer instead of 10000 floats.</p>

<p>Formula is the same in both cases, so no impact on accuracy should be there. </p>
","<neural-network><keras><loss-function><encoding>","54","54","54","0","23398","656"
"22338","{
  ""id"": 22338,
  ""title"": ""Why are Machine Learning models called black boxes?""
}","<p>The <em>black box</em> thing has nothing to do with the level of expertise of the audience (as long as the audience is human), but with the <em>explainability</em> of the function modelled by the machine learning algorithm.</p>

<p>In logistic regression, there is a very simple relationship between inputs and outputs. You can sometimes understand why a certain sample was incorrectly catalogued (e.g. because the value of certain component of the input vector was too low).</p>

<p>The same applies to decision trees: you can <em>follow</em> the logic applied by the tree and understand why a certain element was assigned to one class or the other.</p>

<p>However, deep neural networks are the paradigmatic example of black box algorithms. No one, not even the most expert person in the world grasp the function that is actually modeled by training a neural network. An insight about this can be provided by <a href=""https://blog.openai.com/adversarial-example-research/"" rel=""noreferrer"">adversarial examples</a>: some slight (and unnoticeable by a human) change in a training sample can lead the network to think that it belongs to a totally different label. There are some techniques to create adversarial examples, and some techniques to improve robustness against them. But given that no one actually knows all the relevant properties of the function being modeled by the network, it is always possible to find a novel way to create them.</p>

<p>Humans are also black boxes and <a href=""https://twitter.com/goodfellow_ian/status/870813726751956993?lang=en"" rel=""noreferrer"">we are also sensible to adversarial examples</a>.</p>
","<machine-learning><terminology>","54","56","55","1","14675","13257"
"16908","{
  ""id"": 16908,
  ""title"": ""GBM vs XGBOOST? Key differences?""
}","<p>Quote from the author of <code>xgboost</code>:</p>
<blockquote>
<p>Both xgboost and gbm follows the principle of gradient boosting.  There are however, the difference in modeling details. Specifically,  xgboost used a more regularized model formalization to control over-fitting, which gives it better performance.</p>
<p>We have updated a comprehensive tutorial on introduction to the model, which you might want to take a look at. <a href=""https://xgboost.readthedocs.io/en/latest/tutorials/model.html"" rel=""noreferrer"">Introduction to Boosted Trees</a></p>
<p>The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost. For model, it might be more suitable to be called as regularized gradient boosting.</p>
</blockquote>
<p>Edit: There's a detailed <a href=""https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"" rel=""noreferrer"">guide</a> of xgboost which shows more differences.</p>
<h2>References</h2>
<p><a href=""https://www.quora.com/What-is-the-difference-between-the-R-gbm-gradient-boosting-machine-and-xgboost-extreme-gradient-boosting"" rel=""noreferrer"">https://www.quora.com/What-is-the-difference-between-the-R-gbm-gradient-boosting-machine-and-xgboost-extreme-gradient-boosting</a></p>
<p><a href=""https://xgboost.readthedocs.io/en/latest/tutorials/model.html"" rel=""noreferrer"">https://xgboost.readthedocs.io/en/latest/tutorials/model.html</a></p>
","<machine-learning><algorithms><xgboost><ensemble-modeling><gbm>","54","54","54","0","28628","3736"
"29726","{
  ""id"": 29726,
  ""title"": ""How to set batch_size, steps_per epoch, and validation steps?""
}","<ul>
<li><strong>batch_size</strong> determines the number of samples in each mini batch. Its maximum is the number of all samples, which makes gradient descent accurate, the loss will decrease towards the minimum if the learning rate is small enough, but iterations are slower. Its minimum is 1, resulting in stochastic gradient descent: Fast but the direction of the gradient step is based only on one example, the loss may jump around. batch_size allows to adjust between the two extremes: accurate gradient direction and fast iteration. Also, the maximum value for batch_size may be limited if your model + data set does not fit into the available (GPU) memory.</li>
<li><strong>steps_per_epoch</strong> the number of batch iterations before a training epoch is considered finished. If you have a training set of fixed size you can ignore it but it may be useful if you have a <em>huge</em> data set or if you are generating random data augmentations on the fly, i.e. if your training set has a (generated) infinite size. If you have the time to go through your whole training data set I recommend to skip this parameter.</li>
<li><strong>validation_steps</strong> similar to steps_per_epoch but on the validation data set instead on the training data. If you have the time to go through your whole validation data set I recommend to skip this parameter.</li>
</ul>
","<machine-learning><keras><cnn><theano>","54","54","54","0","49704","731"
"17139","{
  ""id"": 17139,
  ""title"": ""Adding Features To Time Series Model LSTM""
}","<p>For RNNs (e.g., LSTMs and GRUs), the layer input <em>is</em> a list of timesteps, and each timestep <em>is</em> a feature tensor. That means that you could have a input tensor like this (in Pythonic notation):</p>

<pre><code># Input tensor to RNN
[
    # Timestep 1
    [ temperature_in_paris, value_of_nasdaq, unemployment_rate ],
    # Timestep 2
    [ temperature_in_paris, value_of_nasdaq, unemployment_rate ],
    # Timestep 3
    [ temperature_in_paris, value_of_nasdaq, unemployment_rate ],
    ...
]
</code></pre>

<p>So absolutely, you can have multiple features at each timestep. In my mind, weather is a time series feature: where I live, it happens to be a function of time. So it would be quite reasonable to encode weather information as one of your features in each timestep (with an appropriate encoding, like cloudy=0, sunny=1, etc.).</p>

<p>If you have non-time-series data, then it doesn't really make sense to pass it through the LSTM, though. Maybe the LSTM will work anyway, but even if it does, it will probably come at the cost of higher loss / lower accuracy per training time.</p>

<p>Alternatively, you can introduce this sort of ""extra"" information into your model outside of the LSTM by means of additional layers. You might have a data flow like this:</p>

<pre><code>TIME_SERIES_INPUT ------&gt; LSTM -------\
                                       *---&gt; MERGE ---&gt; [more processing]
AUXILIARY_INPUTS --&gt; [do something] --/
</code></pre>

<p>So you would merge your auxiliary inputs into the LSTM outputs, and continue your network from there. Now your model is simply multi-input.</p>

<p>For example, let's say that in your particular application, you only keep the last output of the LSTM output sequence. Let's say that it is a vector of length 10. You auxiliary input might be your encoded weather (so a scalar). Your merge layer could simply append the auxiliary weather information onto the end of the LSTM output vector to produce a single vector of length 11. But you don't <em>need</em> to just keep the last LSTM output timestep: if the LSTM outputted 100 timesteps, each with a 10-vector of features, you could still tack on your auxiliary weather information, resulting in 100 timesteps, each consisting of a vector of 11 datapoints.</p>

<p>The Keras documentation on its <a href=""https://keras.io/getting-started/functional-api-guide/"" rel=""noreferrer"">functional API</a> has a good overview of this.</p>

<p>In other cases, as @horaceT points out, you may want to condition the LSTM on non-temporal data. For example, predict the weather tomorrow, given location. In this case, here are three suggestions, each with positive/negatives:</p>

<ol>
<li><p>Have the first timestep contain your conditioning data, since it will effectively ""set"" the internal/hidden state of your RNN. Frankly, I would <strong>not</strong> do this, for a bunch of reasons: your conditioning data needs to be the same shape as the rest of your features, makes it harder to create stateful RNNs (in terms of being really careful to track how you feed data into the network), the network may ""forget"" the conditioning data with enough time (e.g., long training sequences, or long prediction sequences), etc.</p></li>
<li><p>Include the data as part of the temporal data itself. So each feature vector at a particular timestep includes ""mostly"" time-series data, but then has the conditioning data appended to the end of each feature vector. Will the network learn to recognize this? Probably, but even then, you are creating a harder learning task by polluting the sequence data with non-sequential information. So I would also <strong>discourage</strong> this.</p></li>
<li><p>Probably the <strong>best</strong> approach would be to directly affect the hidden state of the RNN at time zero. This is the approach taken by <a href=""https://arxiv.org/abs/1412.2306"" rel=""noreferrer"">Karpathy and Fei-Fei</a> and by <a href=""https://arxiv.org/abs/1411.4555"" rel=""noreferrer"">Vinyals et al</a>. This is how it works:</p>

<ol>
<li>For each training sample, take your condition variables $\vec{x}$.</li>
<li>Transform/reshape your condition variables with an affine transformation to get it into the right shape as the internal state of the RNN: $\vec{v} = \mathbf{W} \vec{x} + \vec{b}$ (these $\mathbf{W}$ and $\vec{b}$ are trainable weights). You can obtain it with a Dense layer in keras.</li>
<li>For the very first timestep, add $\vec{v}$ to the hidden state of the RNN when calculating its value.</li>
</ol>

<p>This approach is the most ""theoretically"" correct, since it properly conditions the RNN on your non-temporal inputs, naturally solves the shape problem, and also avoids polluting your inputs timesteps with additional, non-temporal information. The downside is that this approach often requires graph-level control of your architecture, so if you are using a higher-level abstraction like Keras, you will find it hard to implement unless you add your own layer type.</p></li>
</ol>
","<machine-learning><neural-network><deep-learning><time-series>","53","53","53","0","29083","1026"
"11361","{
  ""id"": 11361,
  ""title"": ""Merging multiple data frames row-wise in PySpark""
}","<p>Stolen from: <a href=""https://stackoverflow.com/questions/33743978/spark-union-of-multiple-rdds"">https://stackoverflow.com/questions/33743978/spark-union-of-multiple-rdds</a></p>

<p>Outside of chaining unions this is the only way to do it for DataFrames.</p>

<pre><code>from functools import reduce  # For Python 3.x
from pyspark.sql import DataFrame

def unionAll(*dfs):
    return reduce(DataFrame.unionAll, dfs)

unionAll(td2, td3, td4, td5, td6, td7, td8, td9, td10)
</code></pre>

<p>What happens is that it takes all the objects that you passed as parameters and reduces them using unionAll (this reduce is from Python, not the Spark reduce although they work similarly) which eventually reduces it to one DataFrame.</p>

<p>If instead of DataFrames they are normal RDDs you can pass a list of them to the union function of your SparkContext</p>

<p>EDIT: For your purpose I propose a different method, since you would have to repeat this whole union 10 times for your different folds for crossvalidation, I would add labels for which fold a row belongs to and just filter your DataFrame for every fold based on the label</p>
","<python><apache-spark><cross-validation><pyspark>","53","53","53","0","14904","8748"
"40908","{
  ""id"": 40908,
  ""title"": ""When to use One Hot Encoding vs LabelEncoder vs DictVectorizor?""
}","<p>While <a href=""https://datascience.stackexchange.com/a/9447/29575"">AN6U5</a> has given a very good answer, I wanted to add a few points for future reference. When considering <strong>One Hot Encoding</strong>(OHE) and <strong>Label Encoding</strong>, we must try and understand what model you are trying to build. Namely the two categories of model we will be considering are:</p>

<ol>
<li><strong>Tree Based Models</strong>: Gradient Boosted Decision Trees and Random Forests.</li>
<li><strong>Non-Tree Based Models</strong>: Linear, kNN or Neural Network based.</li>
</ol>

<p>Let's consider when to apply OHE and when to apply Label Encoding while building tree based models.</p>

<p>We apply OHE when:</p>

<ol>
<li>When the values that are <strong>close to each other</strong> in the label encoding correspond to target values that aren't close (non - linear data).</li>
<li>When the categorical <strong>feature is not ordinal</strong> (dog,cat,mouse).</li>
</ol>

<p>We apply Label encoding when:</p>

<ol>
<li>The categorical <strong>feature is ordinal</strong> (Jr. kg, Sr. kg, Primary school, high school ,etc).</li>
<li>When we can come up with a label encoder that <strong>assigns close labels to similar categories</strong>: This leads to less splits in the tress hence reducing the execution time.</li>
<li>When the number of categorical features in the dataset is huge: One-hot encoding a categorical feature with huge number of values can lead to (1) high memory consumption and (2) the case when non-categorical features are rarely used by model. You can deal with the 1st case if you employ sparse matrices. The 2nd case can occur if you build a tree using only a subset of features. For example, if you have 9 numeric features and 1 categorical with 100 unique values and you one-hot-encoded that categorical feature, you will get 109 features. If a tree is built with only a subset of features, initial 9 numeric features will rarely be used. In this case, you can increase the parameter controlling size of this subset. In xgboost it is called colsample_bytree, in sklearn's Random Forest max_features.</li>
</ol>

<p>In case you want to continue with OHE, as @AN6U5 suggested, you might want to combine PCA with OHE.</p>

<p>Lets consider when to apply OHE and Label Encoding while building non tree based models.</p>

<p>To apply Label encoding, the dependance between feature and target must be linear in order for Label Encoding to be utilised effectively.</p>

<p>Similarly, in case the dependance is non-linear, you might want to use OHE for the same.</p>

<p>Note: Some of the explanation has been referenced from <strong>How to Win a Data Science Competition</strong> from Coursera.</p>
","<scikit-learn><categorical-data><feature-engineering>","52","52","52","0","61253","691"
"20176","{
  ""id"": 20176,
  ""title"": ""What are deconvolutional layers?""
}","<p>Step by step math explaining how transpose convolution does 2x upsampling with 3x3 filter and stride of 2:</p>

<p><a href=""https://i.stack.imgur.com/GlqLM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/GlqLM.png"" alt=""enter image description here""></a></p>

<p>The simplest TensorFlow snippet to validate the math:
</p>

<pre><code>import tensorflow as tf
import numpy as np

def test_conv2d_transpose():
    # input batch shape = (1, 2, 2, 1) -&gt; (batch_size, height, width, channels) - 2x2x1 image in batch of 1
    x = tf.constant(np.array([[
        [[1], [2]], 
        [[3], [4]]
    ]]), tf.float32)

    # shape = (3, 3, 1, 1) -&gt; (height, width, input_channels, output_channels) - 3x3x1 filter
    f = tf.constant(np.array([
        [[[1]], [[1]], [[1]]], 
        [[[1]], [[1]], [[1]]], 
        [[[1]], [[1]], [[1]]]
    ]), tf.float32)

    conv = tf.nn.conv2d_transpose(x, f, output_shape=(1, 4, 4, 1), strides=[1, 2, 2, 1], padding='SAME')

    with tf.Session() as session:
        result = session.run(conv)

    assert (np.array([[
        [[1.0], [1.0],  [3.0], [2.0]],
        [[1.0], [1.0],  [3.0], [2.0]],
        [[4.0], [4.0], [10.0], [6.0]],
        [[3.0], [3.0],  [7.0], [4.0]]]]) == result).all()
</code></pre>
","<neural-network><convnet><convolution>","52","52","52","0","34204","621"
"689","{
  ""id"": 689,
  ""title"": ""What are some standard ways of computing the distance between documents?""
}","<p>There's a number of different ways of going about this depending on exactly how much semantic information you want to retain and how easy your documents are to tokenize (html documents would probably be pretty difficult to tokenize, but you could conceivably do something with tags and context.)</p>

<p>Some of them have been mentioned by ffriend, and the paragraph vectors by user1133029 is a really solid one, but I just figured I would go into some more depth about plusses and minuses of different approaches.</p>

<ul>
<li><a href=""http://en.wikipedia.org/wiki/Cosine_similarity"">Cosine Distance</a> - Tried a true, cosine distance is probably the most common distance metric used generically across multiple domains. With that said, there's very little information in cosine distance that can actually be mapped back to anything semantic, which seems to be non-ideal for this situation.</li>
<li><a href=""http://en.wikipedia.org/wiki/Levenshtein_distance"">Levenshtein Distance</a> - Also known as <code>edit distance</code>, this is usually just used on the individual token level (words, bigrams, etc...). In general I wouldn't recommend this metric as it not only discards any semantic information, but also tends to treat very different word alterations very similarly, but it is an extremely common metric for this kind of thing</li>
<li><a href=""http://en.wikipedia.org/wiki/Latent_semantic_analysis"">LSA</a> - Is a part of a large arsenal of techniques when it comes to evaluating document similarity called <code>topic modeling</code>. LSA has gone out of fashion pretty recently, and in my experience, it's not quite the strongest topic modeling approach, but it is relatively straightforward to implement and has a few open source implementations</li>
<li><a href=""http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"">LDA</a> - Is also a technique used for <code>topic modeling</code>, but it's different from <code>LSA</code> in that it actually learns internal representations that tend to be more smooth and intuitive. In general, the results you get from <code>LDA</code> are better for modeling document similarity than <code>LSA</code>, but not quite as good for learning how to discriminate strongly between topics.</li>
<li><a href=""http://en.wikipedia.org/wiki/Pachinko_allocation"">Pachinko Allocation</a> - Is a really neat extension on top of LDA. In general, this is just a significantly improved version of <code>LDA</code>, with the only downside being that it takes a bit longer to train and open-source implementations are a little harder to come by</li>
<li><a href=""https://code.google.com/p/word2vec/"">word2vec</a> - Google has been working on a series of techniques for intelligently reducing words and documents to more reasonable vectors than the sparse vectors yielded by techniques such as <code>Count Vectorizers</code> and <code>TF-IDF</code>. Word2vec is great because it has a number of open source implementations. Once you have the vector, any other similarity metric (like cosine distance) can be used on top of it with significantly more efficacy.</li>
<li><a href=""http://cs.stanford.edu/~quocle/paragraph_vector.pdf"">doc2vec</a> - Also known as <code>paragraph vectors</code>, this is the latest and greatest in a series of papers by Google, looking into dense vector representations of documents. The <code>gensim</code> library in python has an implementation of <code>word2vec</code> that is straightforward enough that it can pretty reasonably be leveraged to build <code>doc2vec</code>, but make sure to keep the license in mind if you want to go down this route</li>
</ul>

<p>Hope that helps, let me know if you've got any questions.</p>
","<machine-learning><data-mining><nlp><text-mining><similarity>","51","51","51","0","548","4069"
"5152","{
  ""id"": 5152,
  ""title"": ""Deep Learning vs gradient boosting: When to use what?""
}","<p>Why restrict yourself to those two approaches? Because they're cool? I would always start with a simple linear classifier \ regressor. So in this case a Linear SVM or Logistic Regression, preferably with an algorithm implementation that can take advantage of sparsity due to the size of the data. It will take a long time to run a DL algorithm on that dataset, and I would only normally try deep learning on specialist problems where there's some hierarchical structure in the data, such as images or text. It's overkill for a lot of simpler learning problems, and takes a lot of time and expertise to learn and also DL algorithms are very slow to train. Additionally, just because you have 50M rows, doesn't mean you need to use the entire dataset to get good results. Depending on the data, you may get good results with a sample of a few 100,000 rows or a few million. I would start simple, with a small sample and a linear classifier, and get more complicated from there if the results are not satisfactory. At least that way you'll get a baseline. We've often found simple linear models to out perform more sophisticated models on most tasks, so you want to always start there.</p>
","<machine-learning><classification><deep-learning>","51","51","51","0","1301","946"
"759","{
  ""id"": 759,
  ""title"": ""Tools and protocol for reproducible data science using Python""
}","<p>The topic of <em>reproducible research</em> (RR) is <strong>very popular</strong> today and, consequently, is <strong>huge</strong>, but I hope that my answer will be <strong>comprehensive enough</strong> as an answer and will provide enough information for <strong>further research</strong>, should you decide to do so.</p>

<p>While Python-specific tools for RR certainly exist out there, I think it makes more sense to focus on more <strong>universal tools</strong> (you never know for sure what programming languages and computing environments you will be working with in the future). Having said that, let's take a look what tools are available per your list.</p>

<p>1) <strong>Tools for data version control</strong>. Unless you plan to work with (very) <em>big data</em>, I guess, it would make sense to use the same <code>git</code>, which you use for source code version control. The infrastructure is already there. Even if your files are binary and big, this advice might be helpful: <a href=""https://stackoverflow.com/questions/540535/managing-large-binary-files-with-git"">https://stackoverflow.com/questions/540535/managing-large-binary-files-with-git</a>.</p>

<p>2) <strong>Tools for managing RR workflows and experiments</strong>. Here's a list of most popular tools in this category, to the best of my knowledge (in the descending order of popularity):</p>

<ul>
<li><p><em>Taverna Workflow Management System</em> (<a href=""http://www.taverna.org.uk"" rel=""noreferrer"">http://www.taverna.org.uk</a>) - very solid, if a little too complex, set of tools. The major tool is a Java-based desktop software. However, it is compatible with online workflow repository portal <em>myExperiment</em> (<a href=""http://www.myexperiment.org"" rel=""noreferrer"">http://www.myexperiment.org</a>), where user can store and share their RR workflows. Web-based RR portal, fully compatible with <em>Taverna</em> is called <em>Taverna Online</em>, but it is being developed and maintained by totally different organization in Russia (referred there to as <em>OnlineHPC</em>: <a href=""http://onlinehpc.com"" rel=""noreferrer"">http://onlinehpc.com</a>).</p></li>
<li><p><em>The Kepler Project</em> (<a href=""https://kepler-project.org"" rel=""noreferrer"">https://kepler-project.org</a>)</p></li>
<li><p><em>VisTrails</em> (<a href=""http://vistrails.org"" rel=""noreferrer"">http://vistrails.org</a>)</p></li>
<li><p><em>Madagascar</em> (<a href=""http://www.reproducibility.org"" rel=""noreferrer"">http://www.reproducibility.org</a>)</p></li>
</ul>

<p><strong>EXAMPLE</strong>. Here's an interesting article on scientific workflows with an example of the <strong>real</strong> workflow design and data analysis, based on using <em>Kepler</em> and <em>myExperiment</em> projects: <a href=""http://f1000research.com/articles/3-110/v1"" rel=""noreferrer"">http://f1000research.com/articles/3-110/v1</a>.</p>

<p>There are many RR tools that implement <em>literate programming</em> paradigm, exemplified by <code>LaTeX</code> software family. Tools that help in report generation and presentation is also a large category, where <code>Sweave</code> and <code>knitr</code> are probably the most well-known ones. <code>Sweave</code> is a tool, focused on R, but it can be integrated with Python-based projects, albeit with some additional effort (<a href=""https://stackoverflow.com/questions/2161152/sweave-for-python"">https://stackoverflow.com/questions/2161152/sweave-for-python</a>). I think that <code>knitr</code> might be a better option, as it's modern, has extensive support by popular tools (such as <code>RStudio</code>) and is language-neutral (<a href=""http://yihui.name/knitr/demo/engines"" rel=""noreferrer"">http://yihui.name/knitr/demo/engines</a>).</p>

<p>3) <strong>Protocol and suggested directory structure</strong>. If I understood correctly what you implied by using term <em>protocol</em> (<em>workflow</em>), generally I think that standard RR data analysis workflow consists of the following sequential phases: <em>data collection</em> => <em>data preparation</em> (cleaning, transformation, merging, sampling) => <em>data analysis</em> => <em>presentation of results</em> (generating reports and/or presentations). Nevertheless, every workflow is project-specific and, thus, some specific tasks might require adding additional steps.</p>

<p>For sample directory structure, you may take a look at documentation for R package <code>ProjectTemplate</code> (<a href=""http://projecttemplate.net"" rel=""noreferrer"">http://projecttemplate.net</a>), as an attempt to automate data analysis workflows and projects:</p>

<p><img src=""https://i.stack.imgur.com/0B2vo.png"" alt=""enter image description here""></p>

<p>4) <strong>Automated build/run tools</strong>. Since my answer is focused on universal (language-neutral) RR tools, the most popular tools is <code>make</code>. Read the following article for some reasons to use <code>make</code> as the preferred RR workflow automation tool: <a href=""http://bost.ocks.org/mike/make"" rel=""noreferrer"">http://bost.ocks.org/mike/make</a>. Certainly, there are other <strong>similar</strong> tools, which either improve some aspects of <code>make</code>, or add some additional features. For example: <code>ant</code> (officially, Apache Ant: <a href=""http://ant.apache.org"" rel=""noreferrer"">http://ant.apache.org</a>), <code>Maven</code> (""next generation <code>ant</code>"": <a href=""http://maven.apache.org"" rel=""noreferrer"">http://maven.apache.org</a>), <code>rake</code> (<a href=""https://github.com/ruby/rake"" rel=""noreferrer"">https://github.com/ruby/rake</a>), <code>Makepp</code> (<a href=""http://makepp.sourceforge.net"" rel=""noreferrer"">http://makepp.sourceforge.net</a>). For a comprehensive list of such tools, see Wikipedia: <a href=""http://en.wikipedia.org/wiki/List_of_build_automation_software"" rel=""noreferrer"">http://en.wikipedia.org/wiki/List_of_build_automation_software</a>.</p>
","<python><tools><version-control>","51","51","51","0","2452","6478"
"30882","{
  ""id"": 30882,
  ""title"": ""When is precision more important over recall?""
}","<ul>
<li>For rare cancer data modeling, anything that doesn't account for false-negatives is a crime. <strong>Recall</strong> is a better measure than precision.</li>
<li>For YouTube recommendations, false-negatives is less of a concern. <strong>Precision</strong> is better here.</li>
</ul>
","<machine-learning><evaluation>","51","51","51","0","9123","3169"
"628","{
  ""id"": 628,
  ""title"": ""When is a Model Underfitted?""
}","<p>A model underfits when it is too simple with regards to the data it is trying to model.</p>

<p>One way to detect such situation is to use the <a href=""http://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma"" rel=""noreferrer"">bias–variance approach</a>, which can represented like this:</p>

<p><img src=""https://i.stack.imgur.com/t0zit.png"" alt=""enter image description here""></p>

<p>Your model is underfitted when you have a high bias.</p>

<hr>

<p>To know whether you have a too high bias or a too high variance, you view the phenomenon in terms of training and test errors:</p>

<p>High bias: This learning curve shows high error on both the training and test sets, so the algorithm is suffering from high bias:</p>

<p><img src=""https://i.stack.imgur.com/KFjM4.png"" alt=""enter image description here""></p>

<p>High variance: This learning curve shows a large gap between training and test set errors, so the algorithm is suffering from high variance.</p>

<p><img src=""https://i.stack.imgur.com/Ypj9y.png"" alt=""enter image description here""></p>

<p>If an algorithm is suffering from high variance:</p>

<ul>
<li>more data will probably help</li>
<li>otherwise reduce the model complexity</li>
</ul>

<p>If an algorithm is suffering from high bias:</p>

<ul>
<li>increase the model complexity</li>
</ul>

<p>I would advise to watch <a href=""https://www.coursera.org/course/ml"" rel=""noreferrer"">Coursera' Machine Learning course</a>, section ""10: Advice for applying Machine Learning"", from which I took the above graphs.</p>
","<efficiency><algorithms><parameter>","50","50","50","0","843","5095"
"16801","{
  ""id"": 16801,
  ""title"": ""How to count the number of missing values in each row in Pandas dataframe?""
}","<p>When using pandas, try to avoid performing operations in a loop, including <code>apply</code>, <code>map</code>, <code>applymap</code> etc. That's slow!</p>

<p>A DataFrame object has two axes: “axis 0” and “axis 1”. “axis 0” represents rows and “axis 1” represents columns.</p>

<p>If you want to count the missing values in each column, try:</p>

<p><code>df.isnull().sum()</code> as default or <code>df.isnull().sum(axis=0)</code></p>

<p>On the other hand, you can count in each row (which is your question) by:</p>

<p><code>df.isnull().sum(axis=1)</code></p>

<p>It's roughly 10 times faster than Jan van der Vegt's solution(BTW he counts valid values, rather than missing values):</p>

<pre><code>In [18]: %timeit -n 1000 df.apply(lambda x: x.count(), axis=1)
1000 loops, best of 3: 3.31 ms per loop

In [19]: %timeit -n 1000 df.isnull().sum(axis=1)
1000 loops, best of 3: 329 µs per loop
</code></pre>
","<python><pandas>","49","49","49","0","28628","3736"
"13396","{
  ""id"": 13396,
  ""title"": ""The difference between `Dense` and `TimeDistributedDense` of `Keras`""
}","<p>Let's say you have time-series data with $N$ rows and $700$ columns which you want to feed to a <code>SimpleRNN(200, return_sequence=True)</code> layer in Keras. Before you feed that to the RNN, you need to reshape the previous data to a 3D tensor. So it becomes a $N \times 700 \times 1$.</p>

<p>$ $ </p>

<p><img src=""https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png"" alt=""unrolled RNN""></p>

<p><sup>The image is taken from <a href=""https://colah.github.io/posts/2015-08-Understanding-LSTMs"" rel=""noreferrer"">https://colah.github.io/posts/2015-08-Understanding-LSTMs</a></sup></p>

<p>$ $</p>

<p>In RNN, your columns (the ""700 columns"") is the timesteps of RNN. Your data is processed from $t=1 \  to \ 700$. After feeding the data to the RNN, now it have 700 outputs which are $h_1$ to $h_{700}$, not $h_1$ to $h_{200}$. Remember that now the shape of your data is $N \times 700 \times 200$ which is <strong>samples (the rows) x timesteps (the columns) x channels</strong>.</p>

<p>And then, when you apply a <code>TimeDistributedDense</code>, you're applying a <code>Dense</code> layer on each timestep, which means you're applying a <code>Dense</code> layer on each $h_1$, $h_2$,...,$h_t$ respectively. Which means: actually you're  applying the fully-connected operation on each of its channels (the ""200"" one) respectively, from $h_1$ to $h_{700}$. The 1st ""$1 \times 1 \times 200$"" until the 700th ""$1 \times 1 \times 200$"".</p>

<p>Why are we doing this? Because you don't want to flatten the RNN output.</p>

<p>Why not flattening the RNN output? Because you want to keep each timestep values separate.</p>

<p>Why keep each timestep values separate? Because: </p>

<ul>
<li>you're only want to interacting the values between its own timestep</li>
<li>you don't want to have a random interaction between different timesteps and channels.</li>
</ul>
","<machine-learning><neural-network><keras>","49","49","49","0","9465","2018"
"25737","{
  ""id"": 25737,
  ""title"": ""Multi GPU in Keras""
}","<p>From the <a href=""https://keras.io/getting-started/faq/#how-can-i-run-a-keras-model-on-multiple-gpus"" rel=""nofollow noreferrer"">Keras FAQs</a>, below is copy-pasted code to enable 'data parallelism'. I.e. having each of your GPUs process a different subset of your data independently.</p>
<pre><code>from keras.utils import multi_gpu_model

# Replicates `model` on 8 GPUs.
# This assumes that your machine has 8 available GPUs.
parallel_model = multi_gpu_model(model, gpus=8)
parallel_model.compile(loss='categorical_crossentropy',
                       optimizer='rmsprop')

# This `fit` call will be distributed on 8 GPUs.
# Since the batch size is 256, each GPU will process 32 samples.
parallel_model.fit(x, y, epochs=20, batch_size=256)
</code></pre>
<p>Note that this appears to be valid only for the Tensorflow backend at the time of writing.</p>
<p><strong>Update (Feb 2018)</strong>:</p>
<p>Keras now accepts automatic gpu selection using multi_gpu_model, so you don't have to hardcode the number of gpus anymore. Details in this <a href=""https://github.com/keras-team/keras/pull/9226"" rel=""nofollow noreferrer"">Pull Request</a>. In other words, this enables code that looks like this:</p>
<pre><code>try:
    model = multi_gpu_model(model)
except:
    pass
</code></pre>
<p>But to be more <a href=""https://www.python.org/dev/peps/pep-0020/"" rel=""nofollow noreferrer"">explicit</a>, you can stick with something like:</p>
<pre><code>parallel_model = multi_gpu_model(model, gpus=None)
</code></pre>
<p><strong>Bonus</strong>:</p>
<p>To check if you really are utilizing all of your GPUs, specifically NVIDIA ones, you can monitor your usage in the terminal using:</p>
<pre><code>watch -n0.5 nvidia-smi
</code></pre>
<p>References:</p>
<ul>
<li><a href=""https://keras.io/utils/#multi_gpu_model"" rel=""nofollow noreferrer"">https://keras.io/utils/#multi_gpu_model</a></li>
<li><a href=""https://stackoverflow.com/questions/8223811/top-command-for-gpus-using-cuda"">https://stackoverflow.com/questions/8223811/top-command-for-gpus-using-cuda</a></li>
</ul>
","<python><deep-learning><tensorflow><keras><gpu>","49","49","49","0","42531","616"
"45166","{
  ""id"": 45166,
  ""title"": ""How to get accuracy, F1, precision and recall, for a keras model?""
}","<p>Metrics have been removed from Keras core. You need to calculate them manually. They removed them on <a href=""https://github.com/keras-team/keras/wiki/Keras-2.0-release-notes"" rel=""noreferrer"">2.0 version</a>. Those metrics are all global metrics, but Keras works in batches. As a result, it might be more misleading than helpful. </p>

<p>However, if you really need them, you can do it like this</p>

<pre class=""lang-py prettyprint-override""><code>from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

# compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])

# fit the model
history = model.fit(Xtrain, ytrain, validation_split=0.3, epochs=10, verbose=0)

# evaluate the model
loss, accuracy, f1_score, precision, recall = model.evaluate(Xtest, ytest, verbose=0)
</code></pre>
","<machine-learning><neural-network><deep-learning><classification><keras>","49","49","49","0","201","3560"
"12594","{
  ""id"": 12594,
  ""title"": ""How to interpret the output of XGBoost importance?""
}","<p>From your question, I'm assuming that you're using xgboost to fit boosted trees for binary classification. The importance matrix is actually a data.table object with the first column listing the names of all the features actually used in the boosted trees.</p>

<p>The meaning of the importance data table is as follows:</p>

<ol>
<li>The <strong><em>Gain</em></strong> implies the relative contribution of the corresponding feature to the model calculated by taking each feature's contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction.</li>
<li>The <strong><em>Cover</em></strong> metric means the relative number of observations related to this feature. For example, if you have 100 observations, 4 features and 3 trees, and suppose feature1 is used to decide the leaf node for 10, 5, and 2 observations in tree1, tree2 and tree3 respectively; then the metric will count cover for this feature as 10+5+2 = 17 observations. This will be calculated for all the 4 features and the cover will be 17 expressed as a percentage for all features' cover metrics.</li>
<li>The <strong><em>Frequency</em></strong> (/'Frequence') is the percentage representing the relative number of times a particular feature occurs in the trees of the model. In the above example, if feature1 occurred in 2 splits, 1 split and 3 splits in each of tree1, tree2 and tree3; then the weightage for feature1 will be 2+1+3 = 6. The frequency for feature1 is calculated as its percentage weight over weights of all features.</li>
</ol>

<p>The Gain is the most relevant attribute to interpret the relative importance of each feature.</p>

<p>The measures are all relative and hence all sum up to one, an example from a fitted xgboost model in R is:</p>

<pre><code>&gt; sum(importance<span class=""math-container"">$Frequence)
[1] 1
&gt; sum(importance$</span>Cover)
[1] 1
&gt; sum(importance$Gain)
[1] 1
</code></pre>
","<machine-learning><xgboost>","47","47","47","0","18540","2167"
"34375","{
  ""id"": 34375,
  ""title"": ""Why do people prefer Pandas to SQL?""
}","<p>As much as there is overlap in the application of these two things, this is comparing apples to oranges.</p>

<p>pandas is a data analysis toolkit implemented in Python, a general purpose programming language. SQL is a domain-specific language for querying relational data (usually in an relational database management system which SQLite, MySQL, Oracle, SQL Server, PostgreSQL etc. are examples).</p>

<p>SQL implies </p>

<ul>
<li>working with data in an RDBMS* which may or may not be appropriate for the workload, even if it's just a small SQLite database,</li>
<li>database domain knowledge (as an end user, developer and/or administrator; the suggestion that ""SQL is faster"" I often see is a massive over-simplification), and</li>
<li>overcoming the not-insignificant learning curve in using SQL effectively, particularly in specialist applications such as data analysis (as opposed to creating simple reports of simple data).</li>
</ul>

<blockquote>
  <p>* It's worth underlining the fact that SQL is so domain-specific it's becoming much less relevant to working with increasingly common alternatives to relational databases such as <a href=""https://en.wikipedia.org/wiki/NoSQL"" rel=""noreferrer"">NoSQL</a> databases. This represents a fundamental shift in how data is stored and structured, and there is really no universally common way of accessing it like the development of SQL standardisation aimed to achieve. </p>
</blockquote>

<p>Python on the other hand (pandas is fairly ""pythonic"" so it holds true here) is flexible and accessible to people from various backgrounds. It can be used as a ""scripting language"", as a functional language and a fully featured OOP language. Visualisation capabilities and data source interoperability are built into pandas, but you're free to incorporate whatever Python can do into your workflow (which is most things); the scientific Python ecosystem has ballooned and includes great tools such as <a href=""http://jupyter.org/"" rel=""noreferrer"">Jupyter Notebook</a> and essential <a href=""https://www.scipy.org/"" rel=""noreferrer"">scipy</a> libraries such as <a href=""https://matplotlib.org/"" rel=""noreferrer"">matplotlib</a> and <a href=""http://www.numpy.org/"" rel=""noreferrer"">numpy</a> (which pandas builds on). Significant elements of pandas' data analysis is <a href=""https://www.r-project.org/"" rel=""noreferrer"">R</a>-inspired and you won't generally find statisticians umming and ahhing about whether they use R (or possibly increasingly pandas!) over putting everything in a database and writing their analyses in SQL.</p>

<p>I'm not saying pandas is better than SQL or vice versa, but SQL is a very domain-specific tool whereas pandas is part of a giant, flexible and accessible ecosystem. I work with geospatial data systems, of which relational databases are a huge part, and SQL is a powerful and essential tool. However, pandas is an equally if not more essential part of my day-to-day toolkit and SQL is often relegated to fetching data -- perhaps with some pre-processing -- so I can do things with it in pandas.</p>
","<python><pandas><sql>","47","47","47","0","54992","311"
"2406","{
  ""id"": 2406,
  ""title"": ""Data science without knowledge of a specific topic, is it worth pursuing as a career?""
}","<p>Drew Conway published the <a href=""http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram"" rel=""nofollow noreferrer"">Data Science Venn Diagram</a>, with which I heartily agree:</p>
<p><img src=""https://i.stack.imgur.com/7W0VM.png"" alt=""Data Science Venn Diagram"" /></p>
<p>On the one hand, you should really read his post. On the other hand, I can offer my own experience: my subject matter expertise (which I like better as a term than &quot;Substantive Expertise&quot;, because you should really also have &quot;Substantive Expertise&quot; in math/stats and hacking) is in the retail business, my math/stats are forecasting and inferential statistics, and my hacking skills lie in R.</p>
<p>From this vantage point, I can talk to and understand retailers, and someone who doesn't have at least a passing knowledge of this field will have to face a <em>steep</em> learning curve in a project with retailers. As a side gig, I do statistics in psychology, and it's exactly the same there. And even with quite some knowledge of the hacking/math/stats part of the diagram, I would have a hard time getting up to speed in, say, credit scoring or some other new subject field.</p>
<p>Once you have a certain amount of math/stats and hacking skills, it is <em>much</em> better to acquire a grounding in one or more subjects than in adding <em>yet</em> another programming language to your hacking skills, or <em>yet</em> another machine learning algorithm to your math/stats portfolio. After all, once you have a solid math/stats/hacking grounding, you could if need be learn such new tools from the web or from textbooks in a relative short time period. But the subject matter expertise, on the other hand, you will likely not be able to learn from scratch if you start from zero. And clients will rather work with some data scientist A who understands their specific field than with another data scientist B who first needs to learn the basics - even if B is better in math/stats/hacking.</p>
<p>Of course, all this will also mean that you will never become an expert in <em>either</em> of the three fields. But that's fine, because you are a data scientist, not a programmer or a statistician or a subject matter expert. There will always be people in the three separate circles who you can learn from. Which is part of what I like about data science.</p>
<hr />
<p>EDIT: A little while and a few thoughts later, I'd like to update this post with a new version of the diagram. I still think that Hacking Skills, Math &amp; Statistics Knowledge and Substantive Expertise (shortened to &quot;Programming&quot;, &quot;Statistics&quot; and &quot;Business&quot; for legibility) are important... but I think that the role of <em>Communication</em> is important, too. All the insights you derive by leveraging your hacking, stats and business expertise won't make a bit of a difference unless you can communicate them to people who may <em>not</em> have that unique blend of knowledge. You may need to explain your statistical insights to a business manager who needs to be convinced to spend money or change processes. Or to a programmer who doesn't think statistically. <a href=""https://doi.org/10.1111/1740-9713.01531"" rel=""nofollow noreferrer"">Caulcutt (2021, <em>Significance</em>)</a> is a short article that says much the same, but gives more detail than I do.</p>
<p>So here is the new data science Venn diagram, which also includes communication as one indispensable ingredient. I have labeled the areas in ways that should guarantee maximum flaming, while being easy to remember.</p>
<p>Comment away.</p>
<p><a href=""https://i.stack.imgur.com/aiQeT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aiQeT.png"" alt=""new data science Venn diagram"" /></a></p>
<p>R code:</p>
<pre><code>draw.ellipse &lt;- function(center,angle,semimajor,semiminor,radius,h,s,v,...) {
    shape &lt;- rbind(c(cos(angle),-sin(angle)),c(sin(angle),cos(angle))) %*% diag(c(semimajor,semiminor))
    tt &lt;- seq(0,2*pi,length.out=1000)
    foo &lt;- matrix(center,nrow=2,ncol=length(tt),byrow=FALSE) + shape%*%(radius*rbind(cos(tt),sin(tt)))
    polygon(foo[1,],foo[2,],col=hsv(h,s,v,alpha=0.5),border=&quot;black&quot;,...)
}
name &lt;- function(x,y,label,cex=1.2,...) text(x,y,label,cex=cex,...)

png(&quot;Venn.png&quot;,width=600,height=600)
    opar &lt;- par(mai=c(0,0,0,0),lwd=3,font=2)
        plot(c(0,100),c(0,90),type=&quot;n&quot;,bty=&quot;n&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;)
        draw.ellipse(center=c(30,30),angle=0.75*pi,semimajor=2,semiminor=1,radius=20,h=60/360,s=.068,v=.976)
        draw.ellipse(center=c(70,30),angle=0.25*pi,semimajor=2,semiminor=1,radius=20,h=83/360,s=.482,v=.894)
        draw.ellipse(center=c(48,40),angle=0.7*pi,semimajor=2,semiminor=1,radius=20,h=174/360,s=.397,v=.8)
        draw.ellipse(center=c(52,40),angle=0.3*pi,semimajor=2,semiminor=1,radius=20,h=200/360,s=.774,v=.745)
        
        name(50,90,&quot;The Data Scientist Venn Diagram&quot;,pos=1,cex=2)
        name(8,62,&quot;Communi-\ncation&quot;,cex=1.5,pos=3)
        name(30,78,&quot;Statistics&quot;,cex=1.5)
        name(70,78,&quot;Programming&quot;,cex=1.5)
        name(92,62,&quot;Business&quot;,cex=1.5,pos=3)
        
        name(10,45,&quot;Hot\nAir&quot;)
        name(90,45,&quot;The\nAccountant&quot;)
        name(33,65,&quot;The\nData\nNerd&quot;)
        name(67,65,&quot;The\nHacker&quot;)
        name(27,50,&quot;The\nStats\nProf&quot;)
        name(73,50,&quot;The\nIT\nGuy&quot;)
        name(50,55,&quot;R\nCore\nTeam&quot;)
        name(38,38,&quot;The\nGood\nConsultant&quot;)
        name(62,38,&quot;Drew\nConway's\nData\nScientist&quot;)
        name(50,24,&quot;The\nperfect\nData\nScientist!&quot;)
        name(31,18,&quot;Comp\nSci\nProf&quot;)
        name(69,18,&quot;The\nNumber\nCruncher&quot;)
        name(42,11,&quot;Head\nof IT&quot;)
        name(58,11,&quot;Ana-\nlyst&quot;)
        name(50,5,&quot;The\nSalesperson&quot;)
    par(opar)
dev.off()
</code></pre>
","<education><beginner><career>","47","47","47","0","2853","911"
"23913","{
  ""id"": 23913,
  ""title"": ""Why do we need XGBoost and Random Forest?""
}","<p>It's easier to start with your second question and then go to the first.</p>

<p><strong>Bagging</strong></p>

<p>Random Forest is a bagging algorithm. It reduces variance.</p>

<p>Say that you have very unreliable models, such as Decision Trees. (Why unreliable? Because if you change your data a little bit, the decision tree created can be very different.) In such a case, you can build a robust model (reduce variance) through <strong>bagging</strong> -- bagging is when you create different models by resampling your data to make the resulting model more robust.</p>

<p>Random forest is what we call to bagging applied to decision trees, but it's no different than other bagging algorithm.</p>

<p>Why would you want to do this? It depends on the problem. But usually, it is highly desirable for the model to be stable.</p>

<p><strong>Boosting</strong></p>

<p>Boosting reduces variance, and also reduces bias. It reduces variance because you are using multiple models (bagging). It reduces bias by training the subsequent model by telling him what errors the previous models made (the boosting part).</p>

<p>There are two main algorithms:</p>

<ul>
<li>Adaboost: this is the original algorithm; you tell subsequent models to punish more heavily observations mistaken by the previous models</li>
<li>Gradient boosting: you train each subsequent model using the residuals (the difference between the predicted and true values)</li>
</ul>

<p>In these ensembles, your base learner <strong>must</strong> be weak. If it overfits the data, there won't be any residuals or errors for the subsequent models to build upon. Why are these good models? Well, most competitions in websites like Kaggle have been won using gradient boosting trees. Data science is an empirical science, ""because it works"" is good enough. Anyhow, do notice that boosting models can overfit (albeit empirically it's not very common).</p>

<p>Another reason why gradient boosting, in particular, is also pretty cool: because it makes it very easy to use different loss functions, even when the derivative is not convex. For instance, when using probabilistic forecast, you can use stuff such as the <a href=""https://www.lokad.com/pinball-loss-function-definition"" rel=""noreferrer"">pinball function</a> as your loss function; something which is much harder with neural networks (because the derivative is always constant).</p>

<p>[Interesting historical note: Boosting was originally a theoretical invention motivated by the question ""<a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.192.8630&amp;rep=rep1&amp;type=pdf"" rel=""noreferrer"">can we build a stronger model using weaker models</a>""]</p>

<hr>

<p>Notice: People sometimes confuse random forest and gradient boosting trees, just because both use decision trees, but they are two very different families of ensembles.</p>
","<machine-learning><data-mining><random-forest><decision-trees><xgboost>","47","47","47","0","16853","3182"
"62","{
  ""id"": 62,
  ""title"": ""Why Is Overfitting Bad in Machine Learning?""
}","<p>Overfitting is <em>empirically</em> bad.  Suppose you have a data set which you split in two, test and training.  An overfitted model is one that performs much worse on the test dataset than on training dataset.  It is often observed that models like that also in general perform worse on additional (new) test datasets than models which are not overfitted.  </p>

<p>One way to understand that intuitively is that a model may use some relevant parts of the data (signal) and some irrelevant parts (noise).  An overfitted model uses more of the noise, which increases its performance in the case of known noise (training data) and decreases its performance in the case of novel noise (test data).  The difference in performance between training and test data indicates how much noise the model picks up; and picking up noise directly translates into worse performance on test data (including future data).</p>

<p>Summary: overfitting is bad by definition, this has not much to do with either complexity or ability to generalize, but rather has to do with mistaking noise for signal.</p>

<p>P.S. On the ""ability to generalize"" part of the question, it is very possible to have a model which has inherently limited ability to generalize due to the structure of the model (for example linear SVM, ...) but is still prone to overfitting.  In a sense overfitting is just one way that generalization may fail.</p>
","<machine-learning><predictive-modeling>","47","49","48","1","26","3032"
"65754","{
  ""id"": 65754,
  ""title"": ""Why does Keras need TensorFlow as backend?""
}","<p>This makes more sense when understood in its historical context. These were the chronological events:</p>
<ul>
<li><code>April 2009</code> <a href=""https://github.com/Theano/Theano"" rel=""nofollow noreferrer"">Theano</a> <a href=""https://github.com/Theano/Theano/blob/master/HISTORY.txt#L3574"" rel=""nofollow noreferrer"">0.1 is released</a>. It would dominate the deep learning framework scene for many many years.</li>
<li><code>June 2015</code> Keras <a href=""https://github.com/keras-team/keras/releases/tag/0.1.0"" rel=""nofollow noreferrer"">is created</a> by <a href=""https://fchollet.com/"" rel=""nofollow noreferrer"">François Chollet</a>. The goal was to create an abstraction layer to make Theano easier to use, enabling fast prototyping.</li>
<li><code>August 2015</code> <a href=""https://www.linkedin.com/in/fchollet/"" rel=""nofollow noreferrer"">Google hires François Chollet</a>.</li>
<li><code>November 2015</code> <a href=""https://www.tensorflow.org/"" rel=""nofollow noreferrer"">Tensorflow</a> is released by Google, with much inspiration from Theano and its declarative computational graph paradigm.</li>
<li><code>December 2015</code> Keras <a href=""https://github.com/keras-team/keras/releases/tag/0.3.0"" rel=""nofollow noreferrer"">is refactored</a> to allow for pluggable backend engines, and now it offers backend implementations for Theano and Tensorflow.</li>
</ul>
<p>Other backends were later supported by Keras (CNTK, MxNet), but they never got much traction.</p>
<p>Time passes by and the overlap between Tensorflow and Keras grows. Tensorflow ends up duplicating many of the functionalities in Keras (apart from the multiple APIs within Tensorflow that also had big overlaps).</p>
<ul>
<li><code>September 2017</code> Theano <a href=""https://groups.google.com/forum/#!topic/theano-users/7Poq8BZutbY"" rel=""nofollow noreferrer"">is discontinued</a>.</li>
<li><code>November 2017</code> <a href=""https://github.com/tensorflow/tensorflow/releases/tag/v1.4.0"" rel=""nofollow noreferrer"">Keras is bundled with Tensorflow</a> as <code>tf.keras</code>. From this point on there are 2 different Keras: the one bundled with Tensorflow and the one that supports multiple backend engines. Both are maintained by the same people and are kept in sync at API level.</li>
</ul>
<p>At some point, the roadmap for Tensorflow 2.0 is defined, choosing to pursue an imperative model like <a href=""https://pytorch.org/"" rel=""nofollow noreferrer"">PyTorch</a>. The person leading the Tensorflow API refactoring is François Chollet. This refactoring included a reorganization of the functionality to avoid duplications.</p>
<ul>
<li><code>November 2018</code> some crucial functionalities of Tensorflow <a href=""https://web.archive.org/web/20200202083601/https://keras.io/#multi-backend-keras-and-tfkeras"" rel=""nofollow noreferrer"">are to be moved to</a> <code>tf.keras</code>, <a href=""https://www.reddit.com/r/MachineLearning/comments/9ysmtn/d_debate_on_tensorflow_20_api/"" rel=""nofollow noreferrer"">generating a heated debate</a></li>
<li><code>September 2019</code> <a href=""https://github.com/keras-team/keras/releases/tag/2.3.0"" rel=""nofollow noreferrer"">Keras 2.3 is announced to be the last release of the multi-backend version of Keras</a></li>
</ul>
<p>Now, <strong>THE ANSWER</strong> to your question: Tensorflow is the most used Keras backend because it is the only one with a relevant user base that is under active development and, furthermore, the only version of Keras that is actively developed and maintained is one with Tensorflow.</p>
<p>So, summing up:</p>
<ol>
<li>At the beginning of Keras, the overlap with Tensorflow was small. Tensorflow was a bit difficult to use, and Keras simplified it a lot.</li>
<li>Later, Tensorflow incorporated many functionalities similar to Keras'. Keras became less necessary.</li>
<li>Then, apart from the multi-backend version, Keras was bundled with Tensorflow. Their separation line blurred over the years.</li>
<li>The multi-backend Keras version was discontinued. Now the only Keras is the one bundled with Tensorflow.</li>
</ol>
<p><strong>Update</strong>: the relationship between Keras and Tensorflow is best understood with an example:</p>
<p>The dependency between Keras and Tensorflow is internal to Keras, it is not exposed to the programmer working with Keras. For example, in the source code of Keras, there is an <a href=""https://github.com/keras-team/keras/blob/master/keras/layers/convolutional.py"" rel=""nofollow noreferrer"">implementation of a convolutional layer</a>; this implementation calls package <code>keras.backend</code> to <a href=""https://github.com/keras-team/keras/blob/master/keras/layers/convolutional.py#L157"" rel=""nofollow noreferrer"">actually run the convolution computation</a>; depending on the Keras configuration file, this <a href=""https://github.com/keras-team/keras/tree/f295e8ee39d4ba841ac281a9337d69c7bc5e0eb6/keras/backend"" rel=""nofollow noreferrer"">backend</a> is set to use the Tensorflow backend implementation in <a href=""https://github.com/keras-team/keras/blob/f295e8ee39d4ba841ac281a9337d69c7bc5e0eb6/keras/backend/tensorflow_backend.py"" rel=""nofollow noreferrer""><code>keras.backend.tensorflow_backend.py</code></a>; this Keras file just <a href=""https://github.com/keras-team/keras/blob/f295e8ee39d4ba841ac281a9337d69c7bc5e0eb6/keras/backend/tensorflow_backend.py#L3666"" rel=""nofollow noreferrer"">invokes Tensorflow to compute the convolution</a></p>
","<keras><tensorflow>","47","47","47","0","14675","13257"
"20542","{
  ""id"": 20542,
  ""title"": ""What is \""experience replay\"" and what are its benefits?""
}","<p>The key part of the quoted text is:</p>

<blockquote>
  <p>To perform experience replay we store the agent's experiences $e_t = (s_t,a_t,r_t,s_{t+1})$</p>
</blockquote>

<p>This means instead of running Q-learning on state/action pairs as they occur during simulation or actual experience, the system stores the data discovered for [state, action, reward, next_state] - typically in a large table. Note this does not store associated values - this is the raw data to feed into action-value calculations later.</p>

<p>The learning phase is then logically separate from gaining experience, and based on taking random samples from this table. You still want to interleave the two processes - acting and learning - because improving the policy will lead to different behaviour that should explore actions closer to optimal ones, and you want to learn from those. However, you can split this how you like - e.g. take one step, learn from three random prior steps etc. The Q-Learning targets when using experience replay use the same targets as the online version, so there is no new formula for that. The loss formula given is also the one you would use for DQN without experience replay. The difference is only <em>which</em> s, a, r, s', a' you feed into it.</p>

<p>In DQN, the DeepMind team also maintained two networks and switched which one was learning and which one feeding in current action-value estimates as ""bootstraps"". This helped with stability of the algorithm when using a non-linear function approximator. That's what the bar stands for in ${\theta}^{\overline{\space}}_i$ - it denotes the alternate <em>frozen</em> version of the weights.</p>

<p>Advantages of experience replay:</p>

<ul>
<li><p>More efficient use of previous experience, by learning with it multiple times. This is key when gaining real-world experience is costly, you can get full use of it. The Q-learning updates are incremental and do not converge quickly, so multiple passes with the same data is beneficial, especially when there is low variance in immediate outcomes (reward, next state) given the same state, action pair.</p></li>
<li><p>Better convergence behaviour when training a function approximator. Partly this is because the data is more like <a href=""https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables"" rel=""noreferrer"">i.i.d.</a> data assumed in most supervised learning convergence proofs.</p></li>
</ul>

<p>Disadvantage of experience replay:</p>

<ul>
<li>It is harder to use multi-step learning algorithms, such as Q($\lambda$), which can be tuned to give better learning curves by balancing between bias (due to bootstrapping) and variance (due to delays and randomness in long-term outcomes). Multi-step DQN with experience-replay DQN is one of the extensions explored in the paper <a href=""https://arxiv.org/abs/1710.02298"" rel=""noreferrer"">Rainbow: Combining Improvements in Deep Reinforcement Learning</a>. </li>
</ul>

<p>The approach used in DQN is briefly <a href=""https://www.youtube.com/watch?v=UoPei5o4fps&amp;index=6&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT"" rel=""noreferrer"">outlined by David Silver in parts of this video lecture</a> (around 01:17:00, but worth seeing sections before it). I recommend watching the whole series, which is a graduate level course on reinforcement learning, if you have time.</p>
","<reinforcement-learning><q-learning>","47","47","47","0","836","25808"
"256","{
  ""id"": 256,
  ""title"": ""Do I need to learn Hadoop to be a Data Scientist?""
}","<p>Different people use different tools for different things.  Terms like Data Science are generic for a reason.  A data scientist could spend an entire career without having to learn a particular tool like hadoop.  Hadoop is widely used, but it is not the only platform that is capable of managing and manipulating data, even large scale data.</p>

<p>I would say that a data scientist should be familiar with concepts like MapReduce, distributed systems, distributed file systems, and the like, but I wouldn't judge someone for not knowing about such things.  </p>

<p>It's a big field.  There is a sea of knowledge and most people are capable of learning and being an expert in a single drop.  The key to being a scientist is having the desire to learn and the motivation to know that which you don't already know.</p>

<p>As an example:  I could hand the right person a hundred structured CSV files containing information about classroom performance in one particular class over a decade.  A data scientist would be able to spend a year gleaning insights from the data without ever needing to spread computation across multiple machines.  You could apply machine learning algorithms, analyze it using visualizations, combine it with external data about the region, ethnic makeup, changes to environment over time, political information, weather patterns, etc.  All of that would be ""data science"" in my opinion.  It might take something like hadoop to test and apply anything you learned to data comprising an entire country of students rather than just a classroom, but that final step doesn't necessarily make someone a data scientist.  And not taking that final step doesn't necessarily disqualify someone from being a data scientist.</p>
","<bigdata><apache-hadoop>","46","46","46","0","434","3078"
"11621","{
  ""id"": 11621,
  ""title"": ""RNN vs CNN at a high level""
}","<p>A CNN will learn to recognize patterns across space.  So, as you say, a CNN will learn to recognize components of an image (e.g., lines, curves, etc.) and then learn to combine these components to recognize larger structures (e.g., faces, objects, etc.). </p>

<p>You could say, in a very general way, that a RNN will similarly learn to recognize patterns across time.  So a RNN that is trained to translate text might learn that ""dog"" should be translated differently if preceded by the word ""hot"".</p>

<p>The mechanism by which the two kinds of NNs represent these patterns is different, however.  In the case of a CNN, you are looking for the <em>same</em> patterns on all the different subfields of the image.  In the case of a RNN you are (in the simplest case) feeding the hidden layers from the previous step as an additional input into the next step.  While the RNN builds up memory in this process, it is not looking for the same patterns over different slices of time in the same way that a CNN is looking for the same patterns over different regions of space.</p>

<p>I should also note that when I say ""time"" and ""space"" here, it shouldn't be taken too literally.  You could run a RNN on a single image for image captioning, for instance, and the meaning of ""time"" would simply be the order in which different parts of the image are processed.  So objects initially processed will inform the captioning of later objects processed.</p>
","<machine-learning><neural-network><beginner>","46","46","46","0","18416","891"
"16077","{
  ""id"": 16077,
  ""title"": ""What is the difference between \""equivariant to translation\"" and \""invariant to translation\""""
}","<p>The terms are different:</p>

<ul>
<li><p><em>Equivariant to translation</em> means that a translation of input features results in an equivalent translation of outputs. So if your pattern 0,3,2,0,0 on the input results in 0,1,0,0 in the output, then the pattern 0,0,3,2,0 might lead to 0,0,1,0</p></li>
<li><p><em>Invariant to translation</em> means that a translation of input features doe not change the outputs at all. So if your pattern 0,3,2,0,0 on the input results in 0,1,0 in the output, then the pattern 0,0,3,2,0 would also lead to 0,1,0</p></li>
</ul>

<p>For feature maps in convolutional networks to be useful, they typically need both properties in some balance. The equivariance allows the network to generalise edge, texture, shape detection in different locations. The invariance allows precise location of the detected features to matter less. These are two complementary types of generalisation for many image processing tasks.</p>
","<neural-network><deep-learning><convolution>","46","46","46","0","836","25808"
"32654","{
  ""id"": 32654,
  ""title"": ""What is the use of torch.no_grad in pytorch?""
}","<p>The wrapper <code>with torch.no_grad()</code> temporarily sets all of the <code>requires_grad</code> flags to false. An example is from the <a href=""https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients"" rel=""noreferrer"">official PyTorch tutorial</a>.</p>
<pre><code>x = torch.randn(3, requires_grad=True)
print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print((x ** 2).requires_grad)
</code></pre>
<p>Output:</p>
<pre><code>True
True
False
</code></pre>
<p>I recommend you to read all the tutorials from the link above.</p>
<p>In your example: I guess the author does not want PyTorch to calculate the gradients of the new defined variables w1 and w2 since he just want to update their values.</p>
","<python><pytorch>","45","45","45","0","51541","923"
"1169","{
  ""id"": 1169,
  ""title"": ""How to do SVD and PCA with big data?""
}","<p>First of all, <strong>dimensionality reduction</strong> is used when you have <strong>many covariated dimensions</strong> and want to reduce problem size by rotating data points into new orthogonal basis and taking only axes with largest variance. With 8 variables (columns) your space is already low-dimensional, reducing number of variables further is unlikely to solve technical issues with memory size, but may affect dataset quality a lot. In your concrete case it's more promising to take a look at <a href=""http://en.wikipedia.org/wiki/Online_machine_learning"" rel=""noreferrer""><strong>online learning</strong></a> methods. Roughly speaking, instead of working with the whole dataset, these methods take a little part of them (often referred to as ""mini-batches"") at a time and build a model incrementally. (I personally like to interpret word ""online"" as a reference to some infinitely long source of data from Internet like a Twitter feed, where you just can't load the whole dataset at once). </p>

<p>But what if you really wanted to apply dimensionality reduction technique like PCA to a dataset that doesn't fit into a memory? Normally a dataset is represented as a data matrix <em>X</em> of size <em>n</em> x <em>m</em>, where <em>n</em> is number of observations (rows) and <em>m</em> is a number of variables (columns). Typically problems with memory come from only one of these two numbers. </p>

<h2>Too many observations (n >> m)</h2>

<p>When you have <strong>too many observations</strong>, but the number of variables is from small to moderate, you can <strong>build the covariance matrix incrementally</strong>. Indeed, typical PCA consists of constructing a covariance matrix of size <em>m</em> x <em>m</em> and applying singular value decomposition to it. With <em>m</em>=1000 variables of type float64, a covariance matrix has size 1000*1000*8 ~ 8Mb, which easily fits into memory and may be used with SVD. So you need only to build the covariance matrix without loading entire dataset into memory - <a href=""http://rebcabin.github.io/blog/2013/01/22/covariance-matrices/"" rel=""noreferrer"">pretty tractable task</a>. </p>

<p>Alternatively, you can select a small representative sample from your dataset and <strong>approximate the covariance matrix</strong>. This matrix will have all the same properties as normal, just a little bit less accurate. </p>

<h2>Too many variables (n &lt;&lt; m)</h2>

<p>On another hand, sometimes, when you have <strong>too many variables</strong>, the covariance matrix itself will not fit into memory. E.g. if you work with 640x480 images, every observation has 640*480=307200 variables, which results in a 703Gb covariance matrix! That's definitely not what you would like to keep in memory of your computer, or even in memory of your cluster. So we need to reduce dimensions without building a covariance matrix at all. </p>

<p>My favourite method for doing it is <a href=""http://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf"" rel=""noreferrer""><strong>Random Projection</strong></a>. In short, if you have dataset <em>X</em> of size <em>n</em> x <em>m</em>, you can multiply it by some sparse random matrix <em>R</em> of size <em>m</em> x <em>k</em> (with <em>k</em> &lt;&lt; <em>m</em>) and obtain new matrix <em>X'</em> of a much smaller size <em>n</em> x <em>k</em> with <em>approximately the same properties</em> as the original one. Why does it work? Well, you should know that PCA aims to find set of orthogonal axes (principal components) and project your data onto first <em>k</em> of them. It turns out that sparse random vectors are <em>nearly orthogonal</em> and thus may also be used as a new basis. </p>

<p>And, of course, you don't have to multiply the whole dataset <em>X</em> by <em>R</em> - you can translate every observation <em>x</em> into the new basis separately or in mini-batches.</p>

<p>There's also somewhat similar algorithm called <strong>Random SVD</strong>. I don't have any real experience with it, but you can find example code with explanations <a href=""https://stats.stackexchange.com/a/11934/3305"">here</a>.</p>

<hr>

<p>As a bottom line, here's a short check list for dimensionality reduction of big datasets: </p>

<ol>
<li>If you have not that many dimensions (variables), simply use online learning algorithms. </li>
<li>If there are many observations, but a moderate number of variables (covariance matrix fits into memory), construct the matrix incrementally and use normal SVD. </li>
<li>If number of variables is too high, use incremental algorithms. </li>
</ol>
","<bigdata><data-mining><dimensionality-reduction>","45","45","45","0","1279","2771"
"58846","{
  ""id"": 58846,
  ""title"": ""How to disable GPU with TensorFlow?""
}","<p>I've seen some suggestions elsewhere, but they are old and do not apply very well to newer TF versions. What worked for me was this:</p>

<pre class=""lang-py prettyprint-override""><code>import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
</code></pre>

<p>When that variable is defined and equal to -1, TF uses the CPU even when a CUDA GPU is available.</p>
","<tensorflow><gpu>","45","45","45","0","23556","801"
"296","{
  ""id"": 296,
  ""title"": ""Latent Dirichlet Allocation vs Hierarchical Dirichlet Process""
}","<p>HDP is an extension of LDA, designed to address the case where the number of mixture components (the number of ""topics"" in document-modeling terms) is not known a priori.  So that's the reason why there's a difference.</p>

<p>Using LDA for document modeling, one treats each ""topic"" as a distribution of words in some known vocabulary.  For each document a mixture of topics is drawn from a Dirichlet distribution, and then each word in the document is an independent draw from that mixture (that is, selecting a topic and then using it to generate a word).</p>

<p>For HDP (applied to document modeling), one also uses a Dirichlet process to capture the uncertainty in the number of topics.  So a common base distribution is selected which represents the countably-infinite set of possible topics for the corpus, and then the finite distribution of topics for each document is sampled from this base distribution.</p>

<p>As far as pros and cons, HDP has the advantage that the maximum number of topics can be unbounded and learned from the data rather than specified in advance.  I suppose though it is more complicated to implement, and unnecessary in the case where a bounded number of topics is acceptable.</p>
","<nlp><topic-model><lda>","44","44","44","0","14","2942"
"26481","{
  ""id"": 26481,
  ""title"": ""Why is ReLU used as an activation function?""
}","<p>In mathematics (linear algebra) a function is considered linear whenever a function<span class=""math-container"">$f: A  \rightarrow B$</span> if for every <span class=""math-container"">$x$</span> and <span class=""math-container"">$y$</span> in the domain <span class=""math-container"">$A$</span> has the following property:  <span class=""math-container"">$f(x) + f(y) = f(x+y)$</span>.  By definition the ReLU is <span class=""math-container"">$max(0,x)$</span>.  Therefore, if we split the domain from <span class=""math-container"">$(-\infty, 0]$</span> or <span class=""math-container"">$[0, \infty)$</span> then the function is linear.  However, it's easy to see that <span class=""math-container"">$f(-1) + f(1) \neq f(0)$</span>.  Hence by definition ReLU is not linear.</p>
<p>Nevertheless, ReLU is so close to linear that this often confuses people and wonder how can it be used as a universal approximator.  In my experience, the best way to think about them is like Riemann sums.  You can approximate any continuous functions with lots of little rectangles.  ReLU activations can produced lots of little rectangles.  In fact, in practice, ReLU can make rather complicated shapes and approximate many complicated domains.</p>
<p>I also feel like clarifying another point.  As pointed out by a previous answer, neurons do not die in Sigmoid, but rather vanish.  The reason for this is because at maximum the derivative of the sigmoid function is .25.  Hence, after so many layers you end up multiplying these gradients and the product of very small numbers less than 1 tend to  go to zero very quickly.</p>
<p>Hence if you're building a deep learning network with a lot of layers, your sigmoid functions will essentially stagnant rather quickly and become more or less useless.</p>
<p>The key take away is the vanishing comes from multiplying the gradients not the gradients themselves.</p>
","<machine-learning><neural-network><deep-learning><activation-function>","44","44","44","0","43294","2137"
"449","{
  ""id"": 449,
  ""title"": ""What are the use cases for Apache Spark vs Hadoop""
}","<p>Hadoop means HDFS, YARN, MapReduce, and a lot of other things. Do you mean Spark vs <em>MapReduce</em>? Because Spark runs on/with Hadoop, which is rather the point.</p>

<p>The primary reason to use Spark is for speed, and this comes from the fact that its execution can keep data in memory between stages rather than always persist back to HDFS after a Map or Reduce. This advantage is very pronounced for iterative computations, which have tens of stages each of which is touching the same data. This is where things might be ""100x"" faster. For simple, one-pass ETL-like jobs for which MapReduce was designed, it's not in general faster.</p>

<p>Another reason to use Spark is its nicer high-level language compared to MapReduce. It provides a functional programming-like view that mimics Scala, which is far nicer than writing MapReduce code. (Although you have to either use Scala, or adopt the slightly-less-developed Java or Python APIs for Spark). <a href=""http://crunch.apache.org"">Crunch</a> and <a href=""http://cascading.org"">Cascading</a> already provide a similar abstraction on top of MapReduce, but this is still an area where Spark is nice.</p>

<p>Finally Spark has as-yet-young but promising subprojects for ML, graph analysis, and streaming, which expose a similar, coherent API. With MapReduce, you would have to turn to several different other projects for this (Mahout, Giraph, Storm). It's nice to have it in one package, albeit not yet 'baked'.</p>

<p>Why would you not use Spark? <a href=""https://www.quora.com/Apache-Spark/Assuming-you-have-a-system-with-both-Hadoop-and-Spark-installed-say-under-Yarn-is-there-any-reason-to-use-Hadoop-map-reduce-instead-of-the-equivalent-Spark-commands"">paraphrasing</a> myself:</p>

<ul>
<li>Spark is primarily Scala, with ported Java APIs; MapReduce might be friendlier and more native for Java-based developers</li>
<li>There is more MapReduce expertise out there now than Spark</li>
<li>For the data-parallel, one-pass, ETL-like jobs MapReduce was designed for, MapReduce is lighter-weight compared to the Spark equivalent</li>
<li>Spark is fairly mature, and so is YARN now, but Spark-on-YARN is still pretty new. The two may not be optimally integrated yet. For example until recently I don't think Spark could ask YARN for allocations based on number of cores? That is: MapReduce might be easier to understand, manage and tune</li>
</ul>
","<apache-spark><apache-hadoop><distributed><knowledge-base><cloud-computing>","43","43","43","0","21","6202"
"31791","{
  ""id"": 31791,
  ""title"": ""What is the Q function and what is the V function in reinforcement learning?""
}","<p><span class=""math-container"">$V^\pi(s)$</span> is the state-value function of MDP (Markov Decision Process). It's the expected return starting from state <span class=""math-container"">$s$</span> following policy <span class=""math-container"">$\pi$</span>.</p>
<p>In the expression</p>
<p><span class=""math-container"">$$V^\pi(s) = E_{\pi} \{G_t \vert s_t = s\} $$</span></p>
<p><span class=""math-container"">$G_t$</span> is the total DISCOUNTED reward from time step <span class=""math-container"">$t$</span>, as opposed to <span class=""math-container"">$R_t$</span> which is an immediate return. Here you are taking the expectation of ALL actions according to the policy <span class=""math-container"">$\pi$</span>.</p>
<p><span class=""math-container"">$Q^\pi(s, a)$</span> is the action-value function. It is the expected return starting from state <span class=""math-container"">$s$</span>, following policy <span class=""math-container"">$\pi$</span>, taking action <span class=""math-container"">$a$</span>. It's focusing on the particular action at the particular state.</p>
<p><span class=""math-container"">$$Q^\pi(s, a) = E_\pi \{G_t | s_t = s, a_t = a\}$$</span></p>
<p>The relationship between <span class=""math-container"">$Q^\pi$</span> and <span class=""math-container"">$V^\pi$</span> (the value of being in that state) is</p>
<p><span class=""math-container"">$$V^\pi(s) = \sum_{a ∈ A} \pi (a|s) * Q^\pi(s,a)$$</span></p>
<p>You sum every action-value multiplied by the probability to take that action (the policy <span class=""math-container"">$\pi(a|s)$</span>).</p>
<p>If you think of the grid world example, you multiply the probability of (up/down/right/left) with the one step ahead state value of (up/down/right/left).</p>
","<machine-learning><reinforcement-learning>","43","43","43","0","37932","707"
"6024","{
  ""id"": 6024,
  ""title"": ""Deep learning basics""
}","<p>This <a href=""https://github.com/ChristosChristofidis/awesome-deep-learning"">link</a> contains an amazing amount of deep learning literature.
Summarizing it here(going in the order a beginner ideally should)-
NOTE: All these resources mainly use python.</p>

<p>1) First of all, a basic knowledge of machine learning is required. I found Caltech's Learning from data to be ideal of all the machine learning courses available on the net.</p>

<p>Andrew Ng's Coursera course is pretty good too.</p>

<p>2) For Neural networks, nobody explains it better than <a href=""http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/lecture-12-learning-neural-nets-back-propagation/"">Dr.Patrick Winston</a>.
The assignments should be tried out for better understanding. They are in python.</p>

<p>3) For a better understanding of Neural Networks, <a href=""http://neuralnetworksanddeeplearning.com/"">Michael Nielsen</a>'s course should be done(as suggested by Alexey). It is pretty basic but it works.</p>

<p>4) For deep neural networks, and implementing them faster on GPUs, there are multiple frameworks available, such as <a href=""http://deeplearning.net/software/theano/"">Theano</a>, <a href=""http://caffe.berkeleyvision.org/"">Caffe</a>, <a href=""http://pybrain.org/"">Pybrain</a>, <a href=""http://torch.ch/"">Torch</a>,etc.
Out of these Theano provides a better low level functionality that allows its user to create custom NNs. It is a python library, so being able to use numpy,scikit-learn, matplotlib, scipy along with it is a big plus.
The <a href=""http://deeplearning.net/tutorial/"">deep learning tutorial</a> written by Lisa Lab should be tried out for a better understanding of theano.</p>

<p>5) For Convolutional Neural Networks, follow <a href=""https://cs231n.github.io/"">andrej karpathy's tutorial</a>.</p>

<p>6) For unsupervised learning, follow <a href=""http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial"">here</a> and <a href=""http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/"">here</a>.</p>

<p>7) For an intersection of deep learning and NLP, follow <a href=""http://cs224d.stanford.edu/"">Richard Socher's class</a>.</p>

<p>8) For LSTMs, read <a href=""http://Hochreiter,%20S.,%20&amp;%20Schmidhuber,%20J.%20(1997).%20Long%20short-term%20memory.%20Neural%20computation,%209(8),%201735-1780"">Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780</a> and <a href=""http://Graves,%20Alex.%20Supervised%20sequence%20labelling%20with%20recurrent%20neural%20networks.%20Vol.%20385.%20Springer,%202012."">Graves, Alex. Supervised sequence labelling with recurrent neural networks. Vol. 385. Springer, 2012</a>.</p>

<p>Here is LSTM's <a href=""http://deeplearning.net/tutorial/code/lstm.py"">Theano code</a>.</p>
","<machine-learning><deep-learning>","43","43","43","0","9943","1039"
"22821","{
  ""id"": 22821,
  ""title"": ""Understanding predict_proba from MultiOutputClassifier""
}","<p>Assuming your target is (0,1), then the classifier would output a probability matrix of dimension (N,2). 
The first index refers to the probability that the data belong to class 0, and the second  refers to the probability that the data belong to class 1. </p>

<p>These two would sum to 1.</p>

<p>You can then output the result by:</p>

<pre><code>probability_class_1 = model.predict_proba(X)[:, 1]
</code></pre>

<p>If you have k classes, the output would be (N,k), you would have to specify the probability of which class you want.</p>
","<scikit-learn><random-forest><multilabel-classification>","43","45","44","1","37924","564"
"12597","{
  ""id"": 12597,
  ""title"": ""Does XGBoost handle multicollinearity by itself?""
}","<p>Decision trees are by nature immune to multi-collinearity. For example, if you have 2 features which are 99% correlated, when deciding upon a split the tree will choose only one of them. Other models such as Logistic regression would use both the features.</p>

<p>Since boosted trees use individual decision trees, they also are unaffected by multi-collinearity. However, its a good practice to remove any redundant features from any dataset used for training, irrespective of the model's  algorithm. In your case since you're deriving new features, you could use this approach, evaluate each feature's importance and retain only the best features for your final model.</p>

<p>The importance matrix of an xgboost model is actually a data.table object with the first column listing the names of all the features actually used in the boosted trees. The second column is the Gain metric which implies the relative contribution of the corresponding feature to the model calculated by taking each feature's contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction.</p>
","<feature-selection><correlation><xgboost><gbm>","42","42","42","0","18540","2167"
"9368","{
  ""id"": 9368,
  ""title"": ""Hypertuning XGBoost parameters""
}","<p>Whenever I work with xgboost I often make my own homebrew parameter search but you can do it with the caret package as well like KrisP just mentioned.</p>

<ol>
<li><strong>Caret</strong></li>
</ol>

<p>See this answer on Cross Validated for a thorough explanation on how to use the caret package for hyperparameter search on xgboost.
<a href=""https://stats.stackexchange.com/questions/171043/how-to-tune-hyperparameters-of-xgboost-trees"">How to tune hyperparameters of xgboost trees?</a></p>

<ol start=""2"">
<li><strong>Custom Grid Search</strong></li>
</ol>

<p>I often begin with a few assumptions based on <a href=""https://www.kaggle.com/owenzhang1"">Owen Zhang</a>'s slides on <a href=""http://www.slideshare.net/OwenZhang2/tips-for-data-science-competitions"">tips for data science</a> P. 14</p>

<p><a href=""https://i.stack.imgur.com/9GgQK.jpg""><img src=""https://i.stack.imgur.com/9GgQK.jpg"" alt=""enter image description here""></a></p>

<p>Here you can see that you'll mostly need to tune row sampling, column sampling and maybe maximum tree depth. This is how I do a custom row sampling and column sampling search for a problem I am working on at the moment:</p>

<pre><code>searchGridSubCol &lt;- expand.grid(subsample = c(0.5, 0.75, 1), 
                                colsample_bytree = c(0.6, 0.8, 1))
ntrees &lt;- 100

#Build a xgb.DMatrix object
DMMatrixTrain &lt;- xgb.DMatrix(data = yourMatrix, label = yourTarget)

rmseErrorsHyperparameters &lt;- apply(searchGridSubCol, 1, function(parameterList){

    #Extract Parameters to test
    currentSubsampleRate &lt;- parameterList[[""subsample""]]
    currentColsampleRate &lt;- parameterList[[""colsample_bytree""]]

    xgboostModelCV &lt;- xgb.cv(data =  DMMatrixTrain, nrounds = ntrees, nfold = 5, showsd = TRUE, 
                           metrics = ""rmse"", verbose = TRUE, ""eval_metric"" = ""rmse"",
                           ""objective"" = ""reg:linear"", ""max.depth"" = 15, ""eta"" = 2/ntrees,                               
                           ""subsample"" = currentSubsampleRate, ""colsample_bytree"" = currentColsampleRate)

    xvalidationScores &lt;- as.data.frame(xgboostModelCV)
    #Save rmse of the last iteration
    rmse &lt;- tail(xvalidationScores$test.rmse.mean, 1)

    return(c(rmse, currentSubsampleRate, currentColsampleRate))

})
</code></pre>

<p>And combined with some ggplot2 magic using the results of that apply function you can plot a graphical representation of the search.<a href=""https://i.stack.imgur.com/jaAuy.png""><img src=""https://i.stack.imgur.com/jaAuy.png"" alt=""My xgboost hyperparameter search""></a> </p>

<p>In this plot lighter colors represent lower error and each block represents a unique combination of column sampling and row sampling. So if you want to perform an additional search of say eta (or tree depth) you will end up with one of these plots for each eta parameters tested.</p>

<p>I see you have a different evaluation metric (RMPSE), just plug that in the cross validation function and you'll get the desired result. Besides that I wouldn't worry too much about fine tuning the other parameters because doing so won't improve performance too much, at least not so much compared to spending more time engineering features or cleaning the data.</p>

<ol start=""3"">
<li><strong>Others</strong></li>
</ol>

<p>Random search and Bayesian parameter selection are also possible but I haven't made/found an implementation of them yet.</p>

<p>Here is a good primer on bayesian Optimization of hyperparameters by Max Kuhn creator of caret.</p>

<p><a href=""http://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html"">http://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html</a></p>
","<r><python><xgboost>","42","42","42","0","13023","3130"
"24524","{
  ""id"": 24524,
  ""title"": ""Why should the data be shuffled for machine learning tasks""
}","<p>Shuffling data serves the purpose of reducing variance and making sure that models remain general and overfit less.</p>

<p>The obvious case where you'd shuffle your data is if your data is sorted by their class/target. Here, you will want to shuffle to make sure that your training/test/validation sets are representative of the overall distribution of the data.</p>

<p>For batch gradient descent, the same logic applies. The idea behind batch gradient descent is that by calculating the gradient on a single batch, you will usually get a fairly good estimate of the ""true"" gradient. That way, you save computation time by not having to calculate the ""true"" gradient over the entire dataset every time.</p>

<p>You want to shuffle your data after each epoch because you will always have the risk to create batches that are not representative of the overall dataset, and therefore, your estimate of the gradient will be off. Shuffling your data after each epoch ensures that you will not be ""stuck"" with too many bad batches.</p>

<p>In regular stochastic gradient descent, when each batch has size 1, you still want to shuffle your data after each epoch to keep your learning general. Indeed, if data point 17 is always used after data point 16, its own gradient will be biased with whatever updates data point 16 is making on the model. By shuffling your data, you ensure that each data point creates an ""independent"" change on the model, without being biased by the same points before them.</p>
","<machine-learning><neural-network><deep-learning>","42","42","42","0","38887","4891"
"162","{
  ""id"": 162,
  ""title"": ""Publicly Available Datasets""
}","<p>There are many openly available data sets, one many people often overlook is <a href=""http://www.data.gov/"">data.gov</a>. As mentioned previously Freebase is great, so are all the examples posted by @Rubens</p>
","<open-source><dataset>","42","42","42","0","59","986"
"44","{
  ""id"": 44,
  ""title"": ""Is the R language suitable for Big Data""
}","<p>Actually this is coming around. In the book R in a Nutshell there is even a section on using R with Hadoop for big data processing. There are some work arounds that need to be done because R does all it's work in memory, so you are basically limited to the amount of RAM you have available to you.</p>

<p>A mature project for R and Hadoop is <a href=""https://github.com/RevolutionAnalytics/RHadoop"">RHadoop</a></p>

<p>RHadoop has been divided into several sub-projects, rhdfs, rhbase, rmr2, plyrmr, and quickcheck (<a href=""https://github.com/RevolutionAnalytics/RHadoop/wiki"">wiki</a>).</p>
","<bigdata><r>","42","42","42","0","59","986"
"5370","{
  ""id"": 5370,
  ""title"": ""Data Science in C (or C++)""
}","<blockquote>
  <p>Or must I loose most of the efficiency gained by programming in C by calling on R scripts or other languages?</p>
</blockquote>

<p>Do the opposite: learn C/C++ to write R extensions. Use C/C++ only for the performance critical sections of your new algorithms, use R to build your analysis, import data, make plots etc.</p>

<p>If you want to go beyond R, I'd recommend learning python. There are many libraries available such as <a href=""http://scikit-learn.org/stable/"">scikit-learn</a> for machine learning algorithms or <a href=""http://pybrain.org/"">PyBrain</a> for building Neural Networks etc. (and use pylab/<a href=""http://matplotlib.org/"">matplotlib</a> for plotting and <a href=""http://ipython.org/notebook.html"">iPython notebooks</a> to develop your analyses). Again, C/C++ is useful to implement time critical algorithms as python extensions.</p>
","<machine-learning><bigdata><statistics><programming><c>","41","41","41","0","462","536"
"12833","{
  ""id"": 12833,
  ""title"": ""How are 1x1 convolutions the same as a fully connected layer?""
}","<h1>Your Example</h1>

<p>In your example we have 3 input and 2 output units. To apply convolutions, think of those units having shape: <code>[1,1,3]</code> and <code>[1,1,2]</code>, respectively. In CNN terms, we have <code>3</code> input and <code>2</code> output feature maps, each having spatial dimensions <code>1 x 1</code>. </p>

<p>Applying an <code>n x n</code> convolution to a layer with <code>k</code> feature maps, requires you to have a kernel of shape <code>[n,n,k]</code>. Hence the kernel of you <code>1x1</code> convolutions have shape <code>[1, 1, 3]</code>. You need <code>2</code> of those kernels (or filters) to produce the <code>2</code> output feature maps. Please Note: $1 \times 1$ convolutions really are  $1 \times 1 \times \text{number of channels of the input}$ convolutions. The last one is only rarely mentioned.</p>

<p>Indeed if you choose as kernels and bias:<br>
$$
\begin{align}
w_1 &amp;= 
\begin{pmatrix}
 0 &amp; 1 &amp; 1\\
\end{pmatrix} \in \mathbb{R}^{3}\\
w_2 &amp;= 
\begin{pmatrix}
2 &amp; 3 &amp; 5\\
\end{pmatrix} \in \mathbb{R}^{3}\\
b &amp;= \begin{pmatrix}8\\ 13\end{pmatrix} \in \mathbb{R}^2
\end{align}
$$</p>

<p>The conv-layer will then compute $f(x) = ReLU\left(\begin{pmatrix}w_1 \cdot x\\ w_2 \cdot x\end{pmatrix} + \begin{pmatrix}b_1\\ b_2\end{pmatrix}\right)$ with $x \in \mathbb{R}^3$. </p>

<h1>Transformation in real Code</h1>

<p>For a real-life example, also have a look at my <a href=""https://github.com/MarvinTeichmann/tensorflow-fcn/blob/d04bc268ac6e84f03afc4332d7f54ecff22d1732/fcn32_vgg.py"" rel=""noreferrer"">vgg-fcn</a> implementation. The Code provided in this file takes the VGG weights, but transforms every fully-connected layer into a convolutional layers. The resulting network yields the same output as <code>vgg</code> when applied to input image of shape <code>[244,244,3]</code>. (When applying both networks without padding).  </p>

<p>The transformed convolutional layers are introduced in the function <code>_fc_layer</code> (line 145). They have kernel size <code>7x7</code> for FC6 (which is maximal, as <code>pool5</code> of VGG outputs a feature map of shape <code>[7,7, 512]</code>. Layer <code>FC7</code> and <code>FC8</code> are implemented as <code>1x1</code> convolution.</p>

<h1>""Full Connection Table""</h1>

<p>I am not 100% sure, but he might refer to a filter/kernel which has the same dimension as the input feature map. In both cases (Code and your Example) the spatial dimensions are maximal in the sense, that the spatial dimension of the filter is the same as the spatial dimension as the input.</p>
","<neural-network><convolutional-neural-network>","41","41","41","0","13662","526"
"20193","{
  ""id"": 20193,
  ""title"": ""What is the advantage of keeping batch size a power of 2?""
}","<p>This is a problem of alignment of the virtual processors (VP) onto the physical processors (PP) of the GPU. Since the number of PP is often a power of 2, using a number of VP different from a power of 2 leads to poor performance.<br />
You can see the mapping of the VP onto the PP as a pile of slices of size <em>the number of PP</em>.<br />
Say you've got 16 PP.<br />
You can map 16 VP on them : 1 VP is mapped onto 1 PP.<br />
You can map 32 VP on them : 2 slices of 16 VP, 1 PP will be responsible for 2 VP.<br />
Etc.
During execution, each PP will execute the job of the 1st VP he is responsible for, then the job of the 2nd VP etc.<br />
If you use 17 VP, each PP will execute the job of their 1st PP, then 1 PP will execute the job of the 17th <strong>AND</strong> the other ones will do nothing (precised below).<br />
This is due to the SIMD paradigm (called <em>vector</em> in the 70s) used by GPUs. This is often called Data Parallelism : all the PP do the same thing at the same time but on different data. See <a href=""https://en.wikipedia.org/wiki/SIMD"" rel=""noreferrer"">here</a>.<br />
More precisely, in the example with 17 VP, once the job of the 1st slice done (by all the PPs doing the job of their 1st VP), all the PP will do the same job (2nd VP), <strong>but only one has some data to work on</strong>.<br />
Nothing to do with learning. This is only programming stuff.</p>
","<machine-learning><training>","41","41","41","0","33499","526"
"6855","{
  ""id"": 6855,
  ""title"": ""When to use Random Forest over SVM and vice versa?""
}","<p>I would say, the choice depends very much on what data you have and what is your purpose. A few ""rules of thumb"".</p>

<p>Random Forest is intrinsically suited for multiclass problems, while SVM is intrinsically two-class. For multiclass problem you will need to reduce it into multiple binary classification problems.</p>

<p>Random Forest works well with a mixture of numerical and categorical features. When features are on the various scales, it is also fine. Roughly speaking, with Random Forest you can use data as they are. SVM maximizes the ""margin"" and thus relies on the concept of ""distance"" between different points. It is up to you to decide if ""distance"" is meaningful. As a consequence, one-hot encoding for categorical features is a must-do. Further, min-max or other scaling is highly recommended at preprocessing step. </p>

<p>If you have data with $n$ points and $m$ features, an intermediate step in SVM is constructing an $n\times n$ matrix (think about memory requirements for storage) by calculating $n^2$ dot products (computational complexity).
Therefore, as a rule of thumb, SVM is hardly scalable beyond 10^5 points.
Large number of features (homogeneous features with meaningful distance, pixel of image would be a perfect example) is generally not a problem. </p>

<p>For a classification problem Random Forest gives you probability of belonging to class. SVM gives you distance to the boundary, you still need to convert it to probability somehow if you need probability.</p>

<p>For those problems, where SVM applies, it generally performs better than Random Forest.</p>

<p>SVM gives you ""support vectors"", that is points in each class closest to the boundary between classes. They may be of interest by themselves for interpretation. </p>
","<machine-learning><classification><random-forest><svm>","41","41","41","0","9085","1396"
"86635","{
  ""id"": 86635,
  ""title"": ""Why is it wrong to train and test a model on the same dataset?""
}","<p>Yes, you put it quite correctly.</p>
<p>As a teacher, you wouldn’t give your students an exam that’s got the exact same exercises you have provided as homework: you want to find out whether they (a) have actually understood the intuition behind the methods you taught them and (b) make sure they haven’t just memorised the homework exercises.</p>
","<machine-learning><neural-network><dataset><data><training>","41","41","41","0","102852","1342"
"811","{
  ""id"": 811,
  ""title"": ""Should I go for a 'balanced' dataset or a 'representative' dataset?""
}","<p>I would say the answer depends on your use case. Based on my experience:</p>

<ul>
<li>If you're trying to build a representative model -- one that describes the data rather than necessarily predicts -- then I would suggest using a representative sample of your data.</li>
<li>If you want to build a predictive model, particularly one that performs well by measure of AUC or rank-order and plan to use a basic ML framework (i.e. Decision Tree, SVM, Naive Bayes, etc), then I would suggest you feed the framework a balanced dataset. Much of the literature on class imbalance finds that random undersampling (down sampling the majority class to the size of the minority class) can drive performance gains.</li>
<li>If you're building a predictive model, but are using a more advanced framework (i.e. something that determines sampling parameters via wrapper or a modification of a bagging framework that samples to class equivalence), then I would suggest again feeding the representative sample and letting the algorithm take care of balancing the data for training.</li>
</ul>
","<machine-learning><dataset><class-imbalance>","41","41","41","0","2513","546"
"6335","{
  ""id"": 6335,
  ""title"": ""What is a good way to transform Cyclic Ordinal attributes?""
}","<p>The most logical way to transform hour is into two variables that swing back and forth out of sink. Imagine the position of the end of the hour hand of a 24-hour clock. The <code>x</code> position swings back and forth out of sink with the <code>y</code> position. For a 24-hour clock you can accomplish this with <code>x=sin(2pi*hour/24)</code>,<code>y=cos(2pi*hour/24)</code>.  </p>

<p>You need both variables or the proper movement through time is lost.  This is due to the fact that the derivative of either sin or cos changes in time where as the <code>(x,y)</code> position varies smoothly as it travels around the unit circle.</p>

<p>Finally, consider whether it is worthwhile to add a third feature to trace linear time, which can be constructed my hours (or minutes or seconds) from the start of the first record or a Unix time stamp or something similar.  These three features then provide proxies for both the cyclic and linear progression of time e.g. you can pull out cyclic phenomenon like sleep cycles in people's movement and also linear growth like population vs. time.</p>

<p>Hope this helps!</p>

<p><strong>Adding some relevant example code that I generated for another answer:</strong></p>

<p><strong>Example of if being accomplished:</strong></p>

<pre><code># Enable inline plotting
%matplotlib inline

#Import everything I need...

import numpy as np
import matplotlib as mp

import matplotlib.pyplot as plt
import pandas as pd

# Grab some random times from here: https://www.random.org/clock-times/
# put them into a csv.
from pandas import DataFrame, read_csv
df = read_csv('/Users/angus/Machine_Learning/ipython_notebooks/times.csv',delimiter=':')
df['hourfloat']=df.hour+df.minute/60.0
df['x']=np.sin(2.*np.pi*df.hourfloat/24.)
df['y']=np.cos(2.*np.pi*df.hourfloat/24.)

df
</code></pre>

<p><a href=""https://i.stack.imgur.com/1UTYj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1UTYj.png"" alt=""enter image description here""></a></p>

<pre><code>def kmeansshow(k,X):

    from sklearn import cluster
    from matplotlib import pyplot
    import numpy as np

    kmeans = cluster.KMeans(n_clusters=k)
    kmeans.fit(X)

    labels = kmeans.labels_
    centroids = kmeans.cluster_centers_
    #print centroids

    for i in range(k):
        # select only data observations with cluster label == i
        ds = X[np.where(labels==i)]
        # plot the data observations
        pyplot.plot(ds[:,0],ds[:,1],'o')
        # plot the centroids
        lines = pyplot.plot(centroids[i,0],centroids[i,1],'kx')
        # make the centroid x's bigger
        pyplot.setp(lines,ms=15.0)
        pyplot.setp(lines,mew=2.0)
    pyplot.show()
    return centroids
</code></pre>

<p>Now lets try it out:</p>

<pre><code>kmeansshow(6,df[['x', 'y']].values)
</code></pre>

<p><a href=""https://i.stack.imgur.com/MkVNg.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MkVNg.png"" alt=""enter image description here""></a></p>

<p>You can just barely see that there are some after midnight times included with the before midnight green cluster.  Now lets reduce the number of clusters and show that before and after midnight can be connected in a single cluster in more detail:</p>

<pre><code>kmeansshow(3,df[['x', 'y']].values)
</code></pre>

<p><a href=""https://i.stack.imgur.com/bWg2B.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bWg2B.png"" alt=""enter image description here""></a></p>

<p>See how the blue cluster contains times that are from before and after midnight that are clustered together in the same cluster...</p>

<p>QED!</p>
","<feature-extraction><feature-scaling><featurization>","41","41","41","0","9420","6528"
"42626","{
  ""id"": 42626,
  ""title"": ""Data science related funny quotes""
}","<p>Q: How many machine learning specialists does it take to change a light bulb?</p>

<p>A: Just one, but they require a million light bulbs to train properly.</p>

<p>Q: How many machine learning specialists does it take to change a fluorescent light bulb?</p>

<p>A: That wasn't in the training data!</p>
","<machine-learning><neural-network><deep-learning>","41","41","41","0","37184","1021"
"5389","{
  ""id"": 5389,
  ""title"": ""Best python library for neural networks""
}","<p><a href=""http://tensorflow.org/"">Tensor Flow</a> (<a href=""http://www.tensorflow.org/api_docs/python/nn.html#neural-network"">docs</a>) by Google is another nice framework which has automatic differentiation. I've written down some <a href=""http://martin-thoma.com/tensor-flow-quick/"">quick thoughts about Google Tensor Flow</a> on my blog, together with the MNIST example which they have in their tutorial.</p>

<p>See also: My <a href=""https://martin-thoma.com/tf-xor-tutorial/"">Tensorflow XOR tutorial</a></p>

<p><a href=""https://github.com/benanne/Lasagne"">Lasagne</a> (<a href=""http://lasagne.readthedocs.org/en/latest/"">docs</a>) is very nice, as it uses theano (→ you can use the GPU) and makes it simpler to use. The author of lasagne won the Kaggle Galaxy challenge, as far as I know. It is nice with <a href=""http://pythonhosted.org/nolearn/"">nolearn</a>. Here is an MNIST example network:</p>

<pre><code>#!/usr/bin/env python

import lasagne
from lasagne import layers
from lasagne.updates import nesterov_momentum
from nolearn.lasagne import NeuralNet

import sys
import os
import gzip
import pickle
import numpy


PY2 = sys.version_info[0] == 2

if PY2:
    from urllib import urlretrieve

    def pickle_load(f, encoding):
        return pickle.load(f)
else:
    from urllib.request import urlretrieve

    def pickle_load(f, encoding):
        return pickle.load(f, encoding=encoding)

DATA_URL = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'
DATA_FILENAME = 'mnist.pkl.gz'


def _load_data(url=DATA_URL, filename=DATA_FILENAME):
    """"""Load data from `url` and store the result in `filename`.""""""
    if not os.path.exists(filename):
        print(""Downloading MNIST dataset"")
        urlretrieve(url, filename)

    with gzip.open(filename, 'rb') as f:
        return pickle_load(f, encoding='latin-1')


def load_data():
    """"""Get data with labels, split into training, validation and test set.""""""
    data = _load_data()
    X_train, y_train = data[0]
    X_valid, y_valid = data[1]
    X_test, y_test = data[2]
    y_train = numpy.asarray(y_train, dtype=numpy.int32)
    y_valid = numpy.asarray(y_valid, dtype=numpy.int32)
    y_test = numpy.asarray(y_test, dtype=numpy.int32)

    return dict(
        X_train=X_train,
        y_train=y_train,
        X_valid=X_valid,
        y_valid=y_valid,
        X_test=X_test,
        y_test=y_test,
        num_examples_train=X_train.shape[0],
        num_examples_valid=X_valid.shape[0],
        num_examples_test=X_test.shape[0],
        input_dim=X_train.shape[1],
        output_dim=10,
    )


def nn_example(data):
    net1 = NeuralNet(
        layers=[('input', layers.InputLayer),
                ('hidden', layers.DenseLayer),
                ('output', layers.DenseLayer),
                ],
        # layer parameters:
        input_shape=(None, 28*28),
        hidden_num_units=100,  # number of units in 'hidden' layer
        output_nonlinearity=lasagne.nonlinearities.softmax,
        output_num_units=10,  # 10 target values for the digits 0, 1, 2, ..., 9

        # optimization method:
        update=nesterov_momentum,
        update_learning_rate=0.01,
        update_momentum=0.9,

        max_epochs=10,
        verbose=1,
        )

    # Train the network
    net1.fit(data['X_train'], data['y_train'])

    # Try the network on new data
    print(""Feature vector (100-110): %s"" % data['X_test'][0][100:110])
    print(""Label: %s"" % str(data['y_test'][0]))
    print(""Predicted: %s"" % str(net1.predict([data['X_test'][0]])))


def main():
    data = load_data()
    print(""Got %i testing datasets."" % len(data['X_train']))
    nn_example(data)

if __name__ == '__main__':
    main()
</code></pre>

<p><a href=""http://caffe.berkeleyvision.org/"">Caffe</a> is a C++ library, but has Python bindings. You can do most stuff by configuration files (prototxt). It has a lot of options and can also make use of the GPU.</p>
","<machine-learning><python><neural-network>","40","40","40","0","8820","16400"
"31045","{
  ""id"": 31045,
  ""title"": ""What does Logits in machine learning mean?""
}","<p><strong>Logits interpreted to be the unnormalised</strong> (or not-yet normalised) <strong>predictions</strong> (or outputs) <strong>of a model. These can give results, but we don't normally stop with logits, because interpreting their raw values is not easy.</strong></p>

<p>Have a look at <a href=""https://en.wikipedia.org/wiki/Logit"" rel=""noreferrer"">their definition</a> to help understand how <em>logits</em> are produced.</p>

<h3>Let me explain with an example:</h3>

<p>We want to train a model that learns how to classify cats and dogs, using photos that each contain either one cat or one dog. You build a model give it some of the data you have to approximate a mapping between images and predictions. You then give the model some of the unseen photos in order to test its predictive accuracy on new data. As we have a classification problem (we are trying to put each photo into one of two classes), the model will give us two scores for each input image. A score for how likely it believes the image contains a cat, and then a score for its belief that the image contains a dog.</p>

<p>Perhaps for the first new image, you get logit values out of <code>16.917</code> for a cat and then <code>0.772</code> for a dog. Higher means better, or ('more likely'), so you'd say that a cat is the answer. The correct answer is a cat, so the model worked!</p>

<p>For the second image, the model may say the logit values are 1.004 for a cat and 0.709 for a dog. So once again, our model says we the image contains a cat. The correct answer is once again a cat, so the model worked again!</p>

<p>Now we want to compare the two result. One way to do this is to normalise the scores. That is, <strong>we normalise the logits</strong>! Doing this we gain some insight into the confidence of our model.</p>

<p>Let's using <a href=""https://en.wikipedia.org/wiki/Softmax_function#Softmax_Normalization"" rel=""noreferrer"">the softmax</a>, where all results sum to <code>1</code> and so allow us to think of them as probabilities:</p>

<p><span class=""math-container"">$$\sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}} \hspace{20mm}   for \hspace{5mm} j = 1, …, K.$$</span></p>

<p>For the first test image, we get</p>

<p><span class=""math-container"">$$prob(cat) = \frac{exp(16.917)}{exp(16.917) + exp(0.772)} = 0.9999$$</span>
<span class=""math-container"">$$prob(dog) = \frac{exp(0.772)}{exp(16.917) + exp(0.772)} = 0.0001$$</span></p>

<p>If we do the same for the second image, we get the results:</p>

<p><span class=""math-container"">$$prob(cat) = \frac{exp(1.004)}{exp(1.004) + exp(0.709)} = 0.5732$$</span>
<span class=""math-container"">$$prob(dog) = \frac{exp(0.709)}{exp(1.004) + exp(0.709)} = 0.4268$$</span></p>

<p>The model was not really sure about the second image, as it was very close to 50-50 - a guess!</p>

<p>The last part of the quote from your question likely refers to a neural network as the model. The layers of a neural network commonly take input data, multiply that by some parameters (weights) that we want to learn, then apply a <strong>non-linearity</strong> function, which provides the model with the power to learn non-linear relationships. Without this non-linearity, a neural network would simply be a list of linear operations, performed on some input data, which means it would only be able to learn linear relationships. This would be a massive constraint, meaning the model could always be reduced to a basic linear model.
That being said, it is not considered helpful to apply a non-linearity to the logit outputs of a model, as you are generally going to be cutting out some information, right before a final prediction is made. Have a look for <a href=""https://stats.stackexchange.com/questions/163695/non-linearity-before-final-softmax-layer-in-a-convolutional-neural-network"">related comments in this thread</a>.</p>
","<machine-learning><deep-learning>","40","40","40","0","45264","13123"
"48539","{
  ""id"": 48539,
  ""title"": ""How much of data wrangling is a data scientist's job?""
}","<p>This is a situation that many blogs, companies and papers acknowledge as something real in many cases.</p>
<p>In this paper <a href=""https://openproceedings.org/2016/conf/edbt/paper-94.pdf"" rel=""noreferrer"">Data Wrangling for Big Data: Challenges and Opportunities</a>, there is a quote about it</p>
<blockquote>
<p>data scientists spend from 50 percent to 80 percent of their time</p>
<p>collecting and preparing unruly digital data.</p>
</blockquote>
<p>Also, you can read the source of that quote in this article from The New York Times, <a href=""https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html"" rel=""noreferrer"">For Big-Data Scientists, ‘Janitor Work’ Is Key Hurdle to Insights</a></p>
<p>Unfortunately, the real world is not like Kaggle. You don't get a CSV or Excel file that you can just start the Data Exploration with a little bit of cleaning. You need to find the data in a format that is not suitable for your needs.</p>
<p>What you can do is make use of the old data as much as you can and try to adapt the storing of new data in a process that will be easier for you (or a future colleague) to work with.</p>
","<data-wrangling>","40","40","40","0","201","3560"
"24838","{
  ""id"": 24838,
  ""title"": ""Time Series prediction using LSTMs: Importance of making time series stationary""
}","<p>In general time series are not really different from other machine learning problems - you want your test set to 'look like' your training set, because you want the model you learned on your training set to still be appropriate for your test set.  That's the important underlying concept regarding stationarity.  Time series have the additional complexity that there may be <em>long term</em> structure in your data that your model may not be sophisticated enough to learn.  For example, when using an autoregressive lag of N, we can't learn dependencies over intervals longer than N. Hence, when using simple models like ARIMA, we want data to also be <em>locally</em> stationary.</p>

<ol>
<li><p>As you said, stationary just means the model's statistics don't change over time ('locally' stationary).  ARIMA models are essentially regression models where you use the past N values as input to linear regression to prediction the N+1st value. (At least, that's what the AR part does).  When you learn the model you're learning the regression coefficients.  If you have a time series where you learn the relationship between the past N points and the next point, and then you apply that to a different set of N points to predict the next value, you are implicitly assuming that the same relationship holds between the N predictor points and the following N+1st point you're trying to predict.  That's stationarity.  If you separated your training set into two intervals and trained on them separately, and got two very different models - what would you conclude from that? Do you think you would feel confident applying those models to predict <em>new</em> data? Which one would you use? These issues arise if the data is 'non-stationary'.  </p></li>
<li><p>My take on RNNs is this - you are still learning a pattern from one segment of a time series, and you still want to apply it to another part of the time series to get predictions.  The model learns a simplified representation of the time series - and if that representation applies on the training set but not in the test set, it won't perform well.  However, unlike ARIMA, RNNs are capable of learning nonlinearities, and specialized nodes like LSTM nodes are even better at this.  In particular, LSTMs and GRUs are very good at learning long-term dependencies.  See for example <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""noreferrer"">this blog post</a>.  Effectively this means that what is meant by  'stationarity' is less brittle with RNNs, so it's somewhat less of a concern. To be able to learn long term dependencies, however, you need LOTS of data to train on.</p></li>
</ol>

<p>Ultimately the proof is in the pudding. That is, do model validation like you would with any other machine learning project.  If your model predicts well for hold-out data, you can feel somewhat confident in using it.  But like any other ML project - if your test data is ever significantly different than your training data, your model will not perform well.</p>
","<deep-learning><predictive-modeling><time-series><forecast><lstm>","40","40","40","0","41886","2028"
"12856","{
  ""id"": 12856,
  ""title"": ""How do you visualize neural network architectures?""
}","<h2>Tensorflow, Keras, MXNet, PyTorch</h2>

<p>If the neural network is given as a Tensorflow graph, then you can <a href=""https://www.tensorflow.org/guide/graph_viz"" rel=""noreferrer"">visualize this graph with TensorBoard</a>.</p>

<p>Here is how the MNIST CNN looks like:</p>

<p><a href=""https://i.stack.imgur.com/zJHpV.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zJHpV.png"" alt=""enter image description here""></a></p>

<p>You can add names / scopes (like ""dropout"", ""softmax"", ""fc1"", ""conv1"", ""conv2"") yourself.</p>

<h3>Interpretation</h3>

<p>The following is only about the left graph. I ignore the 4 small graphs on the right half.</p>

<p>Each box is a layer with parameters that can be learned. For inference, information flows from bottom to the top. Ellipses are layers which do not contain learned parameters.</p>

<p>The color of the boxes does not have a meaning.</p>

<p>I'm not sure of the value of the dashed small boxes (""gradients"", ""Adam"", ""save"").</p>
","<machine-learning><neural-network><deep-learning><visualization>","40","42","41","1","8820","16400"
"10616","{
  ""id"": 10616,
  ""title"": ""Number of parameters in an LSTM model""
}","<p>The LSTM has a set of 2 matrices: U and W for each of the (3) gates. The (.) in the diagram indicates multiplication of these matrices with the input <span class=""math-container"">$x$</span> and output <span class=""math-container"">$h$</span>.</p>

<ul>
<li>U has dimensions <span class=""math-container"">$n \times m$</span> </li>
<li>W has dimensions <span class=""math-container"">$n \times n$</span></li>
<li>there is a different set of these matrices for each of the three gates(like <span class=""math-container"">$U_{forget}$</span> for the <em>forget</em> gate etc.)</li>
<li>there is another set of these matrices for updating the cell state S</li>
<li>on top of the mentioned matrices, you need to count the biases (not in the picture)</li>
</ul>

<p>Hence total # parameters = <span class=""math-container"">$4(nm+n^{2} + n)$</span></p>

<p><a href=""https://i.stack.imgur.com/DT6Xc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DT6Xc.png"" alt=""LSTM abstract block""></a></p>
","<deep-learning><rnn>","40","42","41","1","13686","1157"
"24537","{
  ""id"": 24537,
  ""title"": ""Does gradient descent always converge to an optimum?""
}","<p>Gradient Descent is an algorithm which is designed to find the optimal points, but these optimal points are not necessarily global. And yes if it happens that it diverges from a local location it may converge to another optimal point but its probability is not too much. The reason is that the step size might be too large that prompts it recede one optimal point and the probability that it oscillates is much more than convergence.</p>

<p>About gradient descent there are two main perspectives, machine learning era and deep learning era. During machine learning era it was considered that gradient descent will find the local/global optimum but in deep learning era where the dimension of input features are too much it is shown in practice that the probability that all of the features be located in there optimal value at a single point is not too much and rather seeing to have optimal locations in cost functions, most of the time saddle points are observed. This is one of the reasons that training with lots of data and training epochs cause the deep learning models outperform other algorithms. So if you train your model, it will find a detour or will find its way to go downhill and do not stuck in saddle points, but you have to have appropriate step sizes.</p>

<p>For more intuitions I suggest you referring <a href=""https://datascience.stackexchange.com/q/18802/28175"">here</a> and <a href=""https://www.coursera.org/learn/deep-neural-network/lecture/RFANA/the-problem-of-local-optima"" rel=""noreferrer"">here</a>.</p>
","<machine-learning><neural-network><deep-learning><optimization><gradient-descent>","40","40","40","0","28175","12688"
"9493","{
  ""id"": 9493,
  ""title"": ""xgboost: give more importance to recent samples""
}","<p>Just add weights based on your time labels to your xgb.DMatrix.
The following example is written in R but the same principle applies to xgboost on Python or Julia.</p>

<pre><code>data &lt;- data.frame(feature = rep(5, 5),
                   year = seq(2011, 2015), 
                   target = c(1, 0, 1, 0, 0))
weightsData &lt;- 1 + (data$year - max(data$year)) * 5 * 0.01

#Now create the xgboost matrix with your data and weights
xgbMatrix &lt;- xgb.DMatrix(as.matrix(data$feature), 
                         label = data$target, 
                         weight = weightsData)
</code></pre>
","<xgboost><weighted-data>","40","40","40","0","13023","3130"
"40093","{
  ""id"": 40093,
  ""title"": ""What is the reason behind taking log transformation of few continuous variables?""
}","<p>This is done when the variables span several orders of magnitude. Income is a typical example: its distribution is ""power law"", meaning that the vast majority of incomes are small and very few are big. </p>

<p>This type of ""fat tailed"" distribution is studied in logarithmic scale because of the mathematical properties of the logarithm:</p>

<p><span class=""math-container"">$$log(x^n)= n log(x)$$</span></p>

<p>which implies</p>

<p><span class=""math-container"">$$log(10^4) = 4 * log(10)$$</span></p>

<p>and </p>

<p><span class=""math-container"">$$log(10^3) = 3 * log(10)$$</span></p>

<p>which transforms a huge difference <span class=""math-container"">$$ 10^4 - 10^3 $$</span> in a smaller one <span class=""math-container"">$$ 4 - 3 $$</span>
Making the values comparable. </p>
","<machine-learning><python><classification><scikit-learn>","40","40","40","0","60565","621"
"21888","{
  ""id"": 21888,
  ""title"": ""How to use the output of GridSearch?""
}","<p>Decided to go away and find the answers that would satisfy my question, and write them up here for anyone else wondering.</p>
<p>The .best_estimator_ attribute is an instance of the specified model type, which has the 'best' combination of given parameters from the param_grid. Whether or not this instance is useful depends on whether the refit parameter is set to True (it is by default). For example:</p>
<pre><code>clf = GridSearchCV(estimator=RandomForestClassifier(), 
                    param_grid=parameter_candidates,
                    cv=5,
                    refit=True,
                    error_score=0,
                    n_jobs=-1)

clf.fit(training_set, training_classifications)
optimised_random_forest = clf.best_estimator_
return optimised_random_forest
</code></pre>
<p>Will return a RandomForestClassifier. This is all pretty clear from the [documentation][1]. What isn't clear from the documentation is why most examples don't specifically use the .best_estimator_ and instead do this:</p>
<pre><code>clf = GridSearchCV(estimator=RandomForestClassifier(), 
                    param_grid=parameter_candidates,
                    cv=5,
                    refit=True,
                    error_score=0,
                    n_jobs=-1)

clf.fit(training_set, training_classifications)
return clf
</code></pre>
<p>This second approach returns a GridSearchCV instance, with all the bells and whistles of the GridSearchCV such as .best_estimator_, .best_params, etc, which itself can be used like a trained classifier because:</p>
<pre><code>Optimised Random Forest Accuracy:  0.916970802919708
[[139  47]
 [ 44 866]]
GridSearchCV Accuracy:  0.916970802919708
[[139  47]
 [ 44 866]]
</code></pre>
<p>It just uses the same best estimator instance when making predictions. So in practise there's no difference between these two unless you specifically only want the estimator instance itself. As a side note, my differences in metrics were unrelated and down to a buggy class weighting function.
[1]: <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.fit"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.fit</a></p>
","<machine-learning><cross-validation>","40","42","41","1","33603","1572"
"27789","{
  ""id"": 27789,
  ""title"": ""Opening a 20GB file for analysis with pandas""
}","<p>If it's a csv file and you do not need to access all of the data at once when training your algorithm, you can read it in chunks. The <code>pandas.read_csv</code> method allows you to read a file in chunks like this: </p>

<pre><code>import pandas as pd
for chunk in pd.read_csv(&lt;filepath&gt;, chunksize=&lt;your_chunksize_here&gt;)
    do_processing()
    train_algorithm()
</code></pre>

<p>Here is the method's <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""noreferrer"">documentation</a></p>
","<python><bigdata><pandas><anaconda>","39","39","39","0","10076","491"
"32276","{
  ""id"": 32276,
  ""title"": ""What is the difference between bootstrapping and cross-validation?""
}","<p>Both cross validation and bootstrapping are <em>resampling</em> methods. </p>

<ul>
<li>bootstrap resamples with replacement (and usually produces new ""surrogate"" data sets with the same number of cases as the original data set). Due to the drawing with replacement, a bootstrapped data set may contain multiple instances of the same original cases, and may completely omit other original cases.</li>
<li><p>cross validation resamples without replacement and thus produces surrogate data sets that are smaller than the original. These data sets are produced in a systematic way so that after a pre-specified number <span class=""math-container"">$k$</span> of surrogate data sets, each of the <span class=""math-container"">$n$</span> original cases has been left out exactly once. This is called k-fold cross validation or leave-<em>x</em>-out cross validation with <span class=""math-container"">$x = \frac{n}{k}$</span>, e.g. leave-one-out cross validation omits 1 case for each surrogate set, i.e. <span class=""math-container"">$k = n$</span>.</p></li>
<li><p>As the name cross <em>validation</em> suggests, its primary purpose is measuring (generalization) performance of a model. On contrast, bootstrapping is primarily used to establish empirical distribution functions for a widespread range of statistics (widespread as in ranging from, say, the variation of the mean to the variation of models in bagged ensemble models).</p></li>
<li><p>The leave-one-out analogue of the bootstrap procedure is called <em>jackknifing</em> (and is actually older than bootstrapping).</p></li>
<li>The bootstrap analogue to cross validation estimates of generalization error is called <em>out-of-bootstrap</em> estimate (because the test cases are those that were left out of the bootstrap resampled training set). </li>
</ul>

<hr>

<blockquote>
  <p>[cross validation vs. out-of-bootstrap validation] However, I cannot see the main difference between them in terms of performance estimation.</p>
</blockquote>

<p>That intuition is correct: in practice there's often not much of a difference between iterated <span class=""math-container"">$k$</span>-fold cross validation and out-of-bootstrap. With a similar total number of evaluated surrogate models, total error [of the model prediction error measurement] has been found to be similar, although oob typically has more bias and less variance than the corresponding CV estimates.</p>

<p>There are a number of attempts to reduce <a href=""https://en.wikipedia.org/wiki/Out-of-bag_error"" rel=""noreferrer"">oob bias</a> (.632-bootstrap, .632+-bootstrap) but whether they will actually improve the situation depends on the situation at hand.  </p>

<p>Literature:</p>

<ul>
<li><a href=""http://www.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf"" rel=""noreferrer"">Kohavi, R.: A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, Mellish, C. S. (ed.) Artificial Intelligence Proceedings 14<span class=""math-container"">$^th$</span> International Joint Conference, 20 -- 25. August 1995, Montréal, Québec, Canada, Morgan Kaufmann, USA, , 1137 - 1145 (1995).</a></li>
<li><a href=""http://dx.doi.org/10.1016/j.csda.2009.04.009"" rel=""noreferrer""> Kim, J.-H. Estimating classification error rate: Repeated cross-validation, repeated hold-out and bootstrap , Computational Statistics &amp; Data Analysis , 53, 3735 - 3745 (2009). DOI: 10.1016/j.csda.2009.04.009</a></li>
<li><a href=""http://www.sciencedirect.com/science/article/pii/S0169743905000687"" rel=""noreferrer"">Beleites, C.; Baumgartner, R.; Bowman, C.; Somorjai, R.; Steiner, G.; Salzer, R. &amp; Sowa, M. G. Variance reduction in estimating classification error using sparse datasets, Chemom Intell Lab Syst, 79, 91 - 100 (2005). </a></li>
</ul>

<hr>

<blockquote>
  <p>The only thing I could figure out that in case of bootstrapping one could artificially produce virtually arbitrary number of such subsets while for CV the number of instances is a kind of limit for this.</p>
</blockquote>

<p>Yes, there are fewer combinations possible for CV than for bootstrapping. But the limit for CV is probably higher than you are aware of. 
For a data set with  <span class=""math-container"">$n$</span> cases and <span class=""math-container"">$k$</span>-fold cross validation, you have </p>

<ul>
<li>CV  <span class=""math-container"">$\binom{n}{k}$</span> combinations without replacement (for k &lt; n that are far more than the <span class=""math-container"">$k$</span> possibilities that are usually evaluated) vs. </li>
<li>bootstrap/oob <span class=""math-container"">$\binom{2 n - 1}{n}$</span> combinations with replacement (which are again far more than the, say, 100 or 1000 surrogate models that are typically evaluated)</li>
</ul>
","<cross-validation><evaluation>","39","39","39","0","5268","770"
"9870","{
  ""id"": 9870,
  ""title"": ""Neural networks: which cost function to use?""
}","<p>This answer is on the <em>general</em> side of cost functions, not related to TensorFlow, and will mostly address the ""some explanation about this topic"" part of your question.</p>

<p>In most examples/tutorial I followed, the cost function used was somewhat arbitrary. The point was more to introduce the reader to a specific method, not to the cost function specifically. It should not stop you to follow the tutorial to be familiar with the tools, but my answer should help you on how to choose the cost function for your own problems.</p>

<p>If you want answers regarding Cross-Entropy, Logit, L2 norms, or anything specific, I advise you to post multiple, more specific questions. This will increase the probability that someone with specific knowledge will see your question.</p>

<hr>

<p>Choosing the right cost function for achieving the desired result is a critical point of machine learning problems. The basic approach, if you do not know exactly what you want out of your method, is to use <a href=""https://en.wikipedia.org/wiki/Mean_squared_error"" rel=""noreferrer"">Mean Square Error (Wikipedia)</a> for regression problems and Percentage of error for classification problems. However, if you want <em>good</em> results out of your method, you need to <em>define good</em>, and thus define the adequate cost function. This comes from both domain knowledge (what is your data, what are you trying to achieve), and knowledge of the tools at your disposal. </p>

<p>I do not believe I can guide you through the cost functions already implemented in TensorFlow, as I have very little knowledge of the tool, but I can give you an example on how to write and assess different cost functions.</p>

<hr>

<p>To illustrate the various differences between cost functions, let us use the example of the binary classification problem, where we want, for each sample <span class=""math-container"">$x_n$</span>, the class <span class=""math-container"">$f(x_n) \in \{0,1\}$</span>.</p>

<p>Starting with <strong>computational properties</strong>; how two functions measuring the ""same thing"" could lead to different results. Take the following, simple cost function; the percentage of error. If you have <span class=""math-container"">$N$</span> samples, <span class=""math-container"">$f(y_n)$</span> is the predicted class and <span class=""math-container"">$y_n$</span> the true class, you want to minimize</p>

<ul>
<li><span class=""math-container"">$\frac{1}{N} \sum_n \left\{
\begin{array}{ll}
1 &amp; \text{ if } f(x_n) \not= y_n\\
0 &amp; \text{ otherwise}\\
\end{array} \right. = \sum_n y_n[1-f(x_n)] + [1-y_n]f(x_n)$</span>.</li>
</ul>

<p>This cost function has the benefit of being easily interpretable. However, it is not smooth; if you have only two samples, the function ""jumps"" from 0, to 0.5, to 1. This will lead to inconsistencies if you try to use gradient descent on this function. One way to avoid it is to change the cost function to use probabilities of assignment; <span class=""math-container"">$p(y_n = 1 | x_n)$</span>. The function becomes</p>

<ul>
<li><span class=""math-container"">$\frac{1}{N} \sum_n y_n p(y_n = 0 | x_n) + (1 - y_n) p(y_n = 1 | x_n)$</span>.</li>
</ul>

<p>This function is smoother, and will work better with a gradient descent approach. You will get a 'finer' model. However, it has other problem; if you have a sample that is ambiguous, let say that you do not have enough information to say anything better than <span class=""math-container"">$p(y_n = 1 | x_n) = 0.5$</span>. Then, using gradient descent on this cost function will lead to a model which increases this probability as much as possible, and thus, maybe, overfit.</p>

<p>Another problem of this function is that if <span class=""math-container"">$p(y_n = 1 | x_n) = 1$</span> while <span class=""math-container"">$y_n = 0$</span>, you are certain to be right, but you are wrong. In order to avoid this issue, you can take the log of the probability, <span class=""math-container"">$\log p(y_n | x_n)$</span>. As <span class=""math-container"">$\log(0) = \infty$</span> and <span class=""math-container"">$\log(1) = 0$</span>, the following function does not have the problem described in the previous paragraph:</p>

<ul>
<li><span class=""math-container"">$\frac{1}{N} \sum_n y_n \log p(y_n = 0 | x_n) + (1 - y_n) \log p(y_n = 1 | x_n)$</span>.</li>
</ul>

<p>This should illustrate that in order to optimize the <em>same thing</em>, the percentage of error, different definitions might yield different results if they are easier to make sense of, computationally.</p>

<p><strong>It is possible for cost functions <span class=""math-container"">$A$</span> and <span class=""math-container"">$B$</span> to measure the <em>same concept</em>, but <span class=""math-container"">$A$</span> might lead your method to better results than <span class=""math-container"">$B$</span>.</strong></p>

<hr>

<p>Now let see how different costs function can measure different concepts. In the context of information retrieval, as in google search (if we ignore ranking), we want the returned results to</p>

<ul>
<li>have high <em>precision</em>, not return irrelevant information</li>
<li>have high <em>recall</em>, return as much relevant results as possible</li>
<li><a href=""https://en.wikipedia.org/wiki/Precision_and_recall"" rel=""noreferrer"">Precision and Recall (Wikipedia)</a></li>
</ul>

<p>Note that if your algorithm returns <em>everything</em>, it will return every relevant result possible, and thus have high recall, but have very poor precision. On the other hand, if it returns only <em>one</em> element, the one that it is the most certain is relevant, it will have high precision but low recall.</p>

<p>In order to judge such algorithms, the common cost function is the <a href=""https://en.wikipedia.org/wiki/F1_score"" rel=""noreferrer""><span class=""math-container"">$F$</span>-score (Wikipedia)</a>. The common case is the <span class=""math-container"">$F_1$</span>-score, which gives equal weight to precision and recall, but the general case it the <span class=""math-container"">$F_\beta$</span>-score, and you can tweak <span class=""math-container"">$\beta$</span> to get</p>

<ul>
<li>Higher recall, if you use <span class=""math-container"">$\beta &gt; 1$</span></li>
<li>Higher precision, if you use <span class=""math-container"">$\beta &lt; 1$</span>.</li>
</ul>

<p>In such scenario, <strong>choosing the cost function is choosing what trade-off your algorithm should do</strong>.</p>

<p>Another example that is often brought up is the case of medical diagnosis, you can choose a cost function that punishes more false negatives or false positives depending on what is preferable:</p>

<ul>
<li>More healthy people being classified as sick (But then, we might treat healthy people, which is costly and might hurt them if they are actually not sick)</li>
<li>More sick people being classified as healthy (But then, they might die without treatment)</li>
</ul>

<hr>

<p>In conclusion, defining the cost function is defining the goal of your algorithm. The algorithm defines how to get there.</p>

<hr>

<p>Side note: Some cost functions have nice algorithm ways to get to their goals. For example, a nice way to the minimum of the <a href=""https://en.wikipedia.org/wiki/Hinge_loss"" rel=""noreferrer"">Hinge loss (Wikipedia)</a> exists, by solving the dual problem in <a href=""https://en.wikipedia.org/wiki/Support_vector_machine"" rel=""noreferrer"">SVM (Wikipedia)</a></p>
","<machine-learning><python><neural-network><statistics><tensorflow>","39","39","39","0","15501","1256"
"17840","{
  ""id"": 17840,
  ""title"": ""What is Ground Truth""
}","<p>The ground truth is what you measured for your target variable for the training and testing examples.</p>

<p>Nearly all the time you can safely treat this the same as the label.</p>

<p>In some cases it is not precisely the same as the label. For instance if you augment your data set, there is a subtle difference between the ground truth (your actual measurements) and how the augmented examples relate to the labels you have assigned. However, this distinction is not usually a problem.</p>

<p>Ground truth can be wrong. It is a measurement, and there can be errors in it. In some ML scenarios it can also be a subjective measurement where it is difficult define an underlying objective truth - e.g.  expert opinion or analysis, which you are hoping to automate. Any ML model you train will be limited by the quality of the ground truth used to train and test it, and that is part of the explanation on the Wikipedia quote. It is also why published articles about ML should include full descriptions of how the data was collected.</p>
","<machine-learning><neural-network><deep-learning>","38","38","38","0","836","25808"
"18346","{
  ""id"": 18346,
  ""title"": ""Why use both validation set and test set?""
}","<p>Let's assume that you are training a model whose performance depends on a set of hyperparameters. In the case of a neural network, these parameters may be for instance the learning rate or the number of training iterations. </p>

<p>Given a choice of hyperparameter values, you use the <strong>training</strong> set to train the model. But, how do you set the values for the hyperparameters? That's what the <strong>validation</strong> set is for. You can use it to evaluate the performance of your model for different combinations of hyperparameter values (e.g. by means of a grid search process) and keep the best trained model.</p>

<p>But, how does your selected model compares to other different models? Is your neural network performing better than, let's say, a random forest trained with the same combination of training/test data? You cannot compare based on the validation set, because that validation set was part of the fitting of your model. You used it to select the hyperparameter values! </p>

<p>The <strong>test</strong> set allows you to compare different models in an unbiased way, by basing your comparisons in data that were not use in any part of your training/hyperparameter selection process. </p>
","<machine-learning><neural-network><cross-validation>","38","38","38","0","2576","1607"
"23998","{
  ""id"": 23998,
  ""title"": ""Sentence similarity prediction""
}","<p>Your problem can be solved with Word2vec as well as Doc2vec. Doc2vec would give better results because it takes sentences into account while training the model.  </p>

<p><strong>Doc2vec solution</strong><br>
You can train your doc2vec model following this <a href=""https://rare-technologies.com/doc2vec-tutorial/"" rel=""noreferrer"">link</a>. You may want to perform some pre-processing steps like removing all <strong>stop words</strong> (words like ""the"", ""an"", etc. that don't add much meaning to the sentence). Once you trained your model, you can find the similar sentences using following code.  </p>

<pre><code>import gensim  

model = gensim.models.Doc2Vec.load('saved_doc2vec_model')  

new_sentence = ""I opened a new mailbox"".split("" "")  
model.docvecs.most_similar(positive=[model.infer_vector(new_sentence)],topn=5)
</code></pre>

<p>Results:</p>

<pre><code>[('TRAIN_29670', 0.6352514028549194),
 ('TRAIN_678', 0.6344441771507263),
 ('TRAIN_12792', 0.6202734708786011),
 ('TRAIN_12062', 0.6163255572319031),
 ('TRAIN_9710', 0.6056315898895264)]
</code></pre>

<p>The above results are list of tuples for <code>(label,cosine_similarity_score)</code>. You can map outputs to sentences by doing <code>train[29670]</code>.  </p>

<p>Please note that the above approach will only give good results if your doc2vec model contains embeddings for words found in the new sentence. If you try to get similarity for some gibberish sentence like <code>sdsf sdf f sdf sdfsdffg</code>, it will give you few results, but those might not be the actual similar sentences as your trained model may haven't seen these gibberish words while training the model. So try to train your model on as many sentences as possible to incorporate as many words for better results. </p>

<p><strong>Word2vec Solution</strong><br>
If you are using word2vec, you need to calculate the average vector for all words in every sentence and use cosine similarity between vectors.  </p>

<pre><code>def avg_sentence_vector(words, model, num_features, index2word_set):
    #function to average all words vectors in a given paragraph
    featureVec = np.zeros((num_features,), dtype=""float32"")
    nwords = 0

    for word in words:
        if word in index2word_set:
            nwords = nwords+1
            featureVec = np.add(featureVec, model[word])

    if nwords&gt;0:
        featureVec = np.divide(featureVec, nwords)
    return featureVec
</code></pre>

<p>Calculate Similarity </p>

<pre><code>from sklearn.metrics.pairwise import cosine_similarity

#get average vector for sentence 1
sentence_1 = ""this is sentence number one""
sentence_1_avg_vector = avg_sentence_vector(sentence_1.split(), model=word2vec_model, num_features=100)

#get average vector for sentence 2
sentence_2 = ""this is sentence number two""
sentence_2_avg_vector = avg_sentence_vector(sentence_2.split(), model=word2vec_model, num_features=100)

sen1_sen2_similarity =  cosine_similarity(sentence_1_avg_vector,sentence_2_avg_vector)
</code></pre>
","<python><nlp><scikit-learn><similarity><text>","38","38","38","0","39934","666"
"38441","{
  ""id"": 38441,
  ""title"": ""StandardScaler before and after splitting data""
}","<p>In the interest of preventing information about the distribution of the test set <a href=""https://machinelearningmastery.com/data-leakage-machine-learning/"" rel=""noreferrer"">leaking</a> into your model, you should go for option #2 and fit the scaler on your training data only, then standardise both training and test sets with that scaler. By fitting the scaler on the full dataset prior to splitting (option #1), information about the test set is used to transform the training set, which in turn is passed downstream. </p>

<p>As an example, knowing the distribution of the whole dataset might influence how you detect and process outliers, as well as how you parameterise your model. Although the data itself is not exposed, information about the distribution of the data is. As a result, your test set performance is not a true estimate of performance on unseen data. Some further discussion you might find useful is on <a href=""https://stats.stackexchange.com/questions/202287/why-standardization-of-the-testing-set-has-to-be-performed-with-the-mean-and-sd"">Cross Validated</a>.</p>
","<machine-learning><scikit-learn><preprocessing>","38","38","38","0","32697","1448"
"6549","{
  ""id"": 6549,
  ""title"": ""Open source Anomaly Detection in Python""
}","<p>Anomaly Detection or Event Detection can be done in different ways:</p>

<h2>Basic Way</h2>

<p>Derivative! If the deviation of your signal from its past &amp; future is high you most probably have an event. This can be extracted by finding large zero crossings in derivative of the signal.</p>

<h2>Statistical Way</h2>

<p>Mean of anything is its usual, basic behavior. if something deviates from mean it means that it's an event. Please note that mean in time-series is not that trivial and is not a constant but changing according to changes in time-series so you need to see the <em>""moving average""</em> instead of average. It looks like this:</p>

<p><a href=""https://i.stack.imgur.com/GCaNT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/GCaNT.png"" alt=""Events are peaks larger than 1 standard deviation from moving average""></a></p>

<p>The Moving Average code can be found <a href=""https://stackoverflow.com/questions/11352047/finding-moving-average-from-data-points-in-python"">here</a>. In signal processing terminology you are applying a <em>""Low-Pass""</em> filter by applying the moving average.</p>

<p>You can follow the code bellow:</p>

<pre><code>MOV = movingaverage(TimeSEries,5).tolist()
STD = np.std(MOV)
events= []
ind = []
for ii in range(len(TimeSEries)):
    if TimeSEries[ii] &gt; MOV[ii]+STD:
        events.append(TimeSEries[ii])
</code></pre>

<h2>Probabilistic Way</h2>

<p>They are more sophisticated specially for people new to Machine Learning. Kalman Filter is a great idea to <a href=""http://www.hamilton.ie/florian/infocom2008.pdf"" rel=""noreferrer"">find the anomalies</a>. Simpler probabilistic approaches using <em>""Maximum-Likelihood Estimation""</em> also work well but my suggestion is to stay with moving average idea. It works in practice very well.</p>

<p>I hope I could help :)
Good Luck!</p>
","<machine-learning><python><data-mining><anomaly-detection><library>","37","37","37","0","8878","5806"
"14194","{
  ""id"": 14194,
  ""title"": ""What is the difference between model hyperparameters and model parameters?""
}","<p>Hyperparameters and parameters are often used interchangeably but there is a difference between them. You call something a 'hyperparameter' if it cannot be learned within the estimator directly. However, 'parameters' is  more general term. When you say 'passing the parameters to the model', it generally means a combination of hyperparameters along with some other parameters that are not directly related to your estimator but are required for your model.</p>

<p>For example, suppose your are building an SVM classifier in sklearn:</p>

<pre><code>from sklearn import svm
X = [[0, 0], [1, 1]]
y = [0, 1]
clf = svm.SVC(C =0.01, kernel ='rbf', random_state=33)
clf.fit(X, y) 
</code></pre>

<p>In the above code an instance of SVM is your estimator for your model for which the hyperparameters, in this case, are <code>C</code> and <code>kernel</code>. But your model has another parameter which is not a hyperparameter and that is <code>random_state</code>.</p>
","<machine-learning><parameter><hyperparameter><language-model>","37","37","37","0","15412","2851"
"10052","{
  ""id"": 10052,
  ""title"": ""What is the best Keras model for multi-class classification?""
}","<p>Your choices of <code>activation='softmax'</code> in the last layer and compile choice of <code>loss='categorical_crossentropy'</code> are good for a model to predict multiple mutually-exclusive classes.</p>

<p>Regarding more general choices, there is rarely a ""right"" way to construct the architecture. Instead that should be something you test with different meta-params (such as layer sizes, number of layers, amount of drop-out), and should be results-driven (including any limits you might have on resource use for training time/memory use etc). </p>

<p>Use a cross-validation set to help choose a suitable architecture. Once done, to get a more accurate measure of your model's general performance, you should use a separate test set. Data held out from your training set separate to the CV set should be used for this. A reasonable split might be 60/20/20 train/cv/test, depending on how much data you have, and how much you need to report an accurate final figure.</p>

<p>For Question #2, you can either just have two outputs with a softmax final similar to now, or you can have <em>final</em> layer with one output, <code>activation='sigmoid'</code> and <code>loss='binary_crossentropy'</code>.</p>

<p>Purely from a gut feel from what might work with this data, I would suggest trying with <code>'tanh'</code> or <code>'sigmoid'</code> activations in the hidden layer, instead of <code>'relu'</code>, and I would also suggest increasing the number of hidden neurons (e.g. 100) and reducing the amount of dropout (e.g. 0.2). Caveat: Gut feeling on neural network architecture is not scientific. Try it, and test it.</p>
","<python><neural-network><classification><clustering><keras>","37","37","37","0","836","25808"
"156","{
  ""id"": 156,
  ""title"": ""Publicly Available Datasets""
}","<p><a href=""https://www.freebase.com"" rel=""nofollow noreferrer"">Freebase</a> is a free community driven database that spans many interesting topics and contains about 2,5 billion facts in machine readable format. It is also have good API to perform data queries.</p>
<p><a href=""http://www.datapure.co/open-data-sets"" rel=""nofollow noreferrer"">Here</a> is another compiled list of open data sets</p>
","<open-source><dataset>","37","37","37","0","118","634"
"9820","{
  ""id"": 9820,
  ""title"": ""Is there any domain where Bayesian Networks outperform neural networks?""
}","<p>One of the areas where Bayesian approaches are often used, is where one needs interpretability of the prediction system. You don't want to give doctors a Neural net and say that it's 95% accurate. You rather want to explain the assumptions your method makes, as well as the decision process the method uses. </p>

<p>Similar area is when you have a strong prior domain knowledge and want to use it in the system. </p>
","<machine-learning><pgm>","37","37","37","0","15469","486"
"12765","{
  ""id"": 12765,
  ""title"": ""Should a model be re-trained if new observations are available?""
}","<p>When new observations are available, there are three ways to retrain your model:</p>

<ol>
<li><strong>Online:</strong> each time a new observation is available, you use this single data point to further train your model (e.g. load your current model and further train it by doing backpropagation with that single observation). With this method, your model learns in a sequential manner and sort of adapts locally to your data in that it will be more influenced by the recent observations than by older observations. This might be useful in situations where your model needs to dynamically adapt to new patterns in data. It is also useful when you are dealing with extremely large data sets for which training on all of it at once is impossible.</li>
<li><strong>Offline:</strong> you add the new observations to your already existing data set and entirely retrain your model on this new, bigger data set. This generally leads to a better global approximation of the target function and is very popular if you have a fixed data set, or if you don't have new observations to often. However it is unpractical for large data sets.</li>
<li><strong>Batch/mini batch:</strong> this is sort of a middle ground approach. With batch, you wait until you have a batch of $n$ new observations and then train your already existing model on this whole batch. It is not offline as you are not adding this batch to your preexisting data set and then retraining your model on it and it is not online as your are training your model on $n$ observations at once and not just a single one. So it's a bit of both :) <strong>Mini batch</strong> is exactly the same except that the batch size is smaller so it tends towards online learning. Actually online learning is just batch with batch size 1 and offline is batch with batch size the size of the whole data set. </li>
</ol>

<p>Most models today will use <strong>batch/mini batch</strong> and the choice for the size of the batch depends on your application and model. Choosing the right size batch is equivalent to choosing the right frequency with which to re-train your model. If your new observation have a low variance with your existing data, I'd suggest larger batches (256-512 maybe) and if on the contrary new observations tend to vary greatly with your existing data, use small batches (8-256). At the end of the day, batch size is kind of like another hyper-parameter which you need to tune and which is specific to your data</p>
","<machine-learning><predictive-modeling><optimization><training>","37","37","37","0","21426","471"
"37024","{
  ""id"": 37024,
  ""title"": ""Why does adding a dropout layer improve deep/machine learning performance, given that dropout suppresses some neurons from the model?""
}","<p>The function of dropout is to increase the robustness of the model and also to remove any simple dependencies between the neurons.</p>
<p>Neurons are only removed for a single pass forward and backward through the network - meaning their weights are synthetically set to zero for that pass, and so their errors are as well, meaning that the weights are not updated.
Dropout also works as a form of <strong>regularisation</strong>, as it is penalising the model for its complexity, somewhat.</p>
<p>I would recommend having a read of <a href=""http://neuralnetworksanddeeplearning.com/chap3.html"" rel=""nofollow noreferrer"">the Dropout section in Michael Nielsen's Deep Learning book</a> (freely available), which gives nice intuition and also has very helpful interactive diagrams/explanations. He explains that:</p>
<blockquote>
<p>Dropout is a radically different technique for regularization. Unlike L1 and L2 regularization, dropout doesn't rely on modifying the cost function. Instead, in dropout we modify the network itself.</p>
</blockquote>
<p>Here is a <a href=""https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5"" rel=""nofollow noreferrer"">nice summary article</a>. From that article:</p>
<blockquote>
<p>Some Observations:</p>
</blockquote>
<blockquote>
<ul>
<li>Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.</li>
</ul>
</blockquote>
<ul>
<li>Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less.</li>
<li>With H hidden units, each of which can be dropped, we have
2^H possible models. In testing phase, the entire network is considered and each activation is reduced by a factor p.</li>
</ul>
<h3>Example</h3>
<p>Imagine I ask you to make me a cup of tea - you might always use your right hand to pour the water, your left eye to measure the level of water and then your right hand again to stir the tea with a spoon. This would mean your left hand and right eye serve little purpose. Using dropout would e.g. tie your right hand behind your back - forcing you to use your left hand. Now after making me 20 cups of tea, with either one eye or one hand taken out of action, you are better trained at using everything available. Maybe you will later be forced to make tea in a tiny kitchen, where it is only possible to use the kettle with your left arm... and after using dropout, you have experience doing that! You have become more robust to unseen data.</p>
","<machine-learning><deep-learning><keras><regularization><dropout>","36","36","36","0","45264","13123"
"732","{
  ""id"": 732,
  ""title"": ""How to fight underfitting in a deep neural net""
}","<p>The problem with deep networks is that they have lots of hyperparameters to tune and very small solution space. Thus, finding good ones is more like an art rather than engineering task. I would start with working example from tutorial and play around with its parameters to see how results change - this gives a good intuition (though not formal explanation) about dependencies between parameters and results (both - final and intermediate). </p>

<p>Also I found following papers very useful: </p>

<ul>
<li><a href=""http://yosinski.com/media/papers/Yosinski2012VisuallyDebuggingRestrictedBoltzmannMachine.pdf"">Visually Debugging Restricted Boltzmann Machine Training
with a 3D Example</a></li>
<li><a href=""https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf"">A Practical Guide to Training Restricted Boltzmann
Machines</a></li>
</ul>

<p>They both describe RBMs, but contain some insights on deep networks in general. For example, one of key points is that networks need to be debugged layer-wise - if previous layer doesn't provide good representation of features, further layers have almost no chance to fix it. </p>
","<neural-network><deep-learning>","36","36","36","0","1279","2771"
"48814","{
  ""id"": 48814,
  ""title"": ""How to feed LSTM with different input array sizes?""
}","<p>The easiest way is to use <em>Padding and Masking</em>.</p>

<p>There are three general ways to handle variable-length sequences:</p>

<ol>
<li>Padding and masking (which can be used for (3)),</li>
<li>Batch size = 1, and</li>
<li>Batch size > 1, with equi-length samples in each batch.</li>
</ol>

<p><strong>Padding and masking</strong></p>

<p>In this approach, we pad the shorter sequences with a special value to be masked (skipped) later. For example, suppose each timestamp has dimension 2, and <code>-10</code> is the special value, then</p>

<pre><code>X = [

  [[1,    1.1],
   [0.9, 0.95]],  # sequence 1 (2 timestamps)

  [[2,    2.2],
   [1.9, 1.95],
   [1.8, 1.85]],  # sequence 2 (3 timestamps)

]
</code></pre>

<p>will be converted to</p>

<pre><code>X2 = [

  [[1,    1.1],
   [0.9, 0.95],
   [-10, -10]], # padded sequence 1 (3 timestamps)

  [[2,    2.2],
   [1.9, 1.95],
   [1.8, 1.85]], # sequence 2 (3 timestamps)
]
</code></pre>

<p>This way, all sequences would have the same length. Then, we use a <code>Masking</code> layer that skips those special timestamps like they don't exist. A complete example is given at the end.</p>

<p>For cases (2) and (3) you need to set the <code>seq_len</code> of LSTM to <code>None</code>, e.g.</p>

<pre><code>model.add(LSTM(units, input_shape=(None, dimension)))
</code></pre>

<p>this way LSTM accepts batches with different lengths; although samples inside each batch must be the same length. Then, you need to feed a <a href=""https://keras.io/models/model/"" rel=""noreferrer"">custom batch generator</a> to <code>model.fit_generator</code> (instead of <code>model.fit</code>). </p>

<p>I have provided a complete example for simple case (2) (batch size = 1) at the end. Based on this example and the link, you should be able to build a generator for case (3) (batch size > 1). Specifically, we either (a) return <code>batch_size</code> sequences with the same length, or (b) select sequences with almost the same length, and pad the shorter ones the same as case (1), and use a <code>Masking</code> layer before LSTM layer to ignore the padded timestamps, e.g.</p>

<pre><code>model.add(Masking(mask_value=special_value, input_shape=(None, dimension)))
model.add(LSTM(lstm_units))
</code></pre>

<p>where first dimension of <code>input_shape</code> in <code>Masking</code> is again <code>None</code> to allow batches with different lengths.</p>

<p>Here is the code for cases (1) and (2):</p>

<pre><code>from keras import Sequential
from keras.utils import Sequence
from keras.layers import LSTM, Dense, Masking
import numpy as np


class MyBatchGenerator(Sequence):
    'Generates data for Keras'
    def __init__(self, X, y, batch_size=1, shuffle=True):
        'Initialization'
        self.X = X
        self.y = y
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.y)/self.batch_size))

    def __getitem__(self, index):
        return self.__data_generation(index)

    def on_epoch_end(self):
        'Shuffles indexes after each epoch'
        self.indexes = np.arange(len(self.y))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, index):
        Xb = np.empty((self.batch_size, *X[index].shape))
        yb = np.empty((self.batch_size, *y[index].shape))
        # naively use the same sample over and over again
        for s in range(0, self.batch_size):
            Xb[s] = X[index]
            yb[s] = y[index]
        return Xb, yb


# Parameters
N = 1000
halfN = int(N/2)
dimension = 2
lstm_units = 3

# Data
np.random.seed(123)  # to generate the same numbers
# create sequence lengths between 1 to 10
seq_lens = np.random.randint(1, 10, halfN)
X_zero = np.array([np.random.normal(0, 1, size=(seq_len, dimension)) for seq_len in seq_lens])
y_zero = np.zeros((halfN, 1))
X_one = np.array([np.random.normal(1, 1, size=(seq_len, dimension)) for seq_len in seq_lens])
y_one = np.ones((halfN, 1))
p = np.random.permutation(N)  # to shuffle zero and one classes
X = np.concatenate((X_zero, X_one))[p]
y = np.concatenate((y_zero, y_one))[p]

# Batch = 1
model = Sequential()
model.add(LSTM(lstm_units, input_shape=(None, dimension)))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
print(model.summary())
model.fit_generator(MyBatchGenerator(X, y, batch_size=1), epochs=2)

# Padding and Masking
special_value = -10.0
max_seq_len = max(seq_lens)
Xpad = np.full((N, max_seq_len, dimension), fill_value=special_value)
for s, x in enumerate(X):
    seq_len = x.shape[0]
    Xpad[s, 0:seq_len, :] = x
model2 = Sequential()
model2.add(Masking(mask_value=special_value, input_shape=(max_seq_len, dimension)))
model2.add(LSTM(lstm_units))
model2.add(Dense(1, activation='sigmoid'))
model2.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
print(model2.summary())
model2.fit(Xpad, y, epochs=50, batch_size=32)
</code></pre>

<p><strong>Extra notes</strong></p>

<ol>
<li>Note that if we pad without masking, padded value will be regarded as actual value, thus, it becomes noise in data. For example, a padded temperature sequence <code>[20, 21, 22, -10, -10]</code> will be the same as a sensor report with two noisy (wrong) measurements at the end. Model may learn to ignore this noise completely or at least partially, but it is reasonable to clean the data first, i.e. use a mask.</li>
</ol>
","<keras><lstm>","36","36","36","0","67328","7999"
"26119","{
  ""id"": 26119,
  ""title"": ""Merging two different models in Keras""
}","<p>I figured out the answer to my question and here is the code that builds on the above answer.</p>

<pre><code>from keras.layers import Input, Dense
from keras.models import Model
from keras.utils import plot_model

A1 = Input(shape=(30,),name='A1')
A2 = Dense(8, activation='relu',name='A2')(A1)
A3 = Dense(30, activation='relu',name='A3')(A2)

B2 = Dense(40, activation='relu',name='B2')(A2)
B3 = Dense(30, activation='relu',name='B3')(B2)

merged = Model(inputs=[A1],outputs=[A3,B3])
plot_model(merged,to_file='demo.png',show_shapes=True)
</code></pre>

<p>and here is the output structure that I wanted:</p>

<p><a href=""https://i.stack.imgur.com/2xIdb.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2xIdb.png"" alt=""enter image description here""></a></p>
","<machine-learning><python><deep-learning><keras><tensorflow>","36","36","36","0","43921","913"
"64261","{
  ""id"": 64261,
  ""title"": ""Pearson vs Spearman vs Kendall""
}","<p><strong>Correlation</strong> is a bivariate analysis that measures the strength of association between two variables and the direction of the relationship. In terms of the strength of the relationship, the value of the correlation coefficient varies between +1 and -1. A value of ± 1 indicates a perfect degree of association between the two variables. As the correlation coefficient value goes towards 0, the relationship between the two variables will be weaker. The direction of the relationship is indicated by the sign of the coefficient; a + sign indicates a positive relationship and a – sign indicates a negative relationship.</p>
<p><em>Pearson's correlation coefficient</em> and the others are the non-parametric method, <em>Spearman's rank correlation coefficient</em> and <em>Kendall's tau coefficient</em>.</p>
<p><strong>Pearson's Correlation Coefficient</strong></p>
<p><span class=""math-container"">$$
r = \frac{\sum(X - \overline{X})(Y - \overline{Y})}
{\sqrt{\sum(X-\overline{X})^{2}\cdot\sum(Y-\overline{Y})^{2}}}\\
~ \\
\begin{align}
    Where, ~ \overline{X} &amp;= mean ~ of ~ X~variable\\
    \overline{Y} &amp;= mean ~ of ~ Y ~ variable\\
\end{align}
$$</span></p>
<p><strong>Assumptions:</strong></p>
<ul>
<li><p>Each observation should have a pair of values.</p>
</li>
<li><p>Each variable should be continuous.</p>
</li>
<li><p>It should be the absence of outliers.</p>
</li>
<li><p>It assumes linearity and homoscedasticity.</p>
</li>
</ul>
<p><strong>Spearman's Rank Correlation Coefficient</strong></p>
<p><span class=""math-container"">$$\rho = \frac{\sum_{i=1}^{n}(R(x_i) - \overline{R(x)})(R(y_i) - \overline{R(y)})}
{\sqrt{\sum_{i=1}^{n}(R(x_i) - \overline{R(x)})^{2}\cdot\sum_{i=1}^{n}(R(y_i)-\overline{R(y)})^{2}}}
= 1 - \frac{6\sum_{i=1}^{n}(R(x_i) - R(y_i))^{2}}{n(n^{2} - 1)}\\
~ \\
\begin{align}
    Where, ~ R(x_i) &amp;= rank ~ of ~ x_i\\
    R(y_i) &amp;= rank ~ of ~ y_i\\
    \overline{R(x)} &amp;=mean ~ rank ~ of ~ x\\
    \overline{R(y)} &amp;=mean ~ rank ~ of ~ y\\
    n &amp;= number ~ of ~ pairs
\end{align}
$$</span></p>
<p><strong>Assumptions:</strong></p>
<ul>
<li><p>Pairs of observations are independent.</p>
</li>
<li><p>Two variables should be measured on an ordinal, interval or ratio scale.</p>
</li>
<li><p>It assumes that there is a monotonic relationship between the two variables.</p>
</li>
</ul>
<p><strong>Kendall's Tau Coefficient</strong></p>
<p><span class=""math-container"">$$
\tau = \frac{n_c - n_d}{n_c + n_d} = \frac{n_c - n_d}{n(n-1)/2}\\ 
~ \\
\begin{align}
    Where, ~ n_c &amp;= number ~ of ~ concordant ~ pairs\\
    n_d &amp;= number ~ of ~ discordant ~ pairs\\
    n &amp;= number ~ of ~ pairs
\end{align}
$$</span></p>
<p><strong>Assumptions:</strong></p>
<ul>
<li>It's the same as assumptions of <em>Spearman's rank correlation coefficient</em></li>
</ul>
<h1>Comparison of Each Correlation Coefficients</h1>
<p><strong>Pearson correlation vs Spearman and Kendall correlation</strong></p>
<ul>
<li><p>Non-parametric correlations are less powerful because they use less information in their calculations. In the case of <em>Pearson's correlation</em> uses information about the mean and deviation from the mean, while non-parametric correlations use only the ordinal information and scores of pairs.</p>
</li>
<li><p>In the case of non-parametric correlation, it's possible that the X and Y values can be continuous or ordinal, and approximate normal distributions for X and Y are not required. But in the case of <em>Pearson's correlation</em>, it assumes the distributions of X and Y should be normal distribution and also be continuous.</p>
</li>
<li><p>Correlation coefficients only measure linear (<em>Pearson</em>) or monotonic (<em>Spearman</em> and <em>Kendall</em>) relationships.</p>
</li>
</ul>
<p><strong>Spearman correlation vs Kendall correlation</strong></p>
<ul>
<li><p>In the normal case, <em>Kendall correlation</em> is more robust and efficient than <em>Spearman correlation</em>. It means that <em>Kendall correlation</em> is preferred when there are small samples or some outliers.</p>
</li>
<li><p><em>Kendall correlation</em> has a O(n^2) computation complexity comparing with O(n logn) of <em>Spearman correlation</em>, where n is the sample size.</p>
</li>
<li><p><em>Spearman’s rho</em> usually is larger than <em>Kendall’s tau</em>.</p>
</li>
<li><p>The interpretation of <em>Kendall’s tau</em> in terms of the probabilities of observing the agreeable (concordant) and non-agreeable (discordant) pairs is very direct.</p>
</li>
</ul>
<p><a href=""https://www.kaggle.com/kiyoung1027/pearson-vs-spearman-vs-kendall#679923"" rel=""nofollow noreferrer"">Example Python Implementation</a></p>
","<correlation><pearsons-correlation-coefficient><spearmans-rank-correlation><kendalls-tau-coefficient>","36","36","36","0","83275","1597"
"16001","{
  ""id"": 16001,
  ""title"": ""Micro Average vs Macro average Performance in a Multiclass classification setting""
}","<p>This is the <a href=""http://rushdishams.blogspot.in/2011/08/micro-and-macro-average-of-precision.html"" rel=""nofollow noreferrer"">Original Post</a>.</p>
<hr />
<p>In Micro-average method, you sum up the individual true positives, false positives, and false negatives of the system for different sets and the apply them to get the statistics.</p>
<p>Tricky, but I found this very interesting. There are two methods by which you can get such average statistic of information retrieval and classification.</p>
<h2>1. Micro-average Method</h2>
<p>In Micro-average method, you sum up the individual true positives, false positives, and false negatives of the system for different sets and the apply them to get the statistics. For example, for a set of data, the system's</p>
<pre><code>True positive (TP1)  = 12
False positive (FP1) = 9
False negative (FN1) = 3
</code></pre>
<p>Then precision (P1) and recall (R1) will be <span class=""math-container"">$57.14 \%=\frac {TP1}{TP1+FP1}$</span> and <span class=""math-container"">$80\%=\frac {TP1}{TP1+FN1}$</span></p>
<p>and for a different set of data, the system's</p>
<pre><code>True positive (TP2)  = 50
False positive (FP2) = 23
False negative (FN2) = 9
</code></pre>
<p>Then precision (P2) and recall (R2) will be 68.49 and 84.75</p>
<p>Now, the average precision and recall of the system using the Micro-average method is</p>
<p><span class=""math-container"">$\text{Micro-average of precision} = \frac{TP1+TP2}{TP1+TP2+FP1+FP2} = \frac{12+50}{12+50+9+23} = 65.96$</span></p>
<p><span class=""math-container"">$\text{Micro-average of recall} = \frac{TP1+TP2}{TP1+TP2+FN1+FN2} = \frac{12+50}{12+50+3+9} = 83.78$</span></p>
<p>The Micro-average F-Score will be simply the harmonic mean of these two figures.</p>
<h2>2. Macro-average Method</h2>
<p>The method is straight forward. Just take the average of the precision and recall of the system on different sets. For example, the macro-average precision and recall of the system for the given example is</p>
<p><span class=""math-container"">$\text{Macro-average precision} = \frac{P1+P2}{2} = \frac{57.14+68.49}{2} = 62.82$</span>
<span class=""math-container"">$\text{Macro-average recall} = \frac{R1+R2}{2} = \frac{80+84.75}{2} = 82.25$</span></p>
<p>The Macro-average F-Score will be simply the harmonic mean of these two figures.</p>
<p>Suitability
Macro-average method can be used when you want to know how the system performs overall across the sets of data. You should not come up with any specific decision with this average.</p>
<p>On the other hand, micro-average can be a useful measure when your dataset varies in size.</p>
","<multiclass-classification><evaluation>","35","41","38","3","27426","705"
"214","{
  ""id"": 214,
  ""title"": ""Publicly Available Datasets""
}","<p>The following links are available</p>

<ul>
<li><p><a href=""https://www.opensciencedatacloud.org/publicdata/"" rel=""noreferrer"">Public Data Sets</a></p></li>
<li><p><a href=""http://www.google.com/publicdata/directory"" rel=""noreferrer"">Google Public Data Sets</a></p></li>
<li><p><a href=""https://aws.amazon.com/publicdatasets/"" rel=""noreferrer"">Amazon Web Services</a></p></li>
<li><p><a href=""http://www.inside-r.org/howto/finding-data-internet"" rel=""noreferrer"">Finding Data on the Internet</a></p></li>
</ul>
","<open-source><dataset>","35","35","35","0","295","401"
"56677","{
  ""id"": 56677,
  ""title"": ""Can machine learning learn a function like finding maximum from a list?""
}","<p><em>Maybe</em>, but note that this is one of those cases where <em>machine learning is not the answer</em>. There is a tendency to try and shoehorn machine learning into cases where really, bog standard rules-based solutions are faster, simpler and just generally the right choice :P</p>

<blockquote>
  <p>Just because you can, doesn't mean you should</p>
</blockquote>

<p><strong>Edit</strong>: I originally wrote this as ""Yes, but note that..."" but then started to doubt myself, having never seen it done. I tried it out this afternoon and it's certainly doable:</p>

<pre><code>import numpy as np
from keras.models import Model
from keras.layers import Input, Dense, Dropout
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from keras.callbacks import EarlyStopping

# Create an input array of 50,000 samples of 20 random numbers each
x = np.random.randint(0, 100, size=(50000, 20))

# And a one-hot encoded target denoting the index of the maximum of the inputs
y = to_categorical(np.argmax(x, axis=1), num_classes=20)

# Split into training and testing datasets
x_train, x_test, y_train, y_test = train_test_split(x, y)

# Build a network, probaly needlessly complicated since it needs a lot of dropout to
# perform even reasonably well.

i = Input(shape=(20, ))
a = Dense(1024, activation='relu')(i)
b = Dense(512, activation='relu')(a)
ba = Dropout(0.3)(b)
c = Dense(256, activation='relu')(ba)
d = Dense(128, activation='relu')(c)
o = Dense(20, activation='softmax')(d)

model = Model(inputs=i, outputs=o)

es = EarlyStopping(monitor='val_loss', patience=3)

model.compile(optimizer='adam', loss='categorical_crossentropy')

model.fit(x_train, y_train, epochs=15, batch_size=8, validation_data=[x_test, y_test], callbacks=[es])

print(np.where(np.argmax(model.predict(x_test), axis=1) == np.argmax(y_test, axis=1), 1, 0).mean())
</code></pre>

<p>Output is 0.74576, so it's correctly finding the max 74.5% of the time. I have no doubt that that could be improved, but as I say this is not a usecase I would recommend for ML.</p>

<p><strong>EDIT 2</strong>: Actually I re-ran this this morning using sklearn's RandomForestClassifier and it performed significantly better:</p>

<pre><code># instantiation of the arrays is identical

rfc = RandomForestClassifier(n_estimators=1000, verbose=1)
rfc.fit(x_train, y_train)

yhat_proba = rfc.predict_proba(x_test)


# We have some annoying transformations to do because this .predict_proba() call returns the data in a weird format of shape (20, 12500, 2).

for i in range(len(yhat_proba)):
    yhat_proba[i] = yhat_proba[i][:, 1]

pyhat = np.reshape(np.ravel(yhat_proba), (12500,20), order='F')

print(np.where(np.argmax(pyhat, axis=1) == np.argmax(y_test, axis=1), 1, 0).mean())
</code></pre>

<p>And the score here is 94.4% of samples with the max correctly identified, which is pretty good indeed.</p>
","<machine-learning><deep-learning>","35","37","36","1","65767","1604"
"16467","{
  ""id"": 16467,
  ""title"": ""How to set class weights for imbalanced classes in Keras?""
}","<p>I use this kind of rule for <code>class_weight</code> :</p>
<pre><code>import numpy as np
import math

# labels_dict : {ind_label: count_label}
# mu : parameter to tune 

def create_class_weight(labels_dict,mu=0.15):
    total = np.sum(list(labels_dict.values()))
    keys = labels_dict.keys()
    class_weight = dict()
    
    for key in keys:
        score = math.log(mu*total/float(labels_dict[key]))
        class_weight[key] = score if score &gt; 1.0 else 1.0
    
    return class_weight

# random labels_dict
labels_dict = {0: 2813, 1: 78, 2: 2814, 3: 78, 4: 7914, 5: 248, 6: 7914, 7: 248}

create_class_weight(labels_dict)
</code></pre>
<p><code>math.log</code> smooths the weights for very imbalanced classes !
This returns :</p>
<pre><code>{0: 1.0,
 1: 3.749820767859636,
 2: 1.0,
 3: 3.749820767859636,
 4: 1.0,
 5: 2.5931008483842453,
 6: 1.0,
 7: 2.5931008483842453}
</code></pre>
","<deep-learning><classification><keras><weighted-data>","35","35","35","0","28222","451"
"9227","{
  ""id"": 9227,
  ""title"": ""How to prepare/augment images for neural network?""
}","<p>The idea with Neural Networks is that they need little pre-processing since the heavy lifting is done by the algorithm which is the one in charge of learning the features.</p>

<p>The winners of the Data Science Bowl 2015 have a great write-up regarding their approach, so most of this answer's content was taken from:
<a href=""https://benanne.github.io/2015/03/17/plankton.html"">Classifying plankton with deep neural networks</a>. I suggest you read it, specially the part about <strong>Pre-processing and data augmentation</strong>.</p>

<p><strong>- Resize Images</strong></p>

<p>As for different sizes, resolutions or distances you can do the following. You can simply rescale the largest side of each image to a fixed length.</p>

<p>Another option is to use openCV or scipy.
and this will resize the image to have 100 cols (width) and 50 rows (height):</p>

<pre><code>resized_image = cv2.resize(image, (100, 50)) 
</code></pre>

<p>Yet another option is to use scipy module, by using:</p>

<pre><code>small = scipy.misc.imresize(image, 0.5)
</code></pre>

<p><strong>- Data Augmentation</strong></p>

<p>Data Augmentation always improves performance though the amount depends on the dataset. If you want to augmented the data to artificially increase the size of the dataset you can do the following if the case applies (it wouldn't apply if for example were images of houses or people where if you rotate them 180degrees they would lose all information but not if you flip them like a mirror does):</p>

<ul>
<li>rotation: random with angle between 0° and 360° (uniform)</li>
<li>translation: random with shift between -10 and 10 pixels (uniform)</li>
<li>rescaling: random with scale factor between 1/1.6 and 1.6
(log-uniform)</li>
<li>flipping: yes or no (bernoulli)</li>
<li>shearing: random with angle between -20° and 20° (uniform)</li>
<li>stretching: random with stretch factor between 1/1.3 and 1.3
(log-uniform)</li>
</ul>

<p>You can see the results on the Data Science bowl images.</p>

<p><strong>Pre-processed images</strong></p>

<p><a href=""https://i.stack.imgur.com/0S0Y0.png""><img src=""https://i.stack.imgur.com/0S0Y0.png"" alt=""Pre-processed images""></a></p>

<p><strong>augmented versions of the same images</strong></p>

<p><a href=""https://i.stack.imgur.com/KJXZK.png""><img src=""https://i.stack.imgur.com/KJXZK.png"" alt=""enter image description here""></a></p>

<p><strong>-Other techniques</strong></p>

<p>These will deal with other image properties like lighting and are already related to the main algorithm more like a simple pre-processing step. Check the full list on: <a href=""http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/"">UFLDL Tutorial</a></p>
","<neural-network><image-classification><preprocessing><convnet>","35","35","35","0","13023","3130"
"14742","{
  ""id"": 14742,
  ""title"": ""How does Keras calculate accuracy?""
}","<p>For binary classification, the code for accuracy metric is:</p>
<pre><code>K.mean(K.equal(y_true, K.round(y_pred)))
</code></pre>
<p>which suggests that 0.5 is the threshold to distinguish between classes. <em>y_true</em> should of course be 1-hots in this case.</p>
<p>It's a bit different for categorical classification:</p>
<pre><code>K.mean(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)))
</code></pre>
<p>which means &quot;how often predictions have maximum in the same spot as true values&quot;</p>
<p>There is also an option for top-k categorical accuracy, which is similar to one above, but calculates how often target class is within the top-k predictions.</p>
","<neural-network><deep-learning><keras>","34","34","34","0","25504","716"
"33075","{
  ""id"": 33075,
  ""title"": ""How do I compare columns in different data frames?""
}","<p>If you want to check equals values on a certain column let's say Name you can merge both Dataframes to a new one: </p>

<pre><code>mergedStuff = pd.merge(df1, df2, on=['Name'], how='inner')
mergedStuff.head()
</code></pre>

<p>I think this is more efficient and faster then <code>where</code>if you have a big data set</p>
","<pandas><dataframe>","34","36","35","1","46527","644"
"49535","{
  ""id"": 49535,
  ""title"": ""What is GELU activation?""
}","<h3>GELU function</h3>

<p>We can expand the <a href=""https://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_function"" rel=""noreferrer"">cumulative distribution of <span class=""math-container"">$\mathcal{N}(0, 1)$</span></a>, i.e. <span class=""math-container"">$\Phi(x)$</span>, as follows:
<span class=""math-container"">$$\text{GELU}(x):=x{\Bbb P}(X \le x)=x\Phi(x)=0.5x\left(1+\text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)$$</span></p>

<p>Note that this is a <em>definition</em>, not an equation (or a relation). Authors have provided some justifications for this proposal, e.g. a stochastic <em>analogy</em>, however mathematically, this is just a definition.</p>

<p>Here is the plot of GELU:</p>

<p><img src=""https://i.stack.imgur.com/tJI3j.png"" width=""500"" /></p>

<h3>Tanh approximation</h3>

<p>For these type of numerical approximations, the key idea  is to find a similar function (primarily based on experience), parameterize it, and then fit it to a set of points from the original function. </p>

<p>Knowing that <span class=""math-container"">$\text{erf}(x)$</span> is very close to <span class=""math-container"">$\text{tanh}(x)$</span></p>

<p><img src=""https://i.stack.imgur.com/DuvOa.png"" width=""500"" /></p>

<p>and first derivative of <span class=""math-container"">$\text{erf}(\frac{x}{\sqrt{2}})$</span> coincides with that of <span class=""math-container"">$\text{tanh}(\sqrt{\frac{2}{\pi}}x)$</span> at <span class=""math-container"">$x=0$</span>, which is <span class=""math-container"">$\sqrt{\frac{2}{\pi}}$</span>, we proceed to fit
<span class=""math-container"">$$\text{tanh}\left(\sqrt{\frac{2}{\pi}}(x+ax^2+bx^3+cx^4+dx^5)\right)$$</span> (or with more terms) to  a set of points <span class=""math-container"">$\left(x_i, \text{erf}\left(\frac{x_i}{\sqrt{2}}\right)\right)$</span>. </p>

<p>I have fitted this function to 20 samples between <span class=""math-container"">$(-1.5, 1.5)$</span> (<a href=""https://mycurvefit.com/"" rel=""noreferrer"">using this site</a>), and here are the coefficients:</p>

<p><img src=""https://i.stack.imgur.com/7QK9s.png"" width=""300"" /></p>

<p>By setting <span class=""math-container"">$a=c=d=0$</span>, <span class=""math-container"">$b$</span> was estimated to be <span class=""math-container"">$0.04495641$</span>. With more samples from a wider range (that site only allowed 20), coefficient <span class=""math-container"">$b$</span> will be closer to paper's <span class=""math-container"">$0.044715$</span>. Finally we get</p>

<p><span class=""math-container"">$\text{GELU}(x)=x\Phi(x)=0.5x\left(1+\text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)\simeq 0.5x\left(1+\text{tanh}\left(\sqrt{\frac{2}{\pi}}(x+0.044715x^3)\right)\right)$</span></p>

<p>with mean squared error <span class=""math-container"">$\sim 10^{-8}$</span> for <span class=""math-container"">$x \in [-10, 10]$</span>.</p>

<p>Note that if we did not utilize the relationship between the first derivatives, term <span class=""math-container"">$\sqrt{\frac{2}{\pi}}$</span> would have been included in the parameters as follows
<span class=""math-container"">$$0.5x\left(1+\text{tanh}\left(0.797885x+0.035677x^3\right)\right)$$</span>
which is less beautiful (less analytical, more numerical)!</p>

<h3>Utilizing the parity</h3>

<p>As suggested by <a href=""https://datascience.stackexchange.com/questions/49522/what-is-gelu-activation/49535#comment56743_49535"">@BookYourLuck</a>, we can utilize the parity of functions to restrict the space of polynomials in which we search. That is, since <span class=""math-container"">$\text{erf}$</span> is an odd function, i.e. <span class=""math-container"">$f(-x)=-f(x)$</span>, and <span class=""math-container"">$\text{tanh}$</span> is also an odd function, polynomial function <span class=""math-container"">$\text{pol}(x)$</span> inside <span class=""math-container"">$\text{tanh}$</span> should also be odd (should only have odd powers of <span class=""math-container"">$x$</span>) to have
<span class=""math-container"">$$\text{erf}(-x)\simeq\text{tanh}(\text{pol}(-x))=\text{tanh}(-\text{pol}(x))=-\text{tanh}(\text{pol}(x))\simeq-\text{erf}(x)$$</span></p>

<p>Previously, we were fortunate to end up with (almost) zero coefficients for even powers <span class=""math-container"">$x^2$</span> and <span class=""math-container"">$x^4$</span>, however in general, this might lead to low quality approximations that, for example, have a term like <span class=""math-container"">$0.23x^2$</span> that is being cancelled out by extra terms (even or odd) instead of simply opting for <span class=""math-container"">$0x^2$</span>.</p>

<h3>Sigmoid approximation</h3>

<p>A similar relationship holds between <span class=""math-container"">$\text{erf}(x)$</span> and <span class=""math-container"">$2\left(\sigma(x)-\frac{1}{2}\right)$</span> (sigmoid), which is proposed in the paper as another approximation, with mean squared error <span class=""math-container"">$\sim 10^{-4}$</span> for <span class=""math-container"">$x \in [-10, 10]$</span>.</p>

<p><img src=""https://i.stack.imgur.com/mBNGL.png"" width=""600"" /></p>

<p>Here is a Python code for generating data points, fitting the functions, and calculating the mean squared errors:</p>

<pre><code>import math
import numpy as np
import scipy.optimize as optimize


def tahn(xs, a):
    return [math.tanh(math.sqrt(2 / math.pi) * (x + a * x**3)) for x in xs]


def sigmoid(xs, a):
    return [2 * (1 / (1 + math.exp(-a * x)) - 0.5) for x in xs]


print_points = 0
np.random.seed(123)
# xs = [-2, -1, -.9, -.7, 0.6, -.5, -.4, -.3, -0.2, -.1, 0,
#       .1, 0.2, .3, .4, .5, 0.6, .7, .9, 2]
# xs = np.concatenate((np.arange(-1, 1, 0.2), np.arange(-4, 4, 0.8)))
# xs = np.concatenate((np.arange(-2, 2, 0.5), np.arange(-8, 8, 1.6)))
xs = np.arange(-10, 10, 0.001)
erfs = np.array([math.erf(x/math.sqrt(2)) for x in xs])
ys = np.array([0.5 * x * (1 + math.erf(x/math.sqrt(2))) for x in xs])

# Fit tanh and sigmoid curves to erf points
tanh_popt, _ = optimize.curve_fit(tahn, xs, erfs)
print('Tanh fit: a=%5.5f' % tuple(tanh_popt))

sig_popt, _ = optimize.curve_fit(sigmoid, xs, erfs)
print('Sigmoid fit: a=%5.5f' % tuple(sig_popt))

# curves used in https://mycurvefit.com:
# 1. sinh(sqrt(2/3.141593)*(x+a*x^2+b*x^3+c*x^4+d*x^5))/cosh(sqrt(2/3.141593)*(x+a*x^2+b*x^3+c*x^4+d*x^5))
# 2. sinh(sqrt(2/3.141593)*(x+b*x^3))/cosh(sqrt(2/3.141593)*(x+b*x^3))
y_paper_tanh = np.array([0.5 * x * (1 + math.tanh(math.sqrt(2/math.pi)*(x + 0.044715 * x**3))) for x in xs])
tanh_error_paper = (np.square(ys - y_paper_tanh)).mean()
y_alt_tanh = np.array([0.5 * x * (1 + math.tanh(math.sqrt(2/math.pi)*(x + tanh_popt[0] * x**3))) for x in xs])
tanh_error_alt = (np.square(ys - y_alt_tanh)).mean()

# curve used in https://mycurvefit.com:
# 1. 2*(1/(1+2.718281828459^(-(a*x))) - 0.5)
y_paper_sigmoid = np.array([x * (1 / (1 + math.exp(-1.702 * x))) for x in xs])
sigmoid_error_paper = (np.square(ys - y_paper_sigmoid)).mean()
y_alt_sigmoid = np.array([x * (1 / (1 + math.exp(-sig_popt[0] * x))) for x in xs])
sigmoid_error_alt = (np.square(ys - y_alt_sigmoid)).mean()

print('Paper tanh error:', tanh_error_paper)
print('Alternative tanh error:', tanh_error_alt)
print('Paper sigmoid error:', sigmoid_error_paper)
print('Alternative sigmoid error:', sigmoid_error_alt)

if print_points == 1:
    print(len(xs))
    for x, erf in zip(xs, erfs):
        print(x, erf)
</code></pre>

<p>Output:</p>

<pre><code>Tanh fit: a=0.04485
Sigmoid fit: a=1.70099
Paper tanh error: 2.4329173471294176e-08
Alternative tanh error: 2.698034519269613e-08
Paper sigmoid error: 5.6479106346814546e-05
Alternative sigmoid error: 5.704246564663601e-05
</code></pre>
","<activation-function><bert><mathematics>","34","34","34","0","67328","7999"
"21","{
  ""id"": 21,
  ""title"": ""How big is big data?""
}","<p>As you rightly note, these days ""big data"" is something everyone wants to say they've got, which entails a certain looseness in how people define the term.  Generally, though, I'd say you're certainly dealing with big data if the scale is such that it's no longer feasible to manage with more traditional technologies such as RDBMS, at least without complementing them with big data technologies such as Hadoop.</p>

<p>How big your data has to actually be for that to be the case is debatable.  Here's a (somewhat provocative) <a href=""http://www.chrisstucchio.com/blog/2013/hadoop_hatred.html"">blog post</a> that claims that it's not really the case for less than 5 TB of data.  (To be clear, it doesn't claim ""Less than 5 TB isn't big data"", but just ""Less than 5 TB isn't big enough that you need Hadoop"".)</p>

<p>But even on smaller datasets, big data technologies like Hadoop can have other advantages, including being well suited to batch operations, playing well with unstructured data (as well as data whose structure isn't known in advance or could change), horizontal scalability (scaling by adding more nodes instead of beefing up your existing servers), and (as one of the commenters on the above-linked post notes) the ability to integrate your data processing with external data sets (think of a map-reduce where the mapper makes a call to another server).  Other technologies associated with big data, like NoSql databases, emphasize fast performance and consistent availability while dealing with large sets of data, as well also being able to handle semi-unstructured data and to scale horizontally.</p>

<p>Of course, traditional RDBMS have their own advantages including ACID guarantees (Atomicity, Consistency, Isolation, Durability) and better performance for certain operations, as well as being more standardized, more mature, and (for many users) more familiar.  So even for indisputably ""big"" data, it may make sense to load at least a portion of your data into a traditional SQL database and use that in conjunction with big data technologies.</p>

<p>So, a more generous definition would be that you have big data so long as it's big enough that big data technologies provide some added value for you.  But as you can see, that can depend not just on the size of your data but on how you want to work with it and what sort of requirements you have in terms of flexibility, consistency, and performance.  <em>How</em> you're using your data is more relevant to the question than what you're using it <em>for</em> (e.g. data mining).  That said, uses like data mining and machine learning are more likely to yield useful results if you have a big enough data set to work with.</p>
","<bigdata><scalability><efficiency><performance>","34","34","34","0","14","2942"
"19372","{
  ""id"": 19372,
  ""title"": ""Choosing between CPU and GPU for training a neural network""
}","<p>Unlike some of the other answers, I would highly advice against always training on GPUs without any second thought. This is driven by the usage of deep learning methods on images and texts, where the data is very rich (e.g. a lot of pixels = a lot of variables) and the model similarly has many millions of parameters. For other domains, this might not be the case.</p>

<blockquote>
  <p>What is meant by 'small'? For example, would a single-layer MLP with 100 hidden units be 'small'?</p>
</blockquote>

<p>Yes, that is definitely very small by modern standards. Unless you have a GPU suited perfectly for training (e.g. NVIDIA 1080 or NVIDIA Titan), I wouldn't be surprised to find that your CPU was faster.</p>

<p>Note that the complexity of your neural network also depends on your number of input features, not just the number of units in your hidden layer. If your hidden layer has 100 units and each observation in your dataset has 4 input features, then your network is tiny (~400 parameters). If each observation instead has 1M input features as in some medical/biotech contexts, then your network is pretty big in terms of number of parameters. For the remainder of my answer I'm assuming you have quite few input features pr. observation.</p>

<p>One good example I've found of comparing CPU vs. GPU performance was when I trained a poker bot using reinforcement learning. For reinforcement learning you often don't want that many layers in your neural network and we found that we only needed a few layers with few parameters. Moreover, the number of input features was quite low. Initially I trained on a GPU (NVIDIA Titan), but it was taking a long time as reinforcement learning requires a lot of iterations. Luckily, I found that training on my CPU instead made my training go 10x as fast! This is just to say that CPU's can sometimes be better for training.</p>

<blockquote>
  <p>Are there any other criteria that should be considered when deciding whether to train on CPU or GPU?</p>
</blockquote>

<p>It's important to note that while on a GPU you will always want to fill up the entire GPU memory by increasing your batch size, that is not the case on the CPU. On the CPU an increase in batch size will increase the time pr. batch. Therefore, if it's important for you to have a very large batch size (e.g. due to a very noisy signal), it can be beneficial to use a GPU. I haven't experienced this in practice though and normally small batch sizes are preferred.</p>
","<neural-network><deep-learning><gpu>","34","34","34","0","3044","780"
"39201","{
  ""id"": 39201,
  ""title"": ""Adaboost vs Gradient Boosting""
}","<p>Both AdaBoost and Gradient Boosting build weak learners in a sequential fashion.
Originally, AdaBoost was designed in such a way that at every step the sample distribution was adapted to put more weight on misclassified samples and less weight on correctly classified samples. The final prediction is a weighted average of all the weak learners, where more weight is placed on stronger learners.</p>
<p>Later, it was discovered that AdaBoost can also be expressed in terms of the more general framework of additive models with a particular loss function (the exponential loss). See e.g. Chapter 10 in <a href=""https://web.stanford.edu/%7Ehastie/ElemStatLearn/printings/ESLII_print12.pdf"" rel=""noreferrer"">(Hastie) ESL</a>.</p>
<p>Additive modeling tries to solve the following problem for a given loss function <span class=""math-container"">$L$</span>:</p>
<p><span class=""math-container"">$ \min_{\alpha_{n=1:N},\beta_{n=1:N}} L\left(y, \sum_{n=1}^N \alpha_n f(x,\beta_n) \right)$</span></p>
<p>where <span class=""math-container"">$f$</span> could be decision tree stumps. Since the sum inside the loss function makes life difficult, the expression can be approximated in a linear fashion, effectively allowing to move the sum in front of the loss function iteratively minimizing one subproblem at a time:</p>
<p><span class=""math-container"">$ \min_{\alpha_n,\beta_n} L\left(y, f_{n-1}((x) + \alpha_n f_n(x,\beta_n) \right)$</span></p>
<p>For arbitrary loss functions this is still a tricky problem, so we can further approximate this by applying a steepest descent with line search, i.e. we update <span class=""math-container"">$f_n$</span> by taking a step into the direction of the negative gradient.</p>
<p>In order to avoid overfitting on the gradient, the gradient is approximated with a new weak learner. This gives you the gradient boosting algorithm:</p>
<ol>
<li>Start with a constant model <span class=""math-container"">$f_0$</span></li>
<li>Fit a weak learner <span class=""math-container"">$h_n$</span> to the negative gradient of the loss function w.r.t. <span class=""math-container"">$f_{n-1}$</span></li>
<li>Take a step <span class=""math-container"">$\gamma$</span> so that <span class=""math-container"">$f_n= f_{n-1} + \gamma h_n$</span>  minimizes the loss <span class=""math-container"">$L\left(y, f_n(x) \right)$</span></li>
</ol>
<p>The main differences, therefore, are that Gradient Boosting is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function. Hence, Gradient Boosting is much more flexible.</p>
<p>On the other hand, AdaBoost can be interpreted from a much more intuitive perspective and can be implemented without the reference to gradients by reweighting the training samples based on classifications from previous learners.</p>
<p>See also <a href=""https://stats.stackexchange.com/q/164233/127417"">this question</a> for some further references (quote):</p>
<blockquote>
<ul>
<li>In Gradient Boosting, ‘shortcomings’ (of existing weak learners) are identified by gradients.</li>
<li>In Adaboost, ‘shortcomings’ are identified by high-weight data points.</li>
</ul>
</blockquote>
","<algorithms><similarity><ensemble-modeling><boosting>","34","34","34","0","23305","5682"
"17905","{
  ""id"": 17905,
  ""title"": ""Intuitive explanation of Noise Contrastive Estimation (NCE) loss?""
}","<p>Taken from this post:<a href=""https://stats.stackexchange.com/a/245452/154812"">https://stats.stackexchange.com/a/245452/154812</a></p>

<p><strong>The issue</strong></p>

<p>There are some issues with learning the word vectors using an ""standard"" neural network. In this way, the word vectors are learned while the network learns to predict the next word given a window of words (the input of the network).</p>

<p>Predicting the next word is like predicting the class. That is, such a network is just a ""standard"" multinomial (multi-class) classifier. And this network must have as many output neurons as classes there are. When classes are actual words, the number of neurons is, well, huge.</p>

<p>A ""standard"" neural network is usually trained with a cross-entropy cost function which requires the values of the output neurons to represent probabilities - which means that the output ""scores"" computed by the network for each class have to be normalized, converted into actual probabilities for each class. This normalization step is achieved by means of the softmax function. Softmax is very costly when applied to a huge output layer.</p>

<p><strong>The (a) solution</strong></p>

<p>In order to deal with this issue, that is, the expensive computation of the softmax, Word2Vec uses a technique called noise-contrastive estimation. This technique was introduced by [A] (reformulated by [B]) then used in [C], [D], [E] to learn word embeddings from unlabelled natural language text.</p>

<p>The basic idea is to convert a multinomial classification problem (as it is the problem of predicting the next word) to a binary classification problem. That is, instead of using softmax to estimate a true probability distribution of the output word, a binary logistic regression (binary classification) is used instead.</p>

<p>For each training sample, the enhanced (optimized) classifier is fed a true pair (a center word and another word that appears in its context) and a number of kk randomly corrupted pairs (consisting of the center word and a randomly chosen word from the vocabulary). By learning to distinguish the true pairs from corrupted ones, the classifier will ultimately learn the word vectors.</p>

<p>This is important: instead of predicting the next word (the ""standard"" training technique), the optimized classifier simply predicts whether a pair of words is good or bad.</p>

<p>Word2Vec slightly customizes the process and calls it negative sampling. In Word2Vec, the words for the negative samples (used for the corrupted pairs) are drawn from a specially designed distribution, which favours less frequent words to be drawn more often.</p>

<p><strong>References</strong></p>

<p>[A] <a href=""http://www.cs.cmu.edu/~nasmith/papers/smith+eisner.acl05.pdf"" rel=""noreferrer"">(2005) - Contrastive estimation: Training log-linear models on unlabeled data</a></p>

<p>[B] <a href=""http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf"" rel=""noreferrer"">(2010) - Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</a></p>

<p>[C] <a href=""http://dl.acm.org/citation.cfm?id=1390177"" rel=""noreferrer"">(2008) - A unified architecture for natural language processing: Deep neural networks with multitask learning</a></p>

<p>[D] <a href=""https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf"" rel=""noreferrer"">(2012) - A fast and simple algorithm for training neural probabilistic language models</a>.</p>

<p>[E] <a href=""https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf"" rel=""noreferrer"">(2013) - Learning word embeddings efficiently with noise-contrastive estimation</a>.</p>
","<deep-learning><tensorflow><word-embeddings><sampling><loss-function>","34","36","35","1","30425","464"
"10253","{
  ""id"": 10253,
  ""title"": ""When should I use Gini Impurity as opposed to Information Gain (Entropy)?""
}","<p>Generally, your performance will not change whether you use Gini impurity or Entropy. </p>

<p>Laura Elena Raileanu and Kilian Stoffel compared both in ""<a href=""https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf"" rel=""noreferrer"">Theoretical comparison between the gini index and information gain criteria</a>"". The most important remarks were:</p>

<ul>
<li>It only matters in 2% of the cases whether you use gini impurity or entropy. </li>
<li>Entropy might be a little slower to compute (because it makes use of the logarithm). </li>
</ul>

<p>I was once told that both metrics exist because they emerged in different disciplines of science. </p>
","<machine-learning><decision-trees><information-theory>","34","34","34","0","14372","843"
"966","{
  ""id"": 966,
  ""title"": ""What is difference between text classification and topic models?""
}","<p><strong>Text Classification</strong></p>

<p>I give you a bunch of documents, each of which has a label attached. I ask you to learn why you think the contents of the documents have been given these labels based on their words. Then I give you new documents and ask what you think the label for each one should be. The labels have meaning to me, not to you necessarily.</p>

<p><strong>Topic Modeling</strong></p>

<p>I give you a bunch of documents, without labels. I ask you to explain why the documents have the words they do by identifying some topics that each is ""about"". You tell me the topics, by telling me how much of each is in each document, and I decide what the topics ""mean"" if anything.</p>

<p>You'd have to clarify what you me by ""identify one topic"" or ""classify the text"".</p>
","<classification><text-mining><topic-model>","34","34","34","0","21","6202"
"33000","{
  ""id"": 33000,
  ""title"": ""What's the difference between fit and fit_transform in scikit-learn models?""
}","<p>The following explanation is based on <code>fit_transform</code> of <code>Imputer</code> class, but the idea is the same for <code>fit_transform</code> of other scikit_learn classes like <code>MinMaxScaler</code>.</p>
<hr />
<p><code>transform</code> replaces the missing values with a number. By default this number is the means of columns of some data that you choose.
Consider the following example:</p>
<pre><code>imp = Imputer()
# calculating the means
imp.fit([
         [1,      3], 
         [np.nan, 2], 
         [8,      5.5]
        ])
</code></pre>
<p>Now the imputer have learned to use a mean <span class=""math-container"">${{(1+8)}\over {2}} = 4.5$</span> for the first column and mean <span class=""math-container"">${{(2+3+5.5)}\over {3}} = 3.5$</span> for the second column when it gets applied to a two-column data:</p>
<pre><code>X = [[np.nan, 11], 
     [4,      np.nan], 
     [8,      2],
     [np.nan, 1]]
print(imp.transform(X))
</code></pre>
<p>we get</p>
<pre><code>[[4.5, 11], 
 [4, 3.5],
 [8, 2],
 [4.5, 1]]
</code></pre>
<p>So by <code>fit</code> the imputer calculates the means of columns from some data, and by <code>transform</code> it applies those means to some data (which is just replacing missing values with the means). If both these data are the same (i.e. the data for calculating the means and the data that means are applied to) you can use <code>fit_transform</code> which is basically a <code>fit</code> followed by a <code>transform</code>.</p>
<p>Now your questions:</p>
<blockquote>
<p>Why we might need to transform data?</p>
</blockquote>
<p>&quot;For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical&quot; (<a href=""http://scikit-learn.org/dev/modules/impute.html#impute"" rel=""nofollow noreferrer"">source</a>)</p>
<blockquote>
<p>What does it mean fitting model on training data and transforming to test data?</p>
</blockquote>
<p>The <code>fit</code> of an imputer has nothing to do with <code>fit</code> used in model fitting.
So using imputer's <code>fit</code> on training data just calculates means of each column of training data. Using <code>transform</code> on test data then replaces missing values of test data with means that were calculated from training data.</p>
","<python><scikit-learn>","34","34","34","0","50739","441"
"16945","{
  ""id"": 16945,
  ""title"": ""Calculation and Visualization of Correlation Matrix with Pandas""
}","<p>Another alternative is to use the heatmap function in seaborn to plot the covariance. This example uses the Auto data set from the ISLR package in R (the same as in the example you showed).</p>

<pre><code>import pandas.rpy.common as com
import seaborn as sns
%matplotlib inline

# load the R package ISLR
infert = com.importr(""ISLR"")

# load the Auto dataset
auto_df = com.load_data('Auto')

# calculate the correlation matrix
corr = auto_df.corr()

# plot the heatmap
sns.heatmap(corr, 
        xticklabels=corr.columns,
        yticklabels=corr.columns)
</code></pre>

<p><a href=""https://i.stack.imgur.com/AGfW6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/AGfW6.png"" alt=""enter image description here""></a></p>

<p>If you wanted to be even more fancy, you can use <a href=""http://pandas.pydata.org/pandas-docs/stable/style.html"" rel=""noreferrer"">Pandas Style</a>, for example:</p>

<pre><code>cmap = cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector=""th"",
                 props=[(""font-size"", ""7pt"")]),
            dict(selector=""td"",
                 props=[('padding', ""0em 0em"")]),
            dict(selector=""th:hover"",
                 props=[(""font-size"", ""12pt"")]),
            dict(selector=""tr:hover td:hover"",
                 props=[('max-width', '200px'),
                        ('font-size', '12pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_caption(""Hover to magify"")\
    .set_precision(2)\
    .set_table_styles(magnify())
</code></pre>

<p><a href=""https://i.stack.imgur.com/ZmIpq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ZmIpq.png"" alt=""enter image description here""></a></p>
","<python><statistics><visualization><pandas>","34","34","34","0","28965","440"
"30347","{
  ""id"": 30347,
  ""title"": ""Why not always use the ADAM optimization technique?""
}","<p><a href=""https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/"" rel=""noreferrer"">Here</a>’s a blog post reviewing an article claiming SGD is a better generalized adapter than ADAM.</p>
<p>There is often a value to using more than one method (an ensemble), because every method has a weakness.</p>
","<neural-network><optimization>","34","34","34","0","50307","466"
"42625","{
  ""id"": 42625,
  ""title"": ""Data science related funny quotes""
}","<p>Neural Network are not black boxes. They are a big pile of linear algebra :</p>

<p><a href=""https://i.stack.imgur.com/5Q5OC.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5Q5OC.png"" alt=""https://xkcd.com/1838/""></a></p>

<p>image from <a href=""https://xkcd.com/1838/"" rel=""noreferrer"">xkcd</a></p>
","<machine-learning><neural-network><deep-learning>","33","33","33","0","57416","782"
"5450","{
  ""id"": 5450,
  ""title"": ""Do data scientists use Excel?""
}","<p>Most non-technical people often use Excel as a database replacement. I think that's wrong but tolerable. However, someone who is supposedly experienced in data analysis simply can not use Excel as his main tool (excluding the obvious task of looking at the data for the first time). That's because Excel was never intended for that kind of analysis and as a consequence of this, it is incredibly easy to make mistakes in Excel (that's not to say that it is not incredibly easy to make another type of mistakes when using other tools, but Excel aggravates the situation even more.)</p>

<p>To summarize what Excel doesn't have and is a must for any analysis:</p>

<ol>
<li>Reproducibility. A data analysis needs to be reproducible.</li>
<li>Version control. Good for collaboration and also good for reproducibility. Instead of using xls, use csv (still very complex and has lots of edge cases, but csv parsers are fairly good nowadays.) </li>
<li>Testing. If you don't have tests, your code is broken. If your code is broken, your analysis is worse than useless. </li>
<li>Maintainability.</li>
<li>Accuracy. Numerical accuracy, accurate date parsing, among others are really lacking in Excel.</li>
</ol>

<p>More resources:</p>

<p><a href=""http://www.eusprig.org/horror-stories.htm"" rel=""noreferrer"">European Spreadsheet Risks Interest Group - Horror Stories</a></p>

<p><a href=""http://lemire.me/blog/archives/2014/05/23/you-shouldnt-use-a-spreadsheet-for-important-work-i-mean-it/"" rel=""noreferrer"">You shouldn’t use a spreadsheet for important work (I mean it)</a></p>

<p><a href=""http://www.forbes.com/sites/timworstall/2013/02/13/microsofts-excel-might-be-the-most-dangerous-software-on-the-planet/"" rel=""noreferrer"">Microsoft's Excel Might Be The Most Dangerous Software On The Planet</a></p>

<p><a href=""http://randyzwitch.com/excel-destroys-data/"" rel=""noreferrer"">Destroy Your Data Using Excel With This One Weird Trick!</a></p>

<p><a href=""http://www.win-vector.com/blog/2014/11/excel-spreadsheets-are-hard-to-get-right/"" rel=""noreferrer"">Excel spreadsheets are hard to get right</a></p>
","<tools><career><excel>","33","37","35","2","4621","838"
"18259","{
  ""id"": 18259,
  ""title"": ""How to force weights to be non-negative in Linear regression""
}","<p>What you are looking for, is the <a href=""https://en.wikipedia.org/wiki/Non-negative_least_squares"" rel=""noreferrer"">Non-negative least square regression</a>.
It is a simple optimization problem in quadratic programming where your constraint is that all the coefficients(a.k.a weights) should be positive.</p>

<p>Having said that, there is <a href=""https://github.com/scikit-learn/scikit-learn/issues/8191"" rel=""noreferrer"">no standard implementation</a> of Non-negative least squares in Scikit-Learn. <a href=""https://github.com/scikit-learn/scikit-learn/pull/8428"" rel=""noreferrer"">The pull request is still open</a>.</p>

<p>But, looks like <a href=""https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.nnls.html"" rel=""noreferrer"">Scipy has implemented the same</a>. </p>

<p><strong>PS:</strong> I haven't tried the scipy version. I found it solely by googling around.</p>
","<python><scikit-learn><linear-regression>","33","33","33","0","11097","7746"
"54909","{
  ""id"": 54909,
  ""title"": ""Data normalization before or after train-test split?""
}","<p>Normalization across instances should be done after splitting the data between training and test set, using only the data from the training set.</p>
<p>This is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance.</p>
<p><em>[Precision thanks to Neil's comment]</em>
When normalizing the test set, one should apply the normalization parameters previously obtained from the training set as-is. <em>Do not</em> recalculate them on the test set, because they would be inconsistent with the model and this would produce wrong predictions.</p>
","<normalization>","33","33","33","0","64377","15936"
"30679","{
  ""id"": 30679,
  ""title"": ""Role derivative of sigmoid function in neural networks""
}","<p>The use of derivatives in neural networks is for the training process called <strong>backpropagation</strong>. This technique uses <strong>gradient descent</strong> in order to find an optimal set of model parameters in order to minimize a loss function. In your example you must use the <strong>derivative of a sigmoid</strong> because that is the activation that your individual neurons are using.</p>

<hr>

<h1>The loss function</h1>

<p>The essence of machine learning is to optimize a cost function such that we can either minimize or maximize some target function. This is typically called the loss or cost funtion. We typically want to minimize this function. The cost function, <span class=""math-container"">$C$</span>, associates some penalty based on the resulting errors when passing data through your model as a function of the model parameters. </p>

<p>Let's look at the example where we try to label whether an image contains a cat or a dog. If we have a perfect model, we can give the model a picture and it will tell us if it is a cat or a dog. However, no model is perfect and it will make mistakes. </p>

<p>When we train our model to be able to infer meaning from input data we want to minimize the amount of mistakes it makes. So we use a training set, this data contains a lot of pictures of dogs and cats and we have the ground truth label associated with that image. Each time we run a training iteration of the model we calculate the cost (the amount of mistakes) of the model. We will want to minimize this cost.</p>

<p>Many cost functions exist each serving their own purpose. A common cost function that is used is the quadratic cost which is defined as </p>

<p><span class=""math-container"">$C = \frac{1}{N} \sum_{i=0}^{N}(\hat{y} - y)^2$</span>.</p>

<p>This is the square of the difference between the predicted label and the ground truth label for the <span class=""math-container"">$N$</span> images that we trained over. We will want to minimize this in some way.</p>

<h1>Minimizing a loss function</h1>

<p>Indeed most of machine learning is simply a family of frameworks which are capable of determining a distribution by minimizing some cost function. The question we can ask is ""how can we minimize a function""? </p>

<p>Let's minimize the following function </p>

<p><span class=""math-container"">$y = x^2-4x+6$</span>.</p>

<p>If we plot this we can see that there is a minimum at <span class=""math-container"">$x = 2$</span>. To do this analytically we can take the derivative of this function as</p>

<p><span class=""math-container"">$\frac{dy}{dx} = 2x - 4 = 0$</span></p>

<p><span class=""math-container"">$x = 2$</span>.</p>

<p>However, often times finding a global minimum analytically is not feasible. So instead we use some optimization techniques. Here as well many different ways exist such as : Newton-Raphson, grid search, etc. Among these is <strong>gradient descent</strong>. This is the technique used by neural networks.</p>

<h2>Gradient Descent</h2>

<p>Let's use a famously used analogy to understand this. Imagine a 2D minimization problem. This is equivalent of being on a mountainous hike in the wilderness. You want to get back down to the village which you know is at the lowest point. Even if you do not know the cardinal directions of the village. All you need to do is continuously take the steepest way down, and you will eventually get to the village. So we will descend down the surface based on the steepness of the slope.</p>

<p>Let's take our function</p>

<p><span class=""math-container"">$y = x^2-4x+6$</span></p>

<p>we will determine the <span class=""math-container"">$x$</span> for which <span class=""math-container"">$y$</span> is minimized. Gradient descent algorithm first says we will pick a random value for <span class=""math-container"">$x$</span>. Let us initialize at <span class=""math-container"">$x=8$</span>. Then the algorithm will do the following iteratively until we reach convergence.</p>

<p><span class=""math-container"">$x^{new} = x^{old} - \nu \frac{dy}{dx}$</span></p>

<p>where <span class=""math-container"">$\nu$</span> is the learning rate, we can set this to whatever value we will like. However there is a smart way to choose this. Too big and we will never reach our minimum value, and too small we will waste soooo much time before we get there. It is analogous to the size of the steps you want to take down the steep slope. Small steps and you will die on the mountain, you'll never get down. Too large of a step and you risk over shooting the village and ending up the other side of the mountain. <strong>The derivative is the means by which we travel down this slope towards our minimum.</strong></p>

<p><span class=""math-container"">$\frac{dy}{dx} = 2x - 4$</span></p>

<p><span class=""math-container"">$\nu = 0.1$</span></p>

<p>Iteration 1:</p>

<p><span class=""math-container"">$x^{new} = 8 - 0.1(2 * 8 - 4) = 6.8 $</span> <br/>
<span class=""math-container"">$x^{new} = 6.8 - 0.1(2 * 6.8 - 4) = 5.84 $</span> <br/>
<span class=""math-container"">$x^{new} = 5.84 - 0.1(2 * 5.84 - 4) = 5.07 $</span> <br/>
<span class=""math-container"">$x^{new} = 5.07 - 0.1(2 * 5.07 - 4) = 4.45 $</span> <br/>
<span class=""math-container"">$x^{new} = 4.45 - 0.1(2 * 4.45 - 4) = 3.96 $</span> <br/>
<span class=""math-container"">$x^{new} = 3.96 - 0.1(2 * 3.96 - 4) = 3.57 $</span> <br/>
<span class=""math-container"">$x^{new} = 3.57 - 0.1(2 * 3.57 - 4) = 3.25 $</span> <br/>
<span class=""math-container"">$x^{new} = 3.25 - 0.1(2 * 3.25 - 4) = 3.00 $</span> <br/>
<span class=""math-container"">$x^{new} = 3.00 - 0.1(2 * 3.00 - 4) = 2.80 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.80 - 0.1(2 * 2.80 - 4) = 2.64 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.64 - 0.1(2 * 2.64 - 4) = 2.51 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.51 - 0.1(2 * 2.51 - 4) = 2.41 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.41 - 0.1(2 * 2.41 - 4) = 2.32 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.32 - 0.1(2 * 2.32 - 4) = 2.26 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.26 - 0.1(2 * 2.26 - 4) = 2.21 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.21 - 0.1(2 * 2.21 - 4) = 2.16 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.16 - 0.1(2 * 2.16 - 4) = 2.13 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.13 - 0.1(2 * 2.13 - 4) = 2.10 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.10 - 0.1(2 * 2.10 - 4) = 2.08 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.08 - 0.1(2 * 2.08 - 4) = 2.06 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.06 - 0.1(2 * 2.06 - 4) = 2.05 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.05 - 0.1(2 * 2.05 - 4) = 2.04 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.04 - 0.1(2 * 2.04 - 4) = 2.03 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.03 - 0.1(2 * 2.03 - 4) = 2.02 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.02 - 0.1(2 * 2.02 - 4) = 2.02 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.02 - 0.1(2 * 2.02 - 4) = 2.01 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.01 - 0.1(2 * 2.01 - 4) = 2.01 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.01 - 0.1(2 * 2.01 - 4) = 2.01 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.01 - 0.1(2 * 2.01 - 4) = 2.00 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.00 - 0.1(2 * 2.00 - 4) = 2.00 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.00 - 0.1(2 * 2.00 - 4) = 2.00 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.00 - 0.1(2 * 2.00 - 4) = 2.00 $</span> <br/>
<span class=""math-container"">$x^{new} = 2.00 - 0.1(2 * 2.00 - 4) = 2.00 $</span> <br/></p>

<p>And we see that the algorithm converges at <span class=""math-container"">$x = 2$</span>! We have found the minimum.</p>

<hr>

<h1>Applied to neural networks</h1>

<p>The first neural networks only had a single neuron which took in some inputs <span class=""math-container"">$x$</span> and then provide an output <span class=""math-container"">$\hat{y}$</span>. A common function used is the sigmoid function </p>

<p><span class=""math-container"">$\sigma(z) = \frac{1}{1+exp(z)}$</span></p>

<p><span class=""math-container"">$\hat{y}(w^Tx) = \frac{1}{1+exp(w^Tx + b)}$</span></p>

<p>where <span class=""math-container"">$w$</span> is the associated weight for each input <span class=""math-container"">$x$</span> and we have a bias <span class=""math-container"">$b$</span>. We then want to minimize our cost function </p>

<p><span class=""math-container"">$C = \frac{1}{2N} \sum_{i=0}^{N}(\hat{y} - y)^2$</span>.</p>

<h2>How to train the neural network?</h2>

<p>We will use gradient descent to train the weights based on the output of the sigmoid function and we will use some cost function <span class=""math-container"">$C$</span> and train on batches of data of size <span class=""math-container"">$N$</span>. </p>

<p><span class=""math-container"">$C = \frac{1}{2N} \sum_i^N (\hat{y} - y)^2$</span></p>

<p><span class=""math-container"">$\hat{y}$</span> is the predicted class obtained from the sigmoid function and <span class=""math-container"">$y$</span> is the ground truth label. We will use gradient descent to minimize the cost function with respect to the weights <span class=""math-container"">$w$</span>. To make life easier we will split the derivative as follows</p>

<p><span class=""math-container"">$\frac{\partial C}{\partial w} = \frac{\partial C}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial w}$</span>.</p>

<p><span class=""math-container"">$\frac{\partial C}{\partial \hat{y}} = \hat{y} - y$</span></p>

<p>and we have that <span class=""math-container"">$\hat{y} = \sigma(w^Tx)$</span> and the derivative of the sigmoid function is <span class=""math-container"">$\frac{\partial \sigma(z)}{\partial z} = \sigma(z)(1-\sigma(z))$</span> thus we have,</p>

<p><span class=""math-container"">$\frac{\partial \hat{y}}{\partial w} = \frac{1}{1+exp(w^Tx + b)} (1 - \frac{1}{1+exp(w^Tx + b)})$</span>.</p>

<p>So we can then update the weights through gradient descent as </p>

<p><span class=""math-container"">$w^{new} = w^{old} - \eta \frac{\partial C}{\partial w}$</span></p>

<p>where <span class=""math-container"">$\eta$</span> is the learning rate.</p>
","<machine-learning><neural-network>","33","33","33","0","29587","8088"
"1099","{
  ""id"": 1099,
  ""title"": ""Gini coefficient vs Gini impurity - decision trees""
}","<p>No, despite their names they <em>are not</em> equivalent or even that similar. </p>

<ul>
<li><strong>Gini impurity</strong> is a measure of misclassification, which applies in a multiclass classifier context. </li>
<li><strong>Gini coefficient</strong> applies to binary classification and requires a classifier that can in some way rank examples according to the likelihood of being in a positive class. </li>
</ul>

<p>Both could be applied in some cases, but they are different measures for different things. Impurity is what is commonly used in <a href=""https://en.wikipedia.org/wiki/Decision_tree"" rel=""noreferrer"">decision trees</a>.</p>
","<data-mining>","33","33","33","0","21","6202"
"76841","{
  ""id"": 76841,
  ""title"": ""Is Python a viable language to do statistical analysis in?""
}","<p>Python is more &quot;general purpose&quot; while R has a clear(er) focus on statistics. However, most (if not all) things you can do in R can be done in Python as well. The difference is that you need to use additional packages in Python for some things you can do in base R.</p>
<p>Some examples:</p>
<ul>
<li>Data frames are base R while you need to use <a href=""https://pypi.org/project/pandas/"" rel=""nofollow noreferrer"">Pandas</a> in Python.</li>
<li>Linear models (<code>lm</code>) are base R while you need to use <a href=""https://www.statsmodels.org/stable/index.html"" rel=""nofollow noreferrer"">statsmodels</a> or <a href=""https://scikit-learn.org/stable/"" rel=""nofollow noreferrer"">scikit</a> in Python. There are important <a href=""https://datascience.stackexchange.com/questions/74442/why-scikit-and-statsmodel-provide-different-coefficient-of-determination/74445#74445"">conceptional differences</a> to be considered.</li>
<li>For some rather basic mathematical operations you would need to use <a href=""https://numpy.org/"" rel=""nofollow noreferrer"">numpy</a>.</li>
</ul>
<p>Overall this leads to some additional effort (and knowledge) needed to work fluently in Python. I personally often feel more comfortable working with base R since I feel like being &quot;closer to the data&quot; in (base) R.</p>
<p>However, in other cases, e.g. when I use boosting or neural nets, Python seems to have an advantage over R. Many algorithms are developed in <code>C++</code> (e.g. <a href=""https://github.com/jjallaire/deep-learning-with-r-notebooks"" rel=""nofollow noreferrer"">Keras</a>, <a href=""https://lightgbm.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">LightGBM</a>) and adapted to Python and (often later to) R. At least when you work with Windows, this often works better with Python. You can use things like Tensorflow/Keras, LightGBM, Catboost in R, but it sometimes can be daunting to get the additional package running in R (especially with GPU support).</p>
<p>Many packages (or methods) are available for R and Python, such as GLMnet (<a href=""https://web.stanford.edu/%7Ehastie/glmnet/glmnet_alpha.html"" rel=""nofollow noreferrer"">for R</a> / <a href=""https://web.stanford.edu/%7Ehastie/glmnet_python/"" rel=""nofollow noreferrer"">for Python</a>). You can also see based on the Labs of &quot;<a href=""http://faculty.marshall.usc.edu/gareth-james/ISL/index.html"" rel=""nofollow noreferrer"">Introduction to Statistical Learning</a>&quot; - which are available <a href=""http://faculty.marshall.usc.edu/gareth-james/ISL/code.html"" rel=""nofollow noreferrer"">for R</a> and <a href=""https://github.com/JWarmenhoven/ISLR-python"" rel=""nofollow noreferrer"">for Python</a> as well - that there is not so much of a difference between the two languages in terms of <em>what</em> you can do. The difference is more like <em>how</em> things are done.</p>
<p>Finally, since Python is more &quot;general purpose&quot; than R (at least in my view), there are <a href=""https://realpython.com/what-can-i-do-with-python/"" rel=""nofollow noreferrer"">interesting and funny things</a> you can do with Python (beyond statistics) which you cannot do with R (at least it is harder).</p>
","<machine-learning><python><r><statistics><data-analysis>","32","32","32","0","71442","5125"
"293","{
  ""id"": 293,
  ""title"": ""What are the advantages of HDF compared to alternative formats?""
}","<p>Perhaps a good way to paraphrase the question is, what are the advantages compared to alternative formats?  </p>

<p>The main alternatives are, I think: a database, text files, or another packed/binary format.</p>

<p>The database options to consider are probably a columnar store or NoSQL, or for small self-contained datasets SQLite.  The main advantage of the database is the ability to work with data much larger than memory, to have random or indexed access, and to add/append/modify data quickly.  The main *dis*advantage is that it is much slower than HDF, for problems in which the entire dataset needs to be read in and processed.  Another disadvantage is that, with the exception of embedded-style databases like SQLite, a database is a system (requiring admnistration, setup, maintenance, etc) rather than a simple self-contained data store.  </p>

<p>The text file format options are XML/JSON/CSV.  They are cross-platform/language/toolkit, and are a good archival format due to the ability to be self-describing (or obvious :).  If uncompressed, they are huge (10x-100x HDF), but if compressed, they can be fairly space-efficient (compressed XML is about the same as HDF).  The main disadvantage here is again speed: parsing text is much, much slower than HDF.</p>

<p>The other binary formats (npy/npz numpy files, blz blaze files, protocol buffers, Avro, ...) have very similar properties to HDF, except they are less widely supported (may be limited to just one platform: numpy) and may have specific other limitations.  They typically do not offer a compelling advantage.</p>

<p>HDF is a good complement to databases, it may make sense to run a query to produce a roughly memory-sized dataset and then cache it in HDF if the same data would be used more than once.  If you have a dataset which is fixed, and usually processed as a whole, storing it as a collection of appropriately sized HDF files is not a bad option.  If you have a dataset which is updated often, staging some of it as HDF files periodically might still be helpful.</p>

<p>To summarize, HDF is a good format for data which is read (or written) typically as a whole; it is the lingua franca or common/preferred interchange format for many applications due to wide support and compatibility, decent as an archival format, and very fast.</p>

<p>P.S. To give this some practical context, my most recent experience comparing HDF to alternatives, a certain small (much less than memory-sized) dataset took 2 seconds to read as HDF (and most of this is probably overhead from Pandas); ~1 minute to read from JSON; and 1 <em>hour</em> to write to database.  Certainly the database write could be sped up, but you'd better have a good DBA!  This is how it works out of the box.</p>
","<data-formats><hierarchical-data-format>","32","32","32","0","26","3032"
"20203","{
  ""id"": 20203,
  ""title"": ""train_test_split() error: Found input variables with inconsistent numbers of samples""
}","<p>You are running into that error because your <code>X</code> and <code>Y</code> don't have the same length (which is what <code>train_test_split</code> requires), i.e., <code>X.shape[0] != Y.shape[0]</code>. Given your current code:</p>

<pre><code>&gt;&gt;&gt; X.shape
(1, 6, 29)
&gt;&gt;&gt; Y.shape
(29,)
</code></pre>

<p>To fix this error:</p>

<ol>
<li>Remove the extra list from inside of <code>np.array()</code> when defining <code>X</code> or remove the extra dimension afterwards with the following command: <code>X = X.reshape(X.shape[1:])</code>. Now, the shape of <code>X</code> will be (6, 29).</li>
<li>Transpose <code>X</code> by running <code>X = X.transpose()</code> to get equal number of samples in <code>X</code> and <code>Y</code>. Now, the shape of <code>X</code> will be (29, 6) and the shape of <code>Y</code> will be (29,).</li>
</ol>
","<python><scikit-learn><sampling>","32","32","32","0","24000","1064"
"24539","{
  ""id"": 24539,
  ""title"": ""Why should the data be shuffled for machine learning tasks""
}","<p>Based on <a href=""https://datascience.meta.stackexchange.com/q/2258/41663"">What should we do when a question posted on DataScience is a duplicate of a question posted on CrossValidated?</a>, I am reposting my answer to the same question asked on CrossValidated (<a href=""https://stats.stackexchange.com/a/311318/89653"">https://stats.stackexchange.com/a/311318/89653</a>).</p>

<p><strong>Note: throughout this answer I refer to minimization of training loss and I do not discuss stopping criteria such as validation loss. The choice of stopping criteria does not affect the process/concepts described below.</strong></p>

<p>The process of training a neural network is to find the minimum value of a loss function $ℒ_X(W)$, where $W$ represents a matrix (or several matrices) of weights between neurons and $X$ represents the training dataset. I use a subscript for $X$ to indicate that our minimization of $ℒ$ occurs only over the weights $W$ (that is, we are looking for $W$ such that $ℒ$ is minimized) while $X$ is fixed.</p>

<p>Now, if we assume that we have $P$ elements in $W$ (that is, there are $P$ weights in the network), $ℒ$ is a surface in a $P+1$-dimensional space. To give a visual analogue, imagine that we have only two neuron weights ($P=2$). Then $ℒ$ has an easy geometric interpretation: it is a surface in a 3-dimensional space. This arises from the fact that for any given matrices of weights $W$, the loss function can be evaluated on $X$ and that value becomes the elevation of the surface.</p>

<p>But there is the problem of non-convexity; the surface I described will have numerous local minima, and therefore gradient descent algorithms are susceptible to becoming ""stuck"" in those minima while a deeper/lower/better solution may lie nearby. This is likely to occur if $X$ is unchanged over all training iterations, because the surface is fixed for a given $X$; all its features are static, including its various minima.</p>

<p>A solution to this is mini-batch training combined with shuffling. By shuffling the rows and training on only a subset of them during a given iteration, $X$ changes with <em>every</em> iteration, and it is actually quite possible that no two iterations over the entire sequence of training iterations and epochs will be performed on the exact same $X$. The effect is that the solver can easily ""bounce"" out of a local minimum. Imagine that the solver is stuck in a local minimum at iteration $i$ with training mini-batch $X_i$. This local minimum corresponds to $ℒ$ evaluated at a particular value of weights; we'll call it $ℒ_{X_i}(W_i)$. On the next iteration the shape of our loss surface actually changes because we are using $X_{i+1}$, that is, $ℒ_{X_{i+1}}(W_i)$ may take on a very different value from $ℒ_{X_i}(W_i)$ and it is quite possible that it does not correspond to a local minimum! We can now compute a gradient update and continue with training. To be clear: the shape of $ℒ_{X_{i+1}}$ will -- in general -- be different from that of $ℒ_{X_{i}}$. Note that here I am referring to the loss function $ℒ$ evaluated on a training set $X$; it is a complete surface defined over all possible values of $W$, rather than the evaluation of that loss (which is just a scalar) for a specific value of $W$. Note also that if mini-batches are used without shuffling there is still a degree of ""diversification"" of loss surfaces, but there will be a finite (and relatively small) number of unique error surfaces seen by the solver (specifically, it will see the same exact set of mini-batches -- and therefore loss surfaces -- during each epoch).</p>

<p>One thing I deliberately avoided was a discussion of mini-batch sizes, because there are a million opinions on this and it has significant practical implications (greater parallelization can be achieved with larger batches). However, I believe the following is worth mentioning. Because $ℒ$ is evaluated by computing a value for each row of $X$ (and summing or taking the average; i.e., a commutative operator) for a given set of weight matrices $W$, the arrangement of the rows of $X$ <em>has no effect</em> when using full-batch gradient descent (that is, when each batch is the full $X$, and iterations and epochs are the same thing).</p>
","<machine-learning><neural-network><deep-learning>","32","34","33","1","41663","443"
"40904","{
  ""id"": 40904,
  ""title"": ""What's the difference between Sklearn F1 score 'micro' and 'weighted' for a multi class classification problem?""
}","<p>F1Score is a metric to evaluate predictors performance using the formula </p>

<blockquote>
  <p>F1 = 2 * (precision * recall) / (precision + recall)</p>
</blockquote>

<p>where</p>

<blockquote>
  <p>recall = TP/(TP+FN)
  and
  precision = TP/(TP+FP)</p>
</blockquote>

<p>and remember:</p>

<p><a href=""https://i.stack.imgur.com/VxiS5.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/VxiS5.png"" alt=""enter image description here""></a></p>

<p>When you have a multiclass setting, the <em>average</em> parameter in the <code>f1_score</code> <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"" rel=""noreferrer"">function</a> needs to be one of these:</p>

<ul>
<li><strong><em>'weighted'</em></strong></li>
<li><strong><em>'micro'</em></strong></li>
<li><strong><em>'macro'</em></strong></li>
</ul>

<p>The first one, <strong><em>'weighted'</em></strong> calculates de F1 score for each class independently but when it adds them together uses a weight that depends on the number of true labels of each class:</p>

<p><span class=""math-container"">$$F1_{class1}*W_1+F1_{class2}*W_2+\cdot\cdot\cdot+F1_{classN}*W_N$$</span></p>

<p>therefore favouring the majority class.</p>

<p><strong><em>'micro'</em></strong> uses the global number of TP, FN, FP and calculates the F1 directly:</p>

<p><span class=""math-container"">$$F1_{class1+class2+class3}$$</span></p>

<p>no favouring any class in particular.</p>

<p>Finally, <strong><em>'macro'</em></strong> calculates the F1 separated by class but not using weights for the aggregation:</p>

<p><span class=""math-container"">$$F1_{class1}+F1_{class2}+\cdot\cdot\cdot+F1_{classN}$$</span></p>

<p>which resuls in a bigger penalisation when your model does not perform well with the minority classes.</p>

<p>The one to use depends on what you want to achieve. If you are worried with class imbalance I would suggest using <strong>'macro'</strong>. However, it might be also worthwile implementing some of the techniques available to taclke imbalance problems such as downsampling the majority class, upsampling the minority, SMOTE, etc.</p>

<p>Hope this helps!</p>
","<classification><scikit-learn><metric>","32","32","32","0","45166","1612"
"349","{
  ""id"": 349,
  ""title"": ""What do you think of Data Science certifications?""
}","<p>As a former analytics manager and a current lead data scientist, I am very leery of the need for data science certificates.  The term data scientist is pretty vague and the field of data science is in it's infancy.  A certificates implies some sort of uniform standard which is just lacking in data science, it is still very much the wild west. </p>

<p>While a certificate is probably not going to hurt you, I think your time would be better spent developing the experience to know when to use a certain approach, and depth of understanding to be able to explain that approach to a non-technical audience.</p>
","<education>","32","32","32","0","780","881"
"5698","{
  ""id"": 5698,
  ""title"": ""Dimensionality and Manifold""
}","<blockquote>
  <p>What is a dimension?</p>
</blockquote>

<p>To put it simply, if you have a tabular data set with m rows and n columns, then the dimensionality of your data is n:</p>

<blockquote>
  <p>What is a manifold? </p>
</blockquote>

<p>The simplest example is our planet Earth. For us it looks flat, but it really is a sphere. So it's sort of a 2d manifold embedded in the 3d space. </p>

<blockquote>
  <p>What is the difference? </p>
</blockquote>

<p>To answer this question, consider another example of a manifold: </p>

<p><img src=""https://i.stack.imgur.com/FrBXu.png"" alt=""enter image description here""></p>

<p>This is so-called ""swiss roll"". The data points are in 3d, but they all lie on 2d manifold, so the dimensionality of the manifold is 2, while the dimensionality of the input space is 3.</p>

<p>There are many techniques to ""unwrap"" these manifolds. One of them is called <a href=""https://www.google.com/search?q=locally+linear+embedding"" rel=""noreferrer"">Locally Linear Embedding</a>, and this is how it would do that:</p>

<p><img src=""https://i.stack.imgur.com/pagFb.png"" alt=""enter image description here""></p>

<p>Here's a scikit-learn snippet for doing that:</p>

<pre class=""lang-python prettyprint-override""><code>from sklearn.manifold import LocallyLinearEmbedding

lle = LocallyLinearEmbedding(n_neighbors=k, n_components=2)
X_lle = lle.fit_transform(data)
plt.scatter(X_lle[:, 0], X_lle[:, 1], c=color)
plt.show()
</code></pre>
","<machine-learning><dimensionality-reduction>","32","32","32","0","816","2610"
"27390","{
  ""id"": 27390,
  ""title"": ""What does it mean when we say most of the points in a hypercube are at the boundary?""
}","<p>Speaking of '<em><span class=""math-container"">$99\%$</span> of the points in a hypercube</em>' is a bit misleading since a hypercube contains infinitely many points. Let's talk about volume instead.</p>

<p>The volume of a hypercube is the product of its side lengths.
For the 50-dimensional unit hypercube we get <span class=""math-container"">$$\text{Total volume} = \underbrace{1 \times 1 \times \dots \times 1}_{50 \text{ times}} = 1^{50} = 1.$$</span></p>

<p>Now let us exclude the boundaries of the hypercube and look at the '<em>interior</em>' (I put this in quotation marks because the mathematical term <a href=""https://en.wikipedia.org/wiki/Interior_(topology)"" rel=""noreferrer"">interior</a> has a very different meaning). We only keep the points <span class=""math-container"">$x = (x_1, x_2, \dots, x_{50})$</span> that satisfy
<span class=""math-container"">$$
0.05 &lt; x_1 &lt; 0.95 \,\text{ and }\, 0.05 &lt; x_2 &lt; 0.95 \,\text{ and }\, \dots 
\,\text{ and }\, 0.05 &lt; x_{50} &lt; 0.95.
$$</span>
What is the volume of this '<em>interior</em>'? Well, the '<em>interior</em>' is again a hypercube, and the length of each side is <span class=""math-container"">$0.9$</span> (<span class=""math-container"">$=0.95 - 0.05$</span> ... it helps to imagine this in two and three dimensions).
So the volume is <span class=""math-container"">$$\text{Interior volume} = \underbrace{0.9 \times 0.9 \times \dots \times 0.9}_{50 \text{ times}} = 0.9^{50} \approx 0.005.$$</span>
Conclude that the volume of the '<em>boundary</em>' (defined as the unit hypercube without the '<em>interior</em>') is <span class=""math-container"">$1 - 0.9^{50} \approx 0.995.$</span></p>

<p>This shows that <span class=""math-container"">$99.5\%$</span> of the volume of a 50-dimensional hypercube is concentrated on its '<em>boundary</em>'.</p>

<hr>

<p><strong>Follow-up:</strong> <em>ignatius</em> raised an interesting question on how this is connected to probability. Here is an example.</p>

<p>Say you came up with a (machine learning) model that predicts housing prices based on 50 input parameters. All 50 input parameters are <a href=""https://en.wikipedia.org/wiki/Independence_(probability_theory)#For_real_valued_random_variables"" rel=""noreferrer"">independent</a> and <a href=""https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)"" rel=""noreferrer"">uniformly distributed</a> between <span class=""math-container"">$0$</span> and <span class=""math-container"">$1$</span>.</p>

<p>Let us say that your model works very well if none of the input parameters is <em>extreme:</em> As long as every input parameter stays between <span class=""math-container"">$0.05$</span> and <span class=""math-container"">$0.95$</span>, your model predicts the housing price almost perfectly.
But if one or more input parameters are extreme (smaller than <span class=""math-container"">$0.05$</span> or larger than <span class=""math-container"">$0.95$</span>), the predictions of your model are absolutely terrible.</p>

<p>Any given input parameter is extreme with a probability of only <span class=""math-container"">$10\%$</span>. So clearly this is a good model, right?
No! The probability that <em>at least one of the <span class=""math-container"">$50$</span> parameters is extreme</em> is <span class=""math-container"">$1 - 0.9^{50} \approx 0.995.$</span>
So in <span class=""math-container"">$99.5\%$</span> of the cases, your model's prediction is terrible.</p>

<p><strong>Rule of thumb:</strong> <em>In high dimensions, extreme observations are the rule and not the exception.</em></p>
","<machine-learning><mathematics>","32","32","32","0","45507","1546"
"24003","{
  ""id"": 24003,
  ""title"": ""Encoding features like month and hour as categorial or numeric?""
}","<p>Have you considered adding the (sine, cosine) transformation of the time of day variable? This will ensure that the 0 and 23 hour for example are close to each other, thus allowing the cyclical nature of the variable to shine through. </p>

<p>(<a href=""https://medium.com/towards-data-science/top-6-errors-novice-machine-learning-engineers-make-e82273d394db"" rel=""noreferrer"">More Info</a>)</p>
","<machine-learning><feature-extraction><feature-engineering><encoding><numerical>","31","31","31","0","15250","446"
"17770","{
  ""id"": 17770,
  ""title"": ""How to fill missing value based on other columns in Pandas dataframe?""
}","<p>Assuming three columns of your dataframe is <code>a</code>, <code>b</code> and <code>c</code>. This is what you want:</p>

<pre><code>df['c'] = df.apply(
    lambda row: row['a']*row['b'] if np.isnan(row['c']) else row['c'],
    axis=1
)
</code></pre>

<p>Full code:</p>

<pre><code>df = pd.DataFrame(
    np.array([[1, 2, 3], [4, 5, np.nan], [7, 8, 9], [3, 2, np.nan], [5, 6, np.nan]]), 
    columns=['a', 'b', 'c']
)
df['c'] = df.apply(
    lambda row: row['a']*row['b'] if np.isnan(row['c']) else row['c'],
    axis=1
)
</code></pre>
","<pandas>","31","31","31","0","28628","3736"
"44069","{
  ""id"": 44069,
  ""title"": ""What is Monte Carlo dropout?""
}","<p>Let's start with <strong>normal dropout</strong>, i.e. dropout only at training time. Here dropout serves as a <a href=""https://en.wikipedia.org/wiki/Regularization_(mathematics)"" rel=""noreferrer"">regularization</a> to avoid overfitting. During test time, dropout is not applied; instead, all nodes/connections are present, but the weights are adjusted accordingly (e.g. multiplied by the keep ratio, which is <code>1 - dropout_ratio</code>). Such a model during test time can be understood as a <em>average</em> of an ensemble of neural networks.</p>
<p>Notice that for normal dropout, at test time the prediction is <strong>deterministic</strong>. Without other source of randomness, given one test data point, the model will always predict the same label or value.</p>
<p>For <strong>Monte Carlo dropout</strong>, the dropout is applied at both training and test time. At test time, the prediction is no longer deterministic, but depending on which nodes/links you randomly choose to keep. Therefore, given a same datapoint, your model could predict different values each time.</p>
<p>So the primary goal of Monte Carlo dropout is to generate random predictions and interpret them as samples from a probabilistic distribution. In the authors' words, they call it <strong>Bayesian interpretation</strong>.</p>
<p><strong>Example</strong>: suppose you trained an <em>dog</em>/<em>cat</em> image classifier with Monte Carlo dropout. If you feed a same image to the classifier again and again, the classifier may be predicting <em>dog</em> 70% of the times while predicting <em>cat</em> 30% of the time. Therefore you can interpret the result in a probabilistic way: with 70% probability, this image shows a dog.</p>
","<deep-learning><keras><dropout><monte-carlo>","31","31","31","0","14006","2049"
"39319","{
  ""id"": 39319,
  ""title"": ""Difference between OrdinalEncoder and LabelEncoder""
}","<p>Afaik, both have the same functionality. A bit difference is the idea behind. <code>OrdinalEncoder</code> is for converting features, while <code>LabelEncoder</code> is for converting target variable.</p>

<p>That's why <code>OrdinalEncoder</code> can fit data that has the shape of <code>(n_samples, n_features)</code> while <code>LabelEncoder</code> can only fit data that has the shape of <code>(n_samples,)</code> (though in the past one used <code>LabelEncoder</code> within the loop to handle what has been becoming the job of <code>OrdinalEncoder</code> now) </p>
","<machine-learning><python><scikit-learn><preprocessing><encoding>","31","33","32","1","58685","636"
"47","{
  ""id"": 47,
  ""title"": ""Is the R language suitable for Big Data""
}","<p>The main problem with using R for large data sets is the RAM constraint. The reason behind keeping all the data in RAM is that it provides much faster access and data manipulations than would storing on HDDs. If you are willing to take a hit on performance, then yes, it is quite practical to work with <a href=""http://statistics.org.il/wp-content/uploads/2010/04/Big_Memory%20V0.pdf"">large datasets in R</a>.</p>

<ul>
<li>RODBC Package: Allows connecting to external DB from R to retrieve and handle data. Hence, the data being <em>manipulated</em> is restricted to your RAM. The overall data set can go much larger.</li>
<li>The ff package allows using larger than RAM data sets by utilising memory-mapped pages.</li>
<li>BigLM: It builds generalized linear models on big data. It loads data into memory in chunks.</li>
<li>bigmemory : An R package which allows powerful and memory-efficient parallel 
analyses and data mining of massive data sets. It permits storing large objects (matrices etc.) in memory (on the RAM) using external pointer objects to refer to them. </li>
</ul>
","<bigdata><r>","31","31","31","0","62","654"
"73182","{
  ""id"": 73182,
  ""title"": ""What does from_logits=True do in SparseCategoricalcrossEntropy loss function?""
}","<p>The <code>from_logits=True</code> attribute inform the loss function that the output values generated by the model are not normalized, a.k.a. logits. In other words, the softmax function has not been applied on them to produce a probability distribution. Therefore, the output layer in this case does not have a softmax activation function:</p>
<pre><code>out = tf.keras.layers.Dense(n_units)  # &lt;-- linear activation function
</code></pre>
<p>The softmax function would be automatically applied on the output values by the loss function. Therefore, this does not make a difference with the scenario when you use <code>from_logits=False</code> (default) and a softmax activation function on last layer; however, in some cases, this might help with numerical stability during training of the model. You may also find <a href=""https://stackoverflow.com/a/52126567/2099607"">this</a> and <a href=""https://stackoverflow.com/a/61237426/2099607"">this</a> answers relevant and useful about the numerical stability when <code>from_logits=True</code>.</p>
","<machine-learning><python><keras><tensorflow><loss-function>","31","31","31","0","28035","494"
"11684","{
  ""id"": 11684,
  ""title"": ""Applications and differences for Jaccard similarity and Cosine Similarity""
}","<p>The <a href=""https://datascience.stackexchange.com/a/5128/29575"">answer from saq7</a> is wrong, as well as not answering the question.</p>

<p>∥A∥ means the <span class=""math-container"">$L2$</span> norm of <span class=""math-container"">$A$</span>, i.e. the length of the vector in Euclidean
space, not the dimensionality of the vector <span class=""math-container"">$A$</span>.  In other words, you
don't count the 0 bits, you add up only the 1 bits and take the square root.</p>

<p>Sorry I don't have a real answer as to when you should use which metric,
but I can't let the incorrect answer go unchallenged.</p>
","<similarity>","31","31","31","0","18596","411"
"5429","{
  ""id"": 5429,
  ""title"": ""How to generate synthetic dataset using machine learning model learnt with original dataset?""
}","<p>The general approach is to do traditional statistical analysis on your data set to define a multidimensional random process that will generate data with the same statistical characteristics.  The virtue of this approach is that your synthetic data is independent of your ML model, but statistically &quot;close&quot; to your data. (see below for discussion of your alternative)</p>
<p>In essence, you are estimating the multivariate probability distribution associated with the process.  Once you have estimated the distribution, you can generate synthetic data through the Monte Carlo method or similar repeated sampling methods.  If your data resembles some parametric distribution (e.g. lognormal) then this approach is straightforward and reliable.  The tricky part is to estimate the dependence between variables. See: <a href=""https://www.encyclopediaofmath.org/index.php/Multi-dimensional_statistical_analysis"" rel=""nofollow noreferrer"">https://www.encyclopediaofmath.org/index.php/Multi-dimensional_statistical_analysis</a>.</p>
<p>If your data is irregular, then non-parametric methods are easier and probably more robust.  <a href=""https://en.wikipedia.org/wiki/Multivariate_kernel_density_estimation"" rel=""nofollow noreferrer"">Multivariate kernal density estimation</a> is a method that is accessible and appealing to people with ML background. For a general introduction and links to specific methods, see: <a href=""https://en.wikipedia.org/wiki/Nonparametric_statistics"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Nonparametric_statistics</a> .</p>
<p>To validate that this process worked for you, you go through the machine learning process again with the synthesized data, and you should end up with a model that is fairly close to your original.  Likewise, if you put the synthesized data into your ML model, you should get outputs that have similar distribution as your original outputs.</p>
<p>In contrast, you are proposing this:</p>
<blockquote>
<p>[original data --&gt; build machine learning model --&gt; use ml model to generate synthetic data....!!!]</p>
</blockquote>
<p>This accomplishes something different that the method I just described.  This would solve the <em><a href=""https://en.wikipedia.org/wiki/Inverse_problem#Conceptual_understanding"" rel=""nofollow noreferrer"">inverse problem</a></em>: &quot;what inputs could generate any given set of model outputs&quot;. Unless your ML model is over-fitted to your original data, this synthesized data <em>will not</em> look like your original data in every respect, or even most.</p>
<p>Consider a linear regression model. The same linear regression model can have identical fit to data that have very different characteristics. A famous demonstration of this is through <a href=""http://en.wikipedia.org/wiki/Anscombe%27s_quartet"" rel=""nofollow noreferrer"">Anscombe's quartet</a>.</p>
<p><img src=""https://i.stack.imgur.com/P9s4w.png"" alt=""All four sets are identical when examined using simple summary statistics, but vary considerably when graphed"" /></p>
<p>Thought I don't have references, I believe this problem can also arise in logistic regression, generalized linear models, SVM, and K-means clustering.</p>
<p>There are some ML model types (e.g. decision tree) where it's possible to inverse them to generate synthetic data, though it takes some work.  See: <a href=""http://library.cust.edu.pk/IEEEMagazineTransactions_2008/IEEE%20Internet%20Computing/May-June/Generating%20Synthetic%20Data%20to%20Match%20Data%20Mining%20Patterns%20%20Josh%20Eno,%20Pp.%2078-82.pdf"" rel=""nofollow noreferrer"">Generating Synthetic Data to Match Data Mining Patterns</a>.</p>
","<machine-learning><dataset>","31","31","31","0","609","1840"
"756","{
  ""id"": 756,
  ""title"": ""Meaning of latent features?""
}","<p>At the expense of over-simplication, latent features are 'hidden' features to distinguish them from observed features. Latent features are computed from observed features using matrix factorization. An example would be text document analysis. 'words' extracted from the documents are features. If you factorize the data of words you can find 'topics', where 'topic' is a group of words with semantic relevance. Low-rank matrix factorization maps several rows (observed features) to a smaller set of rows (latent features).
To elaborate, the document could have observed features (words) like [sail-boat, schooner, yatch, steamer, cruiser] which would 'factorize' to latent feature (topic) like 'ship' and 'boat'. </p>

<p>[sail-boat, schooner, yatch, steamer, cruiser, ...] -> [ship, boat]</p>

<p>The underlying idea is that latent features are semantically relevant 'aggregates' of observered features. When you have large-scale, high-dimensional, and noisy observered features, it makes sense to build your classifier on latent features.</p>

<p>This is a of course a simplified description to elucidate the concept. You can read the details on Latent Dirichlet Allocation (LDA) or probabilistic Latent Semantic Analysis (pLSA) models for an accurate description.</p>
","<machine-learning><data-mining><recommender-system>","31","31","31","0","2515","1203"
"5388","{
  ""id"": 5388,
  ""title"": ""General approach to extract key text from sentence (nlp)""
}","<p>Shallow <strong>N</strong>atural <strong>L</strong>anguage <strong>P</strong>rocessing technique can be used to extract concepts from sentence.</p>
<p><strong>-------------------------------------------</strong></p>
<p><strong>Shallow NLP technique steps:</strong></p>
<ol>
<li><p>Convert the sentence to lowercase</p>
</li>
<li><p>Remove stopwords (these are common words found in a language. Words like for, very, and, of, are, etc, are common stop words)</p>
</li>
<li><p>Extract n-gram i.e., a contiguous sequence of n items from a given sequence of text (simply increasing n, model can be used to store more context)</p>
</li>
<li><p>Assign a syntactic label (noun, verb etc.)</p>
</li>
<li><p>Knowledge extraction from text through semantic/syntactic analysis approach i.e., try to retain words that hold higher weight in a sentence like Noun/Verb</p>
</li>
</ol>
<p><strong>-------------------------------------------</strong></p>
<p>Lets examine the results of applying the above steps to your given sentence <code>Complimentary gym access for two for the length of stay ($12 value per person per day)</code>.</p>
<p><strong>1-gram Results:</strong> gym, access, length, stay, value, person, day</p>
<pre><code>Summary of step 1 through 4 of shallow NLP:

1-gram          PoS_Tag   Stopword (Yes/No)?    PoS Tag Description
-------------------------------------------------------------------    
Complimentary   NNP                             Proper noun, singular
gym             NN                              Noun, singular or mass
access          NN                              Noun, singular or mass
for             IN         Yes                  Preposition or subordinating conjunction
two             CD                              Cardinal number
for             IN         Yes                  Preposition or subordinating conjunction
the             DT         Yes                  Determiner
length          NN                              Noun, singular or mass
of              IN         Yes                  Preposition or subordinating conjunction
stay            NN                              Noun, singular or mass
($12            CD                              Cardinal number
value           NN                              Noun, singular or mass
per             IN                              Preposition or subordinating conjunction
person          NN                              Noun, singular or mass
per             IN                              Preposition or subordinating conjunction
day)            NN                              Noun, singular or mass

Step 4: Retaining only the Noun/Verbs we end up with gym, access, length, stay, value, person, day
</code></pre>
<p>Lets increase n to store more context and remove stopwords.</p>
<p><strong>2-gram Results:</strong> complimentary gym, gym access, length stay, stay value</p>
<pre><code>Summary of step 1 through 4 of shallow NLP:

2-gram              Pos Tag
---------------------------
access two          NN CD
complimentary gym   NNP NN
gym access          NN NN
length stay         NN NN
per day             IN NN
per person          IN NN
person per          NN IN
stay value          NN NN
two length          CD NN
value per           NN IN

Step 5: Retaining only the Noun/Verb combination we end up with complimentary gym, gym access, length stay, stay value
</code></pre>
<p><strong>3-gram Results:</strong> complimentary gym access, length stay value, person per day</p>
<pre><code>Summary of step 1 through 4 of shallow NLP:

3-gram                      Pos Tag
-------------------------------------
access two length           NN CD NN
complimentary gym access    NNP NN NN
gym access two              NN NN CD
length stay value           NN NN NN
per person per              IN NN IN
person per day              NN IN NN
stay value per              NN NN IN
two length stay             CD NN NN
value per person            NN IN NN


Step 5: Retaining only the Noun/Verb combination we end up with complimentary gym access, length stay value, person per day
</code></pre>
<p><strong>Things to remember:</strong></p>
<ul>
<li>Refer the <a href=""https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"" rel=""nofollow noreferrer"">Penn tree bank</a> to understand PoS tag description</li>
<li>Depending on your <em>data and the business context</em> you can decide the n value to extract n-grams from sentence</li>
<li>Adding domain specific stop words would increase the quality of concept/theme extraction</li>
<li>Deep NLP technique will give better results i.e., rather than n-gram, detect relationships within the sentences and represent/express as complex construction to retain the context. For additional info, see <a href=""https://stats.stackexchange.com/a/133680"">this</a></li>
</ul>
<p><strong>Tools:</strong></p>
<p>You can consider using OpenNLP / StanfordNLP for Part of Speech tagging. Most of the programming language have supporting library for OpenNLP/StanfordNLP. You can choose the language based on your comfort. Below is the sample R code I used for PoS tagging.</p>
<p><strong>Sample R code:</strong></p>
<pre><code>Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7') # for 32-bit version
library(rJava)
require(&quot;openNLP&quot;)
require(&quot;NLP&quot;)

s &lt;- paste(&quot;Complimentary gym access for two for the length of stay $12 value per person per day&quot;)

tagPOS &lt;-  function(x, ...) {
  s &lt;- as.String(x)
    word_token_annotator &lt;- Maxent_Word_Token_Annotator()
    a2 &lt;- Annotation(1L, &quot;sentence&quot;, 1L, nchar(s))
    a2 &lt;- annotate(s, word_token_annotator, a2)
    a3 &lt;- annotate(s, Maxent_POS_Tag_Annotator(), a2)
    a3w &lt;- a3[a3<span class=""math-container"">$type == ""word""]
    POStags &lt;- unlist(lapply(a3w$</span>features, `[[`, &quot;POS&quot;))
    POStagged &lt;- paste(sprintf(&quot;%s/%s&quot;, s[a3w], POStags), collapse = &quot; &quot;)
    list(POStagged = POStagged, POStags = POStags)
  }
  
  tagged_str &lt;-  tagPOS(s)
  tagged_str

#<span class=""math-container"">$POStagged
#[1] ""Complimentary/NNP gym/NN access/NN for/IN two/CD for/IN the/DT length/NN of/IN stay/NN $</span>/<span class=""math-container"">$ 12/CD value/NN per/IN     person/NN per/IN day/NN""
#
#$</span>POStags
#[1] &quot;NNP&quot; &quot;NN&quot;  &quot;NN&quot;  &quot;IN&quot;  &quot;CD&quot;  &quot;IN&quot;  &quot;DT&quot;  &quot;NN&quot;  &quot;IN&quot;  &quot;NN&quot;  &quot;$&quot;   &quot;CD&quot; 
#[13] &quot;NN&quot;  &quot;IN&quot;  &quot;NN&quot;  &quot;IN&quot;  &quot;NN&quot; 
</code></pre>
<p><strong>Additional readings on Shallow &amp; Deep NLP:</strong></p>
<ul>
<li><a href=""http://www.dfki.de/%7Efeiyu/WHIES.pdf"" rel=""nofollow noreferrer"">Integrating Shallow and Deep NLP for Information Extraction</a></li>
</ul>
","<machine-learning><nlp><text-mining><data-cleaning>","31","31","31","0","8465","686"
"18415","{
  ""id"": 18415,
  ""title"": ""Are there any rules for choosing the size of a mini-batch?""
}","<p>In <a href=""https://arxiv.org/abs/1609.04836"" rel=""noreferrer"">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</a> there are a couple of intersting statements:</p>

<blockquote>
  <p>It has been observed in practice that
  when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize [...]</p>
  
  <p>large-batch methods tend to converge to sharp minimizers of the
  training and testing functions—and as is well known, sharp minima lead to poorer
  generalization. n. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation.</p>
</blockquote>

<p>From <a href=""https://arxiv.org/pdf/1707.09725.pdf#page=73"" rel=""noreferrer"">my masters thesis</a>: Hence the choice of the mini-batch size influences:</p>

<ul>
<li><strong>Training time until convergence</strong>: There seems to be a sweet spot. If the batch size is very small (e.g. 8), this time goes up. If the batch size is huge, it is also higher than the minimum.</li>
<li><strong>Training time per epoch</strong>: Bigger computes faster (is efficient)</li>
<li><strong>Resulting model quality</strong>: The lower the better due to better generalization (?)</li>
</ul>

<p>It is important to note <strong>hyper-parameter interactions</strong>: Batch size may interact with other hyper-parameters, most notably learning rate. In some experiments this interaction may make it hard to isolate the effect of batch size alone on model quality. Another strong interaction is with early stopping for regularisation.</p>

<h2>See also</h2>

<ul>
<li><a href=""https://stats.stackexchange.com/a/236393/25741"">this nice answer / related question</a></li>
<li><a href=""https://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf"" rel=""noreferrer"">Efficient Mini-batch Training for Stochastic Optimization</a></li>
<li><a href=""https://svail.github.io/rnn_perf/"" rel=""noreferrer"">this RNN study</a></li>
</ul>
","<deep-learning><neural-network><convolutional-neural-network><optimization>","31","31","31","0","8820","16400"
"10191","{
  ""id"": 10191,
  ""title"": ""Why do cost functions use the square error?""
}","<p>The 1/2 coefficient is merely for convenience; it makes the derivative, which is the function actually being optimized, look nicer. The 1/m is more fundamental; it suggests that we are interested in the <em>mean</em> squared error. This allows you to make fair comparisons when changing the sample size, and prevents overflow. So called ""stochastic"" optimizers use a subset of the data set (m' &lt; m). When you introduce a regularizer (an additive term to the objective function), using the 1/m factor allows you to use the same coefficient for the regularizer regardless of the sample size.</p>

<p>As for the question of why the square and not simply the difference: don't you want underestimates to be penalized similarly to overestimates? Squaring eliminates the effect of the sign of the error. Taking the absolute value (L1 norm) does too, but its derivative is undefined at the origin, so it requires more sophistication to use. The L1 norm has its uses, so keep it in mind, and perhaps ask the teacher if (s)he's going to cover it.</p>
","<machine-learning><linear-regression><loss-function>","31","31","31","0","381","10223"
"998","{
  ""id"": 998,
  ""title"": ""Does scikit-learn have a forward selection/stepwise regression algorithm?""
}","<p>No, scikit-learn does not seem to have a forward selection algorithm. However, it does provide recursive feature elimination, which is a greedy feature elimination algorithm similar to sequential backward selection. See the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"" rel=""nofollow noreferrer"">documentation here</a></p>
","<feature-selection><scikit-learn>","31","31","31","0","2969","611"
"47416","{
  ""id"": 47416,
  ""title"": ""What to set in steps_per_epoch in Keras' fit_generator?""
}","<p>As mentioned in Keras' <a href=""https://keras.io/models/sequential/#fit_generator"" rel=""nofollow noreferrer"">webpage</a> about <code>fit_generator()</code>:</p>
<blockquote>
<p><em><strong>steps_per_epoch</strong></em>: Integer. Total number of steps (batches of samples)
to yield from generator <strong>before declaring one epoch finished</strong> and
starting the next epoch. It should typically be equal to
<strong>ceil(num_samples / batch_size)</strong>. Optional for Sequence: if unspecified,
will use the len(generator) as a number of steps.</p>
</blockquote>
<p>You can set it equal to <code>num_samples // batch_size</code>, which is a typical choice.</p>
<p>However, <code>steps_per_epoch</code> give you the chance to &quot;trick&quot; the generator when updating the learning rate using <code>ReduceLROnPlateau()</code> <a href=""https://keras.io/callbacks/#reducelronplateau"" rel=""nofollow noreferrer"">callback</a>, because this callback checks the drop of the loss once each epoch has finished. If the loss has stagnated for a <code>patience</code> number of consecutive epochs, the callback decreases the learning rate to &quot;slow-cook&quot; the network. If your dataset is huge, as it is usually the case when you need to use generators, you would probably like to decay the learning rate within a single epoch (since it includes a big number of data). This can be achieved by setting <code>steps_per_epoch</code> to a value that is <strong>less than</strong> <code>num_samples // batch_size</code> without affecting the overall number of training epochs of your model.</p>
<p>Imagine this case as using mini-epochs within your normal epochs to change the learning rate because your loss has stagnated. I have found it very useful <a href=""https://github.com/pcko1/Deep-Drug-Coder/blob/master/ddc_pub/ddc_v3.py#L1008-L1011"" rel=""nofollow noreferrer"">in my applications</a>.</p>
<p> </p>
","<keras><epochs>","31","31","31","0","52089","3515"
"13575","{
  ""id"": 13575,
  ""title"": ""Ways to deal with longitude/latitude feature""
}","<p>Lat long coordinates have a problem that they are 2 features that represent a three dimensional space. This means that the long coordinate goes all around, which means the two most extreme values are actually very close together. I've dealt with this problem a few times and what I do in this case is map them to x, y and z coordinates. This means close points in these 3 dimensions are also close in reality. Depending on the use case you can disregard the changes in height and map them to a perfect sphere. These features can then be standardized properly.</p>

<p>To clarify (summarised from the comments):</p>

<pre><code>x = cos(lat) * cos(lon)
y = cos(lat) * sin(lon), 
z = sin(lat) 
</code></pre>
","<machine-learning><python><feature-engineering><feature-scaling><normalization>","31","31","31","0","14904","8748"
"581","{
  ""id"": 581,
  ""title"": ""K-Means clustering for mixed numeric and categorical data""
}","<p>In my opinion, there are solutions to deal with categorical data in clustering. R comes with a specific distance for categorical data. This distance is called <a href=""http://www.rdocumentation.org/packages/StatMatch/versions/1.2.0/topics/gower.dist"" rel=""noreferrer"">Gower</a> and it works pretty well.</p>
","<data-mining><clustering><octave><k-means><categorical-data>","31","31","31","0","1155","563"
"19251","{
  ""id"": 19251,
  ""title"": ""Paper: What's the difference between Layer Normalization, Recurrent Batch Normalization (2016), and Batch Normalized RNN (2015)?""
}","<ul>
<li><p><strong>Layer normalization</strong> (<em>Ba 2016</em>): Does not use batch statistics. Normalize using the statistics collected from all units within a layer of the current sample. Does not work well with ConvNets.</p></li>
<li><p><strong>Recurrent Batch Normalization (BN)</strong> (<em>Cooijmans, 2016</em>; also proposed concurrently by <a href=""https://arxiv.org/abs/1604.03640"" rel=""noreferrer""><em>Qianli Liao &amp; Tomaso Poggio</em></a>, but tested on Recurrent ConvNets, instead of RNN/LSTM): Same as batch normalization. Use different normalization statistics for each time step. You need to store a set of mean and standard deviation for each time step.</p></li>
<li><p><strong>Batch Normalized Recurrent Neural Networks</strong> (<em>Laurent, 2015</em>): batch normalization is only applied between the input and hidden state, but not between hidden states. i.e., normalization is not applied over time.</p></li>
<li><p><strong><a href=""https://arxiv.org/abs/1610.06160"" rel=""noreferrer"">Streaming Normalization</a></strong> (<em>Liao et al. 2016</em>) : it summarizes existing normalizations and overcomes most issues mentioned above. It works well with ConvNets, recurrent learning and online learning (i.e., small mini-batch or one sample at a time): </p></li>
<li><p><strong>Weight Normalization</strong> (<em>Salimans and Kingma 2016</em>): whenever a weight is used, it is divided by its $L2$ norm first, such that the resulting weight has $L2$ norm $1$. That is, output $y = x*(w/|w|)$, where $x$ and $w$ denote the input and weight respectively. A scalar scaling factor $g$ is then multiplied to the output $y = y*g$. But in my experience $g$ seems not essential for performance (also downstream learnable layers can learn this anyway). </p></li>
<li><p><strong>Cosine Normalization</strong> (<em>Luo et al. 2017</em>): weight normalization is very similar to cosine normalization, where the same $L2$ normalization is applied to both weight and input: $y = (x/|x|)*(w/|w|)$. Again, manual or automatic differentiation can compute appropriate gradients of $x$ and $w$. </p></li>
</ul>

<p>Note that both <em>Weight</em> and <em>Cosine Normalization</em> have been extensively used (called normalized dot product) in the 2000s in a class of ConvNets called HMAX (Riesenhuber 1999) to model biological vision. You may find them interesting.</p>

<p>Ref: <a href=""http://maxlab.neuro.georgetown.edu/hmax.html"" rel=""noreferrer"">The HMAX Model Reference</a></p>

<p>Ref: <a href=""http://cbcl.mit.edu/jmutch/cns/"" rel=""noreferrer"">Cortical Network Simulator Reference</a></p>

<p>Ref: <a href=""https://arxiv.org/pdf/1702.05870.pdf"" rel=""noreferrer"">Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks</a>,  Luo Chunjie, Zhan jianfeng, Wang lei, Yang Qiang</p>
","<deep-learning><rnn><normalization><batch-normalization>","30","30","30","0","32711","416"
"742","{
  ""id"": 742,
  ""title"": ""Starting my career as Data Scientist, is Software Engineering experience required?""
}","<p>1) I think that there's no need to question whether your background is adequate for a career in data science. CS degree IMHO is <strong>more than enough</strong> for data scientist from software engineering point of view. Having said that, theoretical knowledge is not very helpful without matching <strong>practical experience</strong>, so I would definitely try to <strong>enrich</strong> my experience through participating in <em>additional school projects, internships or open source projects</em> (maybe ones, focused on data science / machine learning / artificial intelligence).</p>

<p>2) I believe your concern about <strong>focusing</strong> on data science <strong>too early</strong> is unfounded, as long as you will be practicing software engineering either as a part of your data science job, or additionally in your spare time.</p>

<p>3) I find the following <strong>definition of a data scientist</strong> rather accurate and hope it will be helpful in your future career success:</p>

<blockquote>
  <p>A <em>data scientist</em> is someone who is better at statistics than any
  software engineer and better at software engineering than any
  statistician.</p>
</blockquote>

<p>P.S. Today's <strong>enormous</strong> number of various resources on data science topics is mind-blowing, but this <strong>open source curriculum for learning data science</strong> might fill some gaps between your BSc/MSc respective curricula and reality of the data science career (or, at least, provide some direction for further research and maybe answer some of your concerns): <a href=""http://datasciencemasters.org"">http://datasciencemasters.org</a>, or on GitHub: <a href=""https://github.com/datasciencemasters/go"">https://github.com/datasciencemasters/go</a>.</p>
","<education><definitions><career>","30","30","30","0","2452","6478"
"17034","{
  ""id"": 17034,
  ""title"": ""RNN's with multiple features""
}","<p>Recurrent neural networks (RNNs) are designed to learn sequence data. As you guess, they can definitely take multiple features as input! Keras' RNNs take 2D inputs (<em>T</em>, <em>F</em>) of timesteps <em>T</em> and features <em>F</em> (I'm ignoring the batch dimension here).</p>

<p>However, you don't always need or want the intermediate timesteps, <em>t</em> = 1, 2 ... (<em>T</em> - 1). Therefore, Keras flexibly supports both modes. To have it output all <em>T</em> timesteps, pass <code>return_sequences=True</code> to your RNN (e.g., <code>LSTM</code> or <code>GRU</code>) at construction. If you only want the last timestep <em>t</em> = <em>T</em>, then use <code>return_sequences=False</code> (this is the default if you don't pass <code>return_sequences</code> to the constructor).</p>

<p>Below are examples of both of these modes.</p>

<h2>Example 1: Learning the sequence</h2>

<p>Here's a quick example of training a LSTM (type of RNN) which keeps the entire sequence around. In this example, each input data point has 2 timesteps, each with 3 features; the output data has 2 timesteps (because <code>return_sequences=True</code>), each with 4 data points (because that is the size I pass to <code>LSTM</code>).</p>

<pre><code>import keras.layers as L
import keras.models as M

import numpy

# The inputs to the model.
# We will create two data points, just for the example.
data_x = numpy.array([
    # Datapoint 1
    [
        # Input features at timestep 1
        [1, 2, 3],
        # Input features at timestep 2
        [4, 5, 6]
    ],
    # Datapoint 2
    [
        # Features at timestep 1
        [7, 8, 9],
        # Features at timestep 2
        [10, 11, 12]
    ]
])

# The desired model outputs.
# We will create two data points, just for the example.
data_y = numpy.array([
    # Datapoint 1
    [
        # Target features at timestep 1
        [101, 102, 103, 104],
        # Target features at timestep 2
        [105, 106, 107, 108]
    ],
    # Datapoint 2
    [
        # Target features at timestep 1
        [201, 202, 203, 204],
        # Target features at timestep 2
        [205, 206, 207, 208]
    ]
])

# Each input data point has 2 timesteps, each with 3 features.
# So the input shape (excluding batch_size) is (2, 3), which
# matches the shape of each data point in data_x above.
model_input = L.Input(shape=(2, 3))

# This RNN will return timesteps with 4 features each.
# Because return_sequences=True, it will output 2 timesteps, each
# with 4 features. So the output shape (excluding batch size) is
# (2, 4), which matches the shape of each data point in data_y above.
model_output = L.LSTM(4, return_sequences=True)(model_input)

# Create the model.
model = M.Model(input=model_input, output=model_output)

# You need to pick appropriate loss/optimizers for your problem.
# I'm just using these to make the example compile.
model.compile('sgd', 'mean_squared_error')

# Train
model.fit(data_x, data_y)
</code></pre>

<h2>Example 2: Learning the last timestep</h2>

<p>If, on the other hand, you want to train an LSTM which only outputs the last timestep in the sequence, then you need to set <code>return_sequences=False</code> (or just remove it from the constructor entirely, since <code>False</code> is the default). And then your output data (<code>data_y</code> in the example above) needs to be rearranged, since you only need to supply the last timestep. So in this second example, each input data point still has 2 timesteps, each with 3 features. The output data, however, is just a single vector for each data point, because we have flattened everything down to a single timestep. Each of these output vectors still has 4 features, though (because that is the size I pass to <code>LSTM</code>).</p>

<pre><code>import keras.layers as L
import keras.models as M

import numpy

# The inputs to the model.
# We will create two data points, just for the example.
data_x = numpy.array([
    # Datapoint 1
    [
        # Input features at timestep 1
        [1, 2, 3],
        # Input features at timestep 2
        [4, 5, 6]
    ],
    # Datapoint 2
    [
        # Features at timestep 1
        [7, 8, 9],
        # Features at timestep 2
        [10, 11, 12]
    ]
])

# The desired model outputs.
# We will create two data points, just for the example.
data_y = numpy.array([
    # Datapoint 1
    # Target features at timestep 2
    [105, 106, 107, 108],
    # Datapoint 2
    # Target features at timestep 2
    [205, 206, 207, 208]
])

# Each input data point has 2 timesteps, each with 3 features.
# So the input shape (excluding batch_size) is (2, 3), which
# matches the shape of each data point in data_x above.
model_input = L.Input(shape=(2, 3))

# This RNN will return timesteps with 4 features each.
# Because return_sequences=False, it will output 2 timesteps, each
# with 4 features. So the output shape (excluding batch size) is
# (2, 4), which matches the shape of each data point in data_y above.
model_output = L.LSTM(4, return_sequences=False)(model_input)

# Create the model.
model = M.Model(input=model_input, output=model_output)

# You need to pick appropriate loss/optimizers for your problem.
# I'm just using these to make the example compile.
model.compile('sgd', 'mean_squared_error')

# Train
model.fit(data_x, data_y)
</code></pre>
","<machine-learning><neural-network><keras>","30","30","30","0","29083","1026"
"12971","{
  ""id"": 12971,
  ""title"": ""Time series prediction using ARIMA vs LSTM""
}","<p><strong>Statement 1 is correct, statement 2 is correct, but requires elaboration, and statement 3 is incorrect for seasonal ARIMA:</strong></p>

<p>The following might point you in the right direction but hopefully you'll get a few more answers with more depth in the arena of LSTM.</p>

<p>You mention that you have tried both algorithms and that you are simply trying to figure out which one is better, which leads me to think you may be having more trouble with the data science process and cross validation than with the specifics of the models.</p>

<p><strong>Time series in general:</strong></p>

<p>Time series, in general, are difficult to forecast. If they were easy to forecast then all data scientists would be wealthy, having accurately forecast the value of all of the stocks. The reality is that hedge funds, on average, do not outperform the market and that time series forecasting is typically very poor and applies only to very short durations.  The main problems are that there is a lot of noise, there are many hidden influences, models are overly simplistic, influencers do not behave as we think they should, the interplay between linearity and nonlinearity is subtle and confusing, ... ad infinitum. </p>

<p><strong>ARIMA</strong></p>

<p>You are incorrect in your assessment that ARIMA requires stationary time series to forecast on.  <a href=""http://www.slideshare.net/21_venkat/arima-26196965"" rel=""noreferrer"">Non-seasonal ARIMA has three input values to help control for smoothing, stationarity, and forecasting</a> ARIMA(p,d,q), where:</p>

<ul>
<li>p is the number of autoregressive terms,</li>
<li>d is the number of nonseasonal differences needed for stationarity, and</li>
<li>q is the number of lagged forecast errors in the prediction equation.</li>
</ul>

<p>By contrast <a href=""http://people.duke.edu/~rnau/411arim.htm#pdq"" rel=""noreferrer"">seasonal ARIMA has six input values</a> ARIMA(p,d,q,P,D,Q), where:</p>

<ul>
<li>P is the number of seasonal autoregressive terms, </li>
<li>D is the number of seasonal differences, and </li>
<li>Q is the number of seasonal moving-average
terms.</li>
</ul>

<p>Subject to the qualifying statements above, I suggest playing with seasonal ARIMA to get a feel for the intricacies involved in smoothing, de-seasoning, de-trending, de-noiseing, and forecasting.</p>

<p><strong>LSTM</strong></p>

<p>I don't know enough about LSTM to add much here. I will add that red flags tend to be raised when someone begins at data science exercise with deep learning. I suggest learning as much as you can using ARIMA and then applying some of your ARIMA expertise to help you learn LSTM. Neural networks can be a very powerful tool, but they:</p>

<ul>
<li>can take a long time to run,</li>
<li>often require more data to train than other models, and</li>
<li>have lots of input parameters to tune.</li>
</ul>

<p><strong>Cross validation and comparing models:</strong></p>

<p>Time series are fun in that all training data can usually be turned into supervised learning training sets.  Once can simply take a time series and roll back time. That is... pick a point in time and pretend that you don't have any additional data, then produce a forecast and see how well you did.  You can <a href=""https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection"">march through the time series doing this $n$ times in order to get an assessment of the performance of your model</a> and to compare models while taking the <a href=""http://www.stat.tugraz.at/AJS/ausg083+4/08304Kunst.pdf"" rel=""noreferrer"">necessary precautions</a> to <a href=""http://robjhyndman.com/hyndsight/tscvexample/"" rel=""noreferrer"">prevent overfitting</a>.</p>

<p>Hope this helps and good luck!</p>
","<time-series><deep-learning><rnn><prediction>","30","30","30","0","9420","6528"
"43266","{
  ""id"": 43266,
  ""title"": ""Validation loss is not decreasing""
}","<p>The <strong>model is overfitting</strong> right from epoch 10, the validation loss is increasing while the training loss is decreasing.</p>
<p><strong>Dealing with such a Model:</strong></p>
<ol>
<li><strong>Data Preprocessing:</strong> Standardizing and Normalizing the data.</li>
<li><strong>Model compelxity:</strong> Check if the model is too complex. Add dropout, reduce number of layers or number of neurons in each layer.</li>
<li><strong>Learning Rate and Decay Rate:</strong> Reduce the learning rate, a good starting value is usually between 0.0005 to 0.001. Also consider a decay rate of 1e-6.</li>
</ol>
<p>There are many other options as well to reduce overfitting, assuming you are using Keras, visit <a href=""https://keras.io/callbacks/"" rel=""noreferrer"">this link</a>.</p>
","<machine-learning><neural-network><regression><lstm><rnn>","30","30","30","0","45385","495"
"24490","{
  ""id"": 24490,
  ""title"": ""In supervised learning, why is it bad to have correlated features?""
}","<p>(Assuming you are talking about supervised learning)</p>

<p>Correlated features will not always worsen your model, but they will not always improve it either.</p>

<p>There are three main reasons why you would remove correlated features:</p>

<ul>
<li>Make the learning algorithm faster</li>
</ul>

<p>Due to the curse of dimensionality, less features usually mean high improvement in terms of speed.</p>

<p>If speed is not an issue, perhaps don't remove these features right away (see next point)</p>

<ul>
<li>Decrease harmful bias</li>
</ul>

<p>The keyword being harmful. If you have correlated features but they are also correlated to the target, you want to keep them. You can view features as hints to make a good guess, if you have two hints that are essentially the same, but they are good hints, it may be wise to keep them.</p>

<p>Some algorithms like Naive Bayes actually directly benefit from ""positive"" correlated features. And others like random forest may indirectly benefit from them.</p>

<p>Imagine having 3 features A, B, and C. A and B are highly correlated to the target and to each other, and C isn't at all. If you sample out of the 3 features, you have 2/3 chance to get a ""good"" feature, whereas if you remove B for instance, this chance drops to 1/2</p>

<p>Of course, if the features that are correlated are not super informative in the first place, the algorithm may not suffer much.</p>

<p>So moral of the story, removing these features might be necessary due to speed, but remember that you might make your algorithm worse in the process. Also, some algorithms like decision trees have feature selection embedded in them.</p>

<p>A good way to deal with this is to use a wrapper method for feature selection. It will remove redundant features only if they do not contribute directly to the performance. If they are useful like in naive bayes, they will be kept. (Though remember that wrapper methods are expensive and may lead to overfitting)</p>

<ul>
<li>Interpretability of your model</li>
</ul>

<p>If your model needs to be interpretable, you might be forced to make it simpler. Make sure to also remember Occam's razor. If your model is not ""that much"" worse with less features, then you should probably use less features.</p>
","<machine-learning><correlation>","30","30","30","0","38887","4891"
"8252","{
  ""id"": 8252,
  ""title"": ""What makes columnar databases suitable for data science?""
}","<p>A column-oriented database (=columnar data-store) stores the data of a table column by column on the disk, while a row-oriented database stores the data of a table row by row.</p>

<p>There are two main advantages of using a column-oriented database in comparison
with a row-oriented database. The first advantage relates to the amount of data one’s
need to read in case we perform an operation on just a few features. Consider a simple
query:</p>

<pre><code>SELECT correlation(feature2, feature5)
FROM records
</code></pre>

<p>A traditional executor would read the entire table (i.e. all the features):</p>

<p><a href=""https://i.stack.imgur.com/4EZHo.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4EZHo.png"" alt=""enter image description here""></a></p>

<p>Instead, using our column-based approach we just have to read the columns
which are interested in:</p>

<p><a href=""https://i.stack.imgur.com/nGTKQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nGTKQ.png"" alt=""enter image description here""></a></p>

<p>The second advantage, which is also very important for large databases, is that column-based storage allows better compression, since the data in
one specific column is indeed homogeneous than across all the columns.</p>

<p>The main drawback of a column-oriented approach is that manipulating (lookup, update or delete) an entire given row is inefficient. However the situation should
occur rarely in databases for analytics (“warehousing”),
which means most operations are read-only, rarely read many attributes in the same
table and writes are only appends.</p>

<p>Some RDMS offer a column-oriented storage engine option. For example, PostgreSQL
has natively no option to store tables in a column-based fashion, but Greenplum has
created a closed-source one (DBMS2, 2009). Interestingly, Greenplum is also behind
the open-source library for scalable in-database analytics, MADlib (Hellerstein et al.,
2012), which is no coincidence. More recently, CitusDB, a startup working on high speed, analytic database, released their own open-source columnar store extension for
PostgreSQL, CSTORE (Miller, 2014). Google’s system for large scale machine learning
Sibyl also uses column-oriented data format (Chandra et al., 2010). This trend
reflects the growing interest around column-oriented storage for large-scale analytics.
Stonebraker et al. (2005) further discuss the advantages of column-oriented DBMS.</p>

<p>Two concrete use cases: <a href=""http://qr.ae/RoQxgp"" rel=""noreferrer"">How are most datasets for large-scale machine learning stored?</a></p>

<p>(most of the answer comes from Appendix C of: <a href=""http://francky.me/doc/2015beatdbwaveletsgp.pdf"" rel=""noreferrer"">BeatDB: An end-to-end approach to unveil saliencies from massive signal data sets. Franck Dernoncourt, S.M, thesis, MIT Dept of EECS</a>)</p>
","<databases><tools>","29","29","29","0","843","5095"
"52487","{
  ""id"": 52487,
  ""title"": ""Rationale behind most published works in medical imaging trying to reduce false positives""
}","<p>TL;DR: diseases are rare, so the absolute number of false positives is a lot more than that of false negatives.</p>

<p>Let's assume that our system has the same false positive and false negative rate of 1% (pretty good!), and that we're detecting the presence of new cancers this year: 439.2 / 100,000 people, or 0.5% of the population. [<a href=""https://www.cancer.gov/about-cancer/understanding/statistics"" rel=""noreferrer"">source</a>]</p>

<ul>
<li>No cancer, no detection: 99.5% x 99% = 98.5% (98.505%)</li>
<li>No cancer, detection: 99.5% x 1% = 1.0% (0.995%)</li>
<li>Cancer, detection: 0.5% x 99% = 0.5% (0.495%)</li>
<li>Cancer, no detection: 0.5% x 1% = 0.005%</li>
</ul>

<p>So we can see that we have a problem: for everyone who has cancer, two people who didn't have cancer wind up with invasive surgery, chemotherapy or radiotherapy. </p>

<p>For every person who fails to have a present cancer detected, two hundred people receive actively harmful treatment they didn't need and can't really afford.</p>
","<image-classification><image-recognition>","29","29","29","0","74745","391"
"12763","{
  ""id"": 12763,
  ""title"": ""Should a model be re-trained if new observations are available?""
}","<ol>
<li>Once a model is trained and you get new data which can be used for training, you can load the previous model and train onto it. For example, you can save your model as a <code>.pickle</code> file and load it and train further onto it when new data is available. Do note that for the model to predict correctly, <code>the new training data should have a similar distribution as the past data</code>.</li>
<li>Predictions tend to degrade based on the dataset you are using. For example, if you are trying to train using twitter data and you have collected data regarding a product which is widely tweeted that day. But if you use use tweets after some days when that product is not even discussed, it might be biased. <code>The frequency will be dependent on dataset</code> and there is no specific time to state as such. <code>If you observe that your new incoming data is deviating vastly, then it is a good practise to retrain the model</code>.</li>
<li>Optimizing parameters on the aggregated data is not overfitting. Large data doesn't imply overfitting. Use cross validation to check for over-fitting.</li>
</ol>
","<machine-learning><predictive-modeling><optimization><training>","29","29","29","0","21024","2246"
"27067","{
  ""id"": 27067,
  ""title"": ""Number of parameters in an LSTM model""
}","<p>Following previous answers,
The number of parameters of LSTM, taking input vectors of size $m$ and giving output vectors of size $n$ is:</p>

<p>$$4(nm+n^2)$$</p>

<p>However in case your LSTM includes bias vectors, (<a href=""https://keras.io/layers/recurrent/#lstm"" rel=""noreferrer"">this is the default in keras for example</a>), the number becomes:</p>

<p>$$4(nm+n^2 + n)$$</p>
","<deep-learning><rnn>","29","29","29","0","45217","988"
"26945","{
  ""id"": 26945,
  ""title"": ""What exactly is bootstrapping in reinforcement learning?""
}","<p>Bootstrapping in RL can be read as ""using one or more estimated values in the update step <em>for the same kind</em> of estimated value"". </p>

<p>In most TD update rules, you will see something like this SARSA(0) update:</p>

<p><span class=""math-container"">$$Q(s,a) \leftarrow Q(s,a) + \alpha(R_{t+1} + \gamma Q(s',a') - Q(s,a))$$</span></p>

<p>The value <span class=""math-container"">$R_{t+1} + \gamma Q(s',a')$</span> is an estimate for the true value of <span class=""math-container"">$Q(s,a)$</span>, and also called the TD target. It is a bootstrap method because we are in part using a Q value to update another Q value. There is a small amount of real observed data in the form of <span class=""math-container"">$R_{t+1}$</span>, the immediate reward for the step, and also in the state transition <span class=""math-container"">$s \rightarrow s'$</span>.</p>

<p>Contrast with Monte Carlo where the equivalent update rule might be:</p>

<p><span class=""math-container"">$$Q(s,a) \leftarrow Q(s,a) + \alpha(G_{t} - Q(s,a))$$</span></p>

<p>Where <span class=""math-container"">$G_{t}$</span> was the total discounted reward  at time <span class=""math-container"">$t$</span>, assuming in this update, that it started in state <span class=""math-container"">$s$</span>, taking action <span class=""math-container"">$a$</span>, then followed the current policy until the end of the episode. Technically, <span class=""math-container"">$G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}$</span> where <span class=""math-container"">$T$</span> is the time step for the terminal reward and state. Notably, this target value does not use any existing estimates (from other Q values) at all, it only uses a set of observations (i.e., rewards) from the environment. As such, it is guaranteed to be <em>unbiased</em> estimate of the true value of <span class=""math-container"">$Q(s,a)$</span>, as it is technically a <em>sample</em> of <span class=""math-container"">$Q(s,a)$</span>.</p>

<p>The main disadvantage of bootstrapping is that it is biased towards whatever your starting values of <span class=""math-container"">$Q(s',a')$</span> (or <span class=""math-container"">$V(s')$</span>) are. Those are are most likely wrong, and the update system can be unstable as a whole because of too much self-reference and not enough real data - this is a problem with off-policy learning (e.g. Q-learning) using neural networks.</p>

<p>Without bootstrapping, using longer trajectories, there is often <em>high variance</em> instead, which, in practice, means you need more samples before the estimates converge. So, despite the problems with bootstrapping, if it can be made to work, it may learn significantly faster, and is often preferred over Monte Carlo approaches.</p>

<p>You can compromise between Monte Carlo sample based methods and single-step TD methods that bootstrap by using a mix of results from different length trajectories. This is called <a href=""https://en.wikipedia.org/wiki/Temporal_difference_learning#TD-Lambda"" rel=""noreferrer"">TD(<span class=""math-container"">$\lambda$</span>) learning</a>, and there are a variety of specific methods such as SARSA(<span class=""math-container"">$\lambda$</span>) or Q(<span class=""math-container"">$\lambda$</span>).</p>
","<reinforcement-learning>","29","29","29","0","836","25808"
"45253","{
  ""id"": 45253,
  ""title"": ""When to use cosine simlarity over Euclidean similarity""
}","<blockquote>
  <p>When to use cosine similarity over Euclidean similarity</p>
</blockquote>

<p>Cosine similarity looks at the angle between two vectors, euclidian similarity at the distance between two points.</p>

<p>Let's say you are in an e-commerce setting and you want to compare users for product recommendations:</p>

<ul>
<li>User 1 bought 1x eggs, 1x flour and 1x sugar.</li>
<li>User 2 bought 100x eggs, 100x flour and 100x sugar</li>
<li>User 3 bought 1x eggs, 1x Vodka and 1x Red Bull</li>
</ul>

<p>By cosine similarity, user 1 and user 2 are more similar. By euclidean similarity, user 3 is more similar to user 1.</p>

<h2>Questions in the text</h2>

<p>I don't understand the first part.</p>

<blockquote>
  <p>Cosine similarity is specialized in handling scale/length effects. For case 1, context length is fixed -- 4 words, there's no scale effects. In terms of case 2, the term frequency matters, a word appears once is different from a word appears twice, we cannot apply cosine.</p>
</blockquote>

<p>This goes in the right direction, but is not completely true. For example:</p>

<p><span class=""math-container"">$$
\cos \left (\begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}2\\1\end{pmatrix} \right) = \cos \left (\begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}4\\2\end{pmatrix} \right) \neq \cos \left (\begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}5\\2\end{pmatrix} \right)
$$</span></p>

<p>With cosine similarity, the following is true:</p>

<p><span class=""math-container"">$$
\cos \left (\begin{pmatrix}a\\b\end{pmatrix}, \cdot \begin{pmatrix}c\\d\end{pmatrix} \right) = \cos \left (\begin{pmatrix}a\\b\end{pmatrix}, n \cdot \begin{pmatrix}c\\d\end{pmatrix} \right) \text{ with } n \in \mathbb{N}
$$</span></p>

<p>So frequencies are only ignored, if all features are multiplied with the same constant.</p>

<h2>Curse of Dimensionality</h2>

<p>When you look at the table of <a href=""https://martin-thoma.com/curse-of-dimensionality/"" rel=""noreferrer"">my blog post</a>, you can see:</p>

<ul>
<li>The more dimensions I have, the closer the average distance and the maximum distance between randomly placed points become.</li>
<li>Similarly, the average angle between uniformly randomly placed points becomes 90°.</li>
</ul>

<p>So both measures suffer from high dimensionality. More about this: <a href=""https://stats.stackexchange.com/q/341535/25741"">Curse of dimensionality - does cosine similarity work better and if so, why?</a>. A key point:</p>

<ul>
<li>Cosine is essentially the same as Euclidean on normalized data.</li>
</ul>

<h2>Alternatives</h2>

<p>You might be interested in metric learning. The principle is described/used in <em>FaceNet: A Unified Embedding for Face Recognition and Clustering</em> (<a href=""https://www.shortscience.org/paper?bibtexKey=journals/corr/1503.03832#martinthoma"" rel=""noreferrer"">my summary</a>). Instead of taking one of the well-defined and simple metrics. You can learn a metric for the problem domain.</p>
","<machine-learning><nlp><clustering><similarity>","29","29","29","0","8820","16400"
"27587","{
  ""id"": 27587,
  ""title"": ""Is a 100% model accuracy on out-of-sample data overfitting?""
}","<p>High validation scores like accuracy generally mean that you are not overfitting, however it should lead to caution and may indicate something went wrong. It could also mean that the problem is not too difficult and that your model truly performs well. Two things that could go wrong:</p>

<ul>
<li>You didn't split the data properly and the validation data also occured in your training data, meaning it does indicate overfitting because you are not measuring generalization anymore</li>
<li>You use some feature engineering to create additional features and you might have introduced some target leakage, where your rows are using information from it's current target, not just from others in your training set</li>
</ul>
","<r><random-forest><prediction><overfitting>","29","29","29","0","14904","8748"
"34359","{
  ""id"": 34359,
  ""title"": ""Why do people prefer Pandas to SQL?""
}","<p>First, pandas is not that much popular. I use both pandas and SQL. First I try to understand the task- if it can be done in SQL, I prefer SQL because it is more efficient than pandas. Try working on a large data (10,000,000 x 50). Try to do some <em>groupby</em> operation in both SQL and pandas. You will understand.</p>

<p>I use pandas where it comes handy- like splitting a column values into an array and doing some stuff on it (like choosing only some values out of that array). Now this kind of task is relatively hard to code in SQL, but pandas will ease your task.</p>
","<python><pandas><sql>","29","29","29","0","44139","1619"
"10358","{
  ""id"": 10358,
  ""title"": ""What is the Q function and what is the V function in reinforcement learning?""
}","<p>Q-values are a great way to the make actions explicit so  you can deal with problems where the transition function is not available (model-free). However, when your action-space is large, things are not so nice and Q-values are not so convenient. Think of a huge number of actions or even continuous action-spaces.</p>

<p>From a sampling perspective, the dimensionality of <span class=""math-container"">$Q(s, a)$</span> is higher than <span class=""math-container"">$V(s)$</span> so it might get harder to get enough <span class=""math-container"">$(s, a)$</span> samples in comparison with <span class=""math-container"">$(s)$</span>. If you have access to the transition function sometimes <span class=""math-container"">$V$</span> is good. </p>

<p>There are also other uses where both are combined. For instance, the advantage function where <span class=""math-container"">$A(s, a) = Q(s, a) - V(s)$</span>. If you are interested, you can find a recent example using advantage functions here:</p>

<blockquote>
  <p><a href=""http://arxiv.org/abs/1511.06581"" rel=""noreferrer"">Dueling Network Architectures for Deep Reinforcement Learning</a></p>
</blockquote>

<p>by Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot and Nando de Freitas.</p>
","<machine-learning><reinforcement-learning>","29","29","29","0","5041","959"
"30845","{
  ""id"": 30845,
  ""title"": ""after grouping to minimum value in pandas, how to display the matching row result entirely along min() value""
}","<p>In case this can help anyone else. Here is a solution that is more computationally efficient.</p>

<h2>TL;DR version</h2>

<p>If each row already has a unique index, then do this:</p>

<pre><code>&gt;&gt;&gt; df.loc[df.groupby('A')['C'].idxmin()]
</code></pre>

<p>If you've already indexed by 'A', then convert 'A' back into a column first.</p>

<pre><code>&gt;&gt;&gt; df2 = df.reset_index()
&gt;&gt;&gt; df2.loc[df2.groupby('A')['C'].idxmin()]
</code></pre>

<hr>

<p>Step by Step explanation:</p>

<h2>Step 1.</h2>

<p>First, make sure each row in your dataframe is uniquely indexed. This is the default when importing csv data. e.g.</p>

<pre><code>    &gt;&gt;&gt; df = pd.read_csv('questionData.csv'); df
        A       B       C
    0   196512  1325    12.901051
    1   196512  114569  12.926770
    2   196512  118910  12.898335
    3   196512  100688  12.950509
    4   196795  28978   12.780517
    5   196795  34591   12.899411
    6   196795  13078   12.913575
    7   196795  24173   12.876965
    8   196341  118910  12.898335
    9   196341  100688  12.950509
    10  196641  28972   12.780517
    11  196641  34591   12.899411
    12  196346  118910  12.898335
    13  196346  100688  12.950509
    14  196646  28980   12.780517
    15  196646  34591   12.899411
</code></pre>

<p>Aside: If you already converted column 'A' into an index, then you can turn the index back into a column (<a href=""https://stackoverflow.com/questions/20461165/how-to-convert-pandas-index-in-a-dataframe-to-a-column"">https://stackoverflow.com/questions/20461165/how-to-convert-pandas-index-in-a-dataframe-to-a-column</a>) by doing: <code>df.reset_index()</code></p>

<h2>Step 2.</h2>

<p>Use the <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.idxmin.html"" rel=""noreferrer"">pandas.DataFrame.idxmin</a> function to retrieve the indices of the minimum of each group.</p>

<p>The semantics of the example below is this: ""group by 'A', then just look at the 'C' column of each group, and finally return the index corresponding to the minimum 'C' in each group.</p>

<pre><code>&gt;&gt;&gt; indices = df.groupby('A')['C'].idxmin; indices
A
196341     8
196346    12
196512     2
196641    10
196646    14
196795     4
Name: C, dtype: int64
</code></pre>

<h2>Step 3.</h2>

<p>Finally, use the retrieved indices in the original dataframe using <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html"" rel=""noreferrer"">pandas.DataFrame.loc</a> to get the rows of the original dataframe correponding to the minimum values of 'C' in each group that was grouped by 'A'.</p>

<pre><code>&gt;&gt;&gt; df.loc[indices]
    A       B       C
8   196341  118910  12.898335
12  196346  118910  12.898335
2   196512  118910  12.898335
10  196641  28972   12.780517
14  196646  28980   12.780517
4   196795  28978   12.780517
</code></pre>

<p>Note: The groupby('A') operation returns groups sorted by A. Thus 'indices' is sorted by A. If we want the original order, we just have to do</p>

<pre><code>&gt;&gt;&gt; df.loc[indices].sort_index()
    A       B       C
2   196512  118910  12.898335
4   196795  28978   12.780517
8   196341  118910  12.898335
10  196641  28972   12.780517
12  196346  118910  12.898335
14  196646  28980   12.780517
</code></pre>
","<python><pandas><dataframe>","29","29","29","0","51153","391"
"17293","{
  ""id"": 17293,
  ""title"": ""XGBRegressor vs. xgboost.train huge speed difference?""
}","<p><code>xgboost.train</code> will ignore parameter <code>n_estimators</code>, while <code>xgboost.XGBRegressor</code> accepts. In <code>xgboost.train</code>, boosting iterations (i.e. <code>n_estimators</code>) is controlled by <code>num_boost_round</code>(default: 10)</p>

<p>In your case, the first code will do 10 iterations (by default), but the second one will do 1000 iterations. There won't be any big difference if you try to change <code>clf = xg.train(params, dmatrix)</code> into <code>clf = xg.train(params, dmatrix, 1000)</code>, </p>

<h2>References</h2>

<p><a href=""http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train"" rel=""noreferrer"">http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train</a></p>

<p><a href=""http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor"" rel=""noreferrer"">http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor</a></p>
","<machine-learning><python><decision-trees><xgboost><efficiency>","29","29","29","0","28628","3736"
"14234","{
  ""id"": 14234,
  ""title"": ""What is the difference between model hyperparameters and model parameters?""
}","<p>In addition to the answer above.</p>

<p><em>Model parameters</em> are the properties of the training data that are learnt during training by the classifier or other ml model. For example in case of some NLP task: word frequency, sentence length, noun or verb distribution per sentence, the number of specific character n-grams per word, lexical diversity, etc. <em>Model parameters</em> differ for each experiment and depend on the type of data and task at hand. </p>

<p><em>Model hyperparameters</em>, on the other hand, are common for similar models and cannot be learnt during training but are set beforehand. A typical set of hyperparameters for NN include the number and size of the hidden layers, weight initialization scheme, learning rate and its decay, dropout and gradient clipping threshold, etc. </p>
","<machine-learning><parameter><hyperparameter><language-model>","29","29","29","0","22012","1847"
"484","{
  ""id"": 484,
  ""title"": ""Is logistic regression actually a regression algorithm?""
}","<p>Logistic regression is regression, first and foremost. It becomes a classifier by adding a decision rule. I will give an example that goes backwards. That is, instead of taking data and fitting a model, I'm going to start with the model in order to show how this is truly a regression problem.</p>

<p>In logistic regression, we are modeling the log odds, or logit, that an event occurs, which is a continuous quantity. If the probability that event $A$ occurs is $P(A)$, the odds are:</p>

<p>$$\frac{P(A)}{1 - P(A)}$$</p>

<p>The log odds, then, are:</p>

<p>$$\log \left( \frac{P(A)}{1 - P(A)}\right)$$</p>

<p>As in linear regression, we model this with a linear combination of coefficients and predictors:</p>

<p>$$\operatorname{logit} = b_0 + b_1x_1 + b_2x_2 + \cdots$$</p>

<p>Imagine we are given a model of whether a person has gray hair. Our model uses age as the only predictor. Here, our event A = a person has gray hair:</p>

<p>log odds of gray hair = -10 + 0.25 * age</p>

<p>...Regression! Here is some Python code and a plot:</p>

<pre><code>%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

x = np.linspace(0, 100, 100)

def log_odds(x):
    return -10 + .25 * x

plt.plot(x, log_odds(x))
plt.xlabel(""age"")
plt.ylabel(""log odds of gray hair"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/xR0OT.png"" alt=""plot of the log odds for our toy example""></p>

<p>Now, let's make it a classifier. First, we need to transform the log odds to get out our probability $P(A)$. We can use the sigmoid function:</p>

<p>$$P(A) = \frac1{1 + \exp(-\text{log odds}))}$$</p>

<p>Here's the code:</p>

<pre><code>plt.plot(x, 1 / (1 + np.exp(-log_odds(x))))
plt.xlabel(""age"")
plt.ylabel(""probability of gray hair"")
</code></pre>

<p><img src=""https://i.stack.imgur.com/hSpCa.png"" alt=""plot of the probability of gray hair for our toy example""></p>

<p>The last thing we need to make this a classifier is to add a decision rule. One very common rule is to classify a success whenever $P(A) &gt; 0.5$. We will adopt that rule, which implies that our classifier will predict gray hair whenever a person is older than 40 and will predict non-gray hair whenever a person is under 40.</p>

<p>Logistic regression works great as a classifier in more realistic examples too, but before it can be a classifier, it must be a regression technique!</p>
","<algorithms><logistic-regression>","29","29","29","0","1011","1106"
"36087","{
  ""id"": 36087,
  ""title"": ""How to adjust the hyperparameters of MLP classifier to get more perfect performance""
}","<p>If you are using SKlearn, you can use their <a href=""http://scikit-learn.org/stable/modules/grid_search.html"" rel=""noreferrer"">hyper-parameter optimization</a> tools.</p>

<p>For example, you can use:</p>

<ul>
<li><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV"" rel=""noreferrer"">GridSearchCV</a></li>
<li><a href=""http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV"" rel=""noreferrer"">RandomizedSearchCV</a></li>
</ul>

<p>If you use <code>GridSearchCV</code>, you can do the following:</p>

<p>1) Choose your classifier</p>

<pre><code>from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(max_iter=100)
</code></pre>

<p>2) Define a hyper-parameter space to search. (All the values that you want to try out.)</p>

<pre><code>parameter_space = {
    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['constant','adaptive'],
}
</code></pre>

<p>Note: the <code>max_iter=100</code> that you defined on the initializer is not in the grid. So, that number will be constant, while the ones in the grid will be searched.</p>

<p>3) Run the search:</p>

<pre><code>from sklearn.model_selection import GridSearchCV

clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)
clf.fit(DEAP_x_train, DEAP_y_train)
</code></pre>

<p>Note: the parameter <code>n_jobs</code> is to define how many CPU cores from your computer to use (-1 is for all the cores available). The <code>cv</code> is the number of splits for cross-validation.</p>

<p>4) See the best results:</p>

<pre><code># Best paramete set
print('Best parameters found:\n', clf.best_params_)

# All results
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print(""%0.3f (+/-%0.03f) for %r"" % (mean, std * 2, params))
</code></pre>

<p>5) Now you can use the <code>clf</code> to make new predictions. For example, check the performance on your <code>test set</code>.</p>

<pre><code>y_true, y_pred = DEAP_y_test , clf.predict(DEAP_x_test)

from sklearn.metrics import classification_report
print('Results on the test set:')
print(classification_report(y_true, y_pred))
</code></pre>
","<scikit-learn><hyperparameter-tuning><mlp>","29","29","29","0","54395","2988"
"20230","{
  ""id"": 20230,
  ""title"": ""How to decide neural network architecture?""
}","<p>Sadly there is no generic way to determine <em>a priori</em> the best number of neurons and number of layers for a neural network, given just a problem description. There isn't even much guidance to be had determining good values to try as a starting point.</p>

<p>The most common approach seems to be to start with a rough guess based on prior experience about networks used on similar problems. This could be your own experience, or second/third-hand experience you have picked up from a training course, blog or research paper. Then try some variations, and check the performance carefully before picking a best one.</p>

<p>The size and depth of neural networks interact with other <em>hyper-paramaters</em> too, so that changing one thing elsewhere can affect where the best values are. So it is not possible to isolate a ""best"" size and depth for a network then continue to tune other parameters in isolation. For instance, if you have a very deep network, it may work efficiently with the ReLU activation function, but not so well with sigmoid - if you found the best size/shape of network and then tried an experiment with varying activation functions you may come to the wrong conclusion about what works best.</p>

<p>You may sometimes read about ""rules of thumb"" that researchers use when starting a neural network design from scratch. These things might work for your problems or not, but they at least have the advantage of making a start on the problem. The variations I have seen are:</p>

<ul>
<li><p>Create a network with hidden layers similar size order to the input, and all the same size, on the grounds that there is no particular reason to vary the size (unless you are creating an autoencoder perhaps).</p></li>
<li><p>Start simple and build up complexity to see what improves a simple network.</p></li>
<li><p>Try varying depths of network if you expect the output to be explained well by the input data, but with a complex relationship (as opposed to just inherently noisy).</p></li>
<li><p>Try adding some dropout, it's the closest thing neural networks have to magic fairy dust that makes everything better (caveat: adding dropout may improve generalisation, but may also increase required layer sizes and training times). </p></li>
</ul>

<p>If you read these or anything like them in any text, then take them with a pinch of salt. However, at worst they help you get past the blank page effect, and write some kind of network, and get you to start the testing and refinement process.</p>

<hr>

<p>As an aside, try not to get too lost in tuning a neural network when some other approach might be better and save you lots of time. Do consider and use other machine learning and data science approaches. Explore the data, maybe make some plots. Try some simple linear approaches first to get benchmarks to beat, linear regression, logistic regression or softmax regression depending on your problem. Consider using a different ML algorithm to NNs - decision tree based approaches such as XGBoost can be faster and more effective than deep learning on many problems.</p>
","<machine-learning><neural-network>","28","28","28","0","836","25808"
"18819","{
  ""id"": 18819,
  ""title"": ""What is a LB score in machine learning?""
}","<p>In the context of Kaggle, it means <strong>L</strong>eader<strong>B</strong>oard (emphasis mine).</p>
","<machine-learning><accuracy>","28","28","28","0","381","10223"
"52019","{
  ""id"": 52019,
  ""title"": ""What is the difference between semantic segmentation, object detection and instance segmentation?""
}","<ul>
<li><em><strong>Object Detection</strong></em> : is the technology that is related to computer vision and image processing. It's aim? detect objects in an image.</li>
<li><em><strong>Semantic Segmentation</strong></em> : is a technique that detects , for each pixel , the object category it belongs to , all object categories ( labels ) must be known to the model.</li>
<li><em><strong>Instance Segmentation</strong></em> : same as Semantic Segmentation, but dives a bit deeper, it identifies , for each pixel, the object instance it belongs to. The main difference is that differentiates two objects with the same labels in comparison to semantic segmentation.</li>
</ul>
<p>Here's an example of the main difference.</p>
<p><a href=""https://i.stack.imgur.com/MEB9F.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MEB9F.png"" alt=""Differences"" /></a></p>
<p>In the second image where <strong>Semantic Segmentation</strong> is applied, the <strong>category</strong> ( <strong>chair</strong> ) is one of the outputs, all chairs are colored the same. In the third image, the <strong>Instance Segmentation</strong>, goes a step further and separates the instances ( the chairs ) from one another apart from identifying the category ( chair ) in the first step.</p>
<p>Hope this clears it up for you a bit.</p>
","<computer-vision><object-detection>","28","28","28","0","69822","1794"
"10461","{
  ""id"": 10461,
  ""title"": ""Calculation and Visualization of Correlation Matrix with Pandas""
}","<p><strong>I suggest some sort of play on the following:</strong></p>

<p>Using the UCI Abalone data for this example...</p>

<pre><code>import matplotlib
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

# Read file into a Pandas dataframe
from pandas import DataFrame, read_csv
f = 'https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'
df = read_csv(f)
df=df[0:10]
df
</code></pre>

<p><a href=""https://i.stack.imgur.com/efITk.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/efITk.png"" alt=""enter image description here""></a></p>

<p><strong>Correlation matrix plotting function:</strong> </p>

<p># Correlation matric plotting function </p>

<pre><code>def correlation_matrix(df):
    from matplotlib import pyplot as plt
    from matplotlib import cm as cm

    fig = plt.figure()
    ax1 = fig.add_subplot(111)
    cmap = cm.get_cmap('jet', 30)
    cax = ax1.imshow(df.corr(), interpolation=""nearest"", cmap=cmap)
    ax1.grid(True)
    plt.title('Abalone Feature Correlation')
    labels=['Sex','Length','Diam','Height','Whole','Shucked','Viscera','Shell','Rings',]
    ax1.set_xticklabels(labels,fontsize=6)
    ax1.set_yticklabels(labels,fontsize=6)
    # Add colorbar, make sure to specify tick locations to match desired ticklabels
    fig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])
    plt.show()

correlation_matrix(df)
</code></pre>

<p><a href=""https://i.stack.imgur.com/YZsF8.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/YZsF8.png"" alt=""enter image description here""></a></p>

<p>Hope this helps!</p>
","<python><statistics><visualization><pandas>","28","30","29","1","9420","6528"
"47160","{
  ""id"": 47160,
  ""title"": ""Is Gradient Descent central to every optimizer?""
}","<p><strong>No.</strong> Gradient descent is used in optimization algorithms that use the gradient as the basis of its step movement. <code>Adam</code>, <code>Adagrad</code>, and <code>RMSProp</code> all use some form of gradient descent, however they do not make up <em>every</em> optimizer. Evolutionary algorithms such as <a href=""https://en.wikipedia.org/wiki/Particle_swarm_optimization"" rel=""noreferrer"">Particle Swarm Optimization</a> and <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""noreferrer"">Genetic Algorithms</a> are inspired by natural phenomena do not use gradients. Other algorithms, such as <a href=""https://en.wikipedia.org/wiki/Bayesian_optimization"" rel=""noreferrer"">Bayesian Optimization</a>, draw inspiration from statistics. </p>

<p>Check out this visualization of Bayesian Optimization in action: <a href=""https://i.stack.imgur.com/4klPs.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4klPs.gif"" alt=""Bayesian Optimization in action""></a></p>

<p>There are also a few algorithms that combine concepts from evolutionary and gradient-based optimization. </p>

<p>Non-derivative based optimization algorithms can be especially useful in irregular non-convex cost functions, non-differentiable cost functions, or cost functions that have a <a href=""https://en.wikipedia.org/wiki/Semi-differentiability"" rel=""noreferrer"">different left or right derivative</a>. </p>

<p>To understand why one may choose a non-derivative based optimization algorithm. Take a look at the <a href=""https://en.wikipedia.org/wiki/Rastrigin_function"" rel=""noreferrer"">Rastrigin benchmark function</a>. Gradient based optimization is not well suited for optimizing functions with so many local minima. </p>

<p><a href=""https://i.stack.imgur.com/w16DV.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/w16DV.png"" alt=""Rastrigin benchmark function""></a></p>
","<machine-learning><neural-network><deep-learning><optimization><gradient-descent>","28","28","28","0","69397","396"
"9430","{
  ""id"": 9430,
  ""title"": ""Can you explain the difference between SVC and LinearSVC in scikit-learn?""
}","<p>A regular SVM with default values uses a radial basis function as the SVM kernel.  This is basically a Gaussian kernel aka bell-curve.  Meaning that the no man's land between different classes is created with a Gaussian function.  The linear-SVM uses a linear kernel for the basis function, so you can think of this as a ^ shaped function.  It is much less tunable and is basically just a linear interpolation.</p>

<p>People are kind of hammering this question because you didn't provide a lot of information, but looking deeply into what you posted... this hits on some fundamental aspects of really understanding the details of bias and variance and the difference between linear and nonlinear basis functions in SVM.</p>

<p>Check out <a href=""http://blog.fliptop.com/blog/2015/03/02/bias-variance-and-overfitting-machine-learning-overview/"" rel=""noreferrer"">this image</a> describing the four regions of high and low bias and high and low variance.  Obviously the best place to be is  low variance and low bias.</p>

<p><a href=""https://i.stack.imgur.com/jyHqx.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jyHqx.png"" alt=""darts bias-variance image""></a></p>

<p><strong>First lets assess variance -</strong></p>

<p>Now take a look at your plots:
<a href=""https://i.stack.imgur.com/fIVsR.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fIVsR.png"" alt=""enter image description here""></a></p>

<p>The nonlinear basis function has higher variance.  See how it is noisier than the linear kernel!  The linear kernel has lower variance.  See how it is less noisy!  </p>

<p><strong>Now lets assess bias -</strong></p>

<p>Which kernel is more accurate?  We can add the errors that you provided.  The nonlinear kernel has a total error of ~550+325=~875.  The linear kernel has an error of ~690+~50=~740.  So the linear kernel <strong>seems</strong> to do better overall, but they are pretty close overall.  This is were things get tricky!  </p>

<p><strong>Putting it all together</strong></p>

<p>See how the linear kernel did a poor job on 1's and a really great job on 0's.  This is pretty unbalanced.  Where as the nonlinear kernel is more balanced.  It kind of seems like the sweet spot might be to create a balanced model that doesn't have such high variance.  How do we control for high variance?  Bingo - regularization.  We can add regularization to the nonlinear model and we will probably see much better results.  This is the C parameter in scikit learn SVMs, which you will want to increase from the default.  We could also play with the gamma parameter.  Gamma controlls the width of the Gaussian.  Maybe try increasing that one slightly to get less noisy results e.g. a larger no-man's land between classes.</p>

<p>Hope this helps!</p>
","<svm><scikit-learn>","28","28","28","0","9420","6528"
"19366","{
  ""id"": 19366,
  ""title"": ""LightGBM vs XGBoost""
}","<p>LightGBM is a great implementation that is similar to XGBoost but varies in a few specific ways, especially in how it creates the trees.</p>

<p>It offers some different parameters but most of them are very similar to their XGBoost counterparts.</p>

<p>If you use the same parameters, you almost always get a very close score. In most cases, the training will be 2-10 times faster though.</p>

<hr>

<p>Why don't more people use it then?</p>

<p>XGBoost has been around longer and is already installed on many machines. LightGBM is rather new and didn't have a Python wrapper at first. The current version is easier to install and use so no obstacles here.</p>

<p>Many of the more advanced users on Kaggle and similar sites already use LightGBM and for each new competition, it gets more and more coverage. Still, the starter scripts are often based around XGBoost as people just reuse their old code and adjust a few parameters. I'm sure this will increase once there are a few more tutorials and guides on how to use it (most of the non-ScikitLearn guides currently focus on XGBoost or neural networks).</p>
","<xgboost>","28","28","28","0","32873","396"
"2315","{
  ""id"": 2315,
  ""title"": ""Do Random Forest overfit?""
}","<p>Every ML algorithm with high complexity can overfit. However, the OP is asking whether an RF will not overfit when increasing the number of trees in the forest.</p>

<p>In general, ensemble methods reduces the prediction variance to almost nothing, improving the accuracy of the ensemble. If we define the variance of the expected generalization error of an individual randomized model as:</p>

<p><img src=""https://i.stack.imgur.com/ZUORL.gif"" alt=""""></p>

<p>From <a href=""http://arxiv.org/abs/1407.7502"" rel=""noreferrer"">here</a>, the variance of the expected generalization error of an ensemble corresponds to:</p>

<p><img src=""https://i.stack.imgur.com/5Zf9e.gif"" alt=""""></p>

<p>where <code>p(x)</code> is the Pearson’s correlation coefficient between the predictions of two randomized models trained on the same data from two independent seeds. If we increase the number of DT's in the RF, larger <code>M</code>, the variance of the ensemble decreases when <code>ρ(x)&lt;1</code>. Therefore, the variance of an ensemble is strictly smaller than the variance of an individual model. </p>

<p>In a nutshell, increasing the number of individual randomized models in an ensemble will never increase the generalization error.</p>
","<machine-learning><random-forest>","28","28","28","0","4719","536"
"843","{
  ""id"": 843,
  ""title"": ""Data Science Project Ideas""
}","<p>I would try to analyze and solve one or more of the problems published on <strong><a href=""https://www.kaggle.com/competitions"" rel=""nofollow noreferrer"">Kaggle Competitions</a></strong>. Note that the competitions are grouped by their expected <em>complexity</em>, from <code>101</code> (bottom of the list) to <code>Research</code> and <code>Featured</code> (top of the list). A color-coded vertical band is a <em>visual guideline</em> for grouping. You can <strong>assess time</strong> you could spend on a project by <strong>adjusting</strong> the expected <em>length</em> of corresponding competition, based on your <em>skills</em> and <em>experience</em>.</p>
<p>A number of <strong>data science project ideas</strong> can be found by browsing <a href=""https://www.coursolve.org/browse-needs?query=Data%20Science"" rel=""nofollow noreferrer""><code>Coursolve</code></a> webpage.</p>
<p>If you have skills and desire to work on a <strong>real data science project</strong>, focused on <strong>social impacts</strong>, visit <a href=""http://www.datakind.org/projects"" rel=""nofollow noreferrer""><code>DataKind</code></a> projects page. More projects with social impacts focus can be found at <a href=""http://dssg.io/projects"" rel=""nofollow noreferrer""><code>Data Science for Social Good</code></a> webpage.</p>
<p><strong>Science Project Ideas</strong> page at <a href=""http://mynasadata.larc.nasa.gov/804-2"" rel=""nofollow noreferrer""><code>My NASA Data</code></a> site looks like another place to visit for inspiration.</p>
<p>If you would like to use <strong>open data</strong>, this long list of applications on <code>Data.gov</code> can provide you with some interesting <em><a href=""http://www.data.gov/applications"" rel=""nofollow noreferrer"">data science</a></em> project ideas.</p>
","<machine-learning><bigdata><dataset>","28","28","28","0","2452","6478"
"4888","{
  ""id"": 4888,
  ""title"": ""Why are NLP and Machine Learning communities interested in deep learning?""
}","<h3>Why to use deep networks?</h3>

<p>Let's first try to solve very simple classification task. Say, you moderate a web forum which is sometimes flooded with spam messages. These messages are easily identifiable - most often they contain specific words like ""buy"", ""porn"", etc. and a URL to outer resources. You want to create filter that will alert you about such suspecious messages. It turns to be pretty easy - you get list of features (e.g. list of suspicious words and presence of a URL) and train simple logistic regression (a.k.a. perceptron), i.e. model like: </p>

<pre><code>g(w0 + w1*x1 + w2*x2 + ... + wnxn)
</code></pre>

<p>where <code>x1..xn</code> are your features (either presence of specific word or a URL), <code>w0..wn</code> - learned coefficients and <code>g()</code> is a <a href=""http://en.wikipedia.org/wiki/Logistic_function"" rel=""noreferrer"">logistic function</a> to make result be between 0 and 1. It's very simple classifier, but for this simple task it may give very good results, creating linear decision boundary. Assuming you used only 2 features, this boundary may look something like this: </p>

<p><img src=""https://i.stack.imgur.com/opDcs.png"" alt=""linear boundary""></p>

<p>Here 2 axes represent features (e.g. number of occurrences of specific word in a message, normalized around zero), red points stay for spam and blue points - for normal messages, while black line shows separation line. </p>

<p>But soon you notice that some good messages contain a lot of occurrences of word ""buy"", but no URLs, or extended discussion of <a href=""https://stackoverflow.com/questions/713247/what-is-the-best-way-to-programatically-detect-porn-images"">porn detection</a>, not actually refferring to porn movies. Linear decision boundary simply cannot handle such situations. Instead you need something like this: </p>

<p><img src=""https://i.stack.imgur.com/wPF38.png"" alt=""non-linear boundary""></p>

<p>This new non-linear decision boundary is much more <strong>flexible</strong>, i.e. it can fit the data much closer. There are many ways to achieve this non-linearity - you can use polynomial features (e.g. <code>x1^2</code>) or their combination (e.g. <code>x1*x2</code>) or project them out to a higher dimension like in <a href=""http://en.wikipedia.org/wiki/Kernel_method"" rel=""noreferrer"">kernel methods</a>. But in neural networks it's common to solve it by <strong>combining perceptrons</strong> or, in other words, by building <a href=""http://en.wikipedia.org/wiki/Multilayer_perceptron"" rel=""noreferrer""><strong>multilayer perceptron</strong></a>. Non-linearity here comes from logistic function between layers. The more layers, the more sophisticated patterns may be covered by MLP. Single layer (perceptron) can handle simple spam detection, network with 2-3 layers can catch tricky combinations of features, and networks of 5-9 layers, used by large research labs and companies like Google, may model the whole language or detect cats on images. </p>

<p>This is essential reason to have <strong>deep architectures</strong> - they can <em>model more sophisticated patterns</em>. </p>

<h3>Why deep networks are hard to train?</h3>

<p>With only one feature and linear decision boundary it's in fact enough to have only 2 training examples - one positive and one negative. With several features and/or non-linear decision boundary you need several orders more examples to cover all possible cases (e.g. you need not only find examples with <code>word1</code>, <code>word2</code> and <code>word3</code>, but also with all possible their combinations). And in real life you need to deal with hundreds and thousands of features (e.g. words in a language or pixels in an image) and at least several layers to have enough non-linearity. Size of a data set, needed to fully train such networks, easily exceeds 10^30 examples, making it totally impossible to get enough data. In other words, with many features and many layers our decision function becomes <strong>too flexible</strong> to be able to learn it <strong>precisely</strong>.</p>

<p>There are, however, ways to learn it <em>approximately</em>. For example, if we were working in probabilistic settings, then instead of learning frequencies of all combinations of all features we could assume that they are independent and learn only individual frequencies, reducing full and unconstrained <a href=""http://en.wikipedia.org/wiki/Bayes_classifier"" rel=""noreferrer"">Bayes classifier</a> to a <a href=""http://en.wikipedia.org/wiki/Naive_Bayes_classifier"" rel=""noreferrer"">Naive Bayes</a> and thus requiring much, much less data to learn. </p>

<p>In neural networks there were several attempts to (meaningfully) reduce complexity (flexibility) of decision function. For example, convolutional networks, extensively used in image classification, assume only local connections between nearby pixels and thus try only learn combinations of pixels inside small ""windows"" (say, 16x16 pixels = 256 input neurons) as opposed to full images (say, 100x100 pixels = 10000 input neurons). Other approaches include feature engineering, i.e. searching for specific, human-discovered descriptors of input data. </p>

<p>Manually discovered features are very promising actually. In natural language processing, for example, it's sometimes helpful to use special dictionaries (like those containing spam-specific words) or catch negation (e.g. ""<em>not</em> good""). And in computer vision things like <a href=""http://en.wikipedia.org/wiki/SURF"" rel=""noreferrer"">SURF descriptors</a> or <a href=""http://en.wikipedia.org/wiki/Haar-like_features"" rel=""noreferrer"">Haar-like features</a> are almost irreplaceable. </p>

<p>But the problem with manual feature engineering is that it takes literally years to come up with good descriptors. Moreover, these features are often specific </p>

<h3>Unsupervised pretraining</h3>

<p>But it turns out that we can <em>obtain good features automatically</em> right from the data using such algorithms as <strong>autoencoders</strong> and <strong>restricted Boltzmann machines</strong>. I described them in detail in my other <a href=""https://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma/117188#117188"">answer</a>, but in short they allow to <strong>find repeated patterns in the input data</strong> and transform it into higher-level features. For example, given only row pixel values as an input, these algorithms may identify and pass higher whole edges, then from these edges construct figures and so on, until you get really high-level descriptors like variations in faces. </p>

<p><img src=""https://i.stack.imgur.com/oGBRR.jpg"" alt=""deep learning""></p>

<p>After such (unsupervised) pretraining network is usually converted into MLP and used for normal supervised training. Note, that pretraining is done layer-wise. This significantly reduces solution space for learning algorithm (and thus number of training examples needed) as it only needs to learn parameters <em>inside</em> each layer without taking into account other layers. </p>

<h3>And beyond...</h3>

<p>Unsupervised pretraining have been here for some time now, but recently other algorithms were found to improve learning both - together with pretraining and without it. One notable example of such algorithms is <a href=""http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf"" rel=""noreferrer""><strong>dropout</strong></a> - simple technique, that randomly ""drops out"" some neurons during training, creatig some distortion and preventing networks of following data too closely. This is still a hot research topic, so I leave this to a reader. </p>
","<machine-learning><data-mining><neural-network><nlp><deep-learning>","28","28","28","0","1279","2771"
"9311","{
  ""id"": 9311,
  ""title"": ""The cross-entropy error function in neural networks""
}","<p>The first logloss formula you are using is for multiclass log loss, where the $i$ subscript enumerates the different classes in an example. The formula <em>assumes</em> that a single $y_i'$ in each example is 1, and the rest are all 0.</p>

<p>That means the formula only captures error on the target class. It discards any notion of errors that you might consider ""false positive"" and does not care how predicted probabilities are distributed other than predicted probability of the true class.</p>

<p>Another assumption is that $\sum_i y_i = 1$ for the predictions of each example. A softmax layer does this automatically - if you use something different you will need to scale the outputs to meet that constraint.</p>

<h2>Question 1</h2>

<blockquote>
  <p>Isn't it a problem that the $y_i$ (in $log(y_i)$) could be 0?</p>
</blockquote>

<p>Yes that can be a problem, but it is usually not a practical one. A randomly-initialised softmax layer is extremely unlikely to output an exact <code>0</code> in any class. But it is possible, so worth allowing for it. First, don't evaluate $log(y_i)$ for any $y_i'=0$, because the negative classes always contribute 0 to the error. Second, in practical code you can limit the value to something like <code>log( max( y_predict, 1e-15 ) )</code> for numerical stability - in many cases it is not required, but this is sensible defensive programming.</p>

<h2>Question 2</h2>

<blockquote>
  <p>I've learned that cross-entropy is defined as $H_{y'}(y) := - \sum_{i} ({y_i' \log(y_i) + (1-y_i') \log (1-y_i)})$</p>
</blockquote>

<p>This formulation is often used for a network with one output predicting two classes (usually positive class membership for 1 and negative for 0 output). In that case $i$ may only have one value - you can lose the sum over $i$.</p>

<p>If you modify such a network to have two opposing outputs and use softmax plus the first logloss definition, then you can see that in fact it <em>is the same error measurement</em> but folding the error metric for two classes into a single output.</p>

<p>If there is more than one class to predict membership of, and the classes are <em>not exclusive</em> i.e. an example could be any or all of the classes at the same time, then you will need to use this second formulation. For digit recognition that is not the case (a written digit should only have one ""true"" class)</p>
","<machine-learning><tensorflow>","28","28","28","0","836","25808"
"6154","{
  ""id"": 6154,
  ""title"": ""What are deconvolutional layers?""
}","<p>The <a href=""https://cs231n.github.io/"" rel=""noreferrer"">notes that accompany Stanford CS class CS231n</a>: <em>Convolutional Neural Networks for Visual Recognition, by Andrej Karpathy</em>, do an excellent job of explaining convolutional neural networks.</p>

<p>Reading this paper should give you a rough idea about:</p>

<ul>
<li><em><a href=""http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf"" rel=""noreferrer"">Deconvolutional Networks</a>
Matthew D. Zeiler, Dilip Krishnan, Graham W. Taylor and Rob Fergus
Dept. of Computer Science, Courant Institute, New York University</em></li>
</ul>

<p>These <a href=""http://cs.nyu.edu/~fergus/drafts/utexas2.pdf"" rel=""noreferrer"">slides</a> are great for Deconvolutional Networks.</p>
","<neural-network><convnet><convolution>","28","30","29","1","9943","1039"
"49616","{
  ""id"": 49616,
  ""title"": ""Lightgbm vs xgboost vs catboost""
}","<p>On Kaggle, LightGBM is indeed the ""meta"" base learner of almost all of the competitions that have structured datasets right now. This is mostly because of LightGBM's implementation; it doesn't do exact searches for optimal splits like XGBoost does in it's default setting (XGBoost now has this functionality as well but it's still not as fast as LightGBM) but rather through histogram approximations. The result is a slight decrease in predictive performance for a much larger speed increase in training. This means more opportunity for feature engineering/experimentation/model tuning (all of which are key to winning Kaggle competitions) which inevitably yields larger increases in predictive performance (despite using histogram approximations).</p>

<p>CatBoost is not used as much because on average, it it found to be much slower than LightGBM. That being said, CatBoost is different in its implementation of gradient boosting which at times can give slightly more accurate predictions, in particular if you have <a href=""https://arxiv.org/pdf/1706.09516.pdf"" rel=""noreferrer"">large amounts of categorical features</a>. I have never used CatBoost and so I encourage you to read that paper. Regardless, because rapid experimentation is vital in Kaggle competitions, LightGBM tends to be the go to algorithm when first creating strong base learners.</p>

<p>In general, it is important to note that a large amount of approaches I've seen involve combining all three boosting algorithms in a model stack (i.e. ensembling). LightGBM, CatBoost, and XGBoost might be thrown together as three base learners and then combined via. a GLM or neural network. This is done to really squeeze out decimal places on the leaderboard and so I doubt there is any theoretical (or practical) justification for it besides competitions. </p>
","<machine-learning><xgboost>","28","28","28","0","71219","1695"
"38956","{
  ""id"": 38956,
  ""title"": ""How does the validation_split parameter of Keras' fit function work?""
}","<p>You actually would not want to resample your validation set after each epoch. If you did this your model would be trained on every single sample in your dataset and thus this will cause overfitting. You want to always split your data before the training process and then the algorithm should only be trained using the subset of the data for training. </p>

<p>The function as it is designed ensures that the data is separated in such a way that it always trains on the same portion of the data for each epoch. All shuffling is done within the training sample between epochs if that option is chosen. </p>

<p>However, for some datasets getting the last few instances is not useful, specifically if the dataset is regroup based on class. Then the distribution of your classes will be skewed. Thus you will need some kind of random way to extract a subset of the data to get balanced class distributions in the training and validation set. For this I always like to use the sklearn function as follows</p>

<pre><code>from sklearn.model_selection import train_test_split

# Split the data
x_train, x_valid, y_train, y_valid = train_test_split(data, labels, test_size=0.33, shuffle= True)
</code></pre>

<p>It's a nice easy to use function that does what you want. The variables <code>data</code> and <code>labels</code> are standard numpy matrices with the first dimension being the instances.</p>
","<keras><data><cross-validation>","28","28","28","0","29587","8088"
"27794","{
  ""id"": 27794,
  ""title"": ""Opening a 20GB file for analysis with pandas""
}","<p>There are two possibilities: either you <em>need</em> to have all your data in memory for processing (e.g. your machine learning algorithm would want to consume all of it at once), or you can do without it (e.g. your algorithm only needs samples of rows or columns at once).</p>

<p>In the first case, you'll need to <em>solve a memory problem</em>. Increase your memory size, rent a high-memory cloud machine, use inplace operations, provide information about the type of data you are reading in, make sure to delete all unused variables and collect garbage, etc. </p>

<p>It is very probable that 32GB of RAM would not be enough for Pandas to handle your data. Note that the integer ""1"" is just one byte when stored as text but 8 bytes when represented as <code>int64</code> (which is the default when Pandas reads it in from text). You can make the same example with a floating point number ""1.0"" which expands from a 3-byte string to an 8-byte <code>float64</code> by default. You may win some space by letting Pandas know precisely which types to use for each column and forcing the smallest possible representations, but we did not even start speaking of Python's data structure overhead here, which may add an extra pointer or two here or there easily, and pointers are 8 bytes each on a 64-bit machine.</p>

<p>To summarize: no, 32GB RAM is probably not enough for Pandas to handle a 20GB file.</p>

<p>In the second case (which is more realistic and probably applies to you), you need to solve a <em>data management problem</em>. Indeed, having to load all of the data when you really only need parts of it for processing, may be a sign of bad data management. There are multiple options here:</p>

<ol>
<li><p>Use an SQL database. If you can, it is nearly always the first choice and a decently comfortable solution. 20GB sounds like the size most SQL databases would handle well without the need to go distributed even on a (higher-end) laptop. You'll be able to index columns, do basic aggregations via SQL, and get the needed subsamples into Pandas for more complex processing using a simple <code>pd.read_sql</code>. Moving the data to a database will also provide you with an opportunity to think about the <em>actual</em> data types and sizes of your columns.</p></li>
<li><p>If your data is mostly numeric (i.e. arrays or tensors), you may consider holding it in a HDF5 format (see <a href=""http://www.pytables.org/"" rel=""noreferrer"">PyTables</a>), which lets you conveniently read only the necessary slices of huge arrays from disk. Basic <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.load.html"" rel=""noreferrer"">numpy.save and numpy.load</a> achieve the same effect via memory-mapping the arrays on disk as well. For GIS and related raster data there are <a href=""http://www.rasdaman.org/"" rel=""noreferrer"">dedicated</a> <a href=""https://www.paradigm4.com/try_scidb/"" rel=""noreferrer"">databases</a>, which might not connect to pandas as directly as SQL, but should also let you do slices and queries reasonably conveniently.</p></li>
<li><p>Pandas does not support such ""partial"" memory-mapping of HDF5 or numpy arrays, as far as I know. If you still want a kind of a ""pure-pandas"" solution, you can try to work around by ""sharding"": either storing the <em>columns</em> of your huge table separately (e.g. in separate files or in separate ""tables"" of a single HDF5 file) and only loading the necessary ones on-demand, or storing the <em>chunks of rows</em> separately. However, you'd then need to implement the logic for loading the necessary chunks, thus reinventing the bicycles already imlpemented in most SQL databases, so perhaps option 1 would still be easier here.  If your data comes in a CSV, though, you can process it in chunks by specifying the <code>chunksize</code> parameter to <code>pd.read_csv</code>.</p></li>
</ol>
","<python><bigdata><pandas><anaconda>","28","28","28","0","14519","2021"
"26291","{
  ""id"": 26291,
  ""title"": ""Why do convolutional neural networks work?""
}","<p>Actually I guess the question is a bit broad! Anyway. </p>

<h1>Understanding Convolution Nets</h1>

<p>What is learned in <code>ConvNets</code> tries to minimize the cost function to categorize the inputs correctly in classification tasks. All parameter changing and learned filters are in order to achieve the mentioned goal. </p>

<h3>Learned Features in Different Layers</h3>

<p>They try to reduce the cost by learning low level, sometimes meaningless, features like horizontal and vertical lines in their first layers and then stacking them to make abstract shapes, which often have meaning, in their last layers. For illustrating this fig. 1, which has been used from <a href=""http://yosinski.com/deepvis#toolbox"" rel=""noreferrer"">here</a>, can be considered. The input is the bus and the gird shows the activations after passing the input through different filters in the first layer. As it can be seen the red frame which is the activation of a filter, which its parameters have been learned, has been activated for relatively horizontal edges. The blue frame has been activated for relatively vertical edges. It is possible that <code>ConvNets</code> learn unknown filters that are useful and we, as e.g. computer vision practitioners, have not discovered that they may be useful. The best part of these nets is that they try to find appropriate filters by their own and don't use our limited discovered filters. They learn filters to reduce the amount of cost function. As mentioned these filters are not necessarily known. </p>

<p><a href=""https://i.stack.imgur.com/j7QHf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/j7QHf.png"" alt=""**Figure 1.** *Low level activations*""></a></p>

<p>In deeper layers, the features learned in previous layers come together and make shapes which often have meaning. In <a href=""http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf"" rel=""noreferrer"">this paper</a> it has been discussed that these layers may have activations which are meaningful to us or the concepts which have meaning to us, as human beings, may be distributed among other activations. In fig. 2 the green frame shows the activatins of a filter in the fifth layer of a <code>ConvNet</code>. This filter cares about the faces. Suppose that the red one cares about hair. These have meaning. As it can be seen there are other activations that have been activated right in the position of typical faces in the input, the green frame is one of them; The blue frame is another example of these. Accordingly, abstraction of shapes can be learned by a filter or numerous filters. In other words, each concept, like face and its components, can be distributed among the filters. In cases where the concepts are distributed among different layers, if someone look at each of them, they may be sophisticated. The information is distributed among them and for understanding that information all of those filters and their activations have to be considered although they may seem so much complicated.</p>

<p><a href=""https://i.stack.imgur.com/3cGMQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3cGMQ.png"" alt=""**Figure 2.** *High level activations*""></a></p>

<p><code>CNNs</code> should not be considered as black boxes at all. <em>Zeiler et all</em> in <a href=""http://arxiv.org/pdf/1311.2901v3.pdf"" rel=""noreferrer"">this amazing paper</a> have discussed the <em>development of better models is reduced to trial and error</em> if you don't have understanding of what is done inside these nets. This paper tries to visualize the feature maps in <code>ConvNets</code>.</p>

<h3>Capability to Handle Different Transformations to Generalize</h3>

<p><code>ConvNets</code> use <code>pooling</code> layers not only to reduce the number of parameters but also to have the capability to be insensitive to the exact position of each feature. Also the use of them enables the layers to learn different features which means first layers learn simple low level features like edges or arcs, and deeper layers learn more complicated features like eyes or eyebrows. <code>Max Pooling</code> e.g. tries to investigate whether a special feature exists in a special region or not. The idea of <code>pooling</code> layers is so useful but it is just capable to handle transition among other transformations. Although filters in different layers try to find different patterns, e.g. a rotated face is learned using different layers than a usual face, <code>CNNs</code> by there own do not have any layer to handle other transformations. To illustrate this suppose that you want to learn simple faces without any rotation with a minimal net. In this case your model may do that perfectly. suppose that you are asked to learn all kind of faces with arbitrary face rotation. In this case your model has to be much more bigger than the previous learned net. The reason is that there have to be filters to learn these rotations in the input. Unfortunately these are not all transformations. Your input may also be distorted too. These cases made <em>Max Jaderberg et all</em> angry. They composed <a href=""https://arxiv.org/abs/1506.02025"" rel=""noreferrer"">this</a> paper to deal with these problems in order to settle down our anger as theirs.</p>

<h3>Convolutional Neural Networks Do Work</h3>

<p>Finally after referring to these points, they work because they try to find patterns in the input data. They stack them to make abstract concepts by there convolution layers. They try to find out whether the input data has each of these concepts or not in there dense layers to figure out which class the input data belongs to. </p>

<p>I add some links which are helpful:</p>

<ul>
<li><a href=""https://github.com/vdumoulin/conv_arithmetic"" rel=""noreferrer"">Understanding convolution operation</a></li>
<li><a href=""https://gist.github.com/akiross/754c7b87a2af8603da78b46cdaaa5598"" rel=""noreferrer"">Understanding arithmetic behind <code>ConvNets</code></a></li>
<li><a href=""http://cs.stanford.edu/people/karpathy/convnetjs/"" rel=""noreferrer"">Useful toolboxes for tracking what happens in these nets</a></li>
</ul>
","<machine-learning><deep-learning><neural-network><cnn><convolutional-neural-network>","27","27","27","0","28175","12688"
"2272","{
  ""id"": 2272,
  ""title"": ""Any Online R console?""
}","<ul>
<li><a href=""http://roncloud.com"" rel=""nofollow noreferrer"">R On Cloud</a> provides a browser-embedded R-console.</li>
</ul>

<p><img src=""https://i.stack.imgur.com/ELpbR.png"" alt=""screenshot-roncloud""></p>

<ul>
<li><a href=""https://jupyter.org/"" rel=""nofollow noreferrer"">Jupyter.org</a> evolved from the <a href=""http://ipython.org/"" rel=""nofollow noreferrer"">IPython Project</a> (the language-agnostic parts of IPython); supports Python 3, Julia, R, Haskell, Ruby, etc.  </li>
</ul>

<p><img src=""https://i.stack.imgur.com/mhHkx.png"" alt=""jupyter screenshot""></p>
","<r><statistics>","27","27","27","0","2961","670"
"42600","{
  ""id"": 42600,
  ""title"": ""What is the relationship between the accuracy and the loss in deep learning?""
}","<p>There is no relationship between these two metrics. <br/>
Loss can be seen as a <strong>distance</strong> between the true values of the problem and the values predicted by the model. Greater the loss is, more huge is the errors you made on the data.</p>
<p>Accuracy can be seen as the <strong>number</strong> of error you made on the data.</p>
<p>That means:<br/></p>
<ul>
<li>a low accuracy and huge loss means you made huge errors on a lot of data<br/></li>
<li>a low accuracy but low loss means you made little errors on a lot of data<br/></li>
<li>a great accuracy with low loss means you made low errors on a few data (best case)<br/></li>
<li>your situation: a great accuracy but a huge loss, means you made huge errors on a few data.</li>
</ul>
<p>For you case, the third model can correctly predict more examples, but on those where it was wrong, it made more errors (the distance between true value and predicted values is more huge).</p>
<p><strong>NOTE:</strong></p>
<p>Don't forget that low or huge loss is a subjective metric, which <strong>depends</strong> on the problem and the data. It's a distance between the true value of the prediction, and the prediction made by the model. It depends also on the loss you use.<br/></p>
<p>Think:<br/></p>
<ul>
<li>If your data are between 0 and 1, a loss of 0.5 is huge, but if your data are between 0 and 255, an error of 0.5 is low.<br/></li>
<li>Maybe think of cancer detection, and probability of detecting a cancer. Maybe an error of 0.1 is huge for this problem, whereas an error f 0.1 for image classification is fine.</li>
</ul>
","<neural-network><deep-learning><keras><tensorflow><metric>","27","27","27","0","57416","782"
"48607","{
  ""id"": 48607,
  ""title"": ""How much of data wrangling is a data scientist's job?""
}","<ol>
<li><p><em>Feels like most of the work is not related to data science at all. Is this accurate?</em></p>

<p>Yes</p></li>
<li><p><em>I know this is not a data-driven company with a high-level data engineering department, but it is my opinion that data science requires minimum levels of data accessibility. Am I wrong?</em></p>

<p>You're not wrong, but such are the realities of real life.</p></li>
<li><p><em>Is this type of setup common for a company with serious data science needs?</em></p>

<p>Yes</p></li>
</ol>

<p>From a technical standpoint, you need to look into ETL solutions that can make your life easier. Sometimes one tool can be much faster than another to read certain data. E.g. R's readxl is orders of mangnitudes faster than python's pandas at reading xlsx files; you could use R to import the files, then save them to a Python-friendly format (parquet, SQL, etc). I know you're not working on xlsx files and I have no idea if you use Python - it was just an example.</p>

<p>From a practical standpoint, two things:</p>

<ul>
<li><p>First of all, understand what is technically possible. In many cases,
the people telling you know are IT-illiterate people who worry about
business or compliance considerations, but have no concept of what is
and isn't feasible from an IT standpoint. Try to speak to the DBAs or
to whoever manages the data infrastructure. Understand what is
technically possible. THEN, only then, try to find a compromise. E.g.
they won't give you access to their system, but I presume there is a
database behind it? Maybe they can extract the data to some other
formats? Maybe they can extract the SQL statements that define the
data types etc?</p></li>
<li><p>Business people are more likely to help you if you can make the case that doing so is in THEIR interest. If they don't even believe in what you're doing, tough luck...</p></li>
</ul>
","<data-wrangling>","27","27","27","0","70863","286"
"56725","{
  ""id"": 56725,
  ""title"": ""Can machine learning learn a function like finding maximum from a list?""
}","<p><strong>Yes.</strong>
Very importantly, YOU decide the architecture of a machine learning solution. Architectures and training procedures don't write themselves; they must be designed or templated and the training follows as a means of discovering a parameterization of the architecture fitting to a set of data points.</p>

<p>You can construct a very simple architecture that actually includes a maximum function:</p>

<pre><code>net(x) = a * max(x) + b * min(x)
</code></pre>

<p>where <em>a</em> and <em>b</em> are learned parameters. </p>

<p>Given enough training samples and a reasonable training routine, this very simple architecture will learn very quickly to set a to 1 and b to zero for your task.</p>

<p>Machine learning often takes the form of entertaining multiple hypotheses about featurization and transformation of input data points, and learning to preserve only those hypotheses that are correlated with the target variable. The hypotheses are encoded explicitly in the architecture and sub-functions available in a parameterized algorithm, or as the assumptions encoded in a ""parameterless"" algorithm.</p>

<p>For example, the choice to use dot products and nonlinearities as is common in vanilla neural network ML is somewhat arbitrary; it expresses the encompassing hypothesis that a function can be constructed using a predetermined compositional network structure of linear transformations and threshold functions. Different parameterizations of that network embody different hypotheses about which linear transformations to use. Any toolbox of functions can be used and a machine learner's job is to discover through differentiation or trial and error or some other repeatable signal which functions or features in its array best minimize an error metric. In the example given above, the learned network simply reduces to the maximum function itself, whereas an undifferentiated network could alternatively ""learn"" a minimum function. These functions can be expressed or approximated via other means, as in the linear or neural net regression function in another answer. In sum, it really depends on which functions or LEGO pieces you have in your ML architecture toolbox.</p>
","<machine-learning><deep-learning>","27","29","28","1","31848","409"
"48374","{
  ""id"": 48374,
  ""title"": ""What loss function to use for imbalanced classes (using PyTorch)?""
}","<blockquote>
  <p>What kind of loss function would I use here?</p>
</blockquote>

<p>Cross-entropy is the go-to loss function for classification tasks, either balanced or imbalanced. It is the first choice when no preference is built from domain knowledge yet.</p>

<blockquote>
  <p>This would need to be weighted I suppose? How does that work in practice?</p>
</blockquote>

<p>Yes. Weight of class <span class=""math-container"">$c$</span> is the size of largest class divided by the size of class <span class=""math-container"">$c$</span>. </p>

<p>For example, If class 1 has 900, class 2 has 15000, and class 3 has 800 samples, then their weights would be 16.67, 1.0, and 18.75 respectively. </p>

<p>You can also use the smallest class as nominator, which gives 0.889, 0.053, and 1.0 respectively. This is only a re-scaling, the relative weights are the same.</p>

<blockquote>
  <p>Is this the right approach to begin with or are there other / better
  methods I could use?</p>
</blockquote>

<p>Yes, this is the right approach.</p>

<p><strong>EDIT</strong>:</p>

<p>Thanks to @Muppet, we can also use class over-sampling, which is <a href=""https://datascience.stackexchange.com/questions/47423/cnn-imbalanced-classes-class-weights-vs-data-augmentation/47428#47428"">equivalent to using class weights</a>. This is accomplished by <code>WeightedRandomSampler</code> in PyTorch, using the same aforementioned weights.</p>
","<neural-network><pytorch>","27","27","27","0","67328","7999"
"40532","{
  ""id"": 40532,
  ""title"": ""Is pandas now faster than data.table?""
}","<blockquote>
<p>Has anyone done any benchmarks?</p>
</blockquote>
<p>Yes, <a href=""https://github.com/Rdatatable/data.table/wiki/Benchmarks-%3A-Grouping"" rel=""nofollow noreferrer"">the 2014's benchmark in question</a> has turned into foundation for <a href=""https://h2oai.github.io/db-benchmark"" rel=""nofollow noreferrer"">db-benchmark</a> project. Initial step was to reproduce 2014's benchmark on recent version of software, then to make it a continuous benchmark, so it runs routinely and automatically upgrades software before each run. Over time many things have been added. Below is high level diff of the 2014's benchmark comparing to <a href=""https://github.com/h2oai/db-benchmark"" rel=""nofollow noreferrer"">db-benchmark</a> project.</p>
<p>New:</p>
<ul>
<li>continuous benchmark: runs routinely, upgrades software, re-run benchmarking script</li>
<li>more software solutions: spark, python datatable, cuda dataframes, julia dataframes, clickhouse, dask</li>
<li>more data cases
<ul>
<li>two new smaller data sizes: 0.5GB (1e7 rows) and 5GB (1e8 rows)</li>
<li>two more cardinality factors: unbalanced, heavily unbalanced</li>
<li>sortedness</li>
<li>data having NA</li>
</ul>
</li>
<li>advanced <code>groupby</code> questions
<ul>
<li><code>median</code>, <code>sd</code></li>
<li>range v1-v2: <code>max(v1)-min(v2)</code></li>
<li>top 2 rows: <code>order(.); head(.,2)</code></li>
<li>regression: <code>cor(v1, v2)^2</code></li>
<li><code>count</code> and grouping by 6 columns</li>
</ul>
</li>
<li>benchmark task: <code>join</code></li>
</ul>
<p>Changes (see <code>groupby2014</code> task for 2014 fully compliant benchmark script):</p>
<ul>
<li><a href=""https://github.com/h2oai/db-benchmark/issues/20"" rel=""nofollow noreferrer"">using categorical/factor instead of character</a></li>
<li><a href=""https://github.com/h2oai/db-benchmark/issues/93"" rel=""nofollow noreferrer"">cardinality of <code>v2</code> and <code>v3</code> measures increased</a></li>
<li><a href=""https://github.com/h2oai/db-benchmark/issues/40"" rel=""nofollow noreferrer"">function calls are NA-aware</a></li>
<li>aggregated columns are named</li>
<li>order of groups is irrelevant whenever possible</li>
<li>extra call to <code>dim()</code>/<code>.shape</code> is included in timings to force lazy evaluation</li>
<li>machine that runs benchmark has 128GB mem (not 244GB mem)</li>
<li>no 100GB (2e9 rows) data size</li>
</ul>
<p>We are planning to add even more software solutions and benchmark tasks in future.
Feedback is very welcome, feel invited to our issue tracker at <a href=""https://github.com/h2oai/db-benchmark/issues"" rel=""nofollow noreferrer"">https://github.com/h2oai/db-benchmark/issues</a>.</p>
<hr />
<blockquote>
<p>Is pandas now faster than data.table?</p>
</blockquote>
<p>According the our results pandas is not faster than data.table.</p>
<p>I am pasting medium size data 5GB (1e8 rows) groupby benchmark plot taken from the report at <a href=""https://h2oai.github.io/db-benchmark"" rel=""nofollow noreferrer"">h2oai.github.io/db-benchmark</a> as of 20210312. Consult the <a href=""https://h2oai.github.io/db-benchmark/#explore-more-data-cases"" rel=""nofollow noreferrer"">h2oai.github.io/db-benchmark#explore-more-data-cases</a> for other data sizes (1e7, 1e9), data cases (cardinality, NAs, sorted), questions groups (advanced), or tasks (join).</p>
<p>For up-to-date timings please visit <a href=""https://h2oai.github.io/db-benchmark"" rel=""nofollow noreferrer"">https://h2oai.github.io/db-benchmark</a>.</p>
<p><a href=""https://i.stack.imgur.com/WyxZS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WyxZS.png"" alt=""groupby_1e8_1e2_0_0_basic"" /></a></p>
","<python><r><pandas><data><data-table>","27","27","27","0","10588","371"
"499","{
  ""id"": 499,
  ""title"": ""Latent Dirichlet Allocation vs Hierarchical Dirichlet Process""
}","<p>Anecdotally, I've never been impressed with the output from hierarchical LDA. It just doesn't seem to find an optimal level of granularity for choosing the number of topics. I've gotten much better results by running a few iterations of regular LDA, manually inspecting the topics it produced, deciding whether to increase or decrease the number of topics, and continue iterating until I get the granularity I'm looking for.</p>

<p>Remember: hierarchical LDA can't read your mind... it doesn't know what you actually intend to use the topic modeling for. Just like with k-means clustering, you should choose the k that makes the most sense for your use case.</p>
","<nlp><topic-model><lda>","27","27","27","0","819","1481"
"29","{
  ""id"": 29,
  ""title"": ""Is Data Science the Same as Data Mining?""
}","<p><a href=""https://datascience.stackexchange.com/users/36/statsrus"">@statsRus</a> starts to lay the groundwork for your answer in another question <a href=""https://datascience.meta.stackexchange.com/q/86/98307"">What characterises the difference between data science and statistics?</a>:</p>
<blockquote>
<ul>
<li><strong>Data collection</strong>: web scraping and online surveys</li>
<li><strong>Data manipulation</strong>: recoding messy data and extracting meaning from linguistic and social network data</li>
<li><strong>Data scale</strong>: working with extremely large data sets</li>
<li><strong>Data mining</strong>: finding patterns in large, complex data sets, with an emphasis on algorithmic techniques</li>
<li><strong>Data communication</strong>: helping turn &quot;machine-readable&quot; data into &quot;human-readable&quot; information via visualization</li>
</ul>
</blockquote>
<h2>Definition</h2>
<p><a href=""/questions/tagged/data-mining"" class=""post-tag"" title=""show questions tagged &#39;data-mining&#39;"" rel=""tag"">data-mining</a> can be seen as one item (or set of skills and applications) in the toolkit of the data scientist.  I like how he separates the definition of mining from collection in a sort of trade-specific jargon.</p>
<p>However, I think that <em>data-mining</em> would be synonymous with <em>data-collection</em> in a US-English colloquial definition.</p>
<p><em>As to where to go to become proficient?</em>  I think that question is too broad as it is currently stated and would receive answers that are primarily opinion based.  Perhaps if you could refine your question, it might be easier to see what you are asking.</p>
","<data-mining><definitions>","27","27","27","0","53","404"
"17558","{
  ""id"": 17558,
  ""title"": ""make seaborn heatmap bigger""
}","<p>I found out how to increase the size of my plot with the following code...</p>

<pre><code>plt.subplots(figsize=(20,15))
sns.heatmap(corr)
</code></pre>

<p><a href=""https://i.stack.imgur.com/U5vKH.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/U5vKH.jpg"" alt=""enter image description here""></a></p>
","<visualization><pandas><plotting>","27","27","27","0","29897","601"
"24115","{
  ""id"": 24115,
  ""title"": ""What are graph embedding?""
}","<p><strong>What are graph Embeddings ?</strong>
""Graph Embeddings"" is a hot area today in machine learning. It basically means finding ""latent vector representation"" of graphs which captures the topology (in very basic sense) of the graph. We can make this ""vector representation"" rich by also considering the vertex-vertex relationships, edge-information etc. There are roughly two levels of embeddings in the graph (of-course we can anytime define more levels by logically dividing the whole graph into subgraphs of various sizes):</p>

<ul>
<li><strong>Vertex Embeddings</strong> - Here you find latent vector representation of every vertex in the given graph. You can then compare the different vertices by plotting these vectors in the space and interestingly ""similar"" vertices are plotted closer to each other than the ones which are dissimilar or less related. This is the same work that is done in ""DeepWalk"" by Perozzi.</li>
<li><strong>Graph Embeddings</strong> - Here you find the latent vector representation of the whole graph itself. For example, you have a group of chemical compounds for which you want to check which compounds are similar to each other, how many type of compounds are there in the group (clusters) etc. You can use these vectors and plot them in space and find all the above information. This is the work that is done in ""Deep Graph Kernels"" by Yanardag.</li>
</ul>

<p><strong>Applications -</strong>
By looking carefully, embeddings are ""latent"" representations  which means if a graph has a |V| * |V| adjacency matrix where |V| = 1M, its hard to use or process a 1M * 1M numbers in an algorithm. So, latent embedding of dimension 'd', where d &lt;&lt; |V|, would make the adjacency matrix |V| * d and relatively easier to use. Another application could be - Consider a simple scenario where we want to recommend products to the people who have similar interests in a social network. By getting vertex embeddings (here it means vector representation of each person), we can find the similar ones by plotting these vectors and this makes recommendation easy. These are some applications and there are others. You can refer to a nice survey paper - <a href=""https://arxiv.org/pdf/1705.02801.pdf"" rel=""noreferrer"">Graph Embedding Techniques, a Survey</a>.</p>

<p><strong>Where from it all came ?</strong> There has been a lot of works in this area and almost all comes from the groundbreaking research in natural language processing field - ""Word2Vec"" by Mikolov. If you want to get started with the research on graph embeddings, I would recommend to first understand how Word2Vec works. You can find nice explanations - <a href=""https://arxiv.org/pdf/1411.2738.pdf"" rel=""noreferrer"">Word2Vec parameter learning explained</a> and <a href=""https://www.youtube.com/watch?v=ERibwqs9p38"" rel=""noreferrer"">Stanford Lecture</a>. Then you can jump to the papers that you listed. Those works can be categorized as:</p>

<ul>
<li><p>Works based on ""Vertex Embeddings"": - <a href=""http://www.perozzi.net/publications/14_kdd_deepwalk.pdf"" rel=""noreferrer"">DeepWalk</a>, <a href=""https://arxiv.org/pdf/1607.00653.pdf"" rel=""noreferrer"">Node2Vec</a>, <a href=""https://arxiv.org/pdf/1503.03578.pdf"" rel=""noreferrer"">LINE</a>.</p></li>
<li><p>Works based on ""Graph Embeddings"": - <a href=""https://users.soe.ucsc.edu/~vishy/pubs/YanVis15.pdf"" rel=""noreferrer"">Deep Graph Kernels</a>, <a href=""https://arxiv.org/pdf/1606.08928.pdf"" rel=""noreferrer"">Subgraph2Vec</a>.</p></li>
</ul>
","<graphs>","27","27","27","0","41058","1037"
"38124","{
  ""id"": 38124,
  ""title"": ""What is the difference between upsampling and bi-linear upsampling in a CNN?""
}","<p>In the context of image processing, upsampling is a technique for increasing the size of an image. </p>

<p>For example, say you have an image with a height and width of $64$ pixels each (totaling $64 \times 64 = 4096$ pixels). You want to resize this image to a height and width of 256 pixels (totaling $256 \times 256 = 65536$ pixels). In the new, larger image you only know the value of $1$ out of every $16$ pixels. How are you going to calculate the values for the rest?</p>

<p><img src=""https://www.safaribooksonline.com/library/view/deep-learning-for/9781788295628/assets/a4df8c96-4e64-450f-b891-9efb18fc7368.png"" alt=""""></p>

<p>Well, the methods that do that for you are called upsampling techniques. The most common are:</p>

<ul>
<li><p><a href=""https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation"" rel=""noreferrer"">Nearest-Neighbor</a>: Copies the value from the nearest pixel.</p></li>
<li><p><a href=""https://en.wikipedia.org/wiki/Bilinear_interpolation"" rel=""noreferrer"">Bilinear</a>: Uses all nearby pixels to calculate the pixel's value, using linear interpolations.</p></li>
<li><p><a href=""https://en.wikipedia.org/wiki/Bicubic_interpolation"" rel=""noreferrer"">Bicubic</a>: Again uses all nearby pixels to calculate the pixel's values, through polynomial interpolations. Usually produces a smoother surface than the previous techniques, but its harder to compute.</p></li>
<li><p>Other more complex resampling algorithms, e.g. <a href=""https://en.wikipedia.org/wiki/Lanczos_resampling"" rel=""noreferrer"">Lanczos</a>.</p></li>
</ul>

<p>An article explaining the differences among image resampling techniques can be found <a href=""http://bigwww.epfl.ch/publications/thevenaz9901.pdf"" rel=""noreferrer"">here</a>.</p>
","<deep-learning><cnn><convolution><convolutional-neural-network>","26","26","26","0","34269","7013"
"14387","{
  ""id"": 14387,
  ""title"": ""How are deep-learning NNs different now (2016) from the ones I studied just 4 years ago (2012)?""
}","<p>You are right in that the basic concept of a deep NN hasn't changed since 2012.  But there have been a variety of improvements to the ways in which deep NNs are trained that have made them qualitatively more powerful.  There are also a wider variety of architectures available today.  I've listed some developments since 2012, grouped by training improvements and architecture improvements: </p>

<h2>Improvements to training deep NNs</h2>

<ul>
<li><p><strong>Hardware</strong>: The most obvious change is just the inexorable progression of Moore's law.  There is more computing power available today.  Cloud computing also makes it easy for people to train large NNs without needing to buy a huge rig.</p></li>
<li><p><strong>Software</strong>: The open source software for deep learning is really enormously improved from 2012.  Back in 2012 there was Theano, maybe Caffe as well.  I'm sure there are some others, too.  But today we also have TensorFlow, Torch, Paddle, and CNTK, all of which are supported by large tech companies.  This is closely related to the hardware bullet point since many of these platforms make it easy to train on GPUs, which drastically speeds up training time.</p></li>
<li><p><strong>Activation functions</strong>: The use of ReLU activation functions is probably more widespread these days, which makes training very deep networks easier.  On the research side, there is a wider variety of activation functions being studied, including <a href=""http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf"" rel=""noreferrer"">leaky ReLU</a>, <a href=""https://arxiv.org/abs/1502.01852"" rel=""noreferrer"">parametric ReLU</a>, and <a href=""https://arxiv.org/abs/1302.4389"" rel=""noreferrer"">maxout units</a>.</p></li>
<li><p><strong>Optimization algorithms</strong>: There are more optimization algorithms around today.  <a href=""http://jmlr.org/papers/v12/duchi11a.html"" rel=""noreferrer"">Adagrad</a> and <a href=""https://arxiv.org/abs/1212.5701"" rel=""noreferrer"">Adadelta</a> just been introduced in 2011 and 2012, respectively.  But we now also have the <a href=""https://arxiv.org/abs/1412.6980"" rel=""noreferrer"">Adam optimizer</a> and it's become a very popular choice.</p></li>
<li><p><strong>Dropout</strong>: In the past few years, <a href=""http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf"" rel=""noreferrer"">dropout</a> has become a standard tool for regularization when training neural networks.  Dropout is a computationally inexpensive form of ensembling for NNs.  In general, a set of models trained on random samples of the dataset will outperform a single model trained on the entire dataset.  This is difficult to do explicitly for NNs because they are so expensive to train.  But a similar effect can be approximated just by randomly ""turning off"" neurons on each step.  Different subgraphs in the NN end up getting trained on different data sets, and thereby learn different things.  Like ensembling, this tends to make the overall NN more robust to overfitting.  Dropout is a simple technique that seems to improve performance in almost every case, so it's now used de rigueur.</p></li>
<li><p><strong>Batch normalization</strong>: It's been known for a while that NNs train best on data that is normalized --- i.e., there is zero mean and unit variance.  In a very deep network, as the data passes through each layer, the inputs will be transformed and will generally drift to a distribution that lacks this nice, normalized property.  This makes learning in these deeper layers more difficult because, from its perspective, its inputs do not have zero mean and unit variance.  The mean could be very large and the variance could be very small.  <a href=""https://arxiv.org/abs/1502.03167v3"" rel=""noreferrer"">Batch normalization</a> addresses this by transforming the inputs to a layer to have zero mean and unit variance.  This seems to be enormously effective in training very deep NNs.</p></li>
<li><p><strong>Theory</strong>: Up until very recently, it was thought that the reason deep NNs are hard to train is that the optimization algorithms get stuck in local minima and have trouble getting out and finding global minima.  In the last four years there have been a number of studies that seem to indicate that this intuition was wrong (e.g., <a href=""https://arxiv.org/abs/1412.6544"" rel=""noreferrer"">Goodfellow et al. 2014</a>).  In the very high dimensional parameter space of a deep NN, local minima tend not to be that much worse than global minima.  The problem is actually that when training, the NN can find itself on a long, wide plateau.  Furthermore, these plateaus can end abruptly in a steep cliff.  If the NN takes small steps, it takes a very long time to learn.  But if the steps are too large, it meets a huge gradient when it runs into the cliff, which undoes all the earlier work.  (This can be avoided with gradient clipping, another post-2012 innovation.)</p></li>
</ul>

<h2>New architectures</h2>

<ul>
<li><p><strong>Residual networks</strong>: Researchers have been able to train incredibly deep networks (more than 1000 layers!) using <a href=""https://arxiv.org/abs/1605.07146"" rel=""noreferrer"">residual networks</a>.  The idea here is that each layer receives not only the output from the previous layer, but also the original input as well.  If trained properly, this encourages each layer to learn something different from the previous layers, so that each additional layer adds information.</p></li>
<li><p><strong>Wide and deep networks</strong>: Wide, shallow networks have a tendency to simply memorize the mapping between their inputs and their outputs.  Deep networks generalize much better.  Usually you want good generalization, but there are some situations, like recommendation systems, in which simple memorization without generalization is important, too.  In these cases you want to provide good, substantive solutions when a user makes a general query, but very precise solutions when the user makes a very specific query.  <a href=""https://arxiv.org/abs/1606.07792"" rel=""noreferrer"">Wide and deep networks</a> are able to fulfill this task nicely.</p></li>
<li><p><strong>Neural turing machine</strong>: A shortcoming of traditional recurrent NNs (whether they be the standard RNN or something more sophisticated like an LSTM) is that their memory is somewhat ""intuitive"".  They manage to remember past inputs by saving the hidden layer activations they produce into the future.  However, sometimes it makes more sense to explicitly store some data.  (This might be the difference between writing a phone number down on a piece of paper vs. remembering that the number had around 7 digits and there were a couple of 3s in there and maybe a dash somewhere in the middle.)  The <a href=""https://arxiv.org/abs/1410.5401"" rel=""noreferrer"">neural Turing machine</a> is a way to try to address this issue.  The idea is that the network can learn to explicitly commit certain facts to a memory bank.  This is not straightforward to do because backprop algorithms require differentiable functions, but committing a datum to a memory address is an inherently discrete operation.  Consequently, neural Turing machines get around this by committing a little bit of data to a distribution of different memory addresses.  These architectures don't seem to work super well yet, but the idea is very important.  Some variant of these will probably become widespread in the future.</p></li>
<li><p><strong>Generative adversarial networks</strong>: <a href=""https://arxiv.org/abs/1406.2661"" rel=""noreferrer"">GANs</a> are a very exciting idea that seems to be seeing a lot of practical use already.  The idea here is to train two NNs simultaneously: one that tries to generate samples from the underlying probability distribution (a generator), and one that tries to distinguish between real data points and the fake data points generated by the generator (a discriminator).  So, for example, if your dataset is a collection of <a href=""https://arxiv.org/abs/1511.06434v2"" rel=""noreferrer"">pictures of bedrooms</a>, the generator will try to make its own pictures of bedrooms, and the discriminator will try to figure out if it's looking at real pictures of bedrooms or fake pictures of bedrooms.  In the end, you have two very useful NNs: one that is really good at classifying images as being bedrooms or not bedrooms, and one that is really good at generating realistic images of bedrooms.</p></li>
</ul>
","<neural-network><deep-learning>","26","26","26","0","18416","891"
"8924","{
  ""id"": 8924,
  ""title"": ""Removing strings after a certain character in a given text""
}","<p>For instance:</p>

<pre><code> rs&lt;-c(""copyright @ The Society of mo"",""I want you to meet me @ the coffeshop"")
 s&lt;-gsub(""@.*"","""",rs)
 s
 [1] ""copyright ""             ""I want you to meet me ""
</code></pre>

<p>Or, if you want to keep the @ character:</p>

<pre><code> s&lt;-gsub(""(@).*"",""\\1"",rs)
 s
 [1] ""copyright @""             ""I want you to meet me @""
</code></pre>

<p>EDIT: If what you want is to remove everything from the last @ on you just have to follow this previous example with the appropriate regex. Example:</p>

<pre><code>rs&lt;-c(""copyright @ The Society of mo located @ my house"",""I want you to meet me @ the coffeshop"")
s&lt;-gsub(""(.*)@.*"",""\\1"",rs)
s
[1] ""copyright @ The Society of mo located "" ""I want you to meet me ""
</code></pre>

<p>Given the matching we are looking for, both sub and gsub will give you the same answer.</p>
","<r><data-cleaning>","26","26","26","0","12603","461"
"6718","{
  ""id"": 6718,
  ""title"": ""Is it necessary to standardize your data before clustering?""
}","<p>Normalization is not always required, but it rarely hurts. </p>

<p>Some examples:</p>

<p><a href=""https://stats.stackexchange.com/a/21226/12359"">K-means</a>:</p>

<blockquote>
  <p>K-means clustering is ""isotropic"" in all directions of space and
  therefore tends to produce more or less round (rather than elongated)
  clusters. In this situation leaving variances unequal is equivalent to
  putting more weight on variables with smaller variance.</p>
</blockquote>

<p>Example in Matlab:</p>

<pre><code>X = [randn(100,2)+ones(100,2);...
     randn(100,2)-ones(100,2)];

% Introduce denormalization
% X(:, 2) = X(:, 2) * 1000 + 500;

opts = statset('Display','final');

[idx,ctrs] = kmeans(X,2,...
                    'Distance','city',...
                    'Replicates',5,...
                    'Options',opts);

plot(X(idx==1,1),X(idx==1,2),'r.','MarkerSize',12)
hold on
plot(X(idx==2,1),X(idx==2,2),'b.','MarkerSize',12)
plot(ctrs(:,1),ctrs(:,2),'kx',...
     'MarkerSize',12,'LineWidth',2)
plot(ctrs(:,1),ctrs(:,2),'ko',...
     'MarkerSize',12,'LineWidth',2)
legend('Cluster 1','Cluster 2','Centroids',...
       'Location','NW')
title('K-means with normalization')
</code></pre>

<p><img src=""https://i.stack.imgur.com/N2unM.png"" alt=""enter image description here""></p>

<p><img src=""https://i.stack.imgur.com/e5G2M.png"" alt=""enter image description here""></p>

<p>(FYI: <a href=""https://www.quora.com/Machine-Learning/How-can-I-detect-if-my-dataset-is-clustered-or-unclustered-i-e-forming-one-single-cluster/answer/Franck-Dernoncourt"" rel=""noreferrer"">How can I detect if my dataset is clustered or unclustered (i.e. forming one single cluster</a>)</p>

<p><a href=""http://www.medwelljournals.com/fulltext/?doi=ijscomp.2009.168.172"" rel=""noreferrer"">Distributed clustering</a>:</p>

<blockquote>
  <p>The comparative analysis shows that the distributed clustering results
  depend on the type of normalization procedure.</p>
</blockquote>

<p><a href=""https://stackoverflow.com/a/4674770/395857"">Artificial neural network (inputs)</a>:</p>

<blockquote>
  <p>If the input variables are combined linearly, as in an MLP, then it is
  rarely strictly necessary to standardize the inputs, at least in
  theory. The reason is that any rescaling of an input vector can be
  effectively undone by changing the corresponding weights and biases,
  leaving you with the exact same outputs as you had before. However,
  there are a variety of practical reasons why standardizing the inputs
  can make training faster and reduce the chances of getting stuck in
  local optima. Also, weight decay and Bayesian estimation can be done
  more conveniently with standardized inputs.</p>
</blockquote>

<p><a href=""http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html"" rel=""noreferrer"">Artificial neural network (inputs/outputs)</a></p>

<blockquote>
  <p>Should you do any of these things to your data? The answer is, it
  depends.</p>
  
  <p>Standardizing either input or target variables tends to make the training
  process better behaved by improving the numerical condition (see 
  <a href=""ftp://ftp.sas.com/pub/neural/illcond/illcond.html"" rel=""noreferrer"">ftp://ftp.sas.com/pub/neural/illcond/illcond.html</a>) of the optimization
  problem and ensuring that various default values involved in
  initialization and termination are appropriate. Standardizing targets
  can also affect the objective function. </p>
  
  <p>Standardization of cases should be approached with caution because it
  discards information. If that information is irrelevant, then
  standardizing cases can be quite helpful. If that information is
  important, then standardizing cases can be disastrous.</p>
</blockquote>

<hr>

<p>Interestingly, changing the measurement units may even lead one to see a very different clustering structure: <a href=""http://rads.stackoverflow.com/amzn/click/0471735787"" rel=""noreferrer"">Kaufman, Leonard, and Peter J. Rousseeuw.. ""Finding groups in data: An introduction to cluster analysis."" (2005).</a></p>

<blockquote>
  <p>In some applications, changing the measurement units may even lead one
  to see a very different clustering structure. For example, the age (in
  years) and height (in centimeters) of four imaginary people are given
  in Table 3 and plotted in Figure 3. It appears that {A, B ) and { C,
  0) are two well-separated clusters. On the other hand, when height is
  expressed in feet one obtains Table 4 and Figure 4, where the obvious
  clusters are now {A, C} and { B, D}. This partition is completely
  different from the first because each subject has received another
  companion. (Figure 4 would have been flattened even more if age had
  been measured in days.)</p>
  
  <p>To avoid this dependence on the choice of measurement units, one has
  the option of  standardizing the data. This converts the original
  measurements to unitless variables.</p>
</blockquote>

<p><img src=""https://i.stack.imgur.com/Ppu5Y.png"" alt=""enter image description here"">
<img src=""https://i.stack.imgur.com/zW6rI.png"" alt=""enter image description here""></p>

<p><a href=""http://rads.stackoverflow.com/amzn/click/0471735787"" rel=""noreferrer"">Kaufman et al.</a> continues with some interesting considerations (page 11):</p>

<blockquote>
  <p>From a philosophical point of view, standardization does not really
  solve the problem. Indeed, the choice of measurement units gives rise
  to relative weights of the variables. Expressing a variable in smaller
  units will lead to a larger range for that variable, which will then
  have a large effect on the resulting structure. On the other hand, by
  standardizing one attempts to give all variables an equal weight, in
  the hope of achieving objectivity. As such, it may be used by a
  practitioner who possesses no prior knowledge. However, it may well be
  that some variables are intrinsically more important than others in a
  particular application, and then the assignment of weights should be
  based on subject-matter knowledge (see, e.g., Abrahamowicz, 1985). On
  the other hand, there have been attempts to devise clustering
  techniques that are independent of the scale of the variables
  (Friedman and Rubin, 1967). The proposal of Hardy and Rasson (1982) is
  to search for a partition that minimizes the total volume of the
  convex hulls of the clusters. In principle such a method is invariant
  with respect to linear transformations of the data, but unfortunately
  no algorithm exists for its implementation (except for an
  approximation that is restricted to two dimensions). Therefore, the
  dilemma of standardization appears unavoidable at present and the
  programs described in this book leave the choice up to the user.</p>
</blockquote>
","<python><clustering><anomaly-detection>","26","26","26","0","843","5095"
"55987","{
  ""id"": 55987,
  ""title"": ""Sparse_categorical_crossentropy vs categorical_crossentropy (keras, accuracy)""
}","<p><strong>The answer, in a nutshell</strong></p>
<p>If your targets are one-hot encoded, use <code>categorical_crossentropy</code>.
Examples of one-hot encodings:</p>
<pre><code>[1,0,0]
[0,1,0] 
[0,0,1]
</code></pre>
<p>But if your targets are integers, use <code>sparse_categorical_crossentropy</code>.
Examples of integer encodings (for the sake of completion):</p>
<pre><code>1
2
3
</code></pre>
","<neural-network><keras><loss-function><encoding>","26","26","26","0","78035","261"
"15766","{
  ""id"": 15766,
  ""title"": ""NLP - why is \""not\"" a stop word?""
}","<p><a href=""https://en.wikipedia.org/wiki/Stop_words"" rel=""noreferrer"">Stop words</a> are usually thought of as ""the most common words in a language"". However, other definitions based on different tasks are possible. </p>

<p>It clearly makes sense to consider 'not' as a stop word if your task is based on word <strong>frequencies</strong> (e.g. <a href=""https://en.wikipedia.org/wiki/Tf%E2%80%93idf"" rel=""noreferrer"">tf–idf</a> analysis for document classification). </p>

<p>If you're concerned with the context (e.g. <a href=""https://en.wikipedia.org/wiki/Sentiment_analysis"" rel=""noreferrer"">sentiment analysis</a>) of the text it might make sense to treat negation words differently. Negation changes the so-called <em>valence</em> of a text. This needs to be treated carefully and is usually not trivial. One example would be the Twitter negation corpus. An explanation of the approach is given in <a href=""https://www.cs.cmu.edu/~ark/EMNLP-2015/proceedings/WASSA/pdf/WASSA14.pdf"" rel=""noreferrer"">this paper</a>.</p>
","<nlp><topic-model><sentiment-analysis>","26","26","26","0","23305","5682"
"22033","{
  ""id"": 22033,
  ""title"": ""Encoding categorical variables using likelihood estimation""
}","<p>I was learning this topic too, and these are what I found:</p>

<ul>
<li><p>This type of encoding is called <em>likelihood encoding</em>, <em>impact coding</em> or <em>target coding</em></p></li>
<li><p>The idea is encoding your categorical variable with the use of target variable (continuous or categorical depending on the task). For example, if you have regression task, you can encode your categorical variable with the mean of the target. For every category, you calculate the corresponding mean of the target (among this category) and replace the value of a category with this mean. </p></li>
<li><p>If you have classification task, you calculate the relative frequency of your target with respect to every category value. </p></li>
<li><p>From a mathematical point of view, this encoding means a probability of your target, conditional on each category value.</p></li>
<li><p>If you do it in a simple way, how I described above, you will probably get a biased estimation. That's why in Kaggle community they usually use 2 levels of cross-validation. Read <a href=""https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/36136#201638"" rel=""nofollow noreferrer"">this comment by raddar here</a>. The corresponding notebook is <a href=""https://www.kaggle.com/raddar/raddar-extratrees"" rel=""nofollow noreferrer"">here</a>.</p></li>
</ul>

<p><strong>The quote:</strong> </p>

<blockquote>
  <p>It's taking mean value of y. But not plain mean, but in cross-validation within cross-validation way;</p>
  
  <p>Let's say we have 20-fold cross validation. we need somehow to calculate mean value of the feature for #1 fold using information from #2-#20 folds only.</p>
  
  <p>So, you take #2-#20 folds, create another cross validation set within it (i did 10-fold). calculate means for every leave-one-out fold (in the end you get 10 means). You average these 10 means and apply that vector for your primary #1 validation set. Repeat that for remaining 19 folds.</p>
  
  <p>It is tough to explain, hard to understand and to master :) But if done correctly it can bring many benefits:)</p>
</blockquote>

<ul>
<li><p>Another implementation of this encoding <a href=""https://www.kaggle.com/tnarik/likelihood-encoding-of-categorical-features"" rel=""nofollow noreferrer"">is here</a>.</p></li>
<li><p>In R library <strong>vtreat</strong> they have implementation of impact encoding. See <a href=""http://www.win-vector.com/blog/2014/08/vtreat-designing-a-package-for-variable-treatment/"" rel=""nofollow noreferrer"">this post</a>.</p></li>
<li><p>In <a href=""https://tech.yandex.com/catboost/doc/dg/concepts/algorithm-main-stages_cat-to-numberic-docpage/"" rel=""nofollow noreferrer"">CatBoost library</a> they have a lot of options for categorical variable encoding including target encoding. </p></li>
<li><p>There is no such encoding in sklearn yet.</p></li>
</ul>

<hr>

<p>UPDATE: There is a nice package for sklearn models and pipelines! <a href=""https://github.com/scikit-learn-contrib/category_encoders"" rel=""nofollow noreferrer"">https://github.com/scikit-learn-contrib/category_encoders</a></p>
","<feature-engineering>","26","26","26","0","37630","361"
"10945","{
  ""id"": 10945,
  ""title"": ""Why is xgboost so much faster than sklearn GradientBoostingClassifier?""
}","<p>Since you mention ""numeric"" features, I guess your features are not categorical and have a high arity (they can take a lot of different values, and thus there are a lot of possible split points). In such a case, growing trees is difficult since there are [a lot of features $\times$ a lot of split points] to evaluate.</p>

<p>My guess is that the biggest effect comes from the fact that XGBoost uses an approximation on the split points. If you have a continuous feature with 10000 possible splits, XGBoost consider only ""the best"" 300 splits by default (this is a simplification). This behavior is controlled by the <code>sketch_eps</code> parameter, and you can read more about it <a href=""https://xgboost.readthedocs.org/en/latest/parameter.html#parameters-for-tree-booster"" rel=""noreferrer"">in the doc</a>. You can try lowering it and check the difference it makes. Since there is no mention of it in the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"" rel=""noreferrer"">scikit-learn documentation</a>, I guess it is not available. You can learn what XGBoost method is in the <a href=""http://arxiv.org/abs/1603.02754"" rel=""noreferrer"">their paper (arxiv)</a>.</p>

<p>XGBoost also uses an approximation on the evaluation of such split points. I do not know by which criterion scikit learn is evaluating the splits, but it could explain the rest of the time difference.</p>

<hr>

<p><strong>Adressing Comments</strong></p>

<p>Regarding the evaluation of split points</p>

<blockquote>
  <p>However, what did you mean by ""XGBoost also uses an approximation on the evaluation of such split points""? as far as I understand, for the evaluation they are using the exact reduction in the optimal objective function, as it appears in eq (7) in the paper.</p>
</blockquote>

<p>In order to evaluate the split point, you would have to compute $L(y,H_{i-1}+h_i)$ where $L$ is the cost function, $y$ the target, $H_{i-1}$ the model built until now, and $h_i$ the current addition. Notice that this is not what XGBoost is doing; they are simplifying the cost function $L$ by a Taylor Expansion, which leads to a very simple function to compute. They have to compute the Gradient and the Hessian of $L$ with respect to $H_{i-1}$, and they can reuse those number for all potential splits at stage $i$, making the overral computation fast. You can check <a href=""https://stats.stackexchange.com/questions/202858/loss-function-approximation-with-taylor-expansion/204377"">Loss function Approximation With Taylor Expansion (CrossValidated Q/A)</a> for more details, or the derivation in their paper.</p>

<p>The point is that they have found a way to approximate $L(y,H_{i-1} + h_i)$ efficiently. If you were to evaluate $L$ fully, without insider knowledge allowing optimisation or avoidance or redundant computation, it would take more time per split. It this regard, it is an approximation. However, other gradient boosting implementations also use a proxy cost functions to evaluate the splits, and I do not know whether XGBoost approximation is quicker in this regards than the others.</p>
","<scikit-learn><xgboost><gbm>","26","26","26","0","15501","1256"
"16703","{
  ""id"": 16703,
  ""title"": ""Confused about how to apply KMeans on my a dataset with features extracted""
}","<p>For clustering, your data must be indeed integers. Moreover, since k-means is using euclidean distance, having categorical column is not a good idea. Therefore you should also encode the column <code>timeOfDay</code> into three dummy variables. Lastly, don't forget to standardize your data. This might be not important in your case, but in general, you risk that the algorithm will be pulled into direction with largest values, which is not what you want.</p>

<p>So I downloaded your data, put into .csv and made a very simple example. You can see that I am using different dataframe for the clustering itself and then once I retrieve the cluster labels, I add them to the previous one.</p>

<p>Note that I omit the variable timestamp - since the value is unique for every record, it will only confuse the algorithm. </p>

<pre><code>import pandas as pd
from scipy import stats
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('C:/.../Dataset.csv',sep=';')

#Make a copy of DF
df_tr = df

#Transsform the timeOfDay to dummies
df_tr = pd.get_dummies(df_tr, columns=['timeOfDay'])

#Standardize
clmns = ['Wattage', 'Duration','timeOfDay_Afternoon', 'timeOfDay_Evening',
         'timeOfDay_Morning']
df_tr_std = stats.zscore(df_tr[clmns])

#Cluster the data
kmeans = KMeans(n_clusters=2, random_state=0).fit(df_tr_std)
labels = kmeans.labels_

#Glue back to originaal data
df_tr['clusters'] = labels

#Add the column into our list
clmns.extend(['clusters'])

#Lets analyze the clusters
print df_tr[clmns].groupby(['clusters']).mean()
</code></pre>

<p>This can tell us what are the differences between the clusters. It shows mean values of the attribute per each cluster. Looks like cluster 0 are evening people with high consumption, whilst 1 are morning people with small consumption.</p>

<pre><code>clusters  Wattage     Duration   timeOfDay_Afternoon  timeOfDay_Evening timeOfDay_Morning   
0         225.000000  85.000000             0.166667           0.833333  0.0 
1         109.666667  30.166667             0.500000           0.000000  0.5
</code></pre>

<p>You asked for visualization as well. This is tricky, because everything above two dimensions is difficult to read. So i put on scatter plot <code>Duration</code> against <code>Wattage</code> and colored the dots based on cluster.</p>

<p>You can see that it looks quite reasonable, except the one blue dot there.</p>

<pre><code>#Scatter plot of Wattage and Duration
sns.lmplot('Wattage', 'Duration', 
           data=df_tr, 
           fit_reg=False, 
           hue=""clusters"",  
           scatter_kws={""marker"": ""D"", 
                        ""s"": 100})
plt.title('Clusters Wattage vs Duration')
plt.xlabel('Wattage')
plt.ylabel('Duration')
</code></pre>

<p><a href=""https://i.stack.imgur.com/jpFkN.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jpFkN.png"" alt=""enter image description here""></a></p>
","<python><clustering><k-means><unsupervised-learning>","26","26","26","0","17290","1561"
"28211","{
  ""id"": 28211,
  ""title"": ""Keras Callback example for saving a model after every epoch?""
}","<p>Setting 'save_weights_only' to False in the Keras <a href=""https://keras.io/callbacks/"" rel=""noreferrer"">callback</a> 'ModelCheckpoint' will save the full model; this example taken from the link above will save a full model every epoch, regardless of performance:</p>

<pre><code>keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)
</code></pre>

<p>Some more examples are found <a href=""https://machinelearningmastery.com/check-point-deep-learning-models-keras/"" rel=""noreferrer"">here</a>, including saving only improved models and loading the saved models. </p>
","<python><keras>","26","26","26","0","32697","1448"
"327","{
  ""id"": 327,
  ""title"": ""Python vs R for machine learning""
}","<p>There is nothing like ""python is better"" or ""R is much better than x"". </p>

<p>The only fact I know is that in the industry allots of people stick to python because that is what they learned at the university. The python community is really active and have a few great frameworks for ML and data mining etc. </p>

<p>But to be honest, if you get a good c programmer he can do the same as people do in python or r, if you got a good java programmer he can also do (near to) everything in java. </p>

<p>So just stick with the language you are comfortable with.</p>
","<machine-learning><r><python>","26","26","26","0","115","579"
"13522","{
  ""id"": 13522,
  ""title"": ""Why do internet companies prefer Java/Python for data scientist job?""
}","<p>There may be a lot of reasons like: </p>

<ol>
<li><p>Workforce flexibility: One Java / Python programmers can be moved to other tasks or projects easily.</p></li>
<li><p>Candidates availability: there are plenty of Java / Python programmers. You do not want to introduce a new programming language to later find out that there are no qualified workers or they are just too expensive. </p></li>
<li><p>Integration and ETL: Sometimes getting the data with the right quality is the hardest part of the project. So it is natural to use the same language as the rest of the systems.</p></li>
<li><p>Business model definition: Most business rules and business models are already written in this languages. </p></li>
<li><p>Just keeping things simple. It is already hard enough to be up-to-date with the technologies. A diverse base of language can be chaotic. R for this, Ruby for that, Scala, Clojure, F#, Swift, Dart... They may need different servers, different pathes, a hell to administer. All have their own IDEs with tools and plugins (not always free).  See some Uncle Bob's points about <a href=""http://blog.cleancoder.com/uncle-bob/2016/05/21/BlueNoYellow.html"">languages choice</a> and <a href=""http://blog.cleancoder.com/uncle-bob/2016/07/27/TheChurn.html"">new technologies</a></p></li>
</ol>

<p>So even if you have a 5% - 15% productivity advantage using R for the specific task, they may prefer a tool that just does the job even if not in the most efficient way. </p>
","<beginner><tools><career><reference-request>","26","26","26","0","23521","361"
"48542","{
  ""id"": 48542,
  ""title"": ""How much of data wrangling is a data scientist's job?""
}","<blockquote>
  <p>Feels like most of the work is not related to data science at all. Is this accurate?</p>
</blockquote>

<p>This is the reality of any data science project. Google actually measured it and published a paper ""Hidden Technical Debt in Machine Learning Systems"" <a href=""https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf"" rel=""noreferrer"">https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf</a></p>

<p><a href=""https://i.stack.imgur.com/R9OIe.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/R9OIe.jpg"" alt=""enter image description here""></a></p>

<p>Result of the paper reflects my experience as well. Vast majority of time is spent in acquiring, cleaning and processing data. </p>
","<data-wrangling>","26","26","26","0","41771","2126"
"34452","{
  ""id"": 34452,
  ""title"": ""What is the difference between fit() and fit_generator() in Keras?""
}","<p>In keras, <code>fit()</code> is much similar to sklearn's fit method, where you pass array of features as x values and target as y values. You pass your whole dataset at once in fit method. Also, use it if you can load whole data into your memory (small dataset).</p>

<p>In <code>fit_generator()</code>, you don't pass the x and y directly, instead they come from a <strong>generator</strong>. As it is written in <a href=""https://keras.io/models/sequential/"" rel=""noreferrer"">keras documentation</a>, generator is used when you want to avoid duplicate data when using multiprocessing. This is for practical purpose, when you have large dataset. </p>

<p>Here is a link to understand more about this- </p>

<p><a href=""https://towardsdatascience.com/keras-a-thing-you-should-know-about-keras-if-you-plan-to-train-a-deep-learning-model-on-a-large-fdd63ce66bd2"" rel=""noreferrer"">A thing you should know about Keras if you plan to train a deep learning model on a large dataset</a></p>

<p>For reference you can check this book- <a href=""https://github.com/hktxt/bookshelf/blob/master/Computer%20Science/Deep%20Learning%20with%20Python%2C%20Fran%C3%A7ois%20Chollet.pdf"" rel=""noreferrer"">https://github.com/hktxt/bookshelf/blob/master/Computer%20Science/Deep%20Learning%20with%20Python%2C%20Fran%C3%A7ois%20Chollet.pdf</a></p>
","<deep-learning><keras>","26","26","26","0","44139","1619"
"5888","{
  ""id"": 5888,
  ""title"": ""How to scale an array of signed integers to range from 0 to 1?""
}","<p>This is called unity-based normalization. If you have a vector $X$, you can obtain a normalized version of it, say $Z$, by doing:</p>

<p>$$Z = \frac{X - \min(X)}{\max(X) - \min(X)}$$ </p>
","<machine-learning><neural-network><feature-scaling><normalization><javascript>","26","26","26","0","9766","543"
"12646","{
  ""id"": 12646,
  ""title"": ""How to count the number of missing values in each row in Pandas dataframe?""
}","<p>You can apply a count over the rows like this:</p>

<pre><code>test_df.apply(lambda x: x.count(), axis=1)
</code></pre>

<p>test_df:</p>

<pre><code>    A   B   C
0:  1   1   3
1:  2   nan nan
2:  nan nan nan
</code></pre>

<p>output:</p>

<pre><code>0:  3
1:  1
2:  0
</code></pre>

<p>You can add the result as a column like this:</p>

<pre><code>test_df['full_count'] = test_df.apply(lambda x: x.count(), axis=1)
</code></pre>

<p>Result:</p>

<pre><code>    A   B   C   full_count
0:  1   1   3   3
1:  2   nan nan 1
2:  nan nan nan 0
</code></pre>
","<python><pandas>","26","26","26","0","14904","8748"
"20082","{
  ""id"": 20082,
  ""title"": ""How do I load FastText pretrained model with Gensim?""
}","<p>Here's the link for the methods available for fasttext implementation in gensim <a href=""https://github.com/piskvorky/gensim/blob/develop/gensim/models/wrappers/fasttext.py"" rel=""noreferrer"">fasttext.py</a></p>

<pre class=""lang-py prettyprint-override""><code>from gensim.models.wrappers import FastText

model = FastText.load_fasttext_format('wiki.simple')

print(model.most_similar('teacher'))
# Output = [('headteacher', 0.8075869083404541), ('schoolteacher', 0.7955552339553833), ('teachers', 0.733420729637146), ('teaches', 0.6839243173599243), ('meacher', 0.6825737357139587), ('teach', 0.6285147070884705), ('taught', 0.6244685649871826), ('teaching', 0.6199781894683838), ('schoolmaster', 0.6037642955780029), ('lessons', 0.5812176465988159)]

print(model.similarity('teacher', 'teaches'))
# Output = 0.683924396754
</code></pre>
","<nlp><gensim>","25","25","25","0","33970","663"
"30642","{
  ""id"": 30642,
  ""title"": ""How do you visualize neural network architectures?""
}","<p>There is an open source project called <a href=""https://github.com/lutzroeder/Netron"" rel=""noreferrer"">Netron</a></p>
<blockquote>
<p>Netron is a viewer for neural network, deep learning and machine learning models.</p>
<p>Netron supports ONNX (.onnx, .pb), Keras (.h5, .keras), CoreML (.mlmodel) and TensorFlow Lite (.tflite). Netron has experimental support for Caffe (.caffemodel), Caffe2 (predict_net.pb), MXNet (-symbol.json), TensorFlow.js (model.json, .pb) and TensorFlow (.pb, .meta).</p>
</blockquote>
<p><a href=""https://i.stack.imgur.com/R7PT1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/R7PT1.png"" alt=""enter image description here"" /></a></p>
","<machine-learning><neural-network><deep-learning><visualization>","25","25","25","0","50932","351"
"6059","{
  ""id"": 6059,
  ""title"": ""Should I use a decision tree or logistic regression for classification?""
}","<p><strong>Long story short</strong>: <em>do what @untitledprogrammer said, try both models and cross-validate to help pick one.</em></p>

<p>Both decision trees (depending on the implementation, e.g. C4.5) and logistic regression should be able to handle continuous and categorical data just fine. For logistic regression, you'll want to <a href=""https://stats.stackexchange.com/questions/115049/why-do-we-need-to-dummy-code-categorical-variables"">dummy code your categorical variables</a>.</p>

<p>As @untitledprogrammer mentioned, it's difficult to know a priori which technique will be better based simply on the types of features you have, continuous or otherwise. It really depends on your specific problem and the data you have. (See <a href=""http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=585893"" rel=""noreferrer"">No Free Lunch Theorem</a>)</p>

<p>You'll want to keep in mind though that a logistic regression model is searching for a single linear decision boundary in your feature space, whereas a decision tree is essentially partitioning your feature space into half-spaces using <em>axis-aligned</em> linear decision boundaries. The net effect is that you have a non-linear decision boundary, possibly more than one. </p>

<p>This is nice when your data points aren't easily separated by a single hyperplane, but on the other hand, decisions trees are so flexible that they can be prone to overfitting. To combat this, you can try pruning. Logistic regression tends to be less susceptible (but not immune!) to overfitting.</p>

<p>Lastly, another thing to consider is that decision trees can automatically take into account interactions between variables, e.g. $xy$ if you have two independent features $x$ and $y$. With logistic regression, you'll have to manually add those interaction terms yourself.</p>

<p>So you have to ask yourself: </p>

<ul>
<li>what kind of decision boundary makes more sense in your particular problem?</li>
<li>how do you want to balance bias and variance?</li>
<li>are there interactions between my features?</li>
</ul>

<p>Of course, it's always a good idea to just try both models and do cross-validation. This will help you find out which one is more likely to have better generalization error.</p>
","<classification><logistic-regression><decision-trees>","25","25","25","0","9576","466"
"2648","{
  ""id"": 2648,
  ""title"": ""Extract most informative parts of text from documents""
}","<p>What you're describing is often achieved using a simple combination of <a href=""http://en.wikipedia.org/wiki/Tf%E2%80%93idf"">TF-IDF</a> and <a href=""http://en.wikipedia.org/wiki/Automatic_summarization#Extraction-based_summarization"">extractive summarization</a>.</p>

<p>In a nutshell, TF-IDF tells you the relative importance of each word in each document, in comparison to the rest of your corpus. At this point, you have a score for each word in each document approximating its ""importance."" Then you can use these individual word scores to compute a composite score for each sentence by summing the scores of each word in each sentence. Finally, simply take the top-N scoring sentences from each document as its summary.</p>

<p>Earlier this year, I put together an iPython Notebook that culminates with an implementation of this in Python using NLTK and Scikit-learn: <a href=""https://github.com/charlieg/A-Smattering-of-NLP-in-Python"">A Smattering of NLP in Python</a>.</p>
","<nlp><text-mining>","25","25","25","0","819","1481"
"26642","{
  ""id"": 26642,
  ""title"": ""How to set the number of neurons and layers in neural networks""
}","<p>The consideration of the number of neurons for each layer and number of layers in <strong>fully connected networks</strong> depends on the feature space of the problem. For illustrating what happens in the two dimensional cases in order to depict, I use 2-d space. I have used images from the works of <a href=""https://scholar.google.com/citations?user=HOauvegAAAAJ&amp;hl=en"" rel=""noreferrer"">a scientist</a>. For understanding other nets like <code>CNN</code> I recommend you taking a look at <a href=""https://datascience.stackexchange.com/a/26291/28175"">here</a>.</p>

<p>Suppose you have just a single neuron, in this case after learning the parameters of the network you will have a linear decision boundary which can separate the space to two individual classes. </p>

<p><a href=""https://i.stack.imgur.com/Yo1rq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Yo1rq.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/gYDvI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gYDvI.png"" alt=""enter image description here""></a></p>

<p>Suppose that you are asked to separate the following data. You will need <code>d1</code> which specifies the upper decision boundary and somehow is doing <code>AND</code> operation to determine whether the input data is on the left side of it or on the right side. Line <code>d2</code> is doing another <code>AND</code> operation which investigates whether the input data is upper than <code>d2</code> or not. In this case <code>d1</code> is trying to understand whether the input is on the left side of line to classify the input as <em>circle</em>, also <code>d2</code> is trying to figure out whether the input is on the right side of the line to classify the input as <em>circle</em>. Now we need another <code>AND</code> operation to wrap up the results of the two lines which are constructed after training their parameters. If the input is on the left side of <code>d1</code> and on the right side of <code>d2</code>, it should be classified as <em>circle</em>.</p>

<p><a href=""https://i.stack.imgur.com/JOB2t.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/JOB2t.png"" alt=""enter image description here""></a></p>

<p>Now suppose that you have the following problem and you are asked to separate the classes. In this case the justification is exactly like the above's.</p>

<p><a href=""https://i.stack.imgur.com/1kIMG.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1kIMG.png"" alt=""enter image description here""></a></p>

<p>For the following data:</p>

<p><a href=""https://i.stack.imgur.com/6hLmG.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6hLmG.png"" alt=""enter image description here""></a></p>

<p>the decision boundary is not convex and is more complex than the previous boundaries. First you have to have a sub-net which finds the inner circles. Then you have to have another sub-net which finds the inner rectangular decision boundary which decides the inputs which are inside of the rectangle are not circle and if they are outside, they are circle. After these, you have to wrap up the results and say if the input data is inside the bigger rectangle and outside of the inner rectangle, it should be classified as <em>circle</em>. You need another <code>AND</code> operation for this purpose. The network would be like this:</p>

<p><a href=""https://i.stack.imgur.com/A8FiS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/A8FiS.png"" alt=""enter image description here""></a></p>

<hr />

<p>Suppose that you are asked to find the following <strong>circled</strong> decision boundary.</p>

<p><a href=""https://i.stack.imgur.com/XXFsf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/XXFsf.png"" alt=""enter image description here""></a></p>

<p>In this case your network would be like the following network which was referred to but with much more neurons in the first hidden layer. </p>

<p><a href=""https://i.stack.imgur.com/UM1rH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/UM1rH.png"" alt=""enter image description here""></a></p>
","<machine-learning><neural-network><deep-learning><hyperparameter><hyperparameter-tuning>","25","25","25","0","28175","12688"
"13221","{
  ""id"": 13221,
  ""title"": ""How to normalize data for Neural Network and Decision Forest""
}","<p>I disagree with the other comments.</p>

<p>First of all, I see no need to normalize data for <strong>decision trees</strong>. Decision trees work by calculating a score (usually entropy) for each different division of the data $(X\leq x_i,X&gt;x_i)$. Applying a transformation to the data that does not change the order of the data makes no difference.</p>

<p><strong>Random forests</strong> are just a bunch of decision trees, so it doesn't change this rationale.</p>

<p><strong>Neural networks</strong> are a different story. First of all, in terms of prediction, it makes no difference. The neural network can easily counter your normalization since it just scales the weights and changes the bias. The big problem is in the training.</p>

<p>If you use an algorithm like <a href=""https://en.wikipedia.org/wiki/Rprop"" rel=""noreferrer"">resilient backpropagation</a> to estimate the weights of the neural network, then it makes <em>no</em> difference. The reason is because it uses the sign of the gradient, not its magnitude, when changing the weights in the direction of whatever minimizes your error. This is the default algorithm for the <code>neuralnet</code> package in R, by the way.</p>

<p>When does it make a difference? When you are using traditional backpropagation with sigmoid activation functions, it can <strong>saturate</strong> the sigmoid derivative.</p>

<p>Consider the sigmoid function (green) and its derivative (blue):</p>

<p><a href=""https://i.stack.imgur.com/OuyeO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/OuyeO.png"" alt=""sigmoid""></a></p>

<p>What happens if you do not normalize your data is that your data is multiplied by the random weights and you get things like $s'(9999)=0$. The derivative of the sigmoid is (approximately) zero and the training process does not move along. The neural network that you end up with is just a neural network with random weights (there is no training).</p>

<p>Does this help us to know what the best normalization function is? But of course! First of all, it is crucial to use a normalization that centers your data because most implementation initialize bias at zero. I would normalize between -0.5 and 0.5, $\frac{X-\min{X}}{\max{X}-\min{X}}-0.5$. But <a href=""https://en.wikipedia.org/wiki/Standard_score"" rel=""noreferrer"">standard score</a> is also good.</p>

<p>The actual normalization is not very crucial because it only influences the initial iterations of the optimization process. As long as it is centered and most of your data is below 1, then it might mean you have to use slightly less or more iterations to get the same result. But the result will be the same, as long as you avoid the saturation problem I mentioned.</p>

<p>There is something not here discussed which is <strong>regularization</strong>. If you use regularization in your objective function, the way you normalize your data <strong>will</strong> affect the resulting model. I'm assuming your are already familiar with this. If you know that one variable is more prone to cause overfitting, your normalization of the data should take this into account. This is of course completely independent of neural networks being used.</p>
","<neural-network><decision-trees><normalization>","25","25","25","0","16853","3182"
"28427","{
  ""id"": 28427,
  ""title"": ""Train Accuracy vs Test Accuracy vs Confusion matrix""
}","<h1>Definitions</h1>

<ul>
<li>Accuracy: The amount of correct classifications / the total amount
of classifications. </li>
<li>The train accuracy: The accuracy of a model on examples it was constructed on. </li>
<li>The test accuracy is the accuracy of a model on examples it hasn't seen. </li>
<li>Confusion matrix: A tabulation of the predicted class (usually
vertically) against the actual class (thus horizontally).</li>
</ul>

<h1>Overfitting</h1>

<p>What I would make up of your results is that your model is <strong>overfitting</strong>. You can tell that from  the large difference in accuracy between the test and train accuracy. Overfitting means that it learned rules  specifically for the train set, those rules do not generalize well beyond the train set.  </p>

<p>Your confusion matrix tells us how much it is overfitting, because your largest class makes up over 90% of the population. Assuming that you test and train set have a similar distribution, any useful model would have to score more than 90% accuracy:  A simple 0R-model would. Your model scores just under 80% on the test set.</p>

<h1>In depth look at the confusion matrix</h1>

<p>If you would look at the confusion matrix relatively (in percentages) it would look like this: </p>

<pre><code>               Actual    TOT
               1    2
Predicted 1 | 77% | 4% | 81%  
Predicted 2 | 17% | 2% | 19%
TOT         | 94% | 6% |
</code></pre>

<p>You can infer from the total in the first row that your model predicts <strong>Class 1</strong> 81% of the time, while the actual occurrence of <strong>Class 1</strong> is 94%. Hence your model is underestimating this class. <em>It could be the case that it learned specific (complex) rules on the train set, that work against you in the test set.</em></p>

<p>It could also be worth noting that even though the false negatives of <strong>Class 1</strong> (17%-point, row 2, column 1)) are hurting your overall performance most, the false negatives of <strong>Class 2</strong> (4%-point, row 1 column 2) are actually more common with respect to the total population of the respective classes (94%, 6%). This means that your model is bad at predicting <strong>Class 1</strong>, but even worse at predicting <strong>Class 2</strong>. The accuracy just for <strong>Class 1</strong> is 77/99 while the accuracy for <strong>Class 2</strong> is 2/6. </p>
","<python><predictive-modeling><accuracy><confusion-matrix><classifier>","25","25","25","0","27432","1324"
"18823","{
  ""id"": 18823,
  ""title"": ""Unbalanced multiclass data with XGBoost""
}","<p><code>scale_pos_weight</code> is used for binary classification as you stated. It is a more generalized solution to handle imbalanced classes. A good approach when assigning a value to <code>scale_pos_weight</code> is:</p>

<pre><code>sum(negative instances) / sum(positive instances)
</code></pre>

<p>For your specific case, there is another option in order to weight individual data points and take their weights into account while working with the booster, and let the optimization happen regarding their weights so that each point is represented equally. You just need to simply use:</p>

<pre><code>xgboost.DMatrix(..., weight = *weight array for individual weights*)
</code></pre>

<p>You can define the weights as you like and by doing so, you can even handle imbalances within classes as well as imbalances across different classes.</p>
","<classification><xgboost><multiclass-classification><class-imbalance>","25","25","25","0","32045","401"
"9448","{
  ""id"": 9448,
  ""title"": ""K-Means clustering for mixed numeric and categorical data""
}","<p>(In addition to the excellent answer by Tim Goodman)</p>

<p>The choice of k-modes is definitely the way to go for stability of the clustering algorithm used.</p>

<ol>
<li><p>The clustering algorithm is free to choose any distance metric / similarity score. Euclidean is the most popular. But any other metric can be used that scales according to the data distribution in each dimension /attribute, for example the Mahalanobis metric.
<a href=""https://i.stack.imgur.com/fjv5L.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fjv5L.jpg"" alt=""Illustrating the distance of data points from the center based on the distance metric used.""></a></p></li>
<li><p>With regards to mixed (numerical and categorical) clustering a good paper that might help is: <a href=""http://users.cis.fiu.edu/~lzhen001/activities/KDD2011Program/docs/p1127.pdf"" rel=""noreferrer"">INCONCO: Interpretable Clustering of Numerical and Categorical Objects</a></p></li>
<li><p>Beyond k-means: Since plain vanilla k-means has already been ruled out as an appropriate approach to this problem, I'll venture beyond to the idea of thinking of clustering as a model fitting problem. Different measures, like information-theoretic metric: Kullback-Liebler divergence work well when trying to converge a parametric model towards the data distribution.
(Of course parametric clustering techniques like GMM are slower than Kmeans, so there are drawbacks to consider)</p></li>
<li><p>Fuzzy k-modes clustering also sounds appealing since fuzzy logic techniques were developed to deal with something like categorical data. See <a href=""http://biosoft.kaist.ac.kr/~dhlee/pubs/papers/2004FuzzyClustgeringCategorical.pdf"" rel=""noreferrer"">Fuzzy clustering of categorical data using fuzzy centroids</a> for more information.</p></li>
</ol>

<p>Also check out: <a href=""http://theory.stanford.edu/~sudipto/mypapers/categorical.pdf"" rel=""noreferrer"">ROCK: A Robust Clustering Algorithm for Categorical Attributes</a></p>
","<data-mining><clustering><octave><k-means><categorical-data>","25","25","25","0","2515","1203"
"797","{
  ""id"": 797,
  ""title"": ""Uses of NoSQL database in data science""
}","<p>To be perfectly honest, most NoSQL databases are not very well suited to applications in big data. For the vast majority of all big data applications, the performance of <a href=""https://en.wikipedia.org/wiki/MongoDB"" rel=""noreferrer"">MongoDB</a> compared to a relational database like <a href=""http://en.wikipedia.org/wiki/MySQL"" rel=""noreferrer"">MySQL</a> is <a href=""http://www.moredevs.com/mysql-vs-mongodb-performance-benchmark/"" rel=""noreferrer"">significantly</a> is poor enough to warrant staying away from something like MongoDB entirely.</p>

<p>With that said, there are a couple of really useful properties of NoSQL databases that certainly work in your favor when you're working with large data sets, though the chance of those benefits outweighing the generally poor performance of NoSQL compared to <a href=""http://en.wikipedia.org/wiki/SQL"" rel=""noreferrer"">SQL</a> for read-intensive operations (most similar to typical big data use cases) is low.</p>

<ul>
<li><strong>No Schema</strong> - If you're working with a lot of unstructured data, it might be hard to actually decide on and rigidly apply a schema. NoSQL databases in general are very supporting of this, and will allow you to insert schema-less documents on the fly, which is certainly not something an SQL database will support.</li>
<li><strong><a href=""http://en.wikipedia.org/wiki/JSON"" rel=""noreferrer"">JSON</a></strong> - If you happen to be working with JSON-style documents instead of with <a href=""http://en.wikipedia.org/wiki/Comma-separated_values"" rel=""noreferrer"">CSV</a> files, then you'll see a lot of advantage in using something like MongoDB for a database-layer. Generally the workflow savings don't outweigh the increased query-times though.</li>
<li><strong>Ease of Use</strong> - I'm not saying that SQL databases are always hard to use, or that <a href=""http://en.wikipedia.org/wiki/Apache_Cassandra"" rel=""noreferrer"">Cassandra</a> is the easiest thing in the world to set up, but in general NoSQL databases are easier to set up and use than SQL databases. MongoDB is a particularly strong example of this, known for being one of the easiest database layers to use (outside of <a href=""http://en.wikipedia.org/wiki/SQLite"" rel=""noreferrer"">SQLite</a>). SQL also deals with a lot of normalization and there's a large legacy of SQL best practices that just generally bogs down the development process.</li>
</ul>

<p>Personally I might suggest you also check out <a href=""http://en.wikipedia.org/wiki/Graph_database"" rel=""noreferrer"">graph databases</a> such as <a href=""http://en.wikipedia.org/wiki/Neo4j"" rel=""noreferrer"">Neo4j</a> that show really good performance for certain types of queries if you're looking into picking out a backend for your data science applications.</p>
","<bigdata><nosql><mongodb>","25","25","25","0","548","4069"
"20078","{
  ""id"": 20078,
  ""title"": ""Word2Vec vs. Sentence2Vec vs. Doc2Vec""
}","<p>Well the names are pretty straight-forward and should give you a clear idea of vector representations.</p>
<p>The  Word2Vec Algorithm builds distributed semantic representation of words. There are two main approaches to training, Continuous Bag of Words and The skip gram model. One involves predicting the context words using a centre word, while the other involves predicting the word using the context words. You can read about it in much detail in Mikolov's  <a href=""https://www.google.co.in/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://web2.cs.columbia.edu/%7Eblei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf&amp;ved=0ahUKEwiQpu_XguXUAhWIpo8KHQuHDNUQFggcMAA&amp;usg=AFQjCNG3xiM98LkL7kkCQwCHfj0Iu8Ngdg"" rel=""nofollow noreferrer"">paper</a>.</p>
<p>The same idea can be extended to sentences and complete documents where instead of learning feature representations for words, you learn it for sentences or documents. However, to get a general idea of a SentenceToVec, think of it as a mathematical average of the word vector representations of all the words in the sentence. You can get a very good approximation just by averaging and without training any SentenceToVec but of course, it has its limitations.</p>
<p>Doc2Vec extends the idea of SentenceToVec or rather Word2Vec because sentences can also be considered as documents. The idea of training remains similar. You can read Mikolov's Doc2Vec <a href=""https://www.google.co.in/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://cs.stanford.edu/%7Equocle/paragraph_vector.pdf&amp;ved=0ahUKEwiQob-GhOXUAhWCqY8KHaiHCnMQFggcMAA&amp;usg=AFQjCNE-2SsR07iQm78dQqVV-6_Y3ERurA"" rel=""nofollow noreferrer"">paper</a> for more details.</p>
<p>Coming to the applications, it would depend on the task. A Word2Vec effectively captures semantic relations between words hence can be used to calculate word similarities or fed as features to various NLP tasks such as sentiment analysis etc. However words can only capture so much, there are times when you need relationships between sentences and documents and not just words. For example, if you are trying to figure out, whether two stack overflow questions are duplicates of each other.</p>
<p>A simple google search will lead you to a number of applications of these algorithms.</p>
","<machine-learning><data-mining><clustering><nlp><unsupervised-learning>","25","25","25","0","21608","1678"
"19791","{
  ""id"": 19791,
  ""title"": ""How to force weights to be non-negative in Linear regression""
}","<p>I use a workaround with Lasso on Scikit Learn (It is definitely not the best way to do things but it works well). Lasso has a parameter <code>positive</code> which can be set to <code>True</code> and force the coefficients to be positive. Further, setting the Regularization coefficient <code>alpha</code> to lie close to 0 makes the Lasso mimic Linear Regression with no regularization. Here's the code:</p>

<pre><code>from sklearn.linear_model import Lasso
lin = Lasso(alpha=0.0001,precompute=True,max_iter=1000,
            positive=True, random_state=9999, selection='random')
lin.fit(X,y)
</code></pre>
","<python><scikit-learn><linear-regression>","25","25","25","0","358","405"
"8545","{
  ""id"": 8545,
  ""title"": ""How to calculate the mean of a dataframe column and find the top 10%""
}","<p>This is the import you need, and how to get the mean for a column named ""RBIs"":</p>

<pre><code>import org.apache.spark.sql.functions._
df.select(avg($""RBIs"")).show()
</code></pre>

<p>For the standard deviation, see
<a href=""https://stackoverflow.com/questions/31789939/calculate-the-standard-deviation-of-grouped-data-in-a-spark-dataframe"">scala - Calculate the standard deviation of grouped data in a Spark DataFrame - Stack Overflow</a></p>

<p>For grouping by percentiles, I suggest defining a new column via a user-defined function (UDF), and using groupBy on that column.  See</p>

<ul>
<li><a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html#udf-registration-moved-to-sqlcontextudf-java--scala"" rel=""noreferrer"">Spark SQL and DataFrames - Spark 1.5.1 Documentation - udf registration</a></li>
</ul>
","<apache-spark><scala>","25","25","25","0","9146","388"
"275","{
  ""id"": 275,
  ""title"": ""Publicly Available Datasets""
}","<p>For time series data in particular, <a href=""http://www.quandl.com/"">Quandl</a> is an excellent resource -- an easily browsable directory of (mostly) clean time series.</p>

<p>One of their coolest features is <a href=""http://blog.quandl.com/blog/quandl-open-data/"">open-data stock prices</a> -- i.e. financial data that can be edited wiki-style, and isn't encumbered by licensing.</p>
","<open-source><dataset>","25","25","25","0","508","131"
"2565","{
  ""id"": 2565,
  ""title"": ""Advantages of AUC vs standard accuracy""
}","<p>AUC and accuracy are fairly different things. AUC applies to binary classifiers that have some notion of a decision threshold internally. For example logistic regression returns positive/negative depending on whether the logistic function is greater/smaller than a threshold, usually 0.5 by default. When you choose your threshold, you have a classifier. You have to choose one.</p>

<p>For a given choice of threshold, you can compute accuracy, which is the proportion of true positives and negatives in the whole data set.</p>

<p>AUC measures how true positive rate (recall) and false positive rate trade off, so in that sense it is already measuring something else. More importantly, AUC is not a function of threshold. It is an evaluation of the classifier as threshold varies over all possible values. It is in a sense a broader metric, testing the quality of the internal value that the classifier generates and then compares to a threshold. It is not testing the quality of a particular choice of threshold.</p>

<p>AUC has a different interpretation, and that is that it's also the probability that a randomly chosen positive example is ranked above a randomly chosen negative example, according to the classifier's internal value for the examples.</p>

<p>AUC is computable even if you have an algorithm that only produces a ranking on examples. AUC is not computable if you truly only have a black-box classifier, and not one with an internal threshold. These would usually dictate which of the two is even available to a problem at hand.</p>

<p>AUC is, I think, a more comprehensive measure, although applicable in fewer situations. It's not strictly better than accuracy; it's different. It depends in part on whether you care more about true positives, false negatives, etc.</p>

<p><em>F-measure is more like accuracy in the sense that it's a function of a classifier and its threshold setting. But it measures precision vs recall (true positive rate), which is not the same as either above.</em></p>
","<machine-learning><accuracy>","25","25","25","0","21","6202"
"768","{
  ""id"": 768,
  ""title"": ""Tools and protocol for reproducible data science using Python""
}","<p>Since I started doing research in academia I was constantly looking for a satisfactory workflow.
I think that I finally found something I am happy with:</p>

<p>1) Put everything under version control, e.g., Git:</p>

<p>For hobby research projects I use GitHub, for research at work I use the private GitLab server that is provided by our university. I also keep my datasets there.</p>

<p>2) I do most of my analyses along with the documentation on IPython notebooks. It is very organized (for me) to have the code, the plots, and the discussion/conclusion all in one document
If I am running larger scripts, I would usually put them into separate script .py files, but I would still execute them from the IPython notebook via the %run magic to add information about the purpose, outcome, and other parameters.</p>

<p>I have written a small cell-magic extension for IPython and IPython notebooks, called ""watermark"" that I use to conveniently create time stamps and keep track of the different package versions I used and also Git hashs</p>

<p>For example</p>

<p><br></p>

<pre><code>%watermark

29/06/2014 01:19:10

CPython 3.4.1
IPython 2.1.0

compiler   : GCC 4.2.1 (Apple Inc. build 5577)
system     : Darwin
release    : 13.2.0
machine    : x86_64
processor  : i386
CPU cores  : 2
interpreter: 64bit
</code></pre>

<p><br></p>

<pre><code>%watermark -d -t

29/06/2014 01:19:11 
</code></pre>

<p><br></p>

<pre><code>%watermark -v -m -p numpy,scipy

CPython 3.4.1
IPython 2.1.0

numpy 1.8.1
scipy 0.14.0

compiler   : GCC 4.2.1 (Apple Inc. build 5577)
system     : Darwin
release    : 13.2.0
machine    : x86_64
processor  : i386
CPU cores  : 2
interpreter: 64bit
</code></pre>

<p>For more info, see the <a href=""http://nbviewer.ipython.org/github/rasbt/python_reference/blob/master/ipython_magic/watermark.ipynb"">documentation here</a>.</p>
","<python><tools><version-control>","25","25","25","0","",""
"1112","{
  ""id"": 1112,
  ""title"": ""Quick guide into training highly imbalanced data sets""
}","<ul>
<li>Max Kuhn covers this well in Ch16 of <em>Applied Predictive Modeling</em>.     </li>
<li>As mentioned in the linked thread, imbalanced data is essentially a cost sensitive training problem. Thus any cost sensitive approach is applicable to imbalanced data.</li>
<li>There are a large number of such approaches. Not all implemented in R: C50, weighted SVMs are options. Jous-boost. Rusboost I think is only available as Matlab code. </li>
<li>I don't use Weka, but believe it has a large number of cost sensitive classifiers. </li>
<li><em>Handling imbalanced datasets: A review</em>: Sotiris Kotsiantis, Dimitris Kanellopoulos, Panayiotis Pintelas'</li>
<li><em>On the Class Imbalance Problem</em>: Xinjian Guo, Yilong Yin, Cailing Dong, Gongping Yang, Guangtong Zhou</li>
</ul>
","<machine-learning><classification><dataset><class-imbalance>","24","24","24","0","3294","356"
"29018","{
  ""id"": 29018,
  ""title"": ""Feature selection vs Feature extraction. Which to use when?""
}","<p>Adding to The answer given by Toros,</p>

<p>These(see below bullets) three are quite similar but with a subtle differences-:(concise and easy to remember)</p>

<ul>
<li><p><strong>feature extraction and feature engineering</strong>: transformation of raw data into features suitable for modeling;</p></li>
<li><p><strong>feature transformation</strong>: transformation of data to improve the accuracy of the algorithm;</p></li>
<li><p><strong>feature selection</strong>: removing unnecessary features.</p></li>
</ul>

<p>Just to add an Example of the same,</p>

<blockquote>
  <p>Feature Extraction and Engineering(we can extract something from them)</p>
</blockquote>

<ul>
<li>Texts(ngrams, word2vec, tf-idf etc)</li>
<li>Images(CNN'S, texts, q&amp;a)</li>
<li>Geospatial data(lat, long etc)</li>
<li>Date and time(day, month, week, year, rolling based)</li>
<li>Time series, web, etc</li>
<li>Dimensional Reduction Techniques (PCA, SVD, Eigen-Faces etc)</li>
<li>Maybe we can use Clustering as well (DBSCAN etc)</li>
<li>.....(And Many Others)</li>
</ul>

<blockquote>
  <p>Feature transformations(transforming them to make sense)</p>
</blockquote>

<ul>
<li>Normalization and changing distribution(Scaling)</li>
<li>Interactions</li>
<li>Filling in the missing values(median filling etc)</li>
<li>.....(And Many Others)</li>
</ul>

<blockquote>
  <p>Feature selection(building your model on these selected features)</p>
</blockquote>

<ul>
<li>Statistical approaches</li>
<li>Selection by modeling</li>
<li>Grid search </li>
<li>Cross Validation</li>
<li>.....(And Many Others)</li>
</ul>

<p>Hope this helps...</p>

<p>Do look at the links shared by others.
They are Quite Nice...</p>
","<feature-selection><feature-extraction><dimensionality-reduction>","24","24","24","0","35644","2268"
"30408","{
  ""id"": 30408,
  ""title"": ""RandomForestClassifier OOB scoring method""
}","<p>In general, the performance of classifiers are compared using accuracy, this is a measure of the number of correctly classified instances divided by the total number of instances. However, from the training data we can get a better approximation of the expected error from our classifier when we are using ensemble learning or bagging techniques.</p>

<h1>Out-of-bag error</h1>

<p>This metric is the accuracy of examples $x_i$ using all the trees in the random forest ensemble for which it was omitted during training. Thus it kind of acts as a semi-testing instance. You can get a sense of how well your classifier can generalize using this metric. </p>

<p>To implement oob in sklearn you need to specify it when creating your Random Forests object as</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier 
forest = RandomForestClassifier(n_estimators = 100, oob_score = True)
</code></pre>

<p>Then we can train the model</p>

<pre><code>forest.fit(X_train, y_train)
print('Score: ', forest.score(X_train, y_train))
</code></pre>

<blockquote>
  <p>Score:  0.979921928817</p>
</blockquote>

<p>As expected the accuracy of the model when evaluating the training set is very high. However, this is meaningless because you can very well be overfitting your data and thus your model is rubbish. However, we can use the out-of-bag score as</p>

<pre><code>print(forest.oob_score_)
</code></pre>

<blockquote>
  <p>0.86453272101</p>
</blockquote>

<p>This is the accuracy whilst evaluating our instances in the training set using only the trees for which they were omitted. Now let's calculate the score on the testing set as</p>

<pre><code>print('Score: ', forest.score(X_test, y_test))
</code></pre>

<blockquote>
  <p>Score:  0.86517733935</p>
</blockquote>

<p>We see that the accuracy measured by oob is very similar to that obtained with the testing set. It thus follows through the theory that the oob accuracy is a better metric by which to evaluate the performance of your model rather than just the score. This is a consequence of bagging models and cannot be done with other types of classifiers. </p>

<h1>Calculating oob using different metrics</h1>

<p>Yes, you can do this! However, it depends how exactly your code is structured. I am not sure how you can include the oob and AUC all together with the <code>cross_val_score</code> function. However, if you are doing the cross validation folds manually you can do the following, the random forests algorithm in sklearn provides you the decision function of the oob as</p>

<pre><code>print(forest.oob_decision_function_)
</code></pre>

<p>The class can then be obtained using</p>

<pre><code>from sklearn import metrics
pred_train = np.argmax(forest.oob_decision_function_,axis=1)
</code></pre>

<p>Then we can calculate the AUC by using the following</p>

<pre><code>metrics.roc_auc_score(y_train, pred_train)
</code></pre>

<blockquote>
  <p>0.86217157846471204</p>
</blockquote>
","<random-forest><scikit-learn>","24","24","24","0","29587","8088"
"24342","{
  ""id"": 24342,
  ""title"": ""How is a splitting point chosen for continuous variables in decision trees?""
}","<p>In order to come up with a split point, the values are sorted, and the mid-points between adjacent values are evaluated in terms of some metric, usually information gain or gini impurity. For your example, lets say we have four examples and the values of the age variable are $(20, 29, 40, 50)$. The midpoints between the values $(24.5, 34.5, 45)$ are evaluated, and whichever split gives the best information gain (or whatever metric you're using) on the training data is used. </p>

<p>You can save some computation time by only checking split points that lie between examples of different classes, because only these splits can be optimal for information gain.</p>
","<classification><data><decision-trees>","24","24","24","0","26562","3590"
"30914","{
  ""id"": 30914,
  ""title"": ""What does \""baseline\"" mean in the context of machine learning?""
}","<p>A baseline is the result of a very basic model/solution. You generally create a baseline and then try to make more complex solutions in order to get a better result.
If you achieve a better score than the baseline, it is good.</p>
","<machine-learning><regression><predictive-modeling><terminology>","24","24","24","0","31086","683"
"9271","{
  ""id"": 9271,
  ""title"": ""Calculating KL Divergence in Python""
}","<p>First of all, <code>sklearn.metrics.mutual_info_score</code> implements <em>mutual information</em> for evaluating clustering results, not <em>pure</em> Kullback-Leibler divergence!</p>

<blockquote>
  <p>This is equal to the Kullback-Leibler divergence of the joint distribution with the product distribution of the marginals.</p>
</blockquote>

<p>KL divergence (and any other such measure) expects the input data to <strong>have a sum of 1</strong>. Otherwise, they are <em>not</em> proper <strong>probability distributions</strong>. If your data does not have a sum of 1, most likely it is usually not proper to use KL divergence! (In some cases, it may be admissible to have a sum of less than 1, e.g. in the case of missing data.)</p>

<p>Also note that it is common to use base 2 logarithms. This only yields a constant scaling factor in difference, but base 2 logarithms are easier to interpret and have a more intuitive scale (0 to 1 instead of 0 to log2=0.69314..., measuring the information in bits instead of nats).</p>

<pre><code>&gt; sklearn.metrics.mutual_info_score([0,1],[1,0])
0.69314718055994529
</code></pre>

<p>as we can clearly see, the MI result of sklearn is scaled using natural logarithms instead of log2. This is an unfortunate choice, as explained above.</p>

<p>Kullback-Leibler divergence is fragile, unfortunately. On above example it is not well-defined: <code>KL([0,1],[1,0])</code> causes a division by zero, and tends to infinity. It is also <em>asymmetric</em>.</p>
","<python><clustering><scikit-learn>","24","24","24","0","924","7449"
"23502","{
  ""id"": 23502,
  ""title"": ""Why ReLU is better than the other activation functions""
}","<p>The biggest advantage of ReLu is indeed non-saturation of its gradient, which greatly accelerates the convergence of stochastic gradient descent compared to the sigmoid / tanh functions (<a href=""http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"" rel=""noreferrer"">paper</a> by Krizhevsky et al).</p>

<p>But it's not the only advantage. <a href=""https://stats.stackexchange.com/a/176905/130598"">Here</a> is a discussion of sparsity effects of ReLu activations and induced regularization. Another nice property is that compared to tanh / sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.</p>

<p>But I'm not convinced that great success of modern neural networks is due to ReLu <em>alone</em>. New initialization techniques, such as Xavier initialization, dropout and (later) batchnorm also played very important role. For example, famous AlexNet used ReLu <em>and</em> dropout.</p>

<p>So to answer your question: ReLu has very nice properties, though <a href=""https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks"">not ideal</a>. But it truly proves itself when combined with other great techniques, which by the way solve non-zero-center problem that you've mentioned.</p>

<p>UPD: ReLu output is not zero-centered indeed and it does hurt the NN performance. But this particular issue can be tackled by other regularization techniques, e.g. batchnorm, which <a href=""https://arxiv.org/abs/1502.03167"" rel=""noreferrer"">normalizes the signal before activation</a>:</p>

<blockquote>
  <p>We add the BN transform immediately before the nonlinearity, by
  normalizing $x = Wu+ b$. ... normalizing it is likely to produce
  activations with a stable distribution.</p>
</blockquote>
","<machine-learning><neural-network><deep-learning><gradient-descent><activation-function>","24","24","24","0","18375","800"
"37350","{
  ""id"": 37350,
  ""title"": ""What is the meaning of term Variance in Machine Learning Model?""
}","<p>It is pretty much what you said. Formally you can say:</p>
<blockquote>
<p>Variance, in the context of Machine Learning, is a type of error that occurs due to a model's sensitivity to small fluctuations in the training set.</p>
</blockquote>
<p><strong>High variance</strong> would cause an algorithm to model the noise in the training set. This is most commonly referred to as <strong><a href=""https://en.wikipedia.org/wiki/Overfitting"" rel=""noreferrer"">overfitting</a></strong>.</p>
<p>When discussing variance in Machine Learning, we also refer to <strong>bias</strong>.</p>
<blockquote>
<p>Bias, in the context of Machine Learning, is a type of error that occurs due to erroneous assumptions in the learning algorithm.</p>
</blockquote>
<p><strong>High bias</strong> would cause an algorithm to <strong>miss</strong> relevant relations between the input features and the target outputs. This is sometimes referred to as <strong>underfitting</strong>.</p>
<p>These terms can be decomposed from the <strong>expected error</strong> of the trained model, given different samples drawn from a training distribution. <a href=""https://suzyahyah.github.io/machine%20learning/2020/09/04/bias-variance.html"" rel=""noreferrer"">See here for a brief mathematical explanation</a> of where the terms come from, and how to formally measure variance in the model.</p>
<p><strong>Relationship between bias and variance:</strong></p>
<p>In most cases, attempting to minimize one of these two errors, would lead to increasing the other. Thus the two are usually seen as a <strong>trade-off</strong>.</p>
<p><img src=""https://qph.fs.quoracdn.net/main-qimg-7945a8dba7fa07be31d9b7ebde3886a0"" alt="""" /></p>
<p><strong>Cause of high bias/variance in ML:</strong></p>
<p>The most common factor that determines the bias/variance of a model is its <strong>capacity</strong> (think of this as how <em>complex</em> the model is).</p>
<ul>
<li><p><strong>Low capacity</strong> models (e.g. linear regression), might miss relevant relations between the features and targets, causing them to have high bias. This is evident in the left figure above.</p>
</li>
<li><p>On the other hand, <strong>high capacity</strong> models (e.g. high-degree polynomial regression, neural networks with many parameters) might model some of the noise, along with any relevant relations in the training set, causing them to have high variance, as seen in the right figure above.</p>
</li>
</ul>
<p><strong>How to reduce the variance in a model?</strong></p>
<p>The easiest and most common way of reducing the variance in a ML model is by applying techniques that limit its effective capacity, i.e. <strong><a href=""https://en.wikipedia.org/wiki/Regularization_(mathematics)"" rel=""noreferrer"">regularization</a></strong>.</p>
<p>The most common forms of regularization are <a href=""https://pdfs.semanticscholar.org/presentation/be32/622ae17839111e0fc58dc2f66b680618f8a8.pdf"" rel=""noreferrer"">parameter norm penalties</a>, which limit the parameter updates during the training phase; <a href=""https://en.wikipedia.org/wiki/Early_stopping"" rel=""noreferrer"">early stopping</a>, which cuts the training short; <a href=""https://en.wikipedia.org/wiki/Pruning_(decision_trees)"" rel=""noreferrer"">pruning</a> for tree-based algorithms; <a href=""https://en.wikipedia.org/wiki/Dropout_(neural_networks)"" rel=""noreferrer"">dropout</a> for neural networks, etc.</p>
<p><strong>Can a model have both low bias and low variance?</strong></p>
<p><strong>Yes</strong>. Likewise a model can have both high bias and high variance, as is illustrated in the figure below.</p>
<p><img src=""https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRxrp2il7lCdp7jI-JYiOhFMN5kviNy3bH0Iwx0z2ixcXhPizrA"" alt="""" /></p>
<p><strong>How can we achieve both low bias and low variance?</strong></p>
<p>In practice the most methodology is:</p>
<ol>
<li>Select an algorithm with a high enough capacity to sufficiently model the problem. In this stage we want to <strong>minimize the bias</strong>, so we aren't concerned about the variance yet.</li>
<li>Regularize the model above, to <strong>minimize its variance</strong>.</li>
</ol>
","<machine-learning><variance>","24","24","24","0","34269","7013"
"12800","{
  ""id"": 12800,
  ""title"": ""Pandas Dataframe to DMatrix""
}","<p>You can use the dataframe's <code>.values</code> method to access raw data once you have manipulated the columns as you need them.</p>

<p>E.g.</p>

<pre><code>train = pd.read_csv(""train.csv"")
target = train['target']
train = train.drop(['ID','target'],axis=1)
test = pd.read_csv(""test.csv"")
test = test.drop(['ID'],axis=1)

xgtrain = xgb.DMatrix(train.values, target.values)
xgtest = xgb.DMatrix(test.values)
</code></pre>

<p>Obviously you may need to change which columns you drop or use as the training target. The above was for a Kaggle competition, so there was no target data for <code>xgtest</code> (it is held back by the organisers).</p>
","<scikit-learn><pandas><xgboost>","23","23","23","0","836","25808"
"25269","{
  ""id"": 25269,
  ""title"": ""Keras difference beetween val_loss and loss during training""
}","<p><code>val_loss</code> is the value of cost function for your cross-validation data and loss is the value of cost function for your training data. On validation data, neurons using drop out do not drop random neurons. The reason is that during training we use drop out in order to add some noise for avoiding over-fitting. During calculating cross-validation, we are in the recall phase and not in the training phase. We use all the capabilities of the network.</p>

<p>Thanks to one of our dear friends, I quote and explain the contents from <a href=""https://keras.io/models/sequential/"" rel=""noreferrer"">here</a> which are useful.</p>

<blockquote>
  <p><code>validation_split</code>: Float between 0 and 1. The fraction of the training
  data to be used as validation data. The model will set apart this
  fraction of the training data, will not train on it, and will evaluate
  the loss and any model metrics on this data at the end of each epoch.
  The validation data is selected from the last samples in the <em>x</em> and
  <em>y</em> data provided, before shuffling.</p>
  
  <p><code>validation_data</code>: tuple (x_val, y_val) or tuple (x_val, y_val,
  val_sample_weights) on which to evaluate the loss and any model
  metrics at the end of each epoch. The model will not be trained on
  this data. This will override  validation_split.</p>
</blockquote>

<p>As you can see</p>

<pre><code>fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)
</code></pre>

<p><code>fit</code> method used in <code>Keras</code> has a parameter named validation_split, which specifies the percentage of data used for evaluating the model which is created after each epoch. After evaluating the model using this amount of data, that will be reported by <code>val_loss</code> if you've set verbose to <code>1</code>; moreover, as the documentation clearly specifies, you can use either <code>validation_data</code> or <code>validation_split</code>. Cross-validation data is used to investigate whether your model over-fits the data or does not. This is what we can understand whether our model has generalization capability or not.</p>
","<machine-learning><deep-learning><keras>","23","23","23","0","28175","12688"
"69292","{
  ""id"": 69292,
  ""title"": ""Collaborating on Jupyter Notebooks""
}","<p><a href=""https://cocalc.com/doc/jupyter-notebook.html"" rel=""noreferrer"">CoCalc</a> provides Jupyter notebooks with <strong><em>realtime collaboration</em></strong>, unlike Colab, Kaggle, etc.  You just make a project drag and drop ipynb and data files, add collaborators, and everybody can edit everything simultaneously.  You can also share content publicly at <a href=""https://share.cocalc.com/share/"" rel=""noreferrer"">the share server</a>.   I think CoCalc is currently the overall most mature of the realtime Jupyter collaboration platforms (and it is the only open source one <a href=""https://github.com/sagemathinc/cocalc-docker"" rel=""noreferrer"">4</a>), but <a href=""http://deepnote.com/"" rel=""noreferrer"">Deepnote</a> is another option that is more focused on data science (but is closed source).</p>
","<jupyter>","23","23","23","0","91199","331"
"9385","{
  ""id"": 9385,
  ""title"": ""K-Means clustering for mixed numeric and categorical data""
}","<p>This question seems really about representation, and not so much about clustering.</p>

<p>Categorical data is a problem for most algorithms in machine learning. Suppose, for example, you have some categorical variable called ""color"" that could take on the values red, blue, or yellow. If we simply encode these numerically as 1,2, and 3 respectively, our algorithm will think that red (1) is actually closer to blue (2) than it is to yellow (3). We need to use a representation that lets the computer understand that these things are all actually equally different. </p>

<p>One simple way is to use what's called a <a href=""https://en.wikipedia.org/wiki/One-hot"" rel=""noreferrer"">one-hot</a> representation, and it's exactly what you thought you should do. Rather than having one variable like ""color"" that can take on three values, we separate it into three variables. These would be ""color-red,"" ""color-blue,"" and ""color-yellow,"" which all can only take on the value 1 or 0. </p>

<p>This increases the dimensionality of the space, but now you could use any clustering algorithm you like. It does sometimes make sense to zscore or whiten the data after doing this process, but the your idea is definitely reasonable.</p>
","<data-mining><clustering><octave><k-means><categorical-data>","23","23","23","0","9483","547"
"44559","{
  ""id"": 44559,
  ""title"": ""What's the difference between fit and fit_transform in scikit-learn models?""
}","<p>These methods are used for dataset transformations in scikit-learn:</p>
<p>Let us take an example for scaling values in a dataset:</p>
<p>Here the <code>fit</code> method, when applied to the training dataset, learns the model parameters (for example, mean and standard deviation). We then need to apply the <code>transform</code>  method on the training dataset to get the transformed (scaled) training dataset. We could also perform both of these steps in one step by applying <code>fit_transform</code> on the training dataset.</p>
<p>Then why do we need 2 separate methods - <code>fit</code> and <code>transform</code>?</p>
<p>In practice, we need to have separate training and testing dataset and that is where having a separate <code>fit</code> and <code>transform</code> method helps. We apply <code>fit</code> on the training dataset and use the <code>transform</code> method on both - the training dataset and the test dataset. Thus the training, as well as the test dataset, are then transformed(scaled) using the model parameters that were learned on applying the <code>fit</code> method to the training dataset.</p>
<p>Example Code:</p>
<pre><code>scaler = preprocessing.StandardScaler().fit(X_train)
scaler.transform(X_train) 
scaler.transform(X_test) 
</code></pre>
","<python><scikit-learn>","23","23","23","0","66483","331"
"39282","{
  ""id"": 39282,
  ""title"": ""Looking for a good package for anomaly detection in time series""
}","<p>I know I'm bit late here, but yes there is a package for anomaly detection along with outlier combination-frameworks.</p>
<p>The package is in Python and its name is <strong>pyod</strong>. <a href=""https://github.com/yzhao062/Pyod"" rel=""nofollow noreferrer"">It</a> is published in JMLR.</p>
<p>It has multiple algorithms for following individual approaches:</p>
<ol>
<li>Linear Models for Outlier Detection (<strong>PCA,vMCD,vOne-Class, and SVM</strong>)</li>
<li>Proximity-Based Outlier Detection Models (<strong>LOF, CBLOF, HBOS, KNN, AverageKNN, and MedianKNN</strong>)</li>
<li>Probabilistic Models for Outlier Detection (<strong>ABOD and FastABOD</strong>)</li>
<li>Outlier Ensembles and Combination Frameworks(<strong>IsolationForest and FeatureBagging</strong>)</li>
<li>Neural Networks and Deep Learning Models (<strong>Auto-encoder with fully connected Neural Network</strong>)</li>
</ol>
<p>Finally, if you're looking specifically for time-series per se, then <a href=""https://github.com/yzhao062/anomaly-detection-resources#34-time-series-outlier-detection"" rel=""nofollow noreferrer"">this</a> github link will be useful.</p>
<p>It has the following list packages for timeseries outlier detection:</p>
<p><a href=""https://github.com/MentatInnovations/datastream.io"" rel=""nofollow noreferrer"">datastream.io</a></p>
<p><a href=""https://github.com/earthgecko/skyline"" rel=""nofollow noreferrer"">skyline</a></p>
<p><a href=""https://github.com/tsurubee/banpei"" rel=""nofollow noreferrer"">banpei</a></p>
<p><a href=""https://github.com/twitter/AnomalyDetection"" rel=""nofollow noreferrer"">AnomalyDetection</a></p>
","<python><time-series><anomaly-detection><bayesian-networks><anomaly>","23","23","23","0","60288","346"
"9794","{
  ""id"": 9794,
  ""title"": ""Scikit-learn: Getting SGDClassifier to predict as well as a Logistic Regression""
}","<p>The comments about iteration number are spot on. The default <code>SGDClassifier</code> <code>n_iter</code> is <code>5</code> meaning you do <code>5 * num_rows</code> steps in weight space. The <a href=""http://scikit-learn.org/stable/modules/sgd.html#tips-on-practical-use"" rel=""noreferrer"">sklearn rule of thumb</a> is ~ 1 million steps for typical data. For your example, just set it to 1000 and it might reach tolerance first. Your accuracy is lower with <code>SGDClassifier</code> because it's hitting iteration limit before tolerance so you are ""early stopping""</p>

<p>Modifying your code quick and dirty I get:</p>



<pre><code># Added n_iter here
params = [{}, {""loss"": ""log"", ""penalty"": ""l2"", 'n_iter':1000}]

for param, Model in zip(params, Models):
    total = 0
    for train_indices, test_indices in kf:
        train_X = X[train_indices, :]; train_Y = Y[train_indices]
        test_X = X[test_indices, :]; test_Y = Y[test_indices]
        reg = Model(**param)
        reg.fit(train_X, train_Y)
        predictions = reg.predict(test_X)
        total += accuracy_score(test_Y, predictions)

    accuracy = total / numFolds
    print ""Accuracy score of {0}: {1}"".format(Model.__name__, accuracy)

Accuracy score of LogisticRegression: 0.96
Accuracy score of SGDClassifier: 0.96
</code></pre>
","<python><logistic-regression><scikit-learn><gradient-descent>","23","23","23","0","92","961"
"17085","{
  ""id"": 17085,
  ""title"": ""What is the difference between Inception v2 and Inception v3?""
}","<p>In the paper <a href=""https://arxiv.org/pdf/1502.03167.pdf"" rel=""noreferrer"">Batch Normalization</a>,Sergey et al,2015. proposed <strong>Inception-v1</strong> architecture which is a variant of the <strong>GoogleNet</strong> in the paper <strong><em>Going deeper with convolutions</em></strong>, and in the meanwhile they introduced Batch Normalization to Inception(BN-Inception).</p>

<blockquote>
  <p>The main difference to the network described in (Szegedy et al.,2014)
  is that the 5x5 convolutional layers are replaced by two consecutive
  layer of 3x3 convolutions with up to 128 filters.</p>
</blockquote>

<p>And in the paper <a href=""https://www.arxiv.org/pdf/1512.00567v1.pdf"" rel=""noreferrer"">Rethinking the Inception Architecture for Computer Vision</a>, the authors proposed Inception-v2 and Inception-v3.</p>

<p>In the <strong>Inception-v2</strong>, they introduced Factorization(factorize convolutions into smaller convolutions) and some minor change into Inception-v1.</p>

<blockquote>
  <p>Note that we have factorized the traditional 7x7 convolution into
  three 3x3 convolutions</p>
</blockquote>

<p>As for <strong>Inception-v3</strong>, it is a variant of Inception-v2 which adds BN-auxiliary.</p>

<blockquote>
  <p>BN auxiliary refers to the version in which the fully connected layer of the auxiliary classifier is also-normalized, not just convolutions. We are refering to the model [Inception-v2 + BN auxiliary] as Inception-v3.</p>
</blockquote>
","<image-classification><computer-vision><convolutional-neural-network><inception>","23","23","23","0","29223","331"
"609","{
  ""id"": 609,
  ""title"": ""Choosing a learning rate""
}","<p>Below is a very good note (page 12) on learning rate in Neural Nets (Back Propagation) by Andrew Ng. You will find details relating to learning rate. </p>

<p><a href=""http://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf"">http://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf</a></p>

<p>For your 4th point, you're right that normally one has to choose a ""balanced"" learning rate, that should neither overshoot nor converge too slowly. One can plot the learning rate w.r.t. the descent of the cost function to diagnose/fine tune. In practice, Andrew normally uses the L-BFGS algorithm (mentioned in page 12) to get a ""good enough"" learning rate.</p>
","<machine-learning><neural-network><deep-learning><optimization><hyperparameter>","23","23","23","0","1224","331"
"8287","{
  ""id"": 8287,
  ""title"": ""Are there any tools for feature engineering?""
}","<p>Very interesting question (+1). While I am not aware of any software tools that currently offer comprehensive functionality for <em>feature engineering</em>, there is definitely a wide range of options in that regard. Currently, as far as I know, feature engineering is still largely a <strong>laborious</strong> and <strong>manual</strong> process (i.e., see <a href=""http://appliedpredictivemodeling.com/blog/2015/7/28/feature-engineering-versus-feature-extraction"">this blog post</a>). Speaking about the feature engineering subject domain, <a href=""http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it"">this excellent article</a> by Jason Brownlee provides a rather comprehensive overview of the topic.</p>

<p>Ben Lorica, Chief Data Scientist and Director of Content Strategy for Data at O'Reilly Media Inc., has written a <a href=""http://radar.oreilly.com/2014/06/streamlining-feature-engineering.html"">very nice article</a>, describing the state-of-art (as of June 2014) approaches, methods, tools and startups in the area of <em>automating</em> (or, as he put it, <em>streamlining</em>) feature engineering.</p>

<p>I took a brief look at some <em>startups</em> that Ben has referenced and a product by <a href=""http://www.skytree.net"">Skytree</a> indeed looks quite impressive, especially in regard to the subject of this question. Having said that, some of their claims sound really suspicious to me (i.e., <em>""Skytree speeds up machine learning methods by up to 150x compared to open source options""</em>). Continuing talking about commercial data science and machine learning offerings, I have to mention solutions by Microsoft, in particular their <em>Azure Machine Learning Studio</em>. This Web-based product is quite powerful and elegant and offers some feature engineering functionality (FEF). For an example of some simple FEF, see <a href=""http://datasciencedojo.com/tutorial-videos/feature-engineering-r-script-with-azure-ml-studio"">this nice video</a>.</p>

<p>Returning to the question, I think that the simplest approach one can apply for automating feature engineering is to use corresponding <strong>IDEs</strong>. Since you (me, too) are interested in R language as a data science backend, I would suggest to check, in addition to RStudio, another similar open source IDE, called <a href=""https://rkward.kde.org"">RKWard</a>. One of the advantages of RKWard vs RStudio is that it supports <a href=""http://api.kde.org/doc/rkwardplugins"">writing plugins</a> for the IDE, thus, enabling data scientists to automate feature engineering and streamline their R-based data analysis.</p>

<p>Finally, on the other side of the spectrum of feature engineering solutions we can find some <strong>research projects</strong>. The two most notable seem to be Stanford University's <a href=""http://i.stanford.edu/hazy/victor/columbus"">Columbus project</a>, described in detail in the <a href=""http://cs.stanford.edu/people/chrismre/papers/mod539-zhang.pdf"">corresponding research paper</a>, and <em>Brainwash</em>, described in <a href=""http://www.cidrdb.org/cidr2013/Papers/CIDR13_Paper82.pdf"">this paper</a>.</p>
","<feature-selection><feature-extraction><feature-construction>","23","23","23","0","2452","6478"
"25027","{
  ""id"": 25027,
  ""title"": ""Strange behavior with Adam optimizer when training for too long""
}","<p>This small instability at the end of convergence is a feature of Adam (and RMSProp) due to how it estimates mean gradient magnitudes over recent steps and divides by them.</p>

<p>One thing Adam does is maintain a rolling geometric mean of recent gradients and squares of the gradients. The squares of the gradients are used to divide (another rolling mean of) the current gradient to decide the current step. However, when your gradient becomes and stays very close to zero, this will make the squares of the gradient become so low that they either have large rounding errors or are effectively zero, which can introduce instability (for instance a long-term stable gradient in one dimension makes a relatively small step from $10^{-10}$ to $10^{-5}$ due to changes in <em>other</em> params), and the step size will start to jump around, before settling again.</p>

<p>This actually makes Adam less stable and worse for your problem than more basic gradient descent, assuming you want to get as numerically close to zero loss as calculations allow for your problem.</p>

<p>In practice on deep learning problems, you don't get this close to convergence (and for some regularisation techniques such as early stopping, you don't want to anyway), so it is usually not a practical concern on the types of problem that Adam was designed for.</p>

<p>You can actually see this occurring for RMSProp in a comparison of different optimisers (RMSProp is the black line - watch the very last steps just as it reaches the target):</p>

<p><a href=""https://i.stack.imgur.com/qAx2i.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/qAx2i.gif"" alt=""enter image description here""></a></p>

<p>You can make Adam more stable and able to get closer to true convergence by reducing the learning rate. E.g. </p>

<pre><code>optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
</code></pre>

<p>It will take longer to optimise. Using <code>lr=1e-5</code> you need to train for 20,000+ iterations before you see the instability and the instability is less dramatic, values hover around $10^{-7}$. </p>
","<perceptron><pytorch>","23","23","23","0","836","25808"
"16466","{
  ""id"": 16466,
  ""title"": ""What is/are the default filters used by Keras Convolution2d()?""
}","<p>By default, the filters <span class=""math-container"">$W$</span> are initialised randomly using the <code>glorot_uniform</code> method, which draws values from a uniform distribution with positive and negative bounds described as so: 
<span class=""math-container"">$$W \sim \mathcal{U}\left(\frac{6}{n_{in} + n_{out}}, \frac{-6}{n_{in} + n_{out}}\right),$$</span></p>

<p>where <span class=""math-container"">$n_{in}$</span> is the number of units that feed into this unit, and <span class=""math-container"">$n_{out}$</span> is the number of units this result is fed to.</p>

<p>When you are using the network to make a prediction, these filters are applied at each layer of the network. That is, a discrete convolution is performed for each filter on each input image, and the results of these convolutions are fed to the next layer of convolutions (or fully connected layer, or whatever else you might have).</p>

<p>During training, the values in the filters are optimised with backpropogation with respect to a loss function. For classification tasks such as recognising digits, usually the cross entropy loss is used. 
Here's a visualisation of some filters learned in the first layer (top) and the filters learned in the second layer (bottom) of a convolutional network:</p>

<p><a href=""https://i.stack.imgur.com/D6pXk.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/D6pXk.jpg"" alt=""conv net filters visualisation""></a></p>

<p>As you can see, the first layer filters basically all act as simple edge detectors, while the second layer filters are more complex. As you go deeper into a network, the filters are able to detect more complex shapes. It gets a little tricky to visualise though, as these filters act on images that have been convolved many times already, and probably don't look much like the original natural image.</p>
","<keras><convolutional-neural-network>","23","23","23","0","26562","3590"
"18655","{
  ""id"": 18655,
  ""title"": ""RNN vs CNN at a high level""
}","<p>Difference between CNN and RNN are as follows :</p>

<h1>CNN:</h1>

<ol>
<li><p>CNN take a fixed size input and generate fixed-size outputs.</p></li>
<li><p>CNN is a type of feed-forward artificial neural network - are variations of multilayer perceptrons which are designed to use minimal amounts of preprocessing.</p></li>
<li><p>CNNs use connectivity pattern between its neurons is inspired by the organization of the animal visual cortex, whose individual neurons are arranged in such a way that they respond to overlapping regions tiling the visual field.</p></li>
<li><p>CNNs are ideal for images and videos processing.</p></li>
</ol>

<h1>RNN:</h1>

<ol>
<li><p>RNN can handle arbitrary input/output lengths.</p></li>
<li><p>RNN, unlike feedforward neural networks, can use their internal memory to process arbitrary sequences of inputs.</p></li>
<li><p>Recurrent neural networks use time-series information (i.e. what I spoke last will impact what I will speak next.)</p></li>
<li><p>RNNs are ideal for text and speech analysis.</p></li>
</ol>
","<machine-learning><neural-network><beginner>","23","23","23","0","30986","331"
"40719","{
  ""id"": 40719,
  ""title"": ""What is the advantage of using log softmax instead of softmax""
}","<p>There are a number of advantages of using <a href=""https://stats.stackexchange.com/questions/289369/log-probabilities-in-reference-to-softmax-classifier"">log softmax over softmax</a> including practical reasons like improved numerical performance and <a href=""https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability"">gradient optimization</a>. These advantages can be extremely important for implementation especially when training a model can be computationally challenging and expensive. At the heart of using log-softmax over softmax is the use of <a href=""https://en.wikipedia.org/wiki/Log_probability"" rel=""noreferrer"">log probabilities</a> over probabilities, which has nice information theoretic interpretations.</p>

<p>When used for classifiers the log-softmax has the effect of heavily penalizing the model when it fails to predict a correct class. Whether or not that penalization works well for solving your problem is open to your testing, so both log-softmax and softmax are worth using.</p>
","<deep-learning><loss-function>","23","23","23","0","61563","587"
"30639","{
  ""id"": 30639,
  ""title"": ""What does it mean to \""share parameters between features and classes\""""
}","<p>I will try to answer this question through <strong>logistic regression</strong>, one of the simplest linear classifiers.</p>

<p>The simplest case of logistic regression is if we have a <strong>binary classification</strong> task ($y \in\{0,1\})$and only <strong>one input feature</strong> ($x \in R$). In this case the output of logistic regression would be:</p>

<p>$$
\hat y = σ(w \cdot x + b)
$$
where $w$ and $b$ are both <strong>scalars</strong>. The output of the model $\hat y \in [0,1]$ corresponds to the probability that $x$ will be of class $1$.</p>

<p>We'll try to break down the phrase <em>""linear classifiers do not share parameters among features and classes""</em> into two parts. We will examine the cases of multiple features and multiple classes separately to see if logistic regression shares parameters for any those tasks:</p>

<p><strong>Do linear classifiers share parameters among features?</strong></p>

<p>In this case, for each example, $y$ is a scalar that takes binary values (like before), while $x$ is a <strong>vector</strong> of length $N$ (where $N$ is the number of features). Here, the the output is a linear combination of the input features (i.e. a weighted sum of these features plus the biases).</p>

<p>$$
\hat y = σ \left(\sum_i^N{(w_i \cdot x_i)} + b\right) \;\; or \;\; σ( \mathbf w \cdot \mathbf x + b)
$$
where $ \mathbf x $ and $ \mathbf w $ are vectors of length $N$. The product $\mathbf x \cdot \mathbf w$ produces a scalar. As you can see from above there is a <strong>separate weight</strong> $w_i$ for each input feature $x_i$ and these weights are <strong>independent</strong> by all means. From this we can conclude that there is <strong>no parameter sharing among features</strong>.</p>

<p><strong>Do linear classifiers share parameters among classes?</strong></p>

<p>In this case $x$ is a scalar, however $y$ is a <strong>vector</strong> of length $M$ (where $M$ is the number of classes). To tackle this, logistic regression essentially produces a separate output $y_j$ for each of the $M$ classes. Each output is a scalar $y_j \in [0,1]$ and corresponds to the probability of $x$ belonging to class $j$.</p>

<p>$$
\mathbf{ \hat y} = w \cdot \mathbf x + \mathbf b, \;\; where \;\; \mathbf{ \hat y} = {\hat y_1, \hat y_2, ..., y_M}
$$</p>

<p>The easiest way to think of this is as $M$ simple <strong>independent</strong> logistic regressions each with an output of:</p>

<p>$$
\hat y_j = σ(w_j \cdot x + b_j)
$$</p>

<p>From the above it is obvious that <strong>no weights are shared among the different classes</strong>.</p>

<p><strong>multi-feature and multi-class</strong>:</p>

<p>By combining the two cases above we can finally reach the most general case of multiple features and multiple classes:</p>

<p>$$
\mathbf{ \hat y} = σ( \mathbf W \cdot \mathbf x + \mathbf b)
$$
where $\mathbf{ \hat y}$ is a vector with a size of $M$, $\mathbf x$ is a vector with a size of $N$, $\mathbf b$ is a vector with a size of $M$ and $W$ is a matrix with a size of $(N \times M)$.</p>

<p>In any case, <strong>linear classifiers do not share any parameters among features or classes</strong>.</p>

<p>To answer your second question, linear classifiers <strong>do have an underlying assumption that features need to be independent</strong>, however this is <strong>not</strong> what the author of the paper intended to say.</p>
","<machine-learning><logistic-regression><multilabel-classification>","23","23","23","0","34269","7013"
"37379","{
  ""id"": 37379,
  ""title"": ""What are kernel initializers and what is their significance?""
}","<p>The neural network needs to start with some weights and then iteratively update them to better values. The term kernel_initializer is a fancy term for which statistical distribution or function to use for initialising the weights. In case of statistical distribution, the library will generate numbers from that statistical distribution and use as starting weights.</p>

<p>For example in the above code, normal distribution will be used to initialise weights. You can use other functions (constants like 1s or 0s) and distributions (uniform) too. All possible options are <a href=""https://keras.io/initializers/#randomnormal"" rel=""noreferrer"">documented here</a>.</p>

<p>Additional explanation: The term kernel is a carryover from other classical methods like SVM. The idea is to transform data in a given input space to another space where the transformation is achieved using kernel functions. We can think of neural network layers as non-linear maps doing these transformations, so the term kernels is used.</p>
","<machine-learning><python><neural-network><deep-learning><keras>","23","23","23","0","23204","1878"
"10705","{
  ""id"": 10705,
  ""title"": ""How to predict probabilities in xgboost?""
}","<p>Just use <code>predict_proba</code> instead of <code>predict</code>. You can leave the objective as <code>binary:logistic</code>.</p>
","<machine-learning><r><predictive-modeling><decision-trees><xgboost>","23","31","27","4","16050","1227"
"703","{
  ""id"": 703,
  ""title"": ""Best python library for neural networks""
}","<p>Pylearn relies on Theano and as mentioned in the other answer to use the library is quite complicated, until you get the hold of it.</p>

<p>In the meantime I would suggest using <a href=""https://github.com/lmjohns3/theano-nets/"">Theanets</a>. It also built on top of Theano, but is much more easier to work with. It might be true, that it doesn't have all the features of Pylearn, but for the basic work it's sufficient.</p>

<p>Also it's open source, so you can add custom networks on the fly, if you dare. :)</p>

<p>EDIT: Dec 2015. Recently I have started using <a href=""http://keras.io/"">Keras</a>. It is a bit lower level than Theanets, but much more powerful. For basic tests the Theanets is appropriate. But if you want to do some research in field of ANN Keras is much more flexible. Plus the Keras can use <strong>Tensorflow</strong> as a backend.</p>
","<machine-learning><python><neural-network>","23","23","23","0","1390","261"
"47803","{
  ""id"": 47803,
  ""title"": ""Can a neural network compute $y = x^2$?""
}","<p>I think the answer of @ShubhamPanchal is a little bit misleading. Yes, it is true that by Cybenko's universal approximation theorem we can approximate <span class=""math-container"">$f(x)=x^2$</span> with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of <span class=""math-container"">$\mathbb{R}^n$</span>, under mild assumptions on the activation function. </p>

<blockquote>
  <p>But the main problem is that the theorem has a <strong>very important
  limitation</strong>. The function needs to be defined on <strong>a compact subsets of
  <span class=""math-container"">$\mathbb{R}^n$</span></strong> (compact subset = bounded + closed subset). <strong>But why
  is this problematic?</strong> When training the function approximator you
  will always have a finite data set. Hence, you will approximate the
  function inside a compact subset of <span class=""math-container"">$\mathbb{R}^n$</span>. But we can always
  find a point <span class=""math-container"">$x$</span> for which the approximation will probably fail. That
  being said. If you only want to approximate <span class=""math-container"">$f(x)=x^2$</span> on a compact
  subset of <span class=""math-container"">$\mathbb{R}$</span> then we can answer your question with <strong>yes</strong>.
  But if you want to approximate <span class=""math-container"">$f(x)=x^2$</span> for all <span class=""math-container"">$x\in \mathbb{R}$</span>
  then the answer is <strong>no</strong> (I exclude the trivial case in which you use
  a quadratic activation function).</p>
</blockquote>

<p><strong>Side remark on Taylor approximation</strong>: You always have to keep in mind that a Taylor approximation is only a local approximation. If you only want to approximate a function in a predefined region then you should be able to use Taylor series. But approximating <span class=""math-container"">$\sin(x)$</span> by the Taylor series evaluated at <span class=""math-container"">$x=0$</span> will give you horrible results for <span class=""math-container"">$x\to 10000$</span> if you don't use enough terms in your Taylor expansion.</p>
","<machine-learning><neural-network>","22","24","23","1","69670","1294"
"29054","{
  ""id"": 29054,
  ""title"": ""Micro Average vs Macro average Performance in a Multiclass classification setting""
}","<p>In a multi-class setting micro-averaged precision and recall are always the same.</p>

<p>$$
P = \frac{\sum_c TP_c}{\sum_c TP_c + \sum_c FP_c}\\
R = \frac{\sum_c TP_c}{\sum_c TP_c + \sum_c FN_c}
$$
where c is the class label.</p>

<p>Since in a multi-class setting you count <em>all</em> false instances it turns out that
$$
\sum_c FP_c = \sum_c FN_c
$$</p>

<p>Hence P = R. In other words, every single False Prediction will be a False Positive for a class, and every Single Negative will be a False Negative for a class. If you treat a binary classification case as a bi-class classification and compute the micro-averaged precision and recall they will be same. </p>

<p>The answer given by Rahul is in the case of averaging binary precision and recall from multiple dataset. In which case the micro-averaged precision and recall are different.</p>
","<multiclass-classification><evaluation>","22","22","22","0","39811","380"
"20192","{
  ""id"": 20192,
  ""title"": ""How to define a custom performance metric in Keras?""
}","<p>You have to use <a href=""https://keras.io/backend/#backend-functions"" rel=""nofollow noreferrer"">Keras backend functions</a>. Unfortunately they do not support the <code>&amp;</code>-operator, so that you have to build a workaround: We generate matrices of the dimension <code>batch_size x 3</code>, where (e.g. for true positive) the first column is the ground truth vector, the second the actual prediction and the third is kind of a label-helper column, that contains in the case of true positive only ones. Then we check which instances are positive instances, are predicted as positive and the label-helper is also positive. Those are the true positives.</p>
<p>We can make this analog with false positives, false negatives and true negatives with some reverse-calculations of the labels.</p>
<p>Your f1-metric may look as follows:</p>
<pre><code>def f1_score(y_true, y_pred):
    &quot;&quot;&quot;
    f1 score

    :param y_true:
    :param y_pred:
    :return:
    &quot;&quot;&quot;
    tp_3d = K.concatenate(
        [
            K.cast(y_true, 'bool'),
            K.cast(K.round(y_pred), 'bool'),
            K.cast(K.ones_like(y_pred), 'bool')
        ], axis=1
    )

    fp_3d = K.concatenate(
        [
            K.cast(K.abs(y_true - K.ones_like(y_true)), 'bool'),
            K.cast(K.round(y_pred), 'bool'),
            K.cast(K.ones_like(y_pred), 'bool')
        ], axis=1
    )

    fn_3d = K.concatenate(
        [
            K.cast(y_true, 'bool'),
            K.cast(K.abs(K.round(y_pred) - K.ones_like(y_pred)), 'bool'),
            K.cast(K.ones_like(y_pred), 'bool')
        ], axis=1
    )

    tp = K.sum(K.cast(K.all(tp_3d, axis=1), 'int32'))
    fp = K.sum(K.cast(K.all(fp_3d, axis=1), 'int32'))
    fn = K.sum(K.cast(K.all(fn_3d, axis=1), 'int32'))

    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    return 2 * ((precision * recall) / (precision + recall))
</code></pre>
<p>Since the Keras-backend calculator returns nan for division by zero, we do not need the if-else-statement for the return statement.</p>
<p><strong>Edit:</strong>
I have found a pretty good idea for a exact implementation. The problem with our first approach is, that it is only &quot;approximated&quot;, since it is computed batchwise and subsequently averaged. One could also calculate this after each epoch with the <code>keras.callback</code>s. Please find the idea <a href=""https://github.com/fchollet/keras/issues/5794"" rel=""nofollow noreferrer"">here</a>.</p>
<p>An example implementation would be:</p>
<pre><code>import keras
import numpy as np
import sklearn.metrics as sklm


class Metrics(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.confusion = []
        self.precision = []
        self.recall = []
        self.f1s = []
        self.kappa = []
        self.auc = []

    def on_epoch_end(self, epoch, logs={}):
        score = np.asarray(self.model.predict(self.validation_data[0]))
        predict = np.round(np.asarray(self.model.predict(self.validation_data[0])))
        targ = self.validation_data[1]

        self.auc.append(sklm.roc_auc_score(targ, score))
        self.confusion.append(sklm.confusion_matrix(targ, predict))
        self.precision.append(sklm.precision_score(targ, predict))
        self.recall.append(sklm.recall_score(targ, predict))
        self.f1s.append(sklm.f1_score(targ, predict))
        self.kappa.append(sklm.cohen_kappa_score(targ, predict))

        return
</code></pre>
<p>To make the network to call this function you simply add it to you callbacks like</p>
<pre><code>metrics = Metrics()
model.fit(
    train_instances.x,
    train_instances.y,
    batch_size,
    epochs,
    verbose=2,
    callbacks=[metrics],
    validation_data=(valid_instances.x, valid_instances.y),
)
</code></pre>
<p>Then you can simply access the members of the <code>metrics</code> variable.</p>
","<tensorflow><keras><evaluation>","22","22","22","0","30835","381"
"9485","{
  ""id"": 9485,
  ""title"": ""XGBoost Linear Regression output incorrect""
}","<p>It seems that XGBoost uses <strong>regression trees</strong> as base learners by default. XGBoost (or Gradient boosting in general) work by combining multiple of these base learners. Regression trees can not extrapolate the patterns in the training data, so any input above 3 or below 1 will not be predicted correctly in your case. Your model is trained to predict outputs for inputs in the interval <code>[1,3]</code>, an input higher than 3 will be given the same output as 3, and an input less than 1 will be given the same output as 1.</p>

<p>Additionally, regression trees do not really see your data as a <em>straight line</em> as they are non-parametric models, which means they can theoretically fit any shape that is more complicated than a straight line. Roughly, a regression tree works by assigning your new input data to some of the training data points it have seen during training, and produce the output based on that. </p>

<p>This is in contrast to parametric regressors (like <strong>linear regression</strong>) which actually look for the best parameters of a hyperplane (straight line in your case) to fit your data. Linear regression <strong>does</strong> see your data as a straight line with a slope and an intercept.</p>

<p>You can change the base learner of your XGBoost model to a GLM (generalized linear model) by adding <code>""booster"":""gblinear""</code> to your model <code>params</code> :</p>

<pre><code>import pandas as pd
import xgboost as xgb

df = pd.DataFrame({'x':[1,2,3], 'y':[10,20,30]})
X_train = df.drop('y',axis=1)
Y_train = df['y']
T_train_xgb = xgb.DMatrix(X_train, Y_train)

params = {""objective"": ""reg:linear"", ""booster"":""gblinear""}
gbm = xgb.train(dtrain=T_train_xgb,params=params)
Y_pred = gbm.predict(xgb.DMatrix(pd.DataFrame({'x':[4,5]})))
print Y_pred
</code></pre>

<p>In general, to debug why your XGBoost model is behaving in a particular way, see the model parameters :</p>

<pre><code>gbm.get_dump()
</code></pre>

<p>If your base learner is linear model, the get_dump output is :</p>

<pre><code>['bias:\n4.49469\nweight:\n7.85942\n']
</code></pre>

<p>In your code above, since you tree base learners, the output will be :</p>

<pre><code>['0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=2.85\n\t\t4:leaf=5.85\n\t2:leaf=8.85\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=1.995\n\t\t4:leaf=4.095\n\t2:leaf=6.195\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=1.3965\n\t\t4:leaf=2.8665\n\t2:leaf=4.3365\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=0.97755\n\t\t4:leaf=2.00655\n\t2:leaf=3.03555\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=0.684285\n\t\t4:leaf=1.40458\n\t2:leaf=2.12489\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=0.478999\n\t\t4:leaf=0.983209\n\t2:leaf=1.48742\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=0.3353\n\t\t4:leaf=0.688247\n\t2:leaf=1.04119\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=0.23471\n\t\t4:leaf=0.481773\n\t2:leaf=0.728836\n',
 '0:[x&lt;3] yes=1,no=2,missing=1\n\t1:[x&lt;2] yes=3,no=4,missing=3\n\t\t3:leaf=0.164297\n\t\t4:leaf=0.337241\n\t2:leaf=0.510185\n',
 '0:[x&lt;2] yes=1,no=2,missing=1\n\t1:leaf=0.115008\n\t2:[x&lt;3] yes=3,no=4,missing=3\n\t\t3:leaf=0.236069\n\t\t4:leaf=0.357129\n']
</code></pre>

<p>Tip : I actually prefer to use xgb.XGBRegressor or xgb.XGBClassifier classes, since they follow the <a href=""http://scikit-learn.org/"" rel=""noreferrer"">sci-kit learn</a> API. And because sci-kit learn has so many machine learning algorithm implementations, using XGB as an additional library does not disturb my workflow only when I use the sci-kit interface of XGBoost.</p>
","<python><linear-regression><xgboost>","22","22","22","0","860","1751"
"345","{
  ""id"": 345,
  ""title"": ""Books about the \""Science\"" in Data Science?""
}","<p>Introductory:</p>

<ul>
<li><a href=""http://rads.stackoverflow.com/amzn/click/1107422221"">Machine Learning: The Art and Science of Algorithms that Make Sense of Data (Flach)</a></li>
<li><a href=""http://rads.stackoverflow.com/amzn/click/1600490069"">Learning From Data (Abu-Mostafa et al.)</a></li>
<li><a href=""http://rads.stackoverflow.com/amzn/click/1461471370"">Introduction to Statistical Learning (James et al.)</a></li>
</ul>

<p>Digging deeper:</p>

<ul>
<li><a href=""http://rads.stackoverflow.com/amzn/click/0387848576"">Elements of Statistical Learning (Hastie et al.)</a></li>
<li><a href=""http://rads.stackoverflow.com/amzn/click/0387310738"">Pattern Recognition and Machine Learning (Bishop)</a></li>
</ul>

<p>Some special interest examples:</p>

<ul>
<li><a href=""http://rads.stackoverflow.com/amzn/click/0521833787"">Convex Optimization (Boyd)</a></li>
<li><a href=""http://www.cs.ucl.ac.uk/staff/d.barber/brml/"">Bayesian Reasoning and Machine Learning (Barber)</a></li>
<li><a href=""http://rads.stackoverflow.com/amzn/click/0262013193"">Probabilistic Graphical Models (Koller)</a></li>
<li><a href=""http://rads.stackoverflow.com/amzn/click/0198538642"">Neural Networks for Pattern Recognition (Bishop)</a></li>
</ul>

<p>Broader reference works on machine learning (not really what you asked for, but for completeness):</p>

<ul>
<li><a href=""http://rads.stackoverflow.com/amzn/click/0262018020"">Machine Learning: A Probabilistic Perspective (Murphy)</a></li>
<li><a href=""http://rads.stackoverflow.com/amzn/click/0136042597"">Artificial Intelligence: A Modern Approach (Russell &amp; Norvig)</a></li>
</ul>

<p>Bonus paper:</p>

<ul>
<li><a href=""http://projecteuclid.org/euclid.ss/1009213726"">Statistical Modeling: The Two Cultures (Breiman)</a></li>
</ul>
","<statistics><reference-request>","22","22","22","0","554","403"
"37370","{
  ""id"": 37370,
  ""title"": ""When should one use L1, L2 regularization instead of dropout layer, given that both serve same purpose of reducing overfitting?""
}","<p>I am unsure there will be a formal way to show which is best in which situations - simply trying out different combinations is likely best!</p>

<p>It is worth noting that Dropout actually does a little bit more than just provide a form of regularisation, in that it is really adding robustness to the network, allowing it to try out many many <em>different</em> networks. This is true because the randomly deactivated neurons are essentially removed for that forward/backward pass, thereby giving the same effect as if you had used a totally different network! Have a look at <a href=""https://datascience.stackexchange.com/questions/37021/why-does-adding-a-dropout-layer-in-keras-improve-machine-learning-performance-g/37024#37024"">this post for a few more pointers regarding the beauty of dropout layers</a>.</p>

<p>$L_1$ versus $L_2$ is easier to explain, simply by noting that $L_2$ treats outliers a little more thoroughly - returning a larger error for those points. Have a look here for <a href=""http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/"" rel=""noreferrer"">more detailed comparisons</a>.</p>
","<keras><overfitting><regularization><dropout>","22","22","22","0","45264","13123"
"55547","{
  ""id"": 55547,
  ""title"": ""In CNN, why do we increase the number of filters in deeper Convolution layers for complex images?""
}","<p>For this you need to understand what filters actually do.</p>
<p>Every layer of filters is there to capture patterns. For example, the first layer of filters captures patterns like edges, corners, dots etc. Subsequent layers combine those patterns to make bigger patterns (like combining edges to make squares, circles, etc.).</p>
<p>Now as we move forward in the layers, the patterns get more complex; <strong>hence there are larger combinations of patterns to capture</strong>. That's why we increase the filter size in subsequent layers to capture as many combinations as possible.</p>
","<neural-network><keras><tensorflow><cnn><convolution>","22","22","22","0","75738","757"
"489","{
  ""id"": 489,
  ""title"": ""Is GLM a statistical or machine learning model?""
}","<p>A GLM is absolutely a statistical model, but statistical models and machine learning techniques are not mutually exclusive. In general, statistics is more concerned with inferring parameters, whereas in machine learning, prediction is the ultimate goal.</p>
","<machine-learning><statistics><glm>","22","22","22","0","1011","1106"
"30","{
  ""id"": 30,
  ""title"": ""How big is big data?""
}","<p>Total amount of data in the world: 2.8 zetabytes in 2012, estimated to reach 8 zetabytes by 2015 (<a href=""http://siliconangle.com/blog/2012/05/21/when-will-the-world-reach-8-zetabytes-of-stored-data-infographic/"">source</a>) and with a doubling time of 40 months. Can't get bigger than that :)</p>

<p>As an example of a single large organization, Facebook pulls in 500 terabytes per day, into a 100 petabyte warehouse, and runs 70k queries per day on it as of 2012 (<a href=""http://gigaom.com/2012/08/22/facebook-is-collecting-your-data-500-terabytes-a-day/"">source</a>)  Their current warehouse is >300 petabytes.</p>

<p>Big data is probably something that is a good fraction of the Facebook numbers (1/100 probably yes, 1/10000 probably not: it's a spectrum not a single number).</p>

<p>In addition to size, some of the features that make it ""big"" are:</p>

<ul>
<li><p>it is actively analyzed, not just stored  (quote ""If you aren’t taking advantage of big data, then you don’t have big data, you have just a pile of data"" Jay Parikh @ Facebook)</p></li>
<li><p>building and running a data warehouse is a major infrastructure project</p></li>
<li><p>it is growing at a significant rate</p></li>
<li><p>it is unstructured or has irregular structure</p></li>
</ul>

<p>Gartner definition: ""Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing"" (The 3Vs)  So they also think ""bigness"" isn't entirely about the size of the dataset, but also about the velocity and structure and the kind of tools needed.</p>
","<bigdata><scalability><efficiency><performance>","22","24","23","1","26","3032"
"722","{
  ""id"": 722,
  ""title"": ""Organized processes to clean data""
}","<p>R contains some <em>standard</em> functions for data manipulation, which can be used for data cleaning, in its <strong>base</strong> package (<code>gsub</code>, <code>transform</code>, etc.), as well as in various third-party packages, such as <strong>stringr</strong>, <strong>reshape</strong>/<strong>reshape2</strong>, and <strong>plyr</strong>/<strong>dplyr</strong>. Examples and best practices of usage for these packages and their functions are described in the following paper: <a href=""http://vita.had.co.nz/papers/tidy-data.pdf"">http://vita.had.co.nz/papers/tidy-data.pdf</a>.</p>

<p>Additionally, R offers some packages specifically <em>focused</em> on data cleaning and transformation:</p>

<ul>
<li><strong>editrules</strong> (<a href=""http://cran.r-project.org/web/packages/editrules/index.html"">http://cran.r-project.org/web/packages/editrules/index.html</a>)</li>
<li><strong>deducorrect</strong> (<a href=""http://cran.r-project.org/web/packages/deducorrect/index.html"">http://cran.r-project.org/web/packages/deducorrect/index.html</a>)</li>
<li><strong>StatMatch</strong> (<a href=""http://cran.r-project.org/web/packages/StatMatch/index.html"">http://cran.r-project.org/web/packages/StatMatch/index.html</a>)</li>
<li><strong>MatchIt</strong> (<a href=""http://cran.r-project.org/web/packages/MatchIt/index.html"">http://cran.r-project.org/web/packages/MatchIt/index.html</a>)</li>
<li><strong>DataCombine</strong> (<a href=""http://cran.r-project.org/web/packages/DataCombine"">http://cran.r-project.org/web/packages/DataCombine</a>)</li>
<li><strong>data.table</strong> (<a href=""http://cran.r-project.org/web/packages/data.table"">http://cran.r-project.org/web/packages/data.table</a>)</li>
</ul>

<p>A comprehensive and coherent approach to <strong>data cleaning</strong> in R, including examples and use of <strong>editrules</strong> and <strong>deducorrect</strong> packages, as well as a description of <em>workflow</em> (<em>framework</em>) of data cleaning in R, is presented in the following paper, which I highly recommend: <a href=""http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf"">http://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf</a>.</p>
","<r><data-cleaning>","22","22","22","0","2452","6478"
"2602","{
  ""id"": 2602,
  ""title"": ""Item based and user based recommendation difference in Mahout""
}","<p><strong>Item Based Algorithm</strong></p>

<pre><code>for every item i that u has no preference for yet

  for every item j that u has a preference for

    compute a similarity s between i and j

    add u's preference for j, weighted by s, to a running average

 return the top items, ranked by weighted average
</code></pre>

<p><strong>User Based Algorithm</strong></p>

<pre><code>for every item i that u has no preference for yet

 for every other user v that has a preference for i

   compute a similarity s between u and v

   add v's preference for i, weighted by s, to a running average

 return the top items, ranked by weighted average
</code></pre>

<p>Item vs User based:</p>

<p>1) Recommenders scale with the number of items or users they must deal with, so there are scenarios in which each type can perform better than the other</p>

<p>2) Similarity estimates between items are more likely to converge over time than similarities between users</p>

<p>3) We can compute and cache similarities that converge, which can give item based recommenders a performance advantage</p>

<p>4) Item based recommenders begin with a list of a user's preferred items and therefore do not need a nearest item neighborhood as user based recommenders do</p>
","<machine-learning><data-mining><algorithms><recommender-system>","22","22","22","0","5043","975"
"32433","{
  ""id"": 32433,
  ""title"": ""How to initialize a new word2vec model with pre-trained model weights?""
}","<p>Thank Abhishek. I've figure it out! Here are my experiments.</p>

<p><strong>1). we plot a easy example:</strong></p>

<pre><code>from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from matplotlib import pyplot
# define training data
sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],
            ['this', 'is', 'the', 'second', 'sentence'],
            ['yet', 'another', 'sentence'],
            ['one', 'more', 'sentence'],
            ['and', 'the', 'final', 'sentence']]
# train model
model_1 = Word2Vec(sentences, size=300, min_count=1)

# fit a 2d PCA model to the vectors
X = model_1[model_1.wv.vocab]
pca = PCA(n_components=2)
result = pca.fit_transform(X)
# create a scatter plot of the projection
pyplot.scatter(result[:, 0], result[:, 1])
words = list(model_1.wv.vocab)
for i, word in enumerate(words):
    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
pyplot.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/ytYGP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ytYGP.png"" alt=""enter image description here""></a></p>

<p>From the above plots, we can see that easy sentences cannot distinguish different words' meaning by distances.</p>

<p><strong>2). Load pre-trained word embedding:</strong></p>

<pre><code>from gensim.models import KeyedVectors

model_2 = Word2Vec(size=300, min_count=1)
model_2.build_vocab(sentences)
total_examples = model_2.corpus_count
model = KeyedVectors.load_word2vec_format(""glove.6B.300d.txt"", binary=False)
model_2.build_vocab([list(model.vocab.keys())], update=True)
model_2.intersect_word2vec_format(""glove.6B.300d.txt"", binary=False, lockf=1.0)
model_2.train(sentences, total_examples=total_examples, epochs=model_2.iter)

# fit a 2d PCA model to the vectors
X = model_2[model_1.wv.vocab]
pca = PCA(n_components=2)
result = pca.fit_transform(X)
# create a scatter plot of the projection
pyplot.scatter(result[:, 0], result[:, 1])
words = list(model_1.wv.vocab)
for i, word in enumerate(words):
    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
pyplot.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/KA5xE.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/KA5xE.png"" alt=""enter image description here""></a></p>

<p>From the above figure, we can see that word embeddings are more meaningful.<br>
Hope this answer will be helpful.</p>
","<python><nlp><word-embeddings><word2vec><gensim>","22","22","22","0","52960","321"
"814","{
  ""id"": 814,
  ""title"": ""Visualizing a graph with a million vertices""
}","<p>I also suggest <code>Gephi</code> software (<a href=""https://gephi.github.io"">https://gephi.github.io</a>), which seems to be quite powerful. Some additional information on using <code>Gephi</code> with <strong>large networks</strong> can be found <a href=""https://forum.gephi.org/viewtopic.php?t=1554"">here</a> and, more generally, <a href=""https://forum.gephi.org/viewforum.php?f=25"">here</a>. <code>Cytoscape</code> (<a href=""http://www.cytoscape.org"">http://www.cytoscape.org</a>) is an alternative to <code>Gephi</code>, being an another popular platform for complex network analysis and visualization.</p>

<p>If you'd like to work with networks <strong>programmatically</strong> (including visualization) in R, Python or C/C++, you can check <code>igraph</code> collection of libraries. Speaking of R, you may find interesting the following blog posts: on <strong>using R with Cytoscape</strong> (<a href=""http://www.vesnam.com/Rblog/viznets1"">http://www.vesnam.com/Rblog/viznets1</a>) and on <strong>using R with Gephi</strong> (<a href=""http://www.vesnam.com/Rblog/viznets2"">http://www.vesnam.com/Rblog/viznets2</a>).</p>

<p>For <strong>extensive lists</strong> of <em>network analysis and visualization software</em>, including some comparison and reviews, you might want to check the following pages: 1) <a href=""http://wiki.cytoscape.org/Network_analysis_links"">http://wiki.cytoscape.org/Network_analysis_links</a>; 2) <a href=""http://www.kdnuggets.com/software/social-network-analysis.html"">http://www.kdnuggets.com/software/social-network-analysis.html</a>; 3) <a href=""http://www.activatenetworks.net/social-network-analysis-sna-software-review"">http://www.activatenetworks.net/social-network-analysis-sna-software-review</a>.</p>
","<visualization><graphs>","22","22","22","0","2452","6478"
"24321","{
  ""id"": 24321,
  ""title"": ""Are there free cloud services to train machine learning models?""
}","<p>There are no <em>unlimited</em> free services*, but some have starting credit or free offers on initial signup. Here are some suggested to date:</p>

<ul>
<li><p>AWS: If specifically deep learning on a large data set, then probably AWS is out - their free offer does not cover machines with enough processing power to tackle deep learning projects. </p></li>
<li><p>Google Cloud might do, the starting credit offer is good enough to do a little deep learning (for maybe a couple of weeks), although they have signup and tax restrictions. </p></li>
<li><p>Azure have a free tier with limited processing and storage options.</p></li>
</ul>

<p>Most free offerings appear to follow the ""Freemium"" model - give you limited service that you can learn to use and maybe like. However not enough to use heavily (for e.g. training an image recogniser or NLP model from scratch) unless you are willing to pay.</p>

<p>This best advice is to shop around for a best starting offer and best price. A review of services is not suitable here, as it will get out of date quickly and not a good use of Stack Exchange. But you can find <a href=""https://www.quora.com/What-is-best-cloud-solution-for-deep-learning"" rel=""noreferrer"">similar questions on Quora</a> and other sites - your best bet is to do a web search for ""cloud compute services for deep learning"" or similar and expect to spend some time comparing notes. A few specialist deep learning services have popped up recently such as <a href=""https://www.nimbix.net/"" rel=""noreferrer"">Nimbix</a> or <a href=""https://www.floydhub.com/"" rel=""noreferrer"">FloydHub</a>, and there are also the big players such as Azure, AWS, Google Cloud.</p>

<p>You won't find anything completely free and unencumbered, and if you want to do this routinely and have time to build and maintain hardware then it is cheaper to buy your own equipment in the long run - at least at a personal level.</p>

<p>To decide whether to pay for cloud or build your own, then consider a typical price for a cloud machine suitable for performing deep learning at around \$1 per hour (prices do vary a lot though, and it is worth shopping around, if only to find a spec that matches your problem). There may be additional fees for storage and data transfer. Compare that to pre-built deep learning machines costing from \$2000, or <a href=""https://medium.com/towards-data-science/build-a-deep-learning-pc-for-1-000-cad-f3c5f26ba134"" rel=""noreferrer"">building your own for \$1000</a> - such machines might not be 100% comparable, but if you are working by yourself then the payback point is going to be after only a few months use. Although don't forget the electricity costs - a powerful machine can draw 0.5kW whilst being heavily used, so this adds up to more than you might expect.</p>

<p>The advantages of cloud computing are that someone else does the maintenance work and takes on the risk of hardware failure. These are valuable services, and priced accordingly. </p>

<hr>

<p>* But see <a href=""https://datascience.stackexchange.com/a/27085"">Jay Speidall's answer</a> about Google's colab service, which appears to be free to use, but may have some T&amp;C limitations which may affect you (for instance I doubt they will be happy for you to run content production of Deep Dream or Style Transfer on it)</p>
","<machine-learning><neural-network><deep-learning><cloud-computing>","22","22","22","0","836","25808"
"27085","{
  ""id"": 27085,
  ""title"": ""Are there free cloud services to train machine learning models?""
}","<p>I want to add one more resource, <a href=""https://colab.research.google.com/"" rel=""noreferrer"">Google Colaboratory</a>. It's a free cloud iPython notebook and gives you free usage of a GPU . I'm not sure of the exact limitations just yet, but it appears you get 12 hours of GPU time per instance and can do this multiple times per month. </p>

<p>This looks like a great resource for students and other non-professionals, especially for smaller jobs that you can run in half a day. It essentially saves you up to $10 per training session, which is a pretty significant resource for machine learning research in my opinion. I seriously hope it doesn't get abused. </p>
","<machine-learning><neural-network><deep-learning><cloud-computing>","22","22","22","0","44299","561"
"23186","{
  ""id"": 23186,
  ""title"": ""Why convolutions always use odd-numbers as filter_size""
}","<p>The convolution operation, simply put, is combination of element-wise product of two matrices. So long as these two matrices agree in dimensions, there shouldn't be a problem, and so I can understand the motivation behind your query.</p>

<p>A.1. However, the intent of convolution is to encode source data matrix (entire image) in terms of a filter or kernel. More specifically, we are trying to encode the pixels in the neighborhood of anchor/source pixels. Have a look at the figure below:
<a href=""https://i.stack.imgur.com/YDusp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/YDusp.png"" alt=""enter image description here""></a>
Typically, we consider every pixel of the source image as anchor/source pixel, but we are not constrained to do this. In fact, it is not uncommon to include a stride, where in we anchor/source pixels are separated by a specific number of pixels.</p>

<p>Okay, so what is the source pixel? It is the anchor point at which the kernel is centered and we are encoding all the neighboring pixels, including the anchor/source pixel. Since, the kernel is symmetrically shaped (not symmetric in kernel values), there are equal number (n) of pixel on all sides (4- connectivity) of the anchor pixel. Therefore, whatever this number of pixels maybe, the length of each side of our symmetrically shaped kernel is 2*n+1 (each side of the anchor + the anchor pixel), and therefore filter/kernels are always odd sized.</p>

<p>What if we decided to break with 'tradition' and used asymmetric kernels? You'd suffer aliasing errors, and so we don't do it. We consider the pixel to be the smallest entity, i.e. there is no sub-pixel concept here.</p>

<p>A.2 
The boundary problem is dealt with using different approaches: some ignore it, some zero pad it, some mirror reflect it. If you are not going to compute an inverse operation, i.e. deconvolution, and are not interested in perfect reconstruction of original image, then you don't care about either loss of information or injection of noise due to the boundary problem. Typically, the pooling operation (average pooling or max pooling) will remove your boundary artifacts anyway. So, feel free to ignore part of your 'input field', your pooling operation will do so for you.</p>

<p>--</p>

<p>Zen of convolution: </p>

<p>In the old-school signal processing domain, when an input signal was convolved or passed through a filter, there was no way of judging a-prior which components of the convolved/filtered response were relevant/informative and which were not. Consequently, the aim was to preserve signal components (all of it) in these transformations. </p>

<p>These signal components are information. Some components are more informative than others. The only reason for this is that we are interested in extracting higher-level information; Information pertinent towards some semantic classes. Accordingly, those signal components that do not provide the information we are specifically interested in can be pruned out. Therefore, unlike old-school dogmas about convolution/filtering, we are free to pool/prune the convolution response as we feel like. The way we feel like doing so is to rigorously remove all data components that are not contributing towards improving our statistical model.</p>
","<deep-learning><computer-vision><convolutional-neural-network><convolution>","22","22","22","0","2515","1203"
"35464","{
  ""id"": 35464,
  ""title"": ""What is weight and bias in deep learning?""
}","<p>Mathematically speaking, Imagine you are a model (No! not that kind, figure 8 ones).</p>
<p><strong>Bias</strong> is simply how biased you are. If you are a Nigerian, and you are asked &quot;Which nationality have the most beautiful women&quot; you might say Nigerian Ladies. We can say it's because you are biased. So your formula is <span class=""math-container"">$Y = WX + nigerian$</span>.</p>
<p>So what do you understand? Biased is that pre-assumption in a model like you have.</p>
<p>As for weight, logically speaking, Weight is your Gradient (as in linear algebra).</p>
<p><strong>What is Gradient?</strong> it's the steepness of the Linear function.</p>
<p><strong>What makes the linear gradient very steep (High positive value)?</strong></p>
<p>It's because little changes in X(input) causes Large differences in Y axis(output). So you (Not as a Model anymore, but a brilliant Mathematician (your alter ego)) or your Computer tries to find this gradient, which you can call weight. The difference is that you use a pencil and graph book to find this, but the black box does its electronic Magic with registers.</p>
<p>In the Machine Learning Process, computers, or you, try to draw many straight lines or Linear functions across the data points.</p>
<p><strong>Why do you try to draw many straight lines?</strong></p>
<p>Because in your graph book/Computer memory, you are trying the see the line that fit appropriately.</p>
<p><strong>How do I or Computer know the line that fits appropriately?</strong></p>
<p>In my secondary school, I was taught to draw a line across the data points, visually checking the line that cuts through perfectly in the middle of all the data point.(Forget those A.I hype, our brains can calculate by just staring at things). But as for the computer, it tries the standard deviation and variance of each line towards the data points. The line with the least deviation (sometimes we call it the error function) is chosen.</p>
<p><strong>Cool! so and what happens</strong></p>
<p>The gradient of that line is calculated, let's say the Weight of the Learning problem is Calculated.</p>
<p>That's Machine Learning at its basic understand and a high school student plotting graph in his/her Graphbook.</p>
","<machine-learning><deep-learning><tensorflow>","22","22","22","0","56152","321"
"47760","{
  ""id"": 47760,
  ""title"": ""Keras vs. tf.keras""
}","<p>From <a href=""https://github.com/keras-team/keras"" rel=""noreferrer"">Keras</a> repo.:</p>

<blockquote>
  <p>Keras is a high-level neural networks API, written in Python and
  capable of running on top of TensorFlow, CNTK, or Theano.</p>
</blockquote>

<p>And</p>

<blockquote>
  <p>Before installing Keras, please install one of its backend engines:
  TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend.</p>
</blockquote>

<p>So Keras is a skin (an API). TensorFlow has decided to include this skin inside itself as <code>tf.keras</code>. Since Keras provides APIs that TensorFlow has already implemented (unless CNTK and Theano overtake TensorFlow which is unlikely), <code>tf.keras</code> would keep up with Keras in terms of API diversity. Therefore, I would suggest to go with <code>tf.keras</code> which keeps you involved with only one, higher quality repo. instead of two, which means less headache.</p>

<blockquote>
  <p>Which one do you choose?!</p>
</blockquote>

<p><code>tf.keras‬</code>.</p>
","<python><deep-learning><keras><tensorflow>","22","22","22","0","67328","7999"
"24083","{
  ""id"": 24083,
  ""title"": ""What are graph embedding?""
}","<p>Graph embedding learns a mapping from a network to a vector space, while preserving relevant network properties.</p>

<p>Vector spaces are more amenable to data science than graphs. Graphs contain edges and nodes, those network relationships can only use a specific subset of mathematics, statistics, and machine learning. Vector spaces have a richer toolset from those domains. Additionally, vector operations are often simpler and faster than the equivalent graph operations.</p>

<p>One example is finding nearest neighbors. You can perform ""hops"" from node to another node in a graph. In many real-world graphs after a couple of hops, there is little meaningful information (e.g., recommendations from friends of friends of friends). However, in vector spaces, you can use distance metrics to get quantitative results (e.g., Euclidian distance or Cosine Similarity). If you have quantitative distance metrics in a meaningful vector space, finding nearest neighbors is straightforward.</p>

<p>""<a href=""https://arxiv.org/abs/1705.02801"" rel=""noreferrer"">Graph Embedding Techniques, Applications, and Performance: A Survey</a>"" is an overview article that goes into greater detail.</p>
","<graphs>","22","22","22","0","1330","13291"
"510","{
  ""id"": 510,
  ""title"": ""Publicly available social network datasets/APIs""
}","<p>A couple of words about social networks APIs. About a year ago I wrote a review of popular social networks’ APIs for researchers. Unfortunately, it is in Russian. Here is a summary:</p>

<p><strong>Twitter</strong> (<a href=""https://dev.twitter.com/docs/api/1.1"">https://dev.twitter.com/docs/api/1.1</a>)</p>

<ul>
<li>almost all data about tweets/texts and users is available;</li>
<li>lack of sociodemographic data;</li>
<li>great streaming API: useful for real time text processing;</li>
<li>a lot of wrappers for programing languages;</li>
<li>getting network structure (connections) is possible, but time-expensive (1 request per 1 minute).</li>
</ul>

<p><strong>Facebook</strong> (<a href=""https://developers.facebook.com/docs/reference/api/"">https://developers.facebook.com/docs/reference/api/</a>)</p>

<ul>
<li>rate limits: about 1 request per second;</li>
<li>well documented, sandbox present;</li>
<li>FQL (SQL-like) and «regular Rest» Graph API;</li>
<li>friendship data and sociodemographic features present;</li>
<li>a lot of data is beyond <em>event horizon</em>: only friends' and friends' of friends data is more or less complete, almost nothing could be investigated about random user;</li>
<li>some strange API bugs, and looks like nobody cares about it (e.g., some features available through FQL, but not through Graph API synonym).</li>
</ul>

<p><strong>Instagram</strong> (<a href=""http://instagram.com/developer/"">http://instagram.com/developer/</a>)</p>

<ul>
<li>rate limits: 5000 requests per hour;</li>
<li>real-time API (like Streaming API for Twitter, but with photos) - connection to it is a little bit tricky: callbacks are used;</li>
<li>lack of sociodemographic data;</li>
<li>photos, filters data available;</li>
<li>unexpected imperfections (e.g., it’s possible to collect only 150 comments to post/photo).</li>
</ul>

<p><strong>Foursquare</strong> (<a href=""https://developer.foursquare.com/overview/"">https://developer.foursquare.com/overview/</a>)</p>

<ul>
<li>rate limits: 5000 requests per hour;</li>
<li>kingdom of geosocial data :)</li>
<li>quite closed from researches because of privacy issues. To collect checkins data one need to build composite parser working with 4sq, bit.ly, and twitter APIs at once;</li>
<li>again: lack of sociodemographic data.</li>
</ul>

<p><strong>Google+</strong> (<a href=""https://developers.google.com/+/api/latest/"">https://developers.google.com/+/api/latest/</a>)</p>

<ul>
<li>about 5 requests per second (try to verify);</li>
<li>main methods: activities and people;</li>
<li>like on Facebook, a lot of personal data for random user is hidden;</li>
<li>lack of user connections data.</li>
</ul>

<p>And out-of-competition: I reviewed social networks for Russian readers, and #1 network here is <a href=""http://en.wikipedia.org/wiki/VK_%28social_network%29""><strong>vk.com</strong></a>. It’s translated to many languages, but popular only in Russia and other CIS countries. API docs link: <a href=""http://vk.com/dev/"">http://vk.com/dev/</a>. And from my point of view, it’s the best choice for homebrew social media research. At least, in Russia. That’s why:</p>

<ul>
<li>rate limits: 3 requests per second;</li>
<li>public text and media data available;</li>
<li>sociodemographic data available: for random user availability level is about 60-70%;</li>
<li>connections between users are also available: almost all friendships data for random user is available;</li>
<li>some special methods: e.g., there is a method to get online/offline status for exact user in realtime, and one could build schedule for his audience.</li>
</ul>
","<open-source><dataset><crawling>","22","22","22","0","941","1089"
"28346","{
  ""id"": 28346,
  ""title"": ""Different Test Set and Training Set Distribution""
}","<blockquote>
  <p>Great question, this is what is known in Machine Learning paradigm as either
  ""Covariate Shift"", or ""Model Drift"" or ""Nonstationarity"" and so on.</p>
</blockquote>

<p>One of the critical assumption one would make to build a machine learning model for future prediction is that unseen data (test) comes from the same distribution as training data! However, in reality this rather simple assumption breaks easily and upcoming data (its distribution) changes over time for many reasons. For those who may not be familiar with this very important problem, I encourage looking <a href=""https://blog.bigml.com/2014/01/03/simple-machine-learning-to-detect-covariate-shift/"" rel=""noreferrer"">here</a> or <a href=""https://www.analyticsvidhya.com/blog/2017/07/covariate-shift-the-hidden-problem-of-real-world-data-science/"" rel=""noreferrer"">post</a>! </p>

<p>To me, your question falls into the same category. Although I do not have the perfect solution (an implementation to offer), but I think you may look: </p>

<ul>
<li>This <a href=""https://maxhalford.github.io/blog/subsampling-1/"" rel=""noreferrer"">blog post</a> gives you a simple way to handle the subsampling of training data with code provided in Python!</li>
<li>Check this <a href=""https://f1000research.com/articles/5-597/v3"" rel=""noreferrer"">research paper</a>. They propose to solve the problem by reweighting the training data so that the distribution of training is closer to the distribution of test using <strong>Kullback-Leibler Importance Estimation Procedure</strong> base on ""<em>Kullback-Leibler divergence</em>"" theorem. I do not know if they provide an implementation or it can be implemented easily, but I think it might worth digging as it sounds a professional way to deal the distribution mismatch. </li>
</ul>

<p><strong>QUICK update (a good solution)</strong>: I found a <a href=""https://github.com/srome/pykliep"" rel=""noreferrer"">Python implementation of KLIEP algorithm</a> of that research paper (last point) to find those weights. It rather seems easy to use! Basically it resamples the training by putting weights (via the KLIEP algorithm) so that the assumption of having a similar distribution of train and test holds true as much as possible. </p>
","<preprocessing>","22","24","23","1","44456","3746"
"25228","{
  ""id"": 25228,
  ""title"": ""How to calculate mAP for detection task for the PASCAL VOC Challenge?""
}","<p>To answer your questions:</p>
<ol>
<li>Yes your approach is right</li>
<li>Of A, B and C the right answer is B.</li>
</ol>
<p>The explanation is the following: In order to calculate Mean Average Precision (mAP) in the context of Object Detection you must compute the Average Precision (AP) for each class, and then compute the mean across all classes. The key here is to compute the AP for each class, in general for computing Precision (P) and Recall (R) you must define what are: True Positives (TP), False Positives (FP), True Negative (TN) and False Negative (FN). In the setting of Object Detection of the Pascal VOC Challenge are the following:</p>
<ul>
<li>TP: are the Bounding Boxes (BB) that the intersection over union (IoU) with the ground truth (GT) is above 0.5</li>
<li>FP: two cases (a) BB that the IoU with GT is below 0.5 (b) the BB that have IoU with a GT that has already been detected.</li>
<li>TN: there are not true negative, the image are expected to contain at least one object</li>
<li>FN: those ground truthes for which the method failed to produce a BB</li>
</ul>
<p>Now each predicted BB have a confidence value for the given class. So the scoring method sort the predictions for decreasing order of confidence and compute the P = TP / (TP + FP) and R = TP / (TP + FN) for each possible rank k = 1 up to the number of predictions. So now you have a (P, R) for each rank those P and R are the &quot;raw&quot; Precision-Recall curve. To compute the interpolated P-R curve foreach value of R you select the maximum P that has a corresponding R' &gt;= R.</p>
<p>There are two different ways to sample P-R curve points according to <a href=""http://host.robots.ox.ac.uk/pascal/VOC/voc2012/htmldoc/devkit_doc.html#SECTION00044000000000000000"" rel=""nofollow noreferrer"">voc devkit doc</a>.
For VOC Challenge before 2010, we select the maximum P obtained for any R' &gt;= R, which R belongs to 0, 0.1, ..., 1 (eleven points). The AP is then the average precision at each of the Recall thresholds. For VOC Challenge 2010 and after, we still select the maximum P for any R' &gt;= R, while R belongs to all unique recall values (include 0 and 1). The AP is then the area size under P-R curve. Notice that in the case that you don't have a value of P with Recall above some of the thresholds the Precision value is 0.</p>
<p>For instance consider the following output of a method given the class &quot;Aeroplane&quot;:</p>
<pre><code>BB  | confidence | GT
----------------------
BB1 |  0.9       | 1
----------------------
BB2 |  0.9       | 1
----------------------
BB3 |  0.7       | 0
----------------------
BB4 |  0.7       | 0
----------------------
BB5 |  0.7       | 1
----------------------
BB6 |  0.7       | 0
----------------------
BB7 |  0.7       | 0
----------------------
BB8 |  0.7       | 1
----------------------
BB9 |  0.7       | 1
----------------------
</code></pre>
<p>Besides it not detected bounding boxes in two images, so we have FN = 2. The previous table is the ordered rank by confidence value of the predictions of the method GT = 1 means is a TP and GT = 0 FP. So TP=5 (BB1, BB2, BB5, BB8 and BB9), FP=5. For the case of rank=3 the precision drops because BB1 was already detected, so even if the object is indeed present it counts as a FP. .</p>
<pre><code>rank=1  precision=1.00 and recall=0.14
----------
rank=2  precision=1.00 and recall=0.29
----------
rank=3  precision=0.66 and recall=0.29
----------
rank=4  precision=0.50 and recall=0.29
----------
rank=5  precision=0.40 and recall=0.29
----------
rank=6  precision=0.50 and recall=0.43
----------
rank=7  precision=0.43 and recall=0.43
----------
rank=8  precision=0.38 and recall=0.43
----------
rank=9  precision=0.44 and recall=0.57
----------
rank=10 precision=0.50 and recall=0.71
----------
</code></pre>
<p>Given the previous results:
If we used the way before voc2010, the interpolated Precision values are 1, 1, 1, 0.5, 0.5, 0.5, 0.5, 0.5, 0, 0, 0. Then AP = 5.5 / 11 = 0.5 for the class of &quot;Aeroplanes&quot;.
Else if we used the way since voc2010, the interpolated Precision values are 1, 1, 1, 0.5, 0.5, 0.5, 0 for seven unique recalls that are 0, 0.14, 0.29, 0.43, 0.57, 0.71, 1.Then AP = (0.14-0)*1 + (0.29-0.14)*1 + (0.43-0.29)*0.5 + (0.57-0.43)*0.5 + (0.71-0.57)*0.5 + (1-0.71)*0 = 0.5 for the class of &quot;Aeroplanes&quot;.</p>
<p>Repeat for each class and then you have the (mAP).</p>
<p>More information can be found in the following links <a href=""https://stats.stackexchange.com/questions/260430/average-precision-in-object-detection"">1</a>, <a href=""https://stackoverflow.com/questions/36274638/map-metric-in-object-detection-and-computer-vision"">2</a>. Also you should check the paper: <a href=""http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham15.pdf"" rel=""nofollow noreferrer"">The PASCAL Visual Object Classes Challenge: A Retrospective</a> for a more detailed explanation.</p>
","<machine-learning><neural-network><svm><computer-vision><object-recognition>","22","22","22","0","26243","2116"
"8866","{
  ""id"": 8866,
  ""title"": ""Bagging vs Dropout in Deep Neural Networks""
}","<p>Bagging and dropout do not achieve quite the same thing, though both are types of model averaging.</p>

<p><a href=""https://en.wikipedia.org/wiki/Bootstrap_aggregating"">Bagging</a> is an operation across your entire dataset which trains models on a subset of the training data.  Thus some training examples are not shown to a given model.</p>

<p><a href=""https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf"">Dropout</a>, by contrast, is applied to features within each training example.  It is true that the result is functionally equivalent to training exponentially many networks (with shared weights!) and then equally weighting their outputs.  But dropout works on the feature space, causing certain features to be unavailable to the network, not full examples.  Because each neuron cannot completely rely on one input, representations in these networks tend to be more distributed and the network is less likely to overfit.</p>
","<machine-learning><neural-network><deep-learning>","22","22","22","0","12473","2947"
"31713","{
  ""id"": 31713,
  ""title"": ""Natural Language to SQL query""
}","<p>If you want to tackle the problem from another perspective, with <em>an end to end learning</em>, such that you don't specify ahead of time this large pipeline you've mentioned earlier, all you care about is the mapping between sentences and their corresponding SQL queries. </p>

<p><strong>Tutorials:</strong>  </p>

<p><a href=""https://blog.einstein.ai/how-to-talk-to-your-database"" rel=""noreferrer"">How to talk to your database</a>  </p>

<p><strong>Papers:</strong>  </p>

<ul>
<li><p><a href=""https://arxiv.org/abs/1709.00103"" rel=""noreferrer"">Seq2SQL: Generating Structured Queries from
Natural Language using Reinforcement Learning</a>  salesforce</p></li>
<li><p><a href=""https://www.ijcai.org/Proceedings/16/Papers/329.pdf"" rel=""noreferrer"">Neural Enquirer: Learning to Query Tables in Natural Language</a></p></li>
</ul>

<p><strong>Dataset:</strong>  </p>

<p><a href=""https://github.com/salesforce/WikiSQL"" rel=""noreferrer"">A large annotated semantic parsing corpus for developing natural language interfaces.</a></p>

<p><strong>Github code:</strong>   </p>

<ol>
<li><a href=""https://github.com/dadashkarimi/seq2sql"" rel=""noreferrer"">seq2sql</a> </li>
<li><a href=""https://github.com/xiaojunxu/SQLNet"" rel=""noreferrer"">SQLNet</a></li>
</ol>

<p>Also, there are commercial solutions like <a href=""https://www.nlsql.com/"" rel=""noreferrer"">nlsql</a>   </p>
","<machine-learning><sql><nlp>","21","21","21","0","44039","868"
"39103","{
  ""id"": 39103,
  ""title"": ""Convert a list of lists into a Pandas Dataframe""
}","<p>Once you have the data:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd

data = [['New York Yankees ', '""Acevedo Juan""  ', 900000, ' Pitcher\n'], 
        ['New York Yankees ', '""Anderson Jason""', 300000, ' Pitcher\n'], 
        ['New York Yankees ', '""Clemens Roger"" ', 10100000, ' Pitcher\n'], 
        ['New York Yankees ', '""Contreras Jose""', 5500000, ' Pitcher\n']]
</code></pre>

<p>You can create dataframe from the transposing the data:</p>

<pre class=""lang-py prettyprint-override""><code>data_transposed = zip(data)
df = pd.DataFrame(data_transposed, columns=[""Team"", ""Player"", ""Salary"", ""Role""])
</code></pre>

<p>Another way:</p>

<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame(data)
df = df.transpose()
df.columns = [""Team"", ""Player"", ""Salary"", ""Role""]
</code></pre>
","<pandas>","21","21","21","0","60069","211"
"13917","{
  ""id"": 13917,
  ""title"": ""xgboost: give more importance to recent samples""
}","<p>On Python you have a nice scikit-learn wrapper, so you can write just like this:</p>

<pre><code>import xgboost as xgb
exgb_classifier = xgb.XGBClassifier()
exgb_classifier.fit(X, y, sample_weight=sample_weights_data)
</code></pre>

<p>More information you can receive from this:
<a href=""http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.fit"" rel=""noreferrer"">http://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.fit</a></p>
","<xgboost><weighted-data>","21","21","21","0","19220","311"
"66351","{
  ""id"": 66351,
  ""title"": ""What would I prefer - an over-fitted model or a less accurate model?""
}","<p>There are a couple of nuances here.</p>

<ol>
<li>Complexity question very important - ocams razor</li>
<li>CV - is this trully the case 84%/83% (test it for train+test with CV)</li>
</ol>

<p>Given this, personal opinion: Second one.</p>

<p>Better to catch general patterns. You already know that first model failed on that because of the train and test difference. 1% says nothing.</p>
","<machine-learning-model><training><supervised-learning><accuracy><overfitting>","21","21","21","0","80885","5112"
"15633","{
  ""id"": 15633,
  ""title"": ""Train/Test Split after performing SMOTE""
}","<p>When you use any sampling technique (specifically synthetic) you divide your data first and then apply synthetic sampling on the training data only. After you do the training, you use the test set (which contains only original samples) to evaluate.</p>
<p>The risk if you use your strategy is having the original sample in training (testing) and the synthetic sample (that was created based on this original sample) in the test (training) set.</p>
","<machine-learning><evaluation><class-imbalance><smote>","21","21","21","0","17696","1799"
"222","{
  ""id"": 222,
  ""title"": ""Publicly Available Datasets""
}","<p><a href=""http://enigma.io"">Enigma</a> is a repository of public available datasets. Its free plan offers public data search, with 10k API calls per month. Not all public databases are listed, but the list is enough for common cases.</p>

<p>I used it for academic research and it saved me a lot of time.</p>

<hr>

<p>Another interesting source of data is the <a href=""http://theunitedstates.io/"">@unitedstates project</a>, containing data and tools to collect them, about the United States (members of Congress, geographic shapes…).</p>
","<open-source><dataset>","21","21","21","0","43","101"
"28159","{
  ""id"": 28159,
  ""title"": ""How to calculate the fold number (k-fold) in cross validation?""
}","<p>The number of folds is usually determined by the number of instances contained in your dataset. For example, if you have 10 instances in your data, 10-fold cross-validation wouldn't make sense. $k$-fold cross validation is used for two main purposes, to tune hyper parameters and to better evaluate the performance of a model.</p>

<p>In both of these cases selecting $k$ depends on the same thing. You must ensure that the training set and testing set are drawn from the same distribution. And that both sets contain sufficient variation such that the underlining distribution is represented. In a 10-fold cross validation with only 10 instances, there would only be 1 instance in the testing set. This instance does not properly represent the variation of the underlying distribution. </p>

<p>That being said, selecting $k$ is not an exact science because it's hard to estimate how well your fold represents your overall dataset. I usually use 5-fold cross validation. This means that 20% of the data is used for testing, this is usually pretty accurate. However, if your dataset size increases dramatically, like if you have over 100,000 instances, it can be seen that a 10-fold cross validation would lead in folds of 10,000 instances. This should be sufficient to reliably test your model. </p>

<p>In short, yes the number of folds depends on the data size. I usually stick with 4- or 5-fold. Make sure to shuffle your data, such that your folds do not contain inherent bias. </p>
","<machine-learning><python><scikit-learn><cross-validation>","21","21","21","0","29587","8088"
"17600","{
  ""id"": 17600,
  ""title"": ""Why are variables of train and test data defined using the capital letter (in Python)?""
}","<p>The X (and sometimes Y) variables are matrices.</p>

<p>In some math notation, it is common practice to write vector variable names as lower case and matrix variable names as upper case. Often these are in bold or have other annotation, but that does not translate well to code. Either way, I believe that the practice has transferred from this notation. </p>

<p>You may also notice in code, when the target variable is a single column of values, it is written <code>y</code>, so you have <code>X, y</code></p>

<p>Of course, this has no special semantic meaning in Python and you are free to ignore the convention. However, because it has become a convention, it may be worth maintaining if you share your code.</p>
","<python><dataset>","21","21","21","0","836","25808"
"39933","{
  ""id"": 39933,
  ""title"": ""Feature Scaling both training and test data""
}","<p>Generally speaking, best practice is to use only the training set to figure out how to scale / normalize, then blindly apply the same transform to the test set.</p>

<p>For example, say you're going to normalize the data by removing the mean and dividing out the variance. If you use the whole dataset to figure out the feature mean and variance, you're using knowledge about the distribution of the test set to set the scale of the training set - 'leaking' information.</p>

<p>The right way to do this is to use only the training set to calculate the mean and variance, normalize the training set, and then at test time, use that same (training) mean and variance to normalize the test set.</p>

<p>As for the point in your question, imagine using the training mean and variance to scale the training set and test mean and variance to scale the test set. Then, for example, a single test example with a value of 1.0 in a particular feature would have a different original value than a training example with a value of 1.0 (because they were scaled differently), but would be treated identically by the model.  This is where the bias would come from.</p>

<p>Hope this helps!</p>
","<machine-learning><data-science-model>","21","21","21","0","54944","1074"
"19332","{
  ""id"": 19332,
  ""title"": ""Deep Neural Network - Backpropogation with ReLU""
}","<p>Working definitions of ReLU function and its derivative:</p>

<p><span class=""math-container"">$ReLU(x) = \begin{cases}
  0, &amp; \text{if } x &lt; 0, \\
  x, &amp; \text{otherwise}.
\end{cases}$</span></p>

<p><span class=""math-container"">$\frac{d}{dx} ReLU(x) = \begin{cases}
  0, &amp; \text{if } x &lt; 0, \\
  1, &amp; \text{otherwise}.
\end{cases}$</span></p>

<p>The derivative is the unit <a href=""https://en.wikipedia.org/wiki/Step_function"" rel=""noreferrer"">step function</a>. This does ignore a problem at <span class=""math-container"">$x=0$</span>, where the gradient is not strictly defined, but that is not a practical concern for neural networks. With the above formula, the derivative at 0 is 1, but you could equally treat it as 0, or 0.5 with no real impact to neural network performance.</p>

<hr>

<p><strong>Simplified network</strong></p>

<p>With those definitions, let's take a look at your example networks.</p>

<p>You are running regression with cost function <span class=""math-container"">$C = \frac{1}{2}(y-\hat{y})^2$</span>. You have defined <span class=""math-container"">$R$</span> as the output of the artificial neuron, but you have not defined an input value. I'll add that for completeness - call it <span class=""math-container"">$z$</span>, add some indexing by layer, and I prefer lower-case for the vectors and upper case for matrices, so <span class=""math-container"">$r^{(1)}$</span> output of the first layer, <span class=""math-container"">$z^{(1)}$</span> for its input and <span class=""math-container"">$W^{(0)}$</span> for the weight connecting the neuron to its input <span class=""math-container"">$x$</span> (in a larger network, that might connect to a deeper <span class=""math-container"">$r$</span> value instead). I have also adjusted the index number for the weight matrix - why that is will become clearer for the larger network. NB I am ignoring having more than neuron in each layer for now.</p>

<p>Looking at your simple 1 layer, 1 neuron network, the feed-forward equations are:</p>

<p><span class=""math-container"">$z^{(1)} = W^{(0)}x$</span></p>

<p><span class=""math-container"">$\hat{y} = r^{(1)} = ReLU(z^{(1)})$</span></p>

<p>The derivative of the cost function w.r.t. an example estimate is:</p>

<p><span class=""math-container"">$\frac{\partial C}{\partial \hat{y}} = \frac{\partial C}{\partial r^{(1)}} = \frac{\partial}{\partial r^{(1)}}\frac{1}{2}(y-r^{(1)})^2 = \frac{1}{2}\frac{\partial}{\partial r^{(1)}}(y^2 - 2yr^{(1)} + (r^{(1)})^2) = r^{(1)} - y$</span></p>

<p>Using the chain rule for back propagation to the pre-transform (<span class=""math-container"">$z$</span>) value:</p>

<p><span class=""math-container"">$\frac{\partial C}{\partial z^{(1)}} = \frac{\partial C}{\partial r^{(1)}} \frac{\partial r^{(1)}}{\partial z^{(1)}} = (r^{(1)} - y)Step(z^{(1)}) = (ReLU(z^{(1)}) - y)Step(z^{(1)})$</span></p>

<p>This <span class=""math-container"">$\frac{\partial C}{\partial z^{(1)}}$</span> is an interim stage and critical part of backprop linking steps together. Derivations often skip this part because clever combinations of cost function and output layer mean that it is simplified. Here it is not.</p>

<p>To get the gradient with respect to the weight <span class=""math-container"">$W^{(0)}$</span>, then it is another iteration of the chain rule:</p>

<p><span class=""math-container"">$\frac{\partial C}{\partial W^{(0)}} = \frac{\partial C}{\partial z^{(1)}} \frac{\partial z^{(1)}}{\partial W^{(0)}} = (ReLU(z^{(1)}) - y)Step(z^{(1)})x = (ReLU(W^{(0)}x) - y)Step(W^{(0)}x)x$</span></p>

<p>. . . because <span class=""math-container"">$z^{(1)} = W^{(0)}x$</span> therefore <span class=""math-container"">$\frac{\partial z^{(1)}}{\partial W^{(0)}} = x$</span></p>

<p>That is the full solution for your simplest network.</p>

<p>However, in a layered network, you also need to carry the same logic down to the next layer. Also, you typically have more than one neuron in a layer.</p>

<hr>

<p><strong>More general ReLU network</strong></p>

<p>If we add in more generic terms, then we can work with two arbitrary layers. Call them Layer <span class=""math-container"">$(k)$</span> indexed by <span class=""math-container"">$i$</span>, and Layer <span class=""math-container"">$(k+1)$</span> indexed by <span class=""math-container"">$j$</span>. The weights are now a matrix. So our feed-forward equations look like this:</p>

<p><span class=""math-container"">$z^{(k+1)}_j = \sum_{\forall i} W^{(k)}_{ij}r^{(k)}_i$</span></p>

<p><span class=""math-container"">$r^{(k+1)}_j = ReLU(z^{(k+1)}_j)$</span></p>

<p>In the output layer, then the initial gradient w.r.t. <span class=""math-container"">$r^{output}_j$</span> is still  <span class=""math-container"">$r^{output}_j - y_j$</span>. However, ignore that for now, and look at the <em>generic</em> way to back propagate, assuming we have already found <span class=""math-container"">$\frac{\partial C}{\partial r^{(k+1)}_j}$</span> - just note that this is ultimately where we get the output cost function gradients from. Then there are 3 equations we can write out following the chain rule:</p>

<p>First we need to get to the neuron input before applying ReLU: </p>

<ol>
<li><span class=""math-container"">$\frac{\partial C}{\partial z^{(k+1)}_j} = \frac{\partial C}{\partial r^{(k+1)}_j} \frac{\partial r^{(k+1)}_j}{\partial z^{(k+1)}_j} = \frac{\partial C}{\partial r^{(k+1)}_j}Step(z^{(k+1)}_j)$</span></li>
</ol>

<p>We also need to propagate the gradient to previous layers, which involves summing up all connected influences to each neuron:</p>

<ol start=""2"">
<li><span class=""math-container"">$\frac{\partial C}{\partial r^{(k)}_i} = \sum_{\forall j} \frac{\partial C}{\partial z^{(k+1)}_j} \frac{\partial z^{(k+1)}_j}{\partial r^{(k)}_i} = \sum_{\forall j} \frac{\partial C}{\partial z^{(k+1)}_j} W^{(k)}_{ij}$</span></li>
</ol>

<p>And we need to connect this to the weights matrix in order to make adjustments later:</p>

<ol start=""3"">
<li><span class=""math-container"">$\frac{\partial C}{\partial W^{(k)}_{ij}} = \frac{\partial C}{\partial z^{(k+1)}_j} \frac{\partial z^{(k+1)}_j}{\partial W^{(k)}_{ij}} = \frac{\partial C}{\partial z^{(k+1)}_j} r^{(k)}_{i}$</span></li>
</ol>

<p>You can resolve these further (by substituting in previous values), or combine them (often steps 1 and 2 are combined to relate pre-transform gradients layer by layer). However the above is the most general form. You can also substitute the <span class=""math-container"">$Step(z^{(k+1)}_j)$</span> in equation 1 for whatever the derivative function is of your current activation function - this is the only place where it affects the calculations.</p>

<hr>

<p>Back to your questions:</p>

<blockquote>
  <p>If this derivation is correct, how does this prevent vanishing?</p>
</blockquote>

<p>Your derivation was not correct. However, that does not completely address your concerns.</p>

<p>The difference between using sigmoid versus ReLU is just in the step function compared to e.g. sigmoid's <span class=""math-container"">$y(1-y)$</span>, applied once per layer. As you can see from the generic layer-by-layer equations above, the gradient of the transfer function appears in one place only. The sigmoid's best case derivative adds a factor of 0.25 (when <span class=""math-container"">$x = 0, y = 0.5$</span>), and it gets worse than that and saturates quickly to near zero derivative away from <span class=""math-container"">$x=0$</span>. The ReLU's gradient is either 0 or 1, and in a healthy network will be 1 often enough to have less gradient loss during backpropagation. This is not guaranteed, but experiments show that ReLU has good performance in deep networks.</p>

<blockquote>
  <p>If there's thousands of layers, there would be a lot of multiplication due to weights, then wouldn't this cause vanishing or exploding gradient?</p>
</blockquote>

<p>Yes this can have an impact too. This can be a problem regardless of transfer function choice. In some combinations, ReLU may help keep exploding gradients under control too, because it does not saturate (so large weight norms will tend to be poor direct solutions and an optimiser is unlikely to move towards them). However, this is not guaranteed.</p>
","<neural-network><backpropagation>","21","21","21","0","836","25808"
"18145","{
  ""id"": 18145,
  ""title"": ""How should the bias be initialized and regularized?""
}","<p>From the <a href=""http://cs231n.github.io/neural-networks-2/"" rel=""nofollow noreferrer"">Stanford CS231N Notes</a>:</p>
<blockquote>
<p>Initializing the biases. It is possible and common to initialize the
biases to be zero, since the asymmetry breaking is provided by the
small random numbers in the weights. For ReLU non-linearities, some
people like to use small constant value such as 0.01 for all biases
because this ensures that all ReLU units fire in the beginning and
therefore obtain and propagate some gradient. However, it is not clear
if this provides a consistent improvement (in fact some results seem
to indicate that this performs worse) and it is more common to simply
use 0 bias initialization.</p>
</blockquote>
<p>In LSTMs it's common to initialize the biases to 1 - see for <a href=""http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf"" rel=""nofollow noreferrer"">example</a>.</p>
","<neural-network>","21","21","21","0","30810","436"
"36620","{
  ""id"": 36620,
  ""title"": ""When to use GRU over LSTM?""
}","<p><strong>FULL GRU Unit</strong></p>

<p><span class=""math-container"">$ \tilde{c}_t = \tanh(W_c [G_r * c_{t-1}, x_t ] + b_c) $</span></p>

<p><span class=""math-container"">$ G_u = \sigma(W_u [ c_{t-1}, x_t ] + b_u) $</span></p>

<p><span class=""math-container"">$ G_r = \sigma(W_r [ c_{t-1}, x_t ] + b_r) $</span></p>

<p><span class=""math-container"">$ c_t = G_u * \tilde{c}_t + (1 - G_u) * c_{t-1} $</span></p>

<p><span class=""math-container"">$ a_t = c_t $</span></p>

<p><strong>LSTM Unit</strong></p>

<p><span class=""math-container"">$ \tilde{c}_t = \tanh(W_c [ a_{t-1}, x_t ] + b_c) $</span></p>

<p><span class=""math-container"">$ G_u = \sigma(W_u [ a_{t-1}, x_t ] + b_u) $</span></p>

<p><span class=""math-container"">$ G_f = \sigma(W_f [ a_{t-1}, x_t ] + b_f) $</span></p>

<p><span class=""math-container"">$ G_o = \sigma(W_o [ a_{t-1}, x_t ] + b_o) $</span></p>

<p><span class=""math-container"">$ c_t = G_u * \tilde{c}_t + G_f * c_{t-1} $</span></p>

<p><span class=""math-container"">$ a_t = G_o * tanh(c_t) $</span></p>

<p>As can be seen from the equations LSTMs have a separate update gate and forget gate. This clearly makes LSTMs more sophisticated but at the same time more complex as well. There is no simple way to decide which to use for your particular use case. You always have to do trial and error to test the performance. However, because GRU is simpler than LSTM, GRUs will take much less time to train and are more efficient.</p>

<p>Credits:Andrew Ng </p>
","<neural-network><deep-learning><lstm><gru>","21","21","21","0","38261","321"
"22618","{
  ""id"": 22618,
  ""title"": ""Convolutional neural network overfitting. Dropout not helping""
}","<p>Ok, so after a lot of experimentation I have managed to get some results/insights.</p>
<p><strong>In the first place, everything being equal, smaller batches in the training set help a lot in order to increase the general performance</strong> of the network, as a negative side, the training process is muuuuuch slower.</p>
<p>Second point, data is important, nothing new here but as I learned while fighting this problem, more data always seems to help a bit.</p>
<p>Third point, dropout is useful in large networks with lots of data and lots of iterations, in my network I applied dropout on the final fully connected layers only, convolution layers did not get dropout applied.</p>
<p>Fourth point (and this is something I am learning over and over): neural networds take A LOT to train, even on good GPUs (I trained this network on  floydhub, which uses quite expensive NVIDIA cards), so <strong>PATIENCE is key</strong>.</p>
<p>Final conclusion: Batch sizes are more important that one might think, apparently it is easier to hit a local minimum when batches are larger.</p>
<p>The code I wrote is available as a <a href=""https://github.com/moriano/loco-learning/blob/master/cats-vs-dogs/cats-vs-dogs.ipynb"" rel=""nofollow noreferrer"">python notebook</a>, I think it is decently documented.</p>
","<deep-learning><neural-network><convolutional-neural-network><image-recognition><dropout>","21","21","21","0","38339","1137"
"28853","{
  ""id"": 28853,
  ""title"": ""IDE alternatives for R programming (RStudio, IntelliJ IDEA, Eclipse, Visual Studio)""
}","<p><strong><a href=""https://r-brain.io/"" rel=""nofollow noreferrer"">RIDE</a></strong> -   R-Brain IDE (RIDE) for R &amp; Python, Other Data Science R IDEs, Other Data Science Python IDEs. Flexible layout. Multiple language support.</p>
<p><strong><a href=""http://jupyter.org/"" rel=""nofollow noreferrer"">Jupyter notebook</a></strong> -   The Jupyter Notebook App is a server-client application that allows editing and running notebook documents via a web browser. The Jupyter Notebook App can be executed on a local desktop</p>
<p><strong><a href=""https://github.com/jupyterlab/jupyterlab"" rel=""nofollow noreferrer"">Jupyter lab</a></strong> - An extensible environment for interactive and reproducible computing, based on the Jupyter Notebook and Architecture.</p>
<p><strong>Radiant</strong> – Open-source platform-independent browser-based interface for business analytics in R, based on the Shiny package and can be run locally or on a server.</p>
<p><strong><a href=""https://www.visualstudio.com/vs/features/rtvs/"" rel=""nofollow noreferrer"">R Tools for Visual Studio (RTVS)</a></strong> -
A free, open-source extension for Visual Studio 2017, RTVS is presently supported only in Visual Studio on Windows and not Visual Studio for Mac.</p>
<p><strong><a href=""https://www.getarchitect.io/"" rel=""nofollow noreferrer"">Architect</a></strong> -  Architect is an integrated development environment (IDE) that focuses specifically on the needs of the data scientist. All data science tasks from analyzing data to writing reports can be performed in a single environment with a common logic.</p>
<p><strong><a href=""https://www.displayr.com/features/"" rel=""nofollow noreferrer"">displayr</a></strong> - Simple and powerful. Automation by menu or code. Elegant visualizations. Instant publishing.
Collaboration. Reproducibility. Auto-updating. Secure cloud platform.</p>
<p><strong><a href=""https://atom.io/packages/rbox"" rel=""nofollow noreferrer"">Rbox</a></strong> - This package is a collection of several packages to run R via Atom editor.</p>
<p><strong>Use below for more IDEs:</strong><br />
<a href=""https://rkward.kde.org/"" rel=""nofollow noreferrer"">RKWard</a> - an easy to use and easily extensible IDE/GUI for R<br />
<a href=""https://sourceforge.net/projects/tinn-r/"" rel=""nofollow noreferrer"">Tinn-R</a> - Tinn-R Editor - GUI for R Language and Environment</p>
<p><a href=""http://r.analyticflow.com/en/"" rel=""nofollow noreferrer"">R AnalyticFlow</a> - data analysis software that utilizes the R environment for statistical computing.<br />
<a href=""https://rgedit.sourceforge.net/"" rel=""nofollow noreferrer"">Rgedit</a> - a text-editor plugin.</p>
<p><a href=""https://github.com/jalvesaq/Nvim-R"" rel=""nofollow noreferrer"">Nvim-R</a> - Vim plugin for editing R code.<br />
<a href=""https://rattle.togaware.com/"" rel=""nofollow noreferrer"">Rattle</a> - A Graphical User Interface for Data Mining using R.</p>
<p><a href=""https://medium.freecodecamp.org/turning-vim-into-an-r-ide-cd9602e8c217"" rel=""nofollow noreferrer"">How to Turn Vim Into an IDE for R</a></p>
","<r><tools><rstudio><programming>","21","21","21","0","14145","336"
"22546","{
  ""id"": 22546,
  ""title"": ""Feature Transformation on Input data""
}","<p>We love the normal form</p>

<p>In most cases we try to make them act like normal. Its not classifiers point of view but its feature extraction view!</p>

<p>Which <strong>Transformation</strong>?</p>

<p>The main criterion in choosing a transformation is: what works with the data? As above examples indicate, it is important to consider as well two questions.</p>

<p>What makes physical (biological, economic, whatever) sense, for example in terms of limiting behaviour as values get very small or very large?
This question often leads to the use of logarithms.</p>

<p>Can we keep dimensions and units simple and convenient? If possible, we prefer measurement scales that are easy to think about. </p>

<p>The cube root of a volume and the square root of an area both have the dimensions of length, so far from complicating matters, such transformations may simplify them. Reciprocals usually have simple units, as mentioned earlier. Often, however, somewhat complicated units are a sacrifice that has to be made.</p>

<p><strong>When to Use What</strong>?</p>

<p>The most useful transformations in introductory data analysis are the reciprocal, logarithm, cube root, square root, and square. In what follows, even when it is not emphasised, it is supposed that transformations are used only over ranges on which they yield (finite) real numbers as results.</p>

<ul>
<li><strong>Reciprocal</strong>: The reciprocal, x to 1/x, with its sibling the negative reciprocal, x to -1/x, is a very strong transformation with a drastic effect on distribution shape. It can not be applied to zero values.  Although it can be applied to negative values, it is not useful unless all values are positive. The reciprocal of a ratio may often be interpreted as easily as the ratio itself:    Example:

<ul>
<li>population density (people per unit area) becomes area per person</li>
<li>persons per doctor becomes doctors per person </li>
<li>rates of erosion become time to erode a unit depth</li>
</ul></li>
</ul>

<p>(In practice, we might want to multiply or divide the results of taking the reciprocal by some constant, such as 1000 or 10000, to get numbers that are easy to manage, but that itself has no effect on skewness or linearity.)</p>

<p>The reciprocal reverses order among values of the same sign: largest becomes smallest, etc. The negative reciprocal preserves order among values of the same sign.</p>

<hr>

<ul>
<li><p><strong>Logarithm</strong>: The logarithm, x log<sub>10</sub> x, or x log <sub>e x or ln x, or x log 2</sub> x, is a strong    transformation with a major effect on distribution shape. It is    commonly used for reducing right skewness and is   often appropriate    for measured variables. It can not be applied to zero or negative    values. One unit on a logarithmic scale means a multiplication by the    base of logarithms being used. Exponential growth or decline.</p>

<ul>
<li>$y = a exp (bx)$</li>
</ul></li>
</ul>

<p>is made linear by
- $ln y = ln a + bx$
       so that the response variable y should be logged. (Here exp() means    raising to the power e, approximately 2.71828, that is the base of    natural logarithms).
       An aside on this exponential growth or decline equation: 
       $x = 0$, and $y = a exp(0) = a$ so that a is the amount or count when    x = 0. If a and b > 0, then y grows at a faster and faster rate (e.g.    compound interest or unchecked population growth), whereas if a > 0    and b &lt; 0, y declines at a slower and slower rate (e.g. radioactive    decay).</p>

<hr>

<ul>
<li><strong>Power functions</strong> :</li>
<li><p>$y = ax^b$ are made linear by $log y = log a + b log x$ 
so that both variables y and x should be logged. An aside on such power<br>
functions: put $x = 0$, and for $b &gt; 0$,</p></li>
<li><p>$y = ax^b = 0$    so the power function for positive b goes through the origin, which    often makes physical or biological or
economic sense. Think:    does    zero for x imply zero for y? This<br>
kind of power function is a    shape    that fits many data sets<br>
rather well.</p>

<ul>
<li>Consider ratios y = p / q where p and q are both positive in       practice.</li>
</ul></li>
<li><p>Examples are:</p>

<ul>
<li>Males / Females  </li>
<li>Dependants / Workers</li>
<li>Downstream length / Downvalley length</li>
</ul></li>
<li><p>Then y is somewhere between 0 and infinity, or in the last case, between 1 and infinity. If p = q, then y = 1. Such definitions often lead to skewed data, because there is a clear lower limit and no clear upper limit. The logarithm, however, namely</p></li>
<li><p>log y = log p / q = log p - log q,
is somewhere between -infinity and infinity and p = q means that log y = 0. Hence the logarithm of such a ratio is likely to be more symmetrically
distributed.</p></li>
</ul>

<hr>

<ul>
<li><p><strong>Cube root</strong>: The cube root, x <sup>1/3</sup>. This is a fairly strong transformation with a substantial effect on distribution shape: it is weaker than the logarithm. It is also used for reducing right skewness, and has the    advantage that it can be applied to zero and negative values.  Note that the cube root of a volume has the units of a length. It is commonly applied to rainfall data.</p>

<ul>
<li><p>Applicability to negative values requires a special note. Consider<br>
(2)(2)(2) = 8 and (-2)(-2)(-2) = -8. These examples show that the<br>
cube root of a negative number has negative sign and the same<br>
absolute value as the cube root of the equivalent positive number. A 
similar property is possessed by any other root whose power is the<br>
reciprocal of an odd positive integer (powers 1/3, 1/5, 1/7, etc.)</p></li>
<li><p>This property is a little delicate. For example, change the power just a
smidgen from 1/3, and we can no longer define the result as a product of
precisely three terms. However, the property is there to be exploited if
useful.</p></li>
</ul></li>
</ul>

<hr>

<ul>
<li><strong>Square root</strong>:The square root, x to $x^(1/2)$ = sqrt(x), is a transformation with a moderate effect on distribution shape: it is
weaker than the logarithm
   and the cube root. It is also used for reducing right skewness, and also
   has the advantage that it can be applied to zero values. Note that the
   square root of an area has the units of a length. It is commonly applied
   to counted data, especially if the values are mostly rather small.</li>
</ul>

<hr>

<ul>
<li><strong>Square</strong>: The square, x to $x^2$, has a moderate effect on distribution shape and it could be used to reduce left skewness. In<br>
practice, the main reason for   using it is to fit a response by a<br>
quadratic function $y = a + b x + cx^2$. Quadratics have a turning<br>
point, either a maximum or a minimum, although the turning point in a
function fitted to data might be far beyond the limits of the<br>
observations. The distance of a body from an origin is a quadratic if
that body is moving under constant acceleration, which gives a very<br>
clear physical justification for using a quadratic. Otherwise<br>
quadratics are typically used solely because they can mimic a<br>
relationship within the data region. Outside that region they may<br>
behave very poorly, because they take on arbitrarily large values for
extreme values of x, and unless the intercept a is constrained to be 
0, they may behave unrealistically close to the origin.

<ul>
<li>Squaring usually makes sense only if the variable concerned is zero
or positive, given that $(-x)^2$ and $x^2$ are identical.</li>
</ul></li>
</ul>

<hr>
","<machine-learning><feature-extraction><feature-scaling>","21","21","21","0","29392","321"
"12664","{
  ""id"": 12664,
  ""title"": ""SVM using scikit learn runs endlessly and never completes execution""
}","<p>SVM solves an optimization problem of quadratic order.</p>
<p>I do not have anything to add that has not been said here. I just want to post a link the sklearn page about <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"" rel=""nofollow noreferrer"">SVC</a> which clarifies what is going on:</p>
<blockquote>
<p>The implementation is based on libsvm. The fit time complexity is more
than quadratic with the number of samples which makes it hard to scale
to dataset with more than a couple of 10000 samples.</p>
</blockquote>
<p>If you do not want to use kernels, and a linear SVM suffices, there is <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html"" rel=""nofollow noreferrer"">LinearSVC</a>. You'll have to normalize your data though, in case you're not doing so already, because it applies regularization to the intercept coefficient, which is not probably what you want. It means if your data average is far from zero, it will not be able to solve it satisfactorily.</p>
<p>What you can also use is stochastic gradient descent to solve the optimization problem. Sklearn features <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"" rel=""nofollow noreferrer"">SGDClassifier</a>. You have to use <code>loss='epsilon_insensitive'</code> to have similar results to linear SVM. See the documentation. I would only use gradient descent as a last resort though because it implies much tweaking of the hyperparameters in order to avoid getting stuck in local minima. Use <code>LinearSVC</code> if you can.</p>
","<python><svm><scikit-learn>","21","21","21","0","16853","3182"
"9265","{
  ""id"": 9265,
  ""title"": ""Calculating KL Divergence in Python""
}","<p>Scipy's <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html"" rel=""nofollow noreferrer"">entropy function</a> will calculate KL divergence if feed two vectors p and q, each representing a probability distribution. If the two vectors aren't pdfs, it will normalize then first.</p>
<p>Mutual information is <a href=""https://en.wikipedia.org/wiki/Mutual_information"" rel=""nofollow noreferrer"">related to, but not the same</a> as KL Divergence.</p>
<p>&quot;This weighted mutual information is a form of weighted KL-Divergence, which is known to take negative values for some inputs, and there are examples where the weighted mutual information also takes negative values&quot;</p>
","<python><clustering><scikit-learn>","21","21","21","0","12473","2947"
"39224","{
  ""id"": 39224,
  ""title"": ""Difference between RMSProp with momentum and Adam Optimizers""
}","<p>(My answer is based mostly on <a href=""https://arxiv.org/abs/1412.6980"" rel=""noreferrer"">Adam: A Method for Stochastic Optimization</a> (the original Adam paper) and on the implementation of rmsprop with momentum in Tensorflow (which is <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc"" rel=""noreferrer"">operator() of struct ApplyRMSProp</a>), as rmsprop is unpublished - it was described in <a href=""https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf"" rel=""noreferrer"">a lecture by Geoffrey Hinton </a>.)</p>

<hr>

<h1>Some Background</h1>

<p>Adam and rmsprop with momentum are both methods (used by a gradient descent algorithm) to determine the step.</p>

<p>Let <span class=""math-container"">$\Delta x^{(t)}_j$</span> be the <span class=""math-container"">$j^{\text{th}}$</span> component of the <span class=""math-container"">$t^{\text{th}}$</span> step. Then:</p>

<ul>
<li>In Adam: <span class=""math-container"">$$\Delta x_{j}^{(t)}=-\frac{\text{learning_rate}}{\sqrt{\text{BCMA}\left(g_{j}^{2}\right)}}\cdot\text{BCMA}\left(g_{j}\right)$$</span>
while:

<ul>
<li><span class=""math-container"">$\text{learning_rate}$</span> is a hyperparameter.</li>
<li><span class=""math-container"">$\text{BCMA}$</span> is short for ""bias-corrected <a href=""https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average"" rel=""noreferrer"">(exponential) moving average</a>"" (I made up the acronym for brevity).

<ul>
<li>All of the moving averages I am going to talk about are exponential moving averages, so I would just refer to them as ""moving averages"".</li>
</ul></li>
<li><span class=""math-container"">$g_j$</span> is the <span class=""math-container"">$j^{\text{th}}$</span> component of the gradient, and so <span class=""math-container"">$\text{BCMA}\left(g_{j}\right)$</span> is a bias-corrected moving average of the <span class=""math-container"">$j^{\text{th}}$</span> components of the gradients that were calculated. Similarly, <span class=""math-container"">$\text{BCMA}\left(g_{j}^{2}\right)$</span> is a bias-corrected moving average of the squares of the <span class=""math-container"">$j^{\text{th}}$</span> components of the gradients that were calculated.</li>
<li>For each moving average, the decay factor (aka smoothing factor) is a hyperparameter.<br/>
Both the <a href=""https://arxiv.org/abs/1412.6980"" rel=""noreferrer"">Adam paper</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer"" rel=""noreferrer"">TensorFlow</a> use the following notation:

<ul>
<li><span class=""math-container"">$\beta_1$</span> is the decay factor for <span class=""math-container"">$\text{BCMA}\left(g_{j}\right)$</span></li>
<li><span class=""math-container"">$\beta_2$</span> is the decay factor for <span class=""math-container"">$\text{BCMA}\left(g^2_{j}\right)$</span></li>
</ul></li>
<li>The denominator is actually <span class=""math-container"">$\sqrt{\text{BCMA}\left(g_{j}^{2}\right)}+\epsilon$</span>, while <span class=""math-container"">$\epsilon$</span> is a small hyperparameter, but I would ignore it for simplicity.</li>
</ul></li>
<li>In rmsprop with momentum: <span class=""math-container"">$$\Delta x_{j}^{\left(t\right)}=\text{momentum_decay_factor}\cdot\Delta x_{j}^{\left(t-1\right)}-\frac{\text{learning_rate}}{\sqrt{\text{MA}\left(g_{j}^{2}\right)}}\cdot g_{j}^{\left(t\right)}$$</span>
while:

<ul>
<li><span class=""math-container"">$\text{momentum_decay_factor}$</span> is a hyperparameter, and I would assume it is in <span class=""math-container"">$(0,1)$</span> (as it usually is).<br/>
In <a href=""https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer"" rel=""noreferrer"">TensorFlow</a>, this is the <code>momentum</code> argument of <code>RMSPropOptimizer</code>.</li>
<li><span class=""math-container"">$g^{(t)}_j$</span> is the <span class=""math-container"">$j^{\text{th}}$</span> component of the gradient in the <span class=""math-container"">$t^{\text{th}}$</span> step.</li>
<li><span class=""math-container"">$\text{MA}\left(g_{j}^{2}\right)$</span> is a moving average of the squares of the <span class=""math-container"">$j^{\text{th}}$</span> components of the gradients that were calculated.<br/>
The decay factor of this moving average is a hyperparameter, and in <a href=""https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer"" rel=""noreferrer"">TensorFlow</a>, this is the <code>decay</code> argument of <code>RMSPropOptimizer</code>.</li>
</ul></li>
</ul>

<h1>High-Level Comparison</h1>

<p>Now we are finally ready to talk about the differences between the two.</p>

<p>The denominator is quite similar (except for the bias-correction, which I explain about later).<br/>
However, the momentum-like behavior that both share (Adam due to <span class=""math-container"">$\text{BCMA}\left(g_{j}\right)$</span>, and rmsprop with momentum due to explicitly taking a fraction of the previous step) is somewhat different.<br/>
E.g. this is how Sebastian Ruder describes this difference in his blog post <a href=""http://ruder.io/optimizing-gradient-descent/index.html#adam"" rel=""noreferrer"">An overview of gradient descent optimization algorithms</a>:</p>

<blockquote>
  <p>Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface [...]</p>
</blockquote>

<p>This description is based on the paper <a href=""https://arxiv.org/abs/1706.08500"" rel=""noreferrer"">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</a>, so check it out if you want to dive deeper.</p>

<p>Next, I would describe 2 simple scenarios to demonstrate the difference in the momentum-like behaviors of the methods.</p>

<p>Lastly, I would describe the difference with regard to bias-correction.</p>

<hr>

<h1>Accumulating Momentum</h1>

<p>Consider the following scenario: The gradient was constant in every step in the recent past, and <span class=""math-container"">$\Delta x_{j}^{(t-1)}=0$</span>. Also, to keep it simple, <span class=""math-container"">$g_{j}&gt;0$</span>.<br/>
I.e. we can imagine our algorithm as a stationary ball on a linear slope.<br/>
What would happen when we use each of the methods?</p>

<p><a href=""https://i.stack.imgur.com/fX5fk.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fX5fk.png"" alt=""scenario 1""></a></p>

<h3>In Adam</h3>

<p>The gradient was constant in the recent past, so <span class=""math-container"">$\text{BCMA}\left(g_{j}^{2}\right)\approx g_{j}^{2}$</span> and <span class=""math-container"">$\text{BCMA}\left(g_{j}\right)\approx g_j$</span>.<br/>
Thus we get:
<span class=""math-container"">$$\begin{gathered}\\
\Delta x_{j}^{(t)}=-\frac{\text{learning_rate}}{\sqrt{g_{j}^{2}}}\cdot g_{j}=-\frac{\text{learning_rate}}{|g_{j}|}\cdot g_{j}\\
\downarrow\\
\Delta x_{j}^{\left(t\right)}=-\text{learning_rate}
\end{gathered}
$$</span></p>

<p>I.e. the ""ball"" immediately starts moving downhill in a constant speed.</p>

<h3>In rmsprop with momentum</h3>

<p>Similarly, we get:
<span class=""math-container"">$$\Delta x_{j}^{\left(t\right)}=\text{momentum_decay_factor}\cdot\Delta x_{j}^{\left(t-1\right)}-\text{learning_rate}$$</span>
This case is a little more complicated, but we can see that:
<span class=""math-container"">$$\begin{gathered}\\
\Delta x_{j}^{\left(t\right)}=-\text{learning_rate}\\
\Delta x_{j}^{\left(t+1\right)}=-\text{learning_rate}\cdot(1+\text{momentum_decay_factor})
\end{gathered}
$$</span>
So the ""ball"" starts accelerating downhill.<br/>
Given that the gradient stays constant, you can prove that if:
<span class=""math-container"">$$-\frac{\text{learning_rate}}{1-\text{momentum_decay_factor}}&lt;\Delta x_{j}^{\left(k\right)}$$</span>
then: <span class=""math-container"">$$-\frac{\text{learning_rate}}{1-\text{momentum_decay_factor}}&lt;\Delta x_{j}^{\left(k+1\right)}&lt;\Delta x_{j}^{\left(k\right)}$$</span>
Therefore, we conclude that the step converges, i.e. <span class=""math-container"">$\Delta x_{j}^{\left(k\right)}\approx \Delta x_{j}^{\left(k-1\right)}$</span> for some <span class=""math-container"">$k&gt;t$</span>, and then:
<span class=""math-container"">$$\begin{gathered}\\
\Delta x_{j}^{\left(k\right)}\approx \text{momentum_decay_factor}\cdot\Delta x_{j}^{\left(k\right)}-\text{learning_rate}\\
\downarrow\\
\Delta x_{j}^{\left(k\right)}\approx -\frac{\text{learning_rate}}{1-\text{momentum_decay_factor}}
\end{gathered}
$$</span>
Thus, the ""ball"" accelerates downhill and approaches a speed <span class=""math-container"">$\frac{1}{1-\text{momentum_decay_factor}}$</span> times as large as the constant speed of Adam's ""ball"". (E.g. for a typical <span class=""math-container"">$\text{momentum_decay_factor}=0.9$</span>, it can approach <span class=""math-container"">$10\times$</span> speed!)</p>

<h1>Changing Direction</h1>

<p>Now, consider a scenario following the previous one:<br/>
After going down the slope (in the previous scenario) for quite some time (i.e. enough time for rmsprop with momentum to reach a nearly constant step size), suddenly a slope with an opposite and smaller constant gradient is reached.<br/>
What would happen when we use each of the methods?<br/></p>

<p><a href=""https://i.stack.imgur.com/hoTiA.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hoTiA.png"" alt=""scenario 2""></a></p>

<p>This time I would just describe the results of my simulation of the scenario (my Python code is at the end of the answer).<br/>
Note that I have chosen for Adam's <span class=""math-container"">$\text{BCMA}\left(g_{j}\right)$</span> a decay factor equal to <span class=""math-container"">$\text{momentum_decay_factor}$</span>. Choosing differently would have changed the following results:</p>

<ul>
<li>Adam is slower to change its direction, and then much slower to get back to the minimum.</li>
<li>However, rmsprop with momentum reaches much further before it changes direction (when both use the same <span class=""math-container"">$\text{learning_rate}$</span>).<br/>
Note that this further reach is because rmsprop with momentum first reaches the opposite slope with much higher speed than Adam. If both reached the opposite slope with the same speed (which would happen if Adam's <span class=""math-container"">$\text{learning_rate}$</span> were <span class=""math-container"">$\frac{1}{1-\text{momentum_decay_factor}}$</span> times as large as that of rmsprop with momentum), then Adam would reach further before changing direction.</li>
</ul>

<h1>Bias-Correction</h1>

<p>What do we mean by a biased/bias-corrected moving average? (Or at least, what does the <a href=""https://arxiv.org/abs/1412.6980"" rel=""noreferrer"">Adam paper</a> mean by that?)</p>

<p>Generally speaking, a moving average is a weighted average of:</p>

<ul>
<li>The moving average of all of the previous terms</li>
<li>The current term</li>
</ul>

<p>Then what is the moving average in the first step?</p>

<ul>
<li>A natural choice for a programmer would be to initialize the ""moving average of all of the previous terms"" to <span class=""math-container"">$0$</span>. We say that in this case the moving average is biased towards <span class=""math-container"">$0$</span>.</li>
<li>When you only have one term, <a href=""https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average"" rel=""noreferrer"">by definition</a> the average should be equal to that term.<br/>
Thus, we say that the moving average is bias-corrected in case the moving average in the first step is the first term (and the moving average works as usual for the rest of the steps).</li>
</ul>

<p>So here is another difference: The moving averages in Adam are bias-corrected, while the moving average in rmsprop with momentum is biased towards <span class=""math-container"">$0$</span>.</p>

<p>For more about the bias-correction in Adam, see section 3 in <a href=""https://arxiv.org/abs/1412.6980"" rel=""noreferrer"">the paper</a> and also <a href=""https://stats.stackexchange.com/a/234686/215801"">this answer</a>.</p>

<h1>Simulation Python Code</h1>



<pre><code>import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

###########################################
# script parameters
def f(x):
    if x &gt; 0:
        return x
    else:
        return -0.1 * x

def f_grad(x):
    if x &gt; 0:
        return 1
    else:
        return -0.1

METHOD_TO_LEARNING_RATE = {
    'Adam': 0.01,
    'GD': 0.00008,
    'rmsprop_with_Nesterov_momentum': 0.008,
    'rmsprop_with_momentum': 0.001,
    'rmsprop': 0.02,
    'momentum': 0.00008,
    'Nesterov': 0.008,
    'Adadelta': None,
    }
X0 = 2
METHOD = 'rmsprop'
METHOD = 'momentum'
METHOD = 'GD'
METHOD = 'rmsprop_with_Nesterov_momentum'
METHOD = 'Nesterov'
METHOD = 'Adadelta'
METHOD = 'rmsprop_with_momentum'
METHOD = 'Adam'
LEARNING_RATE = METHOD_TO_LEARNING_RATE[METHOD]

MOMENTUM_DECAY_FACTOR = 0.9
RMSPROP_SQUARED_GRADS_AVG_DECAY_FACTOR = 0.9
ADADELTA_DECAY_FACTOR = 0.9
RMSPROP_EPSILON = 1e-10
ADADELTA_EPSILON = 1e-6
ADAM_EPSILON = 1e-10
ADAM_SQUARED_GRADS_AVG_DECAY_FACTOR = 0.999
ADAM_GRADS_AVG_DECAY_FACTOR = 0.9

INTERVAL = 9e2
INTERVAL = 1
INTERVAL = 3e2
INTERVAL = 3e1
###########################################

def plot_func(axe, f):
    xs = np.arange(-X0 * 0.5, X0 * 1.05, abs(X0) / 100)
    vf = np.vectorize(f)
    ys = vf(xs)
    return axe.plot(xs, ys, color='grey')

def next_color(color, f):
    color[1] -= 0.01
    if color[1] &lt; 0:
        color[1] = 1
    return color[:]

def update(frame):
    global k, x, prev_step, squared_grads_decaying_avg, \
           squared_prev_steps_decaying_avg, grads_decaying_avg

    if METHOD in ('momentum', 'Nesterov', 'rmsprop_with_momentum',
                  'rmsprop_with_Nesterov_momentum'):
        step_momentum_portion = MOMENTUM_DECAY_FACTOR * prev_step
    if METHOD in ('Nesterov', 'rmsprop_with_Nesterov_momentum'):
        gradient = f_grad(x + step_momentum_portion)
    else:
        gradient = f_grad(x)

    if METHOD == 'GD':
        step = -LEARNING_RATE * gradient
    elif METHOD in ('momentum', 'Nesterov'):
        step = step_momentum_portion - LEARNING_RATE * gradient
    elif METHOD in ('rmsprop', 'rmsprop_with_momentum',
                    'rmsprop_with_Nesterov_momentum'):
        squared_grads_decaying_avg = (
            RMSPROP_SQUARED_GRADS_AVG_DECAY_FACTOR * squared_grads_decaying_avg +
            (1 - RMSPROP_SQUARED_GRADS_AVG_DECAY_FACTOR) * gradient ** 2)
        grads_rms = np.sqrt(squared_grads_decaying_avg + RMSPROP_EPSILON)
        if METHOD == 'rmsprop':
            step = -LEARNING_RATE / grads_rms * gradient
        else:
            assert(METHOD in ('rmsprop_with_momentum',
                              'rmsprop_with_Nesterov_momentum'))
            print(f'LEARNING_RATE / grads_rms * gradient: {LEARNING_RATE / grads_rms * gradient}')
            step = step_momentum_portion - LEARNING_RATE / grads_rms * gradient
    elif METHOD == 'Adadelta':
        gradient = f_grad(x)
        squared_grads_decaying_avg = (
            ADADELTA_DECAY_FACTOR * squared_grads_decaying_avg +
            (1 - ADADELTA_DECAY_FACTOR) * gradient ** 2)
        grads_rms = np.sqrt(squared_grads_decaying_avg + ADADELTA_EPSILON)
        squared_prev_steps_decaying_avg = (
            ADADELTA_DECAY_FACTOR * squared_prev_steps_decaying_avg +
            (1 - ADADELTA_DECAY_FACTOR) * prev_step ** 2)
        prev_steps_rms = np.sqrt(squared_prev_steps_decaying_avg + ADADELTA_EPSILON)
        step = - prev_steps_rms / grads_rms * gradient
    elif METHOD == 'Adam':
        squared_grads_decaying_avg = (
            ADAM_SQUARED_GRADS_AVG_DECAY_FACTOR * squared_grads_decaying_avg +
            (1 - ADAM_SQUARED_GRADS_AVG_DECAY_FACTOR) * gradient ** 2)
        unbiased_squared_grads_decaying_avg = (
            squared_grads_decaying_avg /
            (1 - ADAM_SQUARED_GRADS_AVG_DECAY_FACTOR ** (k + 1)))
        grads_decaying_avg = (
            ADAM_GRADS_AVG_DECAY_FACTOR * grads_decaying_avg +
            (1 - ADAM_GRADS_AVG_DECAY_FACTOR) * gradient)
        unbiased_grads_decaying_avg = (
            grads_decaying_avg /
            (1 - ADAM_GRADS_AVG_DECAY_FACTOR ** (k + 1)))
        step = - (LEARNING_RATE /
                  (np.sqrt(unbiased_squared_grads_decaying_avg) + ADAM_EPSILON) *
                  unbiased_grads_decaying_avg)

    x += step
    prev_step = step
    k += 1

    color = next_color(cur_color, f)

    print(f'k: {k}\n'
          f'x: {x}\n'
          f'step: {step}\n'
          f'gradient: {gradient}\n')

    k_x_marker, = k_and_x.plot(k, x, '.', color=color)
    x_y_marker, = x_and_y.plot(x, f(x), '.', color=color)

    return k_x_marker, x_y_marker

k = 0
x = X0
cur_color = [0, 1, 1]
prev_step = 0
squared_grads_decaying_avg = 0
squared_prev_steps_decaying_avg = 0
grads_decaying_avg = 0

fig, (k_and_x, x_and_y) = plt.subplots(1, 2, figsize=(9,5))
k_and_x.set_xlabel('k')
k_and_x.set_ylabel('x', rotation=0)
x_and_y.set_xlabel('x')
x_and_y.set_ylabel('y', rotation=0)
plot_func(x_and_y, f)
x_and_y.plot(x, f(x), '.', color=cur_color[:])
k_and_x.plot(k, x, '.', color=cur_color[:])
plt.tight_layout()

ani = FuncAnimation(fig, update, blit=False, repeat=False, interval=INTERVAL)
plt.show()
</code></pre>
","<tensorflow><optimization>","21","21","21","0","56981","584"
"80876","{
  ""id"": 80876,
  ""title"": ""Overfitting in Linear Regression""
}","<p>In linear regression overfitting occurs when the model is &quot;too complex&quot;. This usually happens when there are a large number of parameters compared to the number of observations. Such a model will not generalise well to new data. That is, it will perform well on training data, but poorly on test data.</p>
<p>A simple simulation can show this. Here I use R:</p>
<pre><code>&gt; set.seed(2)
&gt; N &lt;- 4
&gt; X &lt;- 1:N
&gt; Y &lt;- X + rnorm(N, 0, 1)
&gt; 
&gt; (m0 &lt;- lm(Y ~ X)) %&gt;% summary()

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  -0.2393     1.8568  -0.129    0.909
X             1.0703     0.6780   1.579    0.255

Residual standard error: 1.516 on 2 degrees of freedom
Multiple R-squared:  0.5548,    Adjusted R-squared:  0.3321 
F-statistic: 2.492 on 1 and 2 DF,  p-value: 0.2552

</code></pre>
<p>Note that we obtain a good estimate of the true value for the coefficient of X. Note the Adjusted R-squared of 0.3321 which is an indication of the model fit.</p>
<p>Now we fit a quadratic model:</p>
<pre><code>&gt; (m1 &lt;- lm(Y ~ X + I(X^2) )) %&gt;% summary()


Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  -4.9893     2.7654  -1.804    0.322
X             5.8202     2.5228   2.307    0.260
I(X^2)       -0.9500     0.4967  -1.913    0.307

Residual standard error: 0.9934 on 1 degrees of freedom
Multiple R-squared:  0.9044,    Adjusted R-squared:  0.7133 
F-statistic: 4.731 on 2 and 1 DF,  p-value: 0.3092

</code></pre>
<p>Now we have a much higher Adjusted R-squared:  0.7133  which may lead us to think that the model is much better. Indeed if we plot the data and the predicted valus from both models we get :</p>
<pre><code>&gt; fun.linear &lt;- function(x) { coef(m0)[1] + coef(m0)[2] * x  }
&gt; fun.quadratic &lt;- function(x) { coef(m1)[1] + coef(m1)[2] * x  + coef(m1)[3] * x^2}
&gt; 
&gt; ggplot(data.frame(X,Y), aes(y = Y, x = X)) + geom_point()  + stat_function(fun = fun.linear) + stat_function(fun = fun.quadratic)
</code></pre>
<p><a href=""https://i.stack.imgur.com/Asj9M.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Asj9M.png"" alt=""enter image description here"" /></a></p>
<p>So on the face of it, the quadratic model looks much better.</p>
<p>Now, if we simulate new data, but use the same model to plot the predictions, we get</p>
<pre><code>&gt; set.seed(6)
&gt; N &lt;- 4
&gt; X &lt;- 1:N
&gt; Y &lt;- X + rnorm(N, 0, 1)
&gt; ggplot(data.frame(X,Y), aes(y = Y, x = X)) + geom_point()  + stat_function(fun = fun.linear) + stat_function(fun = fun.quadratic)
</code></pre>
<p><a href=""https://i.stack.imgur.com/By0Sp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/By0Sp.png"" alt=""enter image description here"" /></a></p>
<p>Clearly the quadratic model is not doing well, whereas the linear model is still reasonable. However, if we simulate more data with an extended range, using the original seed, so that the initial data points are the same as in the first simulation we find:</p>
<pre><code>&gt; set.seed(2)
&gt; N &lt;- 10
&gt; X &lt;- 1:N
&gt; Y &lt;- X + rnorm(N, 0, 1)
&gt; ggplot(data.frame(X,Y), aes(y = Y, x = X)) + geom_point()  + stat_function(fun = fun.linear) + stat_function(fun = fun.quadratic)
</code></pre>
<p><a href=""https://i.stack.imgur.com/ClGE1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ClGE1.png"" alt=""enter image description here"" /></a></p>
<p>Clearly the linear model still performs well, but the quadratic model is hopeless outside the orriginal range. This is because when we fitted the models, we had too many parameters (3) compared to the number of observations (4).</p>
<hr />
<p>Edit: To address the query in the comments to this answer, about a model that does not include higher order terms.</p>
<p>The situation is the same: If the number of parameters approaches the number of observations, the model will be overfitted. With no higher order terms, this will occur when the number of variables / features in the model approaches the number of observations.</p>
<p>Again we can demonstrate this easily with a simulation:</p>
<p>Here we simulate random data data from a normal distribution, such that we have 7 observations and 5 variables / features:</p>
<pre><code>&gt; set.seed(1)
&gt; n.var &lt;- 5
&gt; n.obs &lt;- 7
&gt; 
&gt; dt &lt;- as.data.frame(matrix(rnorm(n.var * n.obs), ncol = n.var))
&gt; dt$Y &lt;- rnorm(nrow(dt))
&gt; 
&gt; lm(Y ~ . , dt) %&gt;% summary()

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  -0.6607     0.2337  -2.827    0.216
V1            0.6999     0.1562   4.481    0.140
V2           -0.4751     0.3068  -1.549    0.365
V3            1.2683     0.3423   3.705    0.168
V4            0.3070     0.2823   1.087    0.473
V5            1.2154     0.3687   3.297    0.187

Residual standard error: 0.2227 on 1 degrees of freedom
Multiple R-squared:  0.9771,    Adjusted R-squared:  0.8627 
</code></pre>
<p>We obtain an adjusted  R-squared of 0.86 which indicates excellent model fit. On purely random data. The model is severely overfitted. By comparison if we double the number of obervations to 14:</p>
<pre><code>&gt; set.seed(1)
&gt; n.var &lt;- 5
&gt; n.obs &lt;- 14
&gt; dt &lt;- as.data.frame(matrix(rnorm(n.var * n.obs), ncol = n.var))
&gt; dt$Y &lt;- rnorm(nrow(dt))
&gt; lm(Y ~ . , dt) %&gt;% summary()

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -0.10391    0.23512  -0.442   0.6702  
V1          -0.62357    0.32421  -1.923   0.0906 .
V2           0.39835    0.27693   1.438   0.1883  
V3          -0.02789    0.31347  -0.089   0.9313  
V4          -0.30869    0.30628  -1.008   0.3430  
V5          -0.38959    0.20767  -1.876   0.0975 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.7376 on 8 degrees of freedom
Multiple R-squared:  0.4074,    Adjusted R-squared:  0.03707 
F-statistic:   1.1 on 5 and 8 DF,  p-value: 0.4296
</code></pre>
<p>..adjusted R squared drops to just 0.037</p>
","<machine-learning><statistics><linear-regression><overfitting>","21","21","21","0","7730","563"
"22226","{
  ""id"": 22226,
  ""title"": ""What is the \""dying ReLU\"" problem in neural networks?""
}","<p>ReLU neurons output zero and have zero derivatives for all negative inputs. So, if the weights in your network always lead to negative inputs into a ReLU neuron, that neuron is effectively not contributing to the network's training. Mathematically, the gradient contribution to the weight updates coming from that neuron is always zero (see the Mathematical Appendix for some details).</p>

<p>What are the chances that your weights will end up producing negative numbers for <em>all</em> inputs into a given neuron? It's hard to answer this in general, but one way in which this can happen is when you make too large of an update to the weights. Recall that neural networks are typically trained by minimizing a loss function $L(W)$ with respect to the weights using gradient descent. That is, weights of a neural network are the ""variables"" of the function $L$ (the loss depends on the dataset, but only implicitly: it is typically the sum over each training example, and each example is effectively a constant). Since the gradient of any function always points in the direction of steepest increase, all we have to do is calculate the gradient of $L$ with respect to the weights $W$ and move in the opposite direction a little bit, then rinse and repeat. That way, we end up at a (local) minimum of $L$. Therefore, if your inputs are on roughly the same scale, a large step in the direction of the gradient can leave you with weights that give similar inputs which can end up being negative.</p>

<p>In general, what happens depends on how information flows through the network. You can imagine that as training goes on, the values neurons produce can drift around and make it possible for the weights to kill all data flow through some of them. (Sometimes, they may leave these unfavorable configurations due to weight updates earlier in the network, though!). I explored this idea in a <a href=""https://intoli.com/blog/neural-network-initialization/"" rel=""noreferrer"">blog post about weight initialization</a> -- which can also contribute to this problem -- and its relation to data flow. I think my point here can be illustrated by a plot from that article:</p>

<p><img src=""https://intoli.com/blog/neural-network-initialization/img/relu-output-progression-violinplot.png"" alt=""Activations in a ReLU MLP with different initialization strategies""></p>

<p>The plot displays activations in a 5 layer Multi-Layer Perceptron with ReLU activations after one pass through the network with different initialization strategies. You can see that depending on the weight configuration, the outputs of your network can be choked off.</p>

<h1>Mathematical Appendix</h1>

<p>Mathematically if $L$ is your network's loss function, $x_j^{(i)}$ is the output of the $j$-th neuron in the $i$-th layer, $f(s) = \max(0, s)$ is the ReLU neuron, and $s^{(i)}_j$ is the linear input into the $(i+1)$-st layer, then by the chain rule the derivative of the loss with respect to a weight connecting the $i$-th and $(i+1)$-st layers is</p>

<p>$$
\frac{\partial L}{\partial w_{jk}^{(i)}} = \frac{\partial L}{\partial x_k^{(i+1)}} \frac{\partial x_k^{(i+1)}}{\partial w_{jk}^{(i)}}\,.
$$</p>

<p>The first term on the right can be computed recursively. The second term on the right is the only place directly involving the weight $w_{jk}^{(i)}$ and can be broken down into</p>

<p>$$
\begin{align*}
\frac{\partial{x_k^{(i+1)}}}{\partial w_{jk}^{(i)}} &amp;= \frac{\partial{f(s^{(i)}_j)}}{\partial s_j^{(i)}} \frac{\partial s_j^{(i)}}{\partial w_{jk}^{(i)}} \\
&amp;=f'(s^{(i)}_j)\, x_j^{(i)}.
\end{align*}
$$</p>

<p>From this you can see that if the outputs are always negative, the weights leading into the neuron are not updated, and the neuron does not contribute to learning.</p>
","<machine-learning><neural-network><deep-learning>","21","21","21","0","37899","311"
"9824","{
  ""id"": 9824,
  ""title"": ""Is there any domain where Bayesian Networks outperform neural networks?""
}","<p>Bayesian networks and neural networks are not exclusive of each other. In fact, Bayesian networks are just another term for ""directed graphical model"". They can be very useful in designing objective functions neural networks. Yann Lecun has pointed this out here: <a href=""https://plus.google.com/+YannLeCunPhD/posts/gWE7Jca3Zoq"">https://plus.google.com/+YannLeCunPhD/posts/gWE7Jca3Zoq</a>.</p>

<p>One example.</p>

<p>The variational auto encoder and derivatives are directed graphical models of the form $$p(x) = \int_z p(x|z)p(z) dz.$$ A neural networks is used to implemented $p(x|z)$ and an approximation to its inverse: $q(z|x) \approx p(z|x)$.</p>
","<machine-learning><pgm>","21","21","21","0","1193","321"
"12859","{
  ""id"": 12859,
  ""title"": ""How do you visualize neural network architectures?""
}","<p>In Caffe you can use  <a href=""https://github.com/BVLC/caffe/blob/master/python/caffe/draw.py"" rel=""nofollow noreferrer"">caffe/draw.py</a> to draw the NetParameter protobuffer:</p>
<p><a href=""https://i.stack.imgur.com/5Z1Cb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5Z1Cb.png"" alt=""enter image description here"" /></a></p>
<p>In Matlab, you can use  <a href=""http://www.mathworks.com/help/nnet/ref/view.html"" rel=""nofollow noreferrer"">view(net)</a></p>
<p><a href=""https://i.stack.imgur.com/rPpfa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rPpfa.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://transcranial.github.io/keras-js/#/inception-v3"" rel=""nofollow noreferrer"">Keras.js</a>:</p>
<p><a href=""https://i.stack.imgur.com/vEfTv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vEfTv.png"" alt=""enter image description here"" /></a></p>
<p>Also, see <a href=""https://redd.it/kmfx2d"" rel=""nofollow noreferrer"">Can anyone recommend a Network Architecture visualization tool? (Reddit/self.MachineLearning)</a>.</p>
","<machine-learning><neural-network><deep-learning><visualization>","21","21","21","0","843","5095"
"29598","{
  ""id"": 29598,
  ""title"": ""How do you visualize neural network architectures?""
}","<p>I would add ASCII visualizations using <a href=""https://github.com/stared/keras-sequential-ascii"" rel=""noreferrer"">keras-sequential-ascii</a> (disclaimer: I am the author).</p>

<p>A small network for CIFAR-10 (from <a href=""https://blog.deepsense.ai/deep-learning-hands-on-image-classification/"" rel=""noreferrer"">this tutorial</a>) would be:</p>

<pre><code>       OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)

           Input   #####     32   32    3
          Conv2D    \|/  -------------------       896     2.1%
            relu   #####     30   30   32
    MaxPooling2D   Y max -------------------         0     0.0%
                   #####     15   15   32
          Conv2D    \|/  -------------------     18496    43.6%
            relu   #####     13   13   64
    MaxPooling2D   Y max -------------------         0     0.0%
                   #####      6    6   64
         Flatten   ||||| -------------------         0     0.0%
                   #####        2304
           Dense   XXXXX -------------------     23050    54.3%
         softmax   #####          10
</code></pre>

<p>For VGG16 it would be:</p>

<pre><code>       OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)

          Input   #####      3  224  224
     InputLayer     |   -------------------         0     0.0%
                  #####      3  224  224
  Convolution2D    \|/  -------------------      1792     0.0%
           relu   #####     64  224  224
  Convolution2D    \|/  -------------------     36928     0.0%
           relu   #####     64  224  224
   MaxPooling2D   Y max -------------------         0     0.0%
                  #####     64  112  112
  Convolution2D    \|/  -------------------     73856     0.1%
           relu   #####    128  112  112
  Convolution2D    \|/  -------------------    147584     0.1%
           relu   #####    128  112  112
   MaxPooling2D   Y max -------------------         0     0.0%
                  #####    128   56   56
  Convolution2D    \|/  -------------------    295168     0.2%
           relu   #####    256   56   56
  Convolution2D    \|/  -------------------    590080     0.4%
           relu   #####    256   56   56
  Convolution2D    \|/  -------------------    590080     0.4%
           relu   #####    256   56   56
   MaxPooling2D   Y max -------------------         0     0.0%
                  #####    256   28   28
  Convolution2D    \|/  -------------------   1180160     0.9%
           relu   #####    512   28   28
  Convolution2D    \|/  -------------------   2359808     1.7%
           relu   #####    512   28   28
  Convolution2D    \|/  -------------------   2359808     1.7%
           relu   #####    512   28   28
   MaxPooling2D   Y max -------------------         0     0.0%
                  #####    512   14   14
  Convolution2D    \|/  -------------------   2359808     1.7%
           relu   #####    512   14   14
  Convolution2D    \|/  -------------------   2359808     1.7%
           relu   #####    512   14   14
  Convolution2D    \|/  -------------------   2359808     1.7%
           relu   #####    512   14   14
   MaxPooling2D   Y max -------------------         0     0.0%
                  #####    512    7    7
        Flatten   ||||| -------------------         0     0.0%
                  #####       25088
          Dense   XXXXX ------------------- 102764544    74.3%
           relu   #####        4096
          Dense   XXXXX -------------------  16781312    12.1%
           relu   #####        4096
          Dense   XXXXX -------------------   4097000     3.0%
        softmax   #####        1000
</code></pre>
","<machine-learning><neural-network><deep-learning><visualization>","21","21","21","0","289","746"
"14646","{
  ""id"": 14646,
  ""title"": ""Convert a pandas column of int to timestamp datatype""
}","<p>You can specify the unit of a pandas <code>to_datetime</code> call.</p>

<p>Stolen from <a href=""https://stackoverflow.com/questions/19231871/convert-unix-time-to-readable-date-in-pandas-dataframe"">here</a>:</p>

<pre><code># assuming `df` is your data frame and `date` is your column of timestamps

df['date'] = pandas.to_datetime(df['date'], unit='s')
</code></pre>

<p>Should work with integer datatypes, which makes sense if the unit is seconds since the epoch.</p>
","<python><time-series><data-cleaning><pandas>","21","21","21","0","24895","957"
"1109","{
  ""id"": 1109,
  ""title"": ""Quick guide into training highly imbalanced data sets""
}","<p>Undersampling the majority class is usually the way to go in such situations.</p>

<p>If you think that you have too few instances of the positive class, you may perform oversampling, for example, sample 5n instances with replacement from the dataset of size n.</p>

<p>Caveats:</p>

<ul>
<li>Some methods may be sensitive to changes in the class distribution, e.g. for Naive Bayes - it affects the prior probabilities. </li>
<li>Oversampling may lead to overfitting  </li>
</ul>
","<machine-learning><classification><dataset><class-imbalance>","20","20","20","0","816","2610"
"23677","{
  ""id"": 23677,
  ""title"": ""How many features to sample using Random Forests""
}","<p>I think in the original paper they suggest using $\log_2(N +1$), but either way the idea is the following:</p>

<p>The number of randomly selected features can influence the generalization error in two ways: selecting many features increases the strength of the individual trees whereas reducing the number of features leads to a lower correlation among the trees increasing the strength of the forest as a whole. </p>

<p>What's interesting is that the authors of <a href=""https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf"" rel=""noreferrer"">Random Forests</a> (pdf) find an empirical difference between classification and regression:</p>

<blockquote>
  <p>An interesting difference between regression and classification is that the
  correlation increases quite slowly as the number of features used increases.</p>
</blockquote>

<p>Therefore, for regression often $N/3$ is recommended, which gives larger values than $\sqrt N$. </p>

<p>In general, there is no clear justification for $\sqrt N$ or $\log N$ for classification problems other than that it has shown that lower correlation among trees can decrease generalization error enough to more than offset the decrease in strength of individual trees. In particular, the authors note that the range where this trade-off can decrease the generalization error is quite large:</p>

<blockquote>
  <p><strong>The in-between
  range is usually large. In this range, as the number of features goes up, the
  correlation increases, but PE*(tree) compensates by decreasing.</strong></p>
</blockquote>

<p>(PE* being the generalization error)</p>

<p>As they say in Elements of Statistical Learning:</p>

<blockquote>
  <p>In practice the best values for these parameters will depend on the problem,
  and they should be treated as tuning parameters.</p>
</blockquote>

<p>One thing your problem can depend on is the number of categorical variables. If you have many categorical variables that are encoded as dummy-variables it usually makes sense to increase the parameter. Again, from the Random Forests paper:</p>

<blockquote>
  <p>When many of the variables are categorical, using a
  low [number of features] results in low correlation, but also low strength. [The number of features] must be
  increased to about two-three times $int(log_2M+1)$ to get enough strength to
  provide good test set accuracy.</p>
</blockquote>
","<statistics><random-forest><optimization><evaluation><sampling>","20","20","20","0","23305","5682"
"18485","{
  ""id"": 18485,
  ""title"": ""When should I use Gini Impurity as opposed to Information Gain (Entropy)?""
}","<p>For the case of a variable with two values, appearing with fractions f and (1-f), <br>
the gini and entropy are given by: <br>
gini = 2*f(1-f) <br>
entropy = f*ln(1/f) + (1-f)*ln(1/(1-f)) <br>
These measures are very similar if scaled to 1.0
(plotting 2*gini and entropy/ln(2) ):</p>

<p><a href=""https://i.stack.imgur.com/ue6OB.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ue6OB.jpg"" alt=""Gini (y4,purple) and Entropy (y3,green) values scaled for comparison""></a></p>
","<machine-learning><decision-trees><information-theory>","20","20","20","0","31384","201"
"15731","{
  ""id"": 15731,
  ""title"": ""When should I use Gini Impurity as opposed to Information Gain (Entropy)?""
}","<blockquote>
  <p>Gini is intended for continuous attributes and Entropy is for attributes that occur in classes  </p>
</blockquote>

<p><strong>Gini</strong> is to minimize misclassification<br>
<strong>Entropy</strong> is for exploratory analysis  </p>

<blockquote>
  <p>Entropy is a little slower to compute</p>
</blockquote>
","<machine-learning><decision-trees><information-theory>","20","20","20","0","14738","303"
"49594","{
  ""id"": 49594,
  ""title"": ""Early stopping on validation loss or on accuracy?""
}","<p><strong>TLDR; Monitor the loss rather than the accuracy</strong></p>

<p>I will answer my own question since I think that the answers received missed the point and someone might have the same problem one day.</p>

<p>First, let me quickly clarify that using early stopping is perfectly normal when training neural networks (see the relevant sections in Goodfellow et al's Deep Learning book, most DL papers, and the documentation for keras' EarlyStopping callback).</p>

<p>Now, regarding the quantity to monitor: prefer the loss to the accuracy. Why? 
The loss quantify how certain the model is about a prediction (basically having a value close to 1 in the right class and close to 0 in the other classes). The accuracy merely account for the number of correct predictions. Similarly, any metrics using hard predictions rather than probabilities have the same problem.</p>

<p>Obviously, whatever metrics you end up choosing, it has to be calculated on a validation set and not a training set (otherwise, you are completely missing the point of using EarlyStopping in the first place)</p>
","<machine-learning><neural-network><deep-learning><classification>","20","20","20","0","50534","914"
"39043","{
  ""id"": 39043,
  ""title"": ""How to use LeakyRelu as activation function in sequence DNN in keras?When it perfoms better than Relu?""
}","<p>You can use the LeakyRelu <strong>layer</strong>, as in the python class, instead of just specifying the string name like in your example. It works similarly to a normal layer. </p>

<p>Import the LeakyReLU and instantiate a model </p>

<pre><code>from keras.layers import LeakyReLU
model = Sequential()

# here change your line to leave out an activation 
model.add(Dense(90))

# now add a ReLU layer explicitly:
model.add(LeakyReLU(alpha=0.05))
</code></pre>

<hr>

<p>Being able to simply write e.g. <code>activation='relu'</code> is made possible because of simple aliases that are created in the source code. </p>

<hr>

<p>For your second question:</p>

<blockquote>
  <p>what are the best general setting for tuning the parameters of LeakyRelu? And when its performance is significantly better than Relu?</p>
</blockquote>

<p>I can't give you optimal settings for the LeakyReLU, I'm afraid - they will be model/data dependent.</p>

<p>The difference between the ReLU and the LeakyReLU is the ability of the latter to retain some degree of the negative values that flow into it, whilst the former simply sets all values less than 0 to be 0.  In theory, this extended output range offers a slightly higher flexibility to the model using it. I'm sure the inventors thought it to be useful and perhaps proved that to be the case for a few benchmarks. In practice, however, people generally just stick to the ReLU, as the benefits of the LeakyReLU are not consistent and the ReLU is cheaper to compute and therefore models train slightly faster. </p>
","<python><deep-learning><keras><tensorflow><activation-function>","20","20","20","0","45264","13123"
"6170","{
  ""id"": 6170,
  ""title"": ""IDE alternatives for R programming (RStudio, IntelliJ IDEA, Eclipse, Visual Studio)""
}","<p><strong>IntelliJ</strong> supports R via this plugin:</p>

<ul>
<li><a href=""https://plugins.jetbrains.com/plugin/6632"" rel=""noreferrer"">https://plugins.jetbrains.com/plugin/6632</a></li>
</ul>

<p>It's a recent project, so RStudio is still more powerful, including its focus on data-friendly environment (plots and data are always in sight).</p>
","<r><tools><rstudio><programming>","20","20","20","0","5279","651"
"68577","{
  ""id"": 68577,
  ""title"": ""What is the bleu score of professional human translators?""
}","<p>The original paper <a href=""https://www.aclweb.org/anthology/P02-1040.pdf"" rel=""noreferrer"">""BLEU: a Method for Automatic Evaluation of Machine Translation""</a> contains a couple of numbers on this:</p>

<blockquote>
  <p>The BLEU metric ranges from 0 to 1. Few translations will attain a score of 1 unless they are identical to a reference translation. For this reason, even
  a human translator will not necessarily score 1. It
  is important to note that the more reference translations per sentence there are, the higher the score is. Thus, one must be cautious making even “rough”
  comparisons on evaluations with different numbers
  of reference translations: on a test corpus of about
  500 sentences (40 general news stories), <strong>a human
  translator scored 0.3468 against four references and
  scored 0.2571 against two references</strong>.</p>
</blockquote>

<p>But as their table 1 (providing the numbers compared to two references, H2 is the one mentioned in the text above) shows there is variance among human BLEU scores:</p>

<p><a href=""https://i.stack.imgur.com/LoI7e.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/LoI7e.jpg"" alt=""enter image description here""></a></p>

<p>Unfortunately, the paper does not qualify the skill level of the translators.</p>
","<nlp><machine-translation>","20","20","20","0","84891","4397"
"259","{
  ""id"": 259,
  ""title"": ""Publicly Available Datasets""
}","<p>I would like to point to <a href=""http://national.census.okfn.org/"">The Open Data Census</a>. It is an initiative of the Open Knowledge Foundation based on contributions from open data advocates and experts around the world. </p>

<p>The value of Open data Census is open, community driven, and systematic effort to collect and update the database of open datasets globally on country and, in some cases, <a href=""http://us-city.census.okfn.org/"">like U.S., on city level</a>. </p>

<p>Also, it presents an opportunity to compare different countries and cities on in selected areas of interest.</p>
","<open-source><dataset>","20","20","20","0","454","313"
"45172","{
  ""id"": 45172,
  ""title"": ""How to get accuracy, F1, precision and recall, for a keras model?""
}","<p>You could use the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"" rel=""noreferrer"">scikit-learn classification report</a>. To convert your labels into a numerical or binary format take a look at the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""noreferrer"">scikit-learn label encoder</a>.</p>

<pre><code>from sklearn.metrics import classification_report

y_pred = model.predict(x_test, batch_size=64, verbose=1)
y_pred_bool = np.argmax(y_pred, axis=1)

print(classification_report(y_test, y_pred_bool))
</code></pre>

<p>which gives you (output copied from the scikit-learn example):</p>

<pre><code>             precision  recall   f1-score    support

 class 0       0.50      1.00      0.67         1
 class 1       0.00      0.00      0.00         1
 class 2       1.00      0.67      0.80         3
</code></pre>
","<machine-learning><neural-network><deep-learning><classification><keras>","20","20","20","0","67106","301"
"202","{
  ""id"": 202,
  ""title"": ""What does the alpha and beta hyperparameters contribute to in Latent Dirichlet allocation?""
}","<p>The Dirichlet distribution is a multivariate distribution. We can denote the parameters of the Dirichlet as a vector of size K of the form ~$\frac{1}{B(a)} \cdot \prod\limits_{i} x_i^{a_{i-1}}$, where $a$ is the vector of size $K$ of the parameters, and $\sum x_i = 1$.</p>

<p>Now the LDA uses some constructs like:</p>

<ul>
<li>a document can have multiple topics (because of this multiplicity, we need the Dirichlet distribution); and there is a Dirichlet distribution which models this relation</li>
<li>words can also belong to multiple topics, when you consider them outside of a document; so here we need another Dirichlet to model this</li>
</ul>

<p>The previous two are distributions which you do not really see from data, this is why is called latent, or hidden.</p>

<p>Now, in Bayesian inference you use the Bayes rule to infer the posterior probability. For simplicity, let's say you have data $x$ and you have a model for this data governed by some parameters $\theta$. In order to infer values for this parameters, in full Bayesian inference you will infer the posterior probability of these parameters using Bayes' rule with $$p(\theta|x) = \frac{p(x|\theta)p(\theta|\alpha)}{p(x|\alpha)} \iff \text{posterior probability} = \frac{\text{likelihood}\times \text{prior probability}}{\text{marginal likelihood}}$$ Note that here comes an $\alpha$. This is your initial belief about this distribution, and is the parameter of the prior distribution. Usually this is chosen in such a way that will have a conjugate prior (so the distribution of the posterior is the same with the distribution of the prior) and often to encode some knowledge if you have one or to have maximum entropy if you know nothing.</p>

<p>The parameters of the prior are called <em>hyperparameters</em>. So, in LDA, both topic distributions, over documents and over words have also correspondent priors, which are denoted usually with alpha and beta, and because are the parameters of the prior distributions are called hyperparameters. </p>

<p>Now about choosing priors. If you plot some Dirichlet distributions you will note that if the individual parameters $\alpha_k$ have the same value, the pdf is symmetric in the simplex defined by the $x$ values, which is the minimum or maximum for pdf is at the center. </p>

<p>If all the $\alpha_k$ have values lower than unit the maximum is found at corners</p>

<p><img src=""https://i.stack.imgur.com/5khZE.png"" width=""200"" height=""200""></p>

<p>or can if all values $\alpha_k$ are the same and greater than 1 the maximum will be found in center like </p>

<p><img src=""https://i.stack.imgur.com/qrfm7.png"" width=""200"" height=""200""></p>

<p>It is easy to see that if values for $\alpha_k$ are not equal the symmetry is broken and the maximum will be found near bigger values. </p>

<p>Additional, please note that values for priors parameters produce smooth pdfs of the distribution as the values of the parameters are near 1. So if you have great confidence that something is clearly distributed in a way you know, with a high degree of confidence, than values far from 1 in absolute value are to be used, if you do not have such kind of knowledge than values near 1 would be encode this lack of knowledge. It is easy to see why 1 plays such a role in Dirichlet distribution from the formula of the distribution itself.</p>

<p>Another way to understand this is to see that prior encode prior-knowledge. In the same time you might think that prior encode some prior seen data. This data was not saw by the algorithm itself, it was saw by you, you learned something, and you can model prior according to what you know (learned). So in the prior parameters (hyperparameters) you encode also how big this data set you apriori saw, because the sum of $\alpha_k$ can be that also as the size of this more or less imaginary data set. So the bigger the prior data set, the bigger is the confidence, the bigger the values of $\alpha_k$ you can choose, the sharper the surface near maximum value, which means also less doubts. </p>

<p>Hope it helped.</p>
","<topic-model><lda><parameter>","20","20","20","0","108","4044"
"6790","{
  ""id"": 6790,
  ""title"": ""Are decision tree algorithms linear or nonlinear""
}","<p>A decision tree is a non-linear mapping of <code>X</code> to <code>y</code>. This is easy to see if you take an arbitrary function and create a tree to its maximum depth.</p>

<p>For example:</p>

<pre><code>if x = 1, y = 1
if x = 2, y = 15
if x = 3, y = 3
if x = 4, y = 27
...
</code></pre>

<p>Of course, this is a completely over-fit tree and won't generalize. But it demonstrates why a decision tree is a non-linear mapping.</p>
","<machine-learning><classification><decision-trees><algorithms><pac-learning>","20","20","20","0","10816","341"
"33054","{
  ""id"": 33054,
  ""title"": ""How do I compare columns in different data frames?""
}","<pre><code>df1.where(df1.values==df2.values).notna()
</code></pre>

<p><code>True</code> entries show common elements. This also reveals the position of the common elements, unlike the solution with <code>merge</code>.</p>
","<pandas><dataframe>","20","22","21","1","52089","3515"
"22722","{
  ""id"": 22722,
  ""title"": ""Latent Dirichlet Allocation vs Hierarchical Dirichlet Process""
}","<p>I wanted to point out, since this is one of the top Google hits for this topic, that Latent Dirichlet Allocation (LDA), Hierarchical Dirichlet Processes (HDP), <em>and</em> hierarchical Latent Dirichlet Allocation (hLDA) are all distinct models.</p>

<p>LDA models documents as dirichlet mixtures of a fixed number of topics- chosen as a parameter of the model by the user- which are in turn dirichlet mixtures of words. This generates a flat, soft probabilistic clustering of terms into topics and documents into topics. </p>

<p>HDP models topics as mixtures of words, much like LDA, but rather than documents being mixtures of a fixed number of topics, the number of topics is generated by a dirichlet process, resulting in the number of topics being a random variable as well. The ""hierarchical"" portion of the name refers to another level being added to the generative model (the dirichlet process producing the number of topics), not the topics themselves- the topics are still flat clusterings.</p>

<p>hLDA, on the other hand, is an adaptation of LDA that models topics as mixtures of a new, distinct level of topics, drawn from dirichlet <em>distributions</em> and not processes. It still treats the number of topics as a hyperparameter, i.e., independent of the data. The difference is that the clustering is now hierarchical- it learns a clustering of the first set of topics themselves, giving a more general, abstract relationships between topics (and hence, words and documents). Think of it like clustering the stack exchanges into math, science, programming, history, etc. as opposed to clustering data science and cross validation into an abstract statistics and programming topic that shares some concepts with, say, software engineering, but the software engineering exchange is clustered on a more concrete level with the computer science exchange, and the similarity between all of the mentioned exchanges doesn't appear as much until the upper layer of clusters.</p>
","<nlp><topic-model><lda>","20","20","20","0","38663","201"
"69998","{
  ""id"": 69998,
  ""title"": ""How can I appropriately handle cleaning of gender data?""
}","<p>There are at least two general considerations to make:</p>

<p><strong>Domain-related</strong></p>

<p>If an attribute potentially has predictive power in your domain and more specifically for your task your models might benefit from a direct encoding. For example: if being trans is correlated with different psychological disorders then I'd include a direct feature for this. This way it is easier for your model to make a prediction since it does not need to combine two features in the first place (e.g. no need to combine ""sex at birth"" and ""gender identification"" to identify a transsexual person (which would not even accurate since ""trans"" is a much broader term than just <code>sex at birth != gender identity</code>)). </p>

<p>Moreover, I'd apply the same thinking to other feature engineering questions. Sex has predictive power for many tasks related to mental disorders, e.g. because mood disorders are more common among women and anti-social personality disorders are more common among men. However, whether these are rather related to the sex at birth or the gender a person identifies with is another question. So if your hypothesis is that in your task the gender a person identifies with is important then, again, it makes sense to include this in addition to the sex at birth.</p>

<p><strong>Model-related</strong></p>

<p>Different models are able to handle predictors differently. For example, tree-based models can more easily work with two separate attributes <code>sex == female</code> and <code>trans == True</code> to implicitly derive <code>trans female == True</code>. However, linear models like neural networks might benefit from having a combined binary feature <code>female trans</code>.</p>
","<machine-learning><data-cleaning><categorical-data>","20","20","20","0","84891","4397"
"767","{
  ""id"": 767,
  ""title"": ""Tools and protocol for reproducible data science using Python""
}","<p>The best reproducibility tool is to make a log of your actions, something like this:</p>

<pre><code>experiment/input ; expected ; observation/output ; current hypothesis and if supported or rejected
exp1 ; expected1 ; obs1 ; some fancy hypothesis, supported
</code></pre>

<p>This can be written down on a paper, but, if your experiments fit in a computational framework, you can use computational tools to partly or completely automate that logging process (particularly by helping you track the input datasets which can be huge, and the output figures).</p>

<p>A great reproducibility tool for Python with a low learning curve is of course <a href=""http://ipython.org/"" rel=""nofollow noreferrer""><strong>IPython/Jupyter Notebook</strong></a> (don't forget the <a href=""https://damontallen.github.io/IPython-quick-ref-sheets/"" rel=""nofollow noreferrer"">%logon and %logstart</a> magics). Tip: to make sure your notebook is reproducible, restart the kernel and try to run all cells from top to bottom (button Run All Cells): if it works, then save everything in an archive file (""freezing""), else, notably if you need to run cells in a non linear and non sequential and <strong>non obvious</strong> fashion to avoid errors, you need to rework a bit.</p>

<p>Another great tool that is very recent (2015) is <a href=""https://github.com/recipy/recipy"" rel=""nofollow noreferrer"">recipy</a>, which is very like sumatra (see below), but made specifically for Python. I don't know if it works with Jupyter Notebooks, but I know the author frequently uses them so I guess that if it's not currently supported, it will be in the future.</p>

<p><a href=""https://en.wikipedia.org/wiki/Git_(software)"" rel=""nofollow noreferrer"">Git</a> is also awesome, and it's not tied to Python. It will help you not only to keep a history of all your experiments, code, datasets, figures, etc. but also provide you with tools to maintain (<a href=""http://www.philandstuff.com/2014/02/09/git-pickaxe.html"" rel=""nofollow noreferrer"">git pickaxe</a>), collaborate (<a href=""http://jfire.io/blog/2012/03/07/code-archaeology-with-git/"" rel=""nofollow noreferrer"">blame</a>) and debug (<a href=""http://blogs.perl.org/users/ovid/2014/07/making-git-bisect-more-useful.html"" rel=""nofollow noreferrer"">git</a>-<a href=""https://lwn.net/Articles/317154/"" rel=""nofollow noreferrer"">bisect</a>) using a scientific method of debugging (called <a href=""https://en.wikipedia.org/wiki/Delta_Debugging"" rel=""nofollow noreferrer"">delta debugging</a>). <a href=""https://matthew-brett.github.io/pydagogue/curious_git.html"" rel=""nofollow noreferrer"">Here's a story</a> of a fictional researcher trying to make his own experiments logging system, until it ends up being a facsimile of Git.</p>

<p>Another general tool working with any language (with a Python API on <a href=""https://pypi.python.org/pypi/Sumatra"" rel=""nofollow noreferrer"">pypi</a>) is <a href=""http://neuralensemble.org/sumatra/"" rel=""nofollow noreferrer""><strong>Sumatra</strong></a>, which is specifically designed to help you do <strong>replicable</strong> research (<em>replicable</em> aims to produce the same results given the exact same code and softwares, whereas <em>reproducibility</em> aims to produce the same results given any medium, which is a lot harder and time consuming and not automatable).</p>

<p>Here is how Sumatra works: for each experiment that you conduct through Sumatra, this software will act like a ""save game state"" often found in videogames. More precisely, it will will save:</p>

<ul>
<li>all the parameters you provided;</li>
<li>the exact sourcecode state of your whole experimental application and config files;</li>
<li>the output/plots/results and also any file produced by your experimental application.</li>
</ul>

<p>It will then construct a database with the timestamp and other metadatas for each of your experiments, that you can later crawl using the webGUI. Since Sumatra saved the full state of your application for a specific experiment at one specific point in time, you can restore the code that produced a specific result at any moment you want, thus you have replicable research at a low cost (except for storage if you work on huge datasets, but you can configure exceptions if you don't want to save everything everytime).</p>

<p>Another awesome tool is <a href=""https://en.wikipedia.org/wiki/Zeitgeist_(free_software)"" rel=""nofollow noreferrer"">GNOME's Zeitgeist</a> (previously coded in Python but now ported to Vala), an all-compassing action journaling system, which records everything you do and it can use machine learning to summarize for a time period you want the relationship between items based on similarity and usage patterns, eg answering questions like <em>""What was most relevant to me, while I was working on project X, for a month last year?""</em>. Interestingly, <a href=""http://zim-wiki.org/"" rel=""nofollow noreferrer"">Zim Desktop Wiki</a>, a note-taking app similar to Evernote, has a plugin to work with Zeitgeist.</p>

<p>In the end, you can use either Git or Sumatra or any other software you want, they will provide you with about the same replicability power, but Sumatra is specifically tailored for scientific research so it provides a few fancy tools like a web GUI to crawl your results, while Git is more tailored towards code maintenance (but it has debugging tools like git-bisect so if your experiments involve codes, it may actually be better). Or of course you can use both!</p>

<p>/EDIT: <a href=""https://datascience.stackexchange.com/a/775"">dsign</a> touched a very important point here: the replicability of your setup is as important as the replicability of your application. In other words, you should at least provide a <strong>full list of the libraries and compilers</strong> you used along with their exact <strong>versions</strong> and the details of your <strong>platform</strong>.</p>

<p>Personally, in scientific computing with Python, I have found that packaging an application along with the libraries is just too painful, thus I now just use an all-in-one scientific python package such as <a href=""https://store.continuum.io/cshop/anaconda/"" rel=""nofollow noreferrer"">Anaconda</a> (with the great package manager <a href=""http://www.continuum.io/blog/conda"" rel=""nofollow noreferrer"">conda</a>), and just advise users to use the same package. Another solution could be to provide a script to automatically generate a <a href=""http://docs.python-guide.org/en/latest/dev/virtualenvs/"" rel=""nofollow noreferrer"">virtualenv</a>, or to package everything using the commercial <a href=""https://datascience.stackexchange.com/a/775"">Docker application as cited by dsign</a> or the opensource <a href=""http://www.vagrantup.com/"" rel=""nofollow noreferrer"">Vagrant</a> (with for example <a href=""http://deeplearning.net/software/pylearn2/#other-methods"" rel=""nofollow noreferrer"">pylearn2-in-a-box</a> which use Vagrant to produce an easily redistributable virtual environment package).</p>

<p>Finally, to really ensure that you have a fully working environment everytime you need, you can make a virtual machine (see VirtualBox), and you can even save the state of the machine (snapshot) with your experiment ready to run inside. Then you can just share this virtual machine with everything included so that anyone can replicate your experiment with your exact setup. This is probably the best way to replicate a software based experiment. Containers might be a more lightweight alternative, but they do not include the whole environment, so that the replication fidelity will be less robust.</p>

<p>/EDIT2: Here's a <a href=""https://www.udacity.com/course/viewer#!/c-cs259/l-48648797/m-48680672"" rel=""nofollow noreferrer"">great video</a> summarizing (for debugging but this can also be applied to research) what is fundamental to do reproducible research: logging your experiments and each other steps of the scientific method, a sort of <em>""explicit experimenting""</em>.</p>
","<python><tools><version-control>","20","20","20","0","2544","303"
"46127","{
  ""id"": 46127,
  ""title"": ""What do \""compile\"", \""fit\"", and \""predict\"" do in Keras sequential models?""
}","<p>Let's first see what we need to do when we want to train a model. </p>

<ol>
<li>First, we want to decide a model architecture, this is the number of hidden layers and activation functions, etc. <strong>(compile)</strong></li>
<li>Secondly, we will want to train our model to get all the paramters to the correct value to map our inputs to our outputs. <strong>(fit)</strong> </li>
<li>Lastly, we will want to use this model to do some feed-forward passes to predict novel inputs. <strong>(predict)</strong></li>
</ol>

<hr>

<p>Let's go through an example using the mnist database.</p>

<pre><code>from __future__ import print_function
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.callbacks import ModelCheckpoint
from keras.models import model_from_json
from keras import backend as K
</code></pre>

<p>Let's load our data. Then I normalize the values of the pixels to be between 0 and 1.</p>

<pre><code>(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
</code></pre>

<p><a href=""https://i.stack.imgur.com/RlUvW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RlUvW.png"" alt=""enter image description here""></a></p>

<p>Now we need to reshape our data to compatible with Keras. We need to add an additional dimension to our data which will act as our channel when passing the data through the deep learning model. I then vectorize the output classes.</p>

<pre><code># The known number of output classes.
num_classes = 10

# Input image dimensions
img_rows, img_cols = 28, 28

# Channels go last for TensorFlow backend
x_train_reshaped = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test_reshaped = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
input_shape = (img_rows, img_cols, 1)

# Convert class vectors to binary class matrices. This uses 1 hot encoding.
y_train_binary = keras.utils.to_categorical(y_train, num_classes)
y_test_binary = keras.utils.to_categorical(y_test, num_classes)
</code></pre>

<p>Now let's define our model. We will use a vanilla CNN for this example.</p>

<pre><code>model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))
</code></pre>

<p>Now we are ready to <strong>compile</strong> our model. This will create a Python object which will build the CNN. This is done by building the computation graph in the correct format based on the Keras backend you are using. I usually use tensorflow over theano. The compilation steps also asks you to define the loss function and kind of optimizer you want to use. These options depend on the problem you are trying to solve, you can find the best techniques usually reading the literature in the field. For a classification task categorical cross-entropy works very well. </p>

<pre><code>model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
</code></pre>

<p>Now we have a Python object that has a model and all its parameters with its initial values. If you try to use predict now with this model your accuracy will be 10%, pure random output. </p>

<p>You can save this model to disk to use later.</p>

<pre><code># Save the model
model_json = model.to_json()
with open(""weights/model.json"", ""w"") as json_file:
    json_file.write(model_json)
</code></pre>

<p>So, now we need to train our model so that the parameters get tuned to provide the correct outputs for a given input. We do this by feeding inputs at the input layer and then getting an output, we then calculate the loss function using the output and use backpropagation to tune the model parameters. This will <strong>fit</strong> the model parameters to the data.</p>

<p>First let's define some callback functions so that we can checkpoint our model and save it model parameters to file each time we get better results. </p>

<pre><code># Save the weights using a checkpoint.
filepath=""weights/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5""
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]

epochs = 4
batch_size = 128
# Fit the model weights.
model.fit(x_train_reshaped, y_train_binary,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          callbacks=callbacks_list,
          validation_data=(x_test_reshaped, y_test_binary))
</code></pre>

<p>Now we have a model architecture and we have a file containing all the model parameters with the best values found to map the inputs to an output. We are now done with the computationally expensive part of deep learning. We can now take our model and use feed-forward passes and <strong>predict</strong> inputs. I prefer to use predict_class, rather than predict because it immediately gives me the class, rather than the output vector. </p>

<pre><code>print('Predict the classes: ')
prediction = model.predict_classes(x_test_reshaped[10:20])
show_imgs(x_test[10:20])
print('Predicted classes: ', prediction)
</code></pre>

<p><a href=""https://i.stack.imgur.com/kQXDO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kQXDO.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Predicted classes:  [0 6 9 0 1 5 9 7 3 4]</p>
</blockquote>

<hr>

<p>The code to print the MNIST database nicely</p>

<pre><code>import matplotlib.pyplot as plt
%matplotlib inline

# utility function for showing images
def show_imgs(x_test, decoded_imgs=None, n=10):
    plt.figure(figsize=(20, 4))
    for i in range(n):
        ax = plt.subplot(2, n, i+1)
        plt.imshow(x_test[i].reshape(28,28))
        plt.gray()
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

        if decoded_imgs is not None:
            ax = plt.subplot(2, n, i+ 1 +n)
            plt.imshow(decoded_imgs[i].reshape(28,28))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
    plt.show()
</code></pre>
","<keras><prediction><backpropagation><cost-function><methods>","20","20","20","0","29587","8088"
"20428","{
  ""id"": 20428,
  ""title"": ""Why do we convert skewed data into a normal distribution""
}","<p>You might want to interpret your coefficients. That is, to be able to say things like ""if I increase my variable $X_1$ by 1, then, on average and all else being equal, $Y$ should increase by $\beta_1$"".</p>

<p>For your coefficients to be interpretable, linear regression assumes a bunch of things.</p>

<p>One of these things is no multicollinearity. That is, your $X$ variables should not be correlated against each other.</p>

<p>Another is <a href=""https://en.wikipedia.org/wiki/Homoscedasticity"" rel=""noreferrer"">Homoscedasticity</a>. The errors your model commits should have the same variance, i.e. you should ensure the linear regression does not make small errors for low values of $X$ and big errors for higher values of $X$. In other words, the difference between what you predict $\hat Y$ and the true values $Y$ should be constant. You can ensure that by making sure that $Y$ follows a Gaussian distribution. (The proof is highly mathematical.)</p>

<p>Depending on your data, you may be able to make it Gaussian. Typical transformations are taking the inverse, the logarithm or square roots. Many others exist of course, it all depends on your data. You have to look at your data, and then do a histogram or run a <a href=""https://en.wikipedia.org/wiki/Normality_test"" rel=""noreferrer"">normality test</a>, such as the Shapiro-Wilk test.</p>

<p>These are all techniques to build an <a href=""https://en.wikipedia.org/wiki/Bias_of_an_estimator"" rel=""noreferrer"">unbiased estimator</a>. I don't think it has anything to do with convergence as others have said (sometimes you may also want to normalize your data, but that is a different topic).</p>

<p>Following the <a href=""https://en.wikipedia.org/wiki/Linear_regression#Assumptions"" rel=""noreferrer"">linear regression assumptions</a> is important if you want to either interpret the coefficients or if you want to use statistical tests in your model. Otherwise, forget about it.</p>

<p>Applying the logarithm or normalizing your data, is also important because linear regression optimization algorithms typically minimize $\|\hat y - y\|^2$, so if you have some big $y$ outliers, your estimator is going to be VERY concerned about minimizing those, since it is concerned about the squared error, not absolute error. Normalizing your data is important in those case and this is why scikit-learn has a <code>normalize</code> option in the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"" rel=""noreferrer"">LinearRegression</a> constructor.</p>
","<regression><feature-extraction><feature-engineering><kaggle><feature-scaling>","20","20","20","0","16853","3182"
"3764","{
  ""id"": 3764,
  ""title"": ""How to deal with time series which change in seasonality or other patterns?""
}","<p>After reading your question, I became curious about the topic of <em>time series clustering</em> and <em>dynamic time warping (DTW)</em>. So, I have performed a limited search and came up with basic understanding (for me) and the following set of IMHO <em>relevant references</em> (for you). I hope that you'll find this useful, but keep in mind that I have intentionally skipped research papers, as I was more interested in <em>practical aspects</em> of the topic.</p>
<p><strong>Resources:</strong></p>
<ul>
<li><strong>UCR Time Series Classification/Clustering</strong>: <a href=""http://www.cs.ucr.edu/%7Eeamonn/time_series_data"" rel=""nofollow noreferrer"">main page</a>, <a href=""http://www.cs.ucr.edu/%7Eeamonn/UCRsuite.html"" rel=""nofollow noreferrer"">software page</a> and <a href=""http://www.cs.ucr.edu/%7Eeamonn/SIGKDD_trillion.pdf"" rel=""nofollow noreferrer"">corresponding paper</a></li>
<li><strong>Time Series Classification and Clustering with Python</strong>: <a href=""http://alexminnaar.com/2014/04/16/Time-Series-Classification-and-Clustering-with-Python.html"" rel=""nofollow noreferrer"">a blog post</a> and an <a href=""http://nbviewer.ipython.org/github/alexminnaar/time-series-classification-and-clustering/blob/master/Time%20Series%20Classification%20and%20Clustering.ipynb"" rel=""nofollow noreferrer"">ipython notebook</a></li>
<li><strong>Capital Bikeshare: Time Series Clustering</strong>: <a href=""http://ofdataandscience.blogspot.com/2013/03/capital-bikeshare-time-series-clustering.html"" rel=""nofollow noreferrer"">another blog post</a></li>
<li><strong>Dynamic Time Warping using rpy and Python</strong>: <a href=""https://nipunbatra.wordpress.com/2013/06/09/dynamic-time-warping-using-rpy-and-python"" rel=""nofollow noreferrer"">another blog post</a></li>
<li><strong>Mining Time-series with Trillions of Points: Dynamic Time Warping at Scale</strong>: <a href=""http://practicalquant.blogspot.com/2012/10/mining-time-series-with-trillions-of.html"" rel=""nofollow noreferrer"">another blog post</a></li>
<li><strong>Time Series Analysis and Mining in R</strong> (to add R to the mix): <a href=""http://rdatamining.wordpress.com/2011/08/23/time-series-analysis-and-mining-with-r"" rel=""nofollow noreferrer"">yet another blog post</a></li>
<li>And, finally, two <strong>tools implementing/supporting DTW</strong>, to top it off: <a href=""http://rdatamining.wordpress.com/2011/08/23/time-series-analysis-and-mining-with-r"" rel=""nofollow noreferrer"">R package</a> and <a href=""http://mlpy.sourceforge.net"" rel=""nofollow noreferrer"">Python module</a></li>
</ul>
","<data-mining><clustering><time-series><beginner>","20","20","20","0","2452","6478"