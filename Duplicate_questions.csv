OId,OTitle,OBody,OTags,DId,DTitle,DBody,DTags
"477","Preference Matching Algorithm","<p>There's this side project I'm working on where I need to structure a solution to the following problem.</p>

<p>I have two groups of people (clients). Group ""A"" intends to buy and group ""B"" intends to sell a determined product ""X"". </p>

<p>The product has a series of attributes x_i and my objective is to facilitate the transaction between ""A"" e ""B"" by matching their preferences. The main idea is to point out to each member of ""A"" a corresponding in ""B"" who’s product better suits his needs, and vice versa. </p>

<p>Some complicating aspects of the problem:</p>

<ol>
<li><p>The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design which is rare among the population and I can’t predict. Can’t previously list all the attributes;</p></li>
<li><p>Attributes might be continuous, binary or non-quantifiable (ex: price, functionality, design).</p></li>
</ol>

<p>Any suggestion on how to approach this problem and solve it in an automated way?<br>
The idea is to really think out of the box here so feel free to ""go wild"" on your suggestions. </p>

<p>I would also appreciate some references to other similar problems if possible. </p>
","<algorithms>","461","Preference Matching Algorithm","<p>There's this side project I'm working on where I need to structure a solution to the following problem.</p>

<p>I have two groups of people (clients). Group <code>A</code> intends to buy and group <code>B</code> intends to sell a determined product <code>X</code>. The product has a series of attributes <code>x_i</code>, and my objective is to facilitate the transaction between <code>A</code> and <code>B</code> by matching their preferences. The main idea is to point out to each member of <code>A</code> a corresponding in <code>B</code> whose product better suits his needs, and vice versa.</p>

<p>Some complicating aspects of the problem:</p>

<ol>
<li><p>The list of attributes is not finite. The buyer might be interested in a very particular characteristic or some kind of design, which is rare among the population and I can't predict. Can't previously list all the attributes;</p></li>
<li><p>Attributes might be continuous, binary, or non-quantifiable (ex: price, functionality, design);</p></li>
</ol>

<p>Any suggestion on how to approach this problem and solve it in an automated way?</p>

<p>I would also appreciate some references to other similar problems if possible.</p>

<hr>

<p>Great suggestions! Many similarities in to the way I'm thinking of approaching the problem.</p>

<p>The main issue on mapping the attributes is that the level of detail to which the product should be described depends on each buyers. Let’s take an example of a car. The product “car” has lots and lots of attributes that range from its performance, mechanical structure, price etc.</p>

<p>Suppose I just want a cheap car, or an electric car. Ok, that's easy to map because they represent main features of this product. But let’s say, for instance, that I want a car with Dual-Clutch transmission or Xenon headlights. Well there might be many cars on the data base with this attributes but I wouldn't ask the seller to fill in this level of detail to their product prior to the information that there is someone looking them. Such a procedure would require every seller fill a complex, very detailed, form just try to sell his car on the platform. Just wouldn't work.</p>

<p>But still, my challenge is to try to be as detailed as necessary in the search to make a good match. So the way I'm thinking is mapping main aspects of the product, those that are probably relevant to everyone, to narrow down de group of potential sellers.</p>

<p>Next step would be a “refined search”. In order to avoid creating a too detailed form I could ask buyers and sellers to write a free text of their specification. And then use some word matching algorithm to find possible matches. Although I understand that this is not a proper solution to the problem because the seller cannot “guess” what the buyer needs. But might get me close.  </p>

<p>The weighting criteria suggested is great. It allows me to quantify the level to which the seller matches the buyer’s needs. The scaling part might be a problem though, because the importance of each attribute varies from client to client. I'm thinking of using some kind of pattern recognition or just asking de buyer to input the level of importance of each attribute.</p>
","<bigdata><text-mining><recommender-system>"
"1174","How to classify and cluster this time series data","<p>I have post already the question few months ago about my project that I'm starting to work on. This post can be see here: 
<a href=""https://datascience.stackexchange.com/questions/211/human-activity-recognition-using-smartphone-data-set-problem"">Human activity recognition using smartphone data set problem</a></p>

<p>Now, I know this is based around multivariate time series analysis and tasks are to classify and cluster the data. I have gathered some materials (e-books, tutorials etc.) on this but still can't see a more detailed picture of how even I should start. Here's the tutorial that looks like it might be helpful but the thing is my data looks differently and I'm not really sure if this can be applied to my work.</p>

<p><a href=""http://little-book-of-r-for-multivariate-analysis.readthedocs.org/en/latest/src/multivariateanalysis.html#scatterplots-of-the-principal-components"" rel=""nofollow noreferrer"">http://little-book-of-r-for-multivariate-analysis.readthedocs.org/en/latest/src/multivariateanalysis.html#scatterplots-of-the-principal-components</a></p>

<p>So basically, my questions are:</p>

<p>How I can start on some very basic analysis? How to read data so it any meaning for me.
Any tips and advises will be much appreciated!
Note: I'm just the beginner in data science.  </p>
","<data-mining><classification><dataset><clustering><time-series>","3738","How to deal with time series which change in seasonality or other patterns?","<h3>Background</h3>
<p>I'm working on a time series data set of energy meter readings. The length of the series varies by meter - for some I have several years, others only a few months, etc. Many display significant seasonality, and often multiple layers - within the day, week, or year.</p>
<p>One of the things I've been working on is clustering of these time series. My work is academic for the moment, and while I'm doing other analysis of the data as well, I have a specific goal to carry out some clustering.</p>
<p>I did some initial work where I calculated various features (percentage used on weekends vs. weekday, percentage used in different time blocks, etc.). I then moved on to looking at using Dynamic Time Warping (DTW) to obtain the distance between different series, and clustering based on the difference values, and I've found several papers related to this.</p>
<h3>Question</h3>
<p>Will the seasonality in a specific series changing cause my clustering to be incorrect? And if so, how do I deal with it?</p>
<p>My concern is that the distances obtained by DTW could be misleading in the cases where the pattern in a time series has changed. This could lead to incorrect clustering.</p>
<p>In case the above is unclear, consider these examples:</p>
<h3>Example 1</h3>
<p>A meter has low readings from midnight until 8AM, the readings then increase sharply for the next hour and stay high from 9AM until 5PM, then decrease sharply over the next hour and then stay low from 6PM until midnight. The meter continues this pattern consistently every day for several months, but then changes to a pattern where readings simply stay at a consistent level throughout the day.</p>
<h3>Example 2</h3>
<p>A meter shows approximately the same amount of energy being consumed each month. After several years, it changes to a pattern where energy usage is higher during the summer months before returning to the usual amount.</p>
<h3>Possible Directions</h3>
<ul>
<li>I've wondered whether I can continue to compare whole time series, but split them and consider them as a separate series if the pattern changes considerably. However, to do this I'd need to be able to detect such changes. Also, I just don't know if this is a suitable way or working with the data.</li>
<li>I've also considered splitting the data and considering it as many separate time series. For instance, I could consider every day/meter combination as a separate series. However, I'd then need to do similarly if I wanted to consider the weekly/monthly/yearly patterns. I <em>think</em> this would work, but it's potentially quite onerous and I'd hate to go down this path if there's a better way that I'm missing.</li>
</ul>
<h3>Further Notes</h3>
<p>These are things that have come up in comments, or things I've thought of due to comments, which might be relevant. I'm putting them here so people don't have to read through everything to get relevant information.</p>
<ul>
<li>I'm working in Python, but have rpy for those places where R is more suitable. I'm not necessarily looking for a Python answer though - if someone has a practical answer of what should be done I'm happy to figure out implementation details myself.</li>
<li>I have a lot of working &quot;rough draft&quot; code - I've done some DTW runs, I've done a couple of different types of clustering, etc. I think I largely understand the direction I'm taking, and what I'm really looking for is related to how I process my data before finding distances, running clustering, etc. Given this, I suspect the answer would be the same whether the distances between series are calculated via DTW or a simpler Euclidean Distance (ED).</li>
<li>I have found these papers especially informative on time series and DTW and they may be helpful if some background is needed to the topic area: <a href=""http://www.cs.ucr.edu/%7Eeamonn/selected_publications.htm"" rel=""noreferrer"">http://www.cs.ucr.edu/~eamonn/selected_publications.htm</a></li>
</ul>
","<data-mining><clustering><time-series><beginner>"
"2316","Machine Learning - Where is the difference between one-class, binary-class and multinominal-class classification?","<p>Where is the difference between one-class, binary-class and multinominal-class classification?</p>

<p>If I like to classify text in lets say four classes and also want the system to be able to tell me that none of these classes matches the unknown/untrained test-data. </p>

<p>Couldn't I just use all the methods that I mentioned above to reach my goal?
e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknow data-set ...</p>

<p>Just by saying, Training-Set for C1 contains class 1 (all good samples for C1) and class 0 (mix of all C2, C3 and C4 as bad samples for C1).</p>

<p>Is unlabeled-data C1 -> 1 or 0</p>

<p>Is unlabeled-data C2 -> 1 or 0
... and so on ...</p>

<p>For multinominal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ...</p>

<p>But where is the difference between this two methods? (except of that I have to use different algorithms)</p>

<p>And how would I define a training-set for the described problem of categorizing data in those four classes using one-class classfication (is that even possible)?</p>

<p>Excuse me if I'm completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =)</p>
","<machine-learning><classification><data-mining>","2313","Machine Learning - Where is the difference between one-class, binary-class and multinominal-class classification?","<p>Where is the difference between one-class, binary-class and multinominal-class classification?</p>

<p>If I like to classify text in lets say four classes and also want the system to be able to tell me that none of these classes matches the unknown/untrained test-data. </p>

<p>Couldn't I just use all the methods that I mentioned above to reach my goal?
e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknow data-set ...</p>

<p>Just by saying, Training-Set for C1 contains class 1 (all good samples for C1) and class 0 (mix of all C2, C3 and C4 as bad samples for C1).</p>

<p>Is unlabeled-data C1 -> 1 or 0</p>

<p>Is unlabeled-data C2 -> 1 or 0
... and so on ...</p>

<p>For multinominal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ...</p>

<p>But where is the difference between this two methods? (except of that I have to use different algorithms)</p>

<p>And how would I define a training-set for the described problem of categorizing data in those four classes using one-class classfication (is that even possible)?</p>

<p>Excuse me if I'm completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =)</p>
","<machine-learning><data-mining><classification><categorical-data>"
"6068","How can an undergraduate learn more about deep learning?","<p>I am an undergraduate who needs to submit a thesis for graduation. I am fairly interested in deep learning, and am working on a project that uses deep learning methods extensively (rCNNs to be precise). The caveat here is that I am working alone, with little help from my advisor. There are hardly any experts on deep learning in my university. How do I go about finishing my project. It is specially intimidating to see that most papers in deep learning are published by multiple accomplished scientists with very little involvement of undergraduate level students. </p>

<p>PS: I am really interested in this topic, so please try not to advise me to switch to an easier topic.</p>
","<machine-learning><deep-learning><research>","2651","Deep learning basics","<p>I am looking for a paper detailing the very basics of deep learning. Ideally like the Andrew Ng course for deep learning. Do you know where I can find this ?</p>
","<machine-learning><deep-learning>"
"9418","Is it common to preprocess image data before sending it through a deep net?","<p>I'm curious as how convolutional neural network are used in practice for object recognition. Is it common to perform data preprocessing before providing the data to the input layer ? If so, what types of preprocessing are common ?</p>
","<image-classification><preprocessing><object-recognition>","5224","How to prepare/augment images for neural network?","<p>I would like to use a neural network for image classification.  I'll start with pre-trained CaffeNet and train it for my application.  </p>

<h1>How should I prepare the input images?</h1>

<p>In this case, all the images are of the same object but with variations (think: quality control).  They are at somewhat different scales/resolutions/distances/lighting conditions (and in many cases I don't know the scale).  Also, in each image there is an area (known) around the object of interest that should be ignored by the network.</p>

<p>I could (for example) crop the center of each image, which is guaranteed to contain a portion of the object of interest and none of the ignored area; but that seems like it would throw away information, and also the results wouldn't be really the same scale (maybe 1.5x variation).</p>

<h1>Dataset augmentation</h1>

<p>I've heard of creating more training data by random crop/mirror/etc, is there a standard method for this?  Any results on how much improvement it produces to classifier accuracy?</p>
","<neural-network><image-classification><preprocessing><convnet>"
"10560","How would you categorise email subjects to find similar emails?","<p>I have a list of email subjects like</p>

<pre><code>&lt;XYZ&gt; commented on &lt;ABC&gt;
Weekly review for &lt;Company&gt;
Your account is ready 
</code></pre>

<p>And I want to find patterns in them so I can group them.</p>

<p>Is there a well known algorithm I can use? </p>
","<algorithms><machine-learning>","10550","How would you categorize email subjects to find similar emails?","<p>I have a list of email subjects like:</p>

<pre><code>&lt;XYZ&gt; commented on &lt;ABC&gt;
Weekly review for &lt;Company&gt;
Your account is ready 
</code></pre>

<p>And I want to find patterns in them so I can group them.</p>

<p>Is there a well known algorithm I can use? </p>

<ul>
<li>Preferably with wide language implementations or easy re-implementation.</li>
<li>The algorithm should be unsupervised.</li>
<li>The number of different emails is not known.</li>
</ul>

<p><em>Update:</em></p>

<p>I think I can break this down into two problems:</p>

<ol>
<li><p>Group subjects by the similar words they use, resulting in the following. Each group should be very distinct from the rest (they should be almost perfectly exclusive) and the algorithm should give relatively small number of groups with good length of the common words.</p>

<pre><code>[commented, on]
[weekly, review]
[your, account, is, ready]
</code></pre></li>
<li><p>Once grouped, it should be easy to find a state automaton that accepts only the group's subject and thus eliminates variable</p></li>
<li><p>Then I can go back and check if there are any intersections and tweak the variables.</p></li>
</ol>

<p>Having said that, is it better to use a completley different approach like neural nets maybe? I have zero experience with those, but if it makes more sense, I am open to learning.</p>
","<text-mining><algorithms>"
"12875","automated topic modeling topic naming","<p>Are there well-known automated methods for deriving a name for each topic obtained through topic modeling? for a specifically given problem at hand I will probably default to an algorithm on top an ontology, on top the topic modeling results. But I'm highly curious as to whether there's something that I am missing, in the general case.</p>
","<topic-model>","9663","How to give name to topics created using LDA?","<p>I have categorized 800,000 documents into 500 categories using the Mahout topic modelling.</p>

<p>Instead of representing the topic using the top 5/10 words for each topics, I want to infer a generic name for the group using any existing algorithm.
For the time being, I have used the following algorithm to arrive at the name for the topic:</p>

<p>For each topic</p>

<ul>
<li>Take all the documents belonging to the topic (using the document-topic distribution output)</li>
<li>Run python nltk to get the noun phrases</li>
<li>Create the TF file from the output</li>
<li>name for the topic is the phrase (limited towards max 5 words)</li>
</ul>

<p>Please suggest a approach to arrive at more relevant name for the topics.</p>
","<machine-learning><data-mining><nlp><text-mining><topic-model>"
"14959","After the training phase, is it better to run neural networks on a GPU or CPU?","<p>My understanding is that GPUs are more efficient for running neural nets, but someone recently suggested GPUs are only needed for the training phase and that once trained, it's actually more efficient to run them on CPUs.</p>

<p>Is this true?</p>
","<machine-learning>","14941","After the training phase, is it better to run neural networks on a GPU or CPU?","<p>My understanding is that GPUs are more efficient for running neural nets, but someone recently suggested to me that GPUs are only needed for the training phase. Once trained, it's actually more efficient to run them on CPUs.</p>

<p>Is this true?</p>
","<neural-network><deep-learning><gpu>"
"16771","Team Collaboration using Jupyter Notebooks?","<p>Is there an alternative to JupyterHub that allows sharing of notebooks across team members. We need the following requirements:</p>

<p>Be able to run the server on a HPC. 
Multiple user login.
Version control with Git or similar service.
Sharing of notebooks between team members.</p>
","<jupyter>","14998","Sharing Jupyter notebooks within a team","<p>I would like to set up a server which could support a data science team in the following way: be a central point for storing, versioning, sharing and possible also executing Jupyter notebooks.</p>

<p>Some desired properties:</p>

<ol>
<li>Different users can access the server and open and execute notebooks that were stored by them or by other team members. The interesting question here is what would be the behavior if user X executes cells in a notebook authored by user Y. I guess the notebook should <em>NOT</em> be changed:</li>
<li>Solution should be self-hosted.</li>
<li>Notebooks should be stored either on the server or on Google drive or on self-hosted instance of owncloud.</li>
<li>(Bonus) Notebooks will be under git versioning control (git may be self-hosted. Cannot be bounded to GitHub or something of that sort).</li>
</ol>

<p>I looked into <a href=""https://github.com/jupyterhub"" rel=""noreferrer"">JupyterHub</a> and <a href=""https://github.com/binder-project/binder"" rel=""noreferrer"">Binder</a>. With the former, I didn't understand how to allow cross users access. The latter seems to only support GitHub as the storage of the notebooks.</p>

<p>Do you have experience with either of the solutions? </p>
","<software-recommendation>"
"17167","Keras del stuck with constant loss and accuracy","<p>I am trying to train a keras CNN against the Street View House Numbers Dataset. You can find the project <a href=""https://github.com/asindico/digitRecognition/tree/9faa600f956e264ca61afe48d64c833f8b58711c"" rel=""nofollow noreferrer"">here</a>. The problem is that during training neither loss nor accuracy change over time. I have tried with 1 Channel (Gray Scale) images, with RGB (3 channels) images, with wider (50,50) and smaller (28,28) images, with more or less filters in the convolutional layers, with wider and smaller patches in the pooling layers, with and without dropout, with bigger and smaller batches, with smaller and bigger learning step for the optimizers, with different optimizers, ...</p>

<p>Still the training gets stuck to constant loss and accuracy</p>
","<machine-learning><python><deep-learning><keras>","17080","Keras CNN with low/constant accuracies","<p>I am dealing with the Street View House Number recognition problem. I am trying to train a CNN with Keras. </p>

<p>Here is how I prepared the input:</p>

<pre><code>from PIL import Image
from PIL import ImageFilter
train_folders = 'sv_train/train'
test_folders = 'test'
extra_folders = 'extra'
SV_IMG_SIZE = 28
SV_CHANNELS = 1
train_imsize = np.ndarray([len(train_data),2])
k = 500
sv_images = []
max_images = 20000#len(train_data)
max_digits = 5
sv_labels = np.ones([max_images, max_digits], dtype=int) * 10 # init to 10 cause it would be no digit
nboxes = [[] for i in range(max_images)]
print (""%d to load"" % len(train_data))
def getBBox(i,perc):
    '''
    Given i, the desired i.png, returns
    x_min, y_min, x_max, y_max,
    the four numbers which define the small rectangular bounding
    box that contains all individual character bounding boxes
    '''
    boxes = train_data[i]['boxes'] 
    x_min=9990
    y_min=9990
    x_max=0
    y_max=0
    for bid,b in enumerate(boxes):
        x_min = b['left'] if b['left'] &lt;= x_min else x_min
        y_min = b['top'] if b['top'] &lt;= y_min else y_min
        x_max = b['left']+b['width'] if  b['left']+b['width'] &gt;= x_max else x_max
        y_max = b['top']+b['height'] if b['top']+b['height'] &gt;= y_max else y_max

    dy = y_max-y_min
    dx = x_max-x_min
    dpy = dy*perc
    dpx = dx*perc
    nboxes[i]=[dpx,dpy,dx,dy]
    return x_min-dpx, y_min-dpy, x_max+dpx, y_max+dpy

for i in range(max_images):
    print ("" \r%d"" % i ,end="""")
    filename = train_data[i]['filename']
    fullname = os.path.join(train_folders, filename)
    boxes = train_data[i]['boxes']
    label = [10,10,10,10,10]
    lb = len(boxes)
    if lb &lt;= max_digits:
        im = Image.open(fullname)
        x_min, y_min, x_max, y_max = getBBox(i,0.3)
        im = im.crop([x_min,y_min,x_max,y_max])
        owidth, oheight = im.size
        wr = SV_IMG_SIZE/float(owidth)
        hr = SV_IMG_SIZE/float(oheight)
        for bid,box in  enumerate(boxes):
            sv_labels[i][max_digits-lb+bid] = int(box['label'])

        box = nboxes[i]
        box[0]*=wr
        box[1]*=wr
        box[2]*=hr
        box[3]*=hr
        im = im.resize((SV_IMG_SIZE,SV_IMG_SIZE),Image.ANTIALIAS)
        img = img - np.mean(img)
        im = im.filter(ImageFilter.EDGE_ENHANCE)
        img = img - np.mean(img)

        array = np.asarray(im)
        array =  array.reshape((SV_IMG_SIZE,SV_IMG_SIZE,3)).astype(np.float32)
        na = np.zeros([SV_IMG_SIZE,SV_IMG_SIZE],dtype=int)
        for x in range (array.shape[0]):
            for y in range (array.shape[1]):
                na[x][y]=np.average(array[x][y][:])
        na = na.reshape(SV_IMG_SIZE,SV_IMG_SIZE,1)
        #print(array.shape)
        sv_images.append(na.astype(np.float32))

sv_train, sv_validation, svt_labels, svv_labels = train_test_split(sv_images, sv_labels, test_size=0.33, random_state=42)
</code></pre>

<p>And here is how I created and trained the model:</p>

<pre><code>model = Sequential()
x = Input((28, 28,1))

y = Convolution2D(16, 3, 3, border_mode=""same"")(x)
#y = MaxPooling2D(pool_size = (2, 2), strides = (2, 2)) (y)
#y = Dropout(0.25)(y)

y = Convolution2D(32, 4, 4, border_mode=""same"")(y)
y = MaxPooling2D(pool_size = (3, 3)) (y)
#y = Dropout(0.25)(y)

y = Convolution2D(64, 5, 5, border_mode=""same"", activation=""relu"")(y)
y = MaxPooling2D((2, 2))(y)
#y = Dropout(0.25)(y)

y = Convolution2D(128, 5, 5, border_mode=""same"", activation=""relu"")(y)
y = MaxPooling2D((2, 2))(y)
#y = Dropout(0.25)(y)


y = Flatten()(y)
y = Dense(1024, activation=""relu"")(y)

digit1 = Dense(11, activation=""softmax"")(y)
digit2 = Dense(11, activation=""softmax"")(y)
digit3 = Dense(11, activation=""softmax"")(y)
digit4 = Dense(11, activation=""softmax"")(y)
digit5 = Dense(11, activation=""softmax"")(y)
model = Model(input=x, output=[digit1, digit2, digit3,digit4,digit5])


model.compile(optimizer='adam',
          loss='sparse_categorical_crossentropy',
          metrics=['accuracy'])
print(model.layers[0].output_shape)
print(model.layers[2].output_shape)
print(model.layers[4].output_shape)
print(model.layers[6].output_shape)
print(model.layers[8].output_shape)

sv_train_labels = [svt_labels[:,0],svt_labels[:,1],svt_labels[:,2],svt_labels[:,3],svt_labels[:,4]]
sv_validation_labels = [svv_labels[:,0],svv_labels[:,1],svv_labels[:,2],svv_labels[:,3],svv_labels[:,4]]

model.fit(sv_train, sv_train_labels, nb_epoch=10, batch_size=64,validation_data=(sv_validation, sv_validation_labels))
</code></pre>

<p>The problem is that I get very low accuracies that remain with the same value at each epoch:</p>

<pre><code>Train on 13400 samples, validate on 6600 samples
Epoch 1/10
13400/13400 [==============================] - 78s - loss: 34.7407 - dense_740_loss: 0.1161 - dense_741_loss: 0.6879 - dense_742_loss: 4.7988 - dense_743_loss: 14.7893 - dense_744_loss: 14.3486 - dense_740_acc: 0.9902 - dense_741_acc: 0.9542 - dense_742_acc: 0.7001 - dense_743_acc: 0.0810 - dense_744_acc: 0.1055 - val_loss: 34.7760 - val_dense_740_loss: 0.0049 - val_dense_741_loss: 0.7131 - val_dense_742_loss: 4.8721 - val_dense_743_loss: 14.8091 - val_dense_744_loss: 14.3769 - val_dense_740_acc: 0.9997 - val_dense_741_acc: 0.9558 - val_dense_742_acc: 0.6977 - val_dense_743_acc: 0.0812 - val_dense_744_acc: 0.1080
Epoch 2/10
13400/13400 [==============================] - 70s - loss: 34.7032 - dense_740_loss: 0.0036 - dense_741_loss: 0.6760 - dense_742_loss: 4.7861 - dense_743_loss: 14.8118 - dense_744_loss: 14.4257 - dense_740_acc: 0.9998 - dense_741_acc: 0.9581 - dense_742_acc: 0.7031 - dense_743_acc: 0.0810 - dense_744_acc: 0.1050 - val_loss: 34.7760 - val_dense_740_loss: 0.0049 - val_dense_741_loss: 0.7131 - val_dense_742_loss: 4.8721 - val_dense_743_loss: 14.8091 - val_dense_744_loss: 14.3769 - val_dense_740_acc: 0.9997 - val_dense_741_acc: 0.9558 - val_dense_742_acc: 0.6977 - val_dense_743_acc: 0.0812 - val_dense_744_acc: 0.1080
</code></pre>
","<machine-learning><python><deep-learning><tensorflow><keras>"
"17233","Installing Orange Package with older Python Version?","<p>I am trying to install an Add-on package from GitHub that contains prototype widgets for Orange Data Mining. I am trying to install it from the GitHub page found <a href=""https://github.com/biolab/orange3-prototypes.git"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I am using the following Terminal code to install this:</p>
<pre><code>git clone http://github.com/biolab/orange3-prototypes.git
</code></pre>
<p>Everything then appears to install correctly and the download shows 100%. Then, however, it throws an error and says:</p>
<blockquote>
<p>Orange requires Python &gt;= 3.4</p>
</blockquote>
<p>I am using Mac OS. Clearly, it is suggesting that I need to use a different version of python to install, however, I already updated my pip install. Any insight into how I can fix this?</p>
","<python><orange><orange3><pip>","17095","How do I integrate Github files to Orange for ML?","<p><a href=""https://i.stack.imgur.com/e7l93.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e7l93.jpg"" alt=""enter image description here"" /></a></p>
<p>I am currently trying to make a facial recognition workflow with Orange but since the widgets are still prototypes they are not all available in Orange through the add-on menu. I found the Github files for the widgets but don't know how to install this so that I can see and use the widgets in Orange.</p>
","<orange><orange3>"
"19836","When to use a Sigmoid function and when to use ReLu in a Convolutional Neural Network","<p>Can someone please refer a good article explaining why we use a Sigmoid activation in the final layer of a neural network and a ReLu activation in the middle layers and input layer while building a Convolutional Neural Network? I am not getting how that results in the right output.</p>
","<machine-learning><deep-learning><keras>","15484","Sigmoid vs Relu function in Convnets","<p>The question is simple: is there any advantage in using sigmoid function in a convolutional neural network? Because every website that talks about CNN uses Relu function.</p>
","<convolutional-neural-network>"
"21918","What's a good Python HMM library?","<p>I've looked at hmmlearn but I'm not sure if it's the best one. </p>
","<python><markov>","8460","Python library to implement Hidden Markov Models","<p>What <em>stable</em> Python library can I use to implement Hidden Markov Models? I need it to be reasonably well documented, because I've never really used this model before.</p>

<p>Alternatively, is there a more direct approach to performing a time-series analysis on a data-set using HMM?</p>
","<python><time-series><markov-process>"
"21986","Why CNN input images are often square shaped?","<p>Why it's so that in convolutional neural networks we generally take the image dimensions of input image to be generally a <em>square?</em> We even do padding to make it happen. Why not different dimensions?</p>

<p>What I understood is that computer computes multiplication and division (by 2) much faster than the rest.</p>

<p>Can someone shed some light on this? </p>

<p>Any link or reference will be appreciated. I already have CS231n notes and lecture slides. </p>
","<neural-network><deep-learning><convolution>","16601","Reason for square images in deep learning","<p>Most of the advanced deep learning models like VGG, ResNet, etc. require square images as input, usually with a pixel size of <span class=""math-container"">$224x224$</span>.</p>

<p>Is there a reason why the input has to be of equal shape, or can I build a convnet model with say <span class=""math-container"">$100x200$</span> as well (if I want to do facIAL recognition for example and I have portrait images)?</p>

<p>Is there increased benefit with a larger pixel size, say <span class=""math-container"">$512x512$</span>?</p>
","<deep-learning><image-classification><image-recognition>"
"22740","Missing Values in Data","<p>I have experienced that most of the datasets contain missing values, which make our task bit challenging.</p>

<p>Please let me know how to fill up those missing values in an efficient way? and is there any specific techniques to handle missing values?</p>
","<data-mining><dataset><data-cleaning><data><missing-data>","12797","Scikit Learn Missing Data - Categorical values","<p>I have a dataset containing categorical features, which has 4 labels, and 4 features. (It is a meta classifier, so outputs from base classifier serve as input into this classifier)</p>
<pre><code>Label  Feat1 Feat2 Feat3 Feat4
 1      1     1      2     2
 2      3     1      2     2 
 3      4     3      3     1     
 4      4     1      2     4
</code></pre>
<p>I'm using scikit learn, and am considering using Naive Bayes or a Decision Tree. The classifier needs to be able to deal with missing features, and I read on scikit learn's <a href=""http://scikit-learn.org/stable/modules/tree.html"" rel=""nofollow noreferrer"">page</a> that Decision Tree does not support missing values.</p>
<p>What I am looking for is advice as to how to approach missing categorical values when using scikit learn. Also, any links to academic papers addressing this would be appreciated.</p>
","<predictive-modeling><scikit-learn><categorical-data><multiclass-classification><missing-data>"
"22901","Loss function that penalizes under-predictions more than over-predictions?","<p>I have a regression problem where I am predicting a continuous variable. Loss functions used most often in these cases (RMSE, MAE, etc.) don't treat over- or under- predictions differently.</p>

<p>I am in a scenario where under-predicting would be a much worse outcome than over-predicting.</p>

<p>What type of loss function would appropriately capture this?</p>
","<machine-learning><predictive-modeling><regression><loss-function>","10471","Linear regression with non-symmetric cost function?","<p>I want to predict some value $Y(x)$ and I am trying to get some prediction $\hat Y(x)$ that optimizes between being as low as possible, but still being larger than $Y(x)$. In other words:
$$\text{cost}\left\{ Y(x) \gtrsim \hat Y(x) \right\}  &gt;&gt; \text{cost}\left\{ \hat Y(x) \gtrsim Y(x) \right\}  $$</p>

<p>I think a simple linear regression should do totally fine.
So I somewhat know how to implement this manually, but I guess I'm not the first one with this kind of problem. Are there any packages/libraries (preferably python) out there doing what I want to do?
What's the keyword I need to look for?</p>

<p>What if I knew a function $Y_0(x) &gt; 0$ where $Y(x) &gt; Y_0(x)$. What's the best way to implement these restrictions?</p>
","<machine-learning><logistic-regression>"
"22929","OneHotEncoder vs. LabelEncoder vs. LabelBinarizer","<p>I have a dataset that I have created in Python with a category called <code>change_label</code> that has values ""buy, hold, sell"" (or alternatively I could use -1,0,1). </p>

<p>I am wondering, what is the best way to fit this data so that machine learning algorithms can be applied to it to make predictions? </p>

<p>My options are label encoding, label binzarization, or one-hot encoding. Basically, I thought that simply using sci-kit learn's <code>LabelBinarizer()</code> would be enough. However, I want to be sure. Any advice? </p>
","<python><scikit-learn><encoding>","9443","When to use One Hot Encoding vs LabelEncoder vs DictVectorizor?","<p>I have been building models with categorical data for a while now and when in this situation I basically default to using scikit-learn's LabelEncoder function to transform this data prior to building a model.</p>

<p>I understand the difference between <code>OHE</code>, <code>LabelEncoder</code> and <code>DictVectorizor</code> in terms of what they are doing to the data, but what is not clear to me is when you might choose to employ one technique over another.  </p>

<p>Are there certain algorithms or situations in which one has advantages/disadvantages with respect to the others?</p>
","<scikit-learn><categorical-data><feature-engineering>"
"23933","How can I deal with circular features like hours?","<p>Assume I want to predict if I'm fit in the morning. One feature is the last time I was online. Now this feature is tricky: If I take the hour, then a classifier might have a difficult time with it because 23 is numerically closer to 20 than to 0, but actually the time 23 o'clock is closer to 0 o'clock.</p>

<p>Is there a transformation to make this more linear? Probably into multiple features? (Well, hopefully not 60 features if I do the same for minutes)</p>
","<feature-engineering>","5990","What is a good way to transform Cyclic Ordinal attributes?","<p>I am having 'hour' field as my attribute, but it takes a cyclic values. How could I transform the feature to preserve the information like '23' and '0' hour are close not far. </p>

<p>One way I could think is to do transformation: <code>min(h, 23-h)</code></p>

<pre><code>Input: [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]

Output: [0 1 2 3 4 5 6 7 8 9 10 11 11 10 9 8 7 6 5 4 3 2 1]
</code></pre>

<p>Is there any standard to handle such attributes?</p>

<p>Update: I will be using superviseed learning, to train random forest classifier!</p>
","<feature-extraction><feature-scaling><featurization>"
"24290","Product Recommendation based on purchase history","<p>I am dealing with problem where i have to increase the sales by product recommendation.I only have customer data and product that they have purchased.No ratings,reviews or feedback is present.What approach fit best for my problem.</p>
","<machine-learning>","19523","Recommender system based on purchase history, not ratings","<p>I'm exploring options for recommender systems optimized for the insurance industry, which would take into account</p>

<p>i) product holdings</p>

<p>ii) user characteristics (segment, age, affluence, etc.).</p>

<p>I want to stress that</p>

<p>a) there are no product ratings available, thus collaborative filtering is not an option</p>

<p>b) recommended products don't have to be similar to products that have already been purchased, thus item-to-item recommendations are most probably not relevant.</p>

<p>Keep in mind that in insurance you rarely want to recommend similar products to those already purchased ones, as someone with the Car insurance is unlikely to want to buy another Motor product, rather Home or maybe Travel, etc. </p>

<p><strong>That's why I want to develop recommendations on similarities between the users based on their purchase history and/or demographics</strong></p>

<p>Ideally, I'd like to be able to implement it in R, if not possible, then in Python. Thanks for help and suggestions!</p>
","<machine-learning><python><r><recommender-system>"
"24405","How to do stepwise regression using sklearn?","<p>I could not find a way to stepwise regression in scikit learn. I have checked all other posts on Stack Exchange on this topic. Answers to all of them suggests using f_regression.</p>

<p>But f_regression does not do stepwise regression but only give F-score and pvalues corresponding to each of the regressors, which is only the first step in stepwise regression.</p>

<p>What to do after 1st regressors with the best f-score is chosen?</p>
","<machine-learning><scikit-learn><regression><feature-selection><linear-regression>","937","Does scikit-learn have a forward selection/stepwise regression algorithm?","<p>I am working on a problem with too many features and training my models takes way too long. I implemented a forward selection algorithm to choose features.</p>
<p>However, I was wondering does scikit-learn have a forward selection/stepwise regression algorithm?</p>
","<feature-selection><scikit-learn>"
"26288","Machine Learning: Why do the error in cost function need to be squared?","<p>I have recently started Andrew NG's Machine Learning course on Coursera and I came across this cost function which is:</p>

<p><a href=""https://i.stack.imgur.com/fp2i0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fp2i0.png"" alt=""enter image description here""></a></p>

<p>Why does the error in the cost function need to be squared? If its purpose is to eliminate the negative sign in the error then why don't we simply use the absolute function?</p>
","<machine-learning><predictive-modeling><statistics>","10188","Why do cost functions use the square error?","<p>I'm just getting started with some machine learning, and until now I have been dealing with linear regression over one variable.</p>

<p>I have learnt that there is a hypothesis, which is:</p>

<p>$h_\theta(x)=\theta_0+\theta_1x$</p>

<p>To find out good values for the parameters $\theta_0$ and $\theta_1$ we want to minimize the difference between the calculated result and the actual result of our test data. So we subtract</p>

<p>$h_\theta(x^{(i)})-y^{(i)}$</p>

<p>for all $i$ from $1$ to $m$. Hence we calculate the sum over this difference and then calculate the average by multiplying the sum by $\frac{1}{m}$. So far, so good. This would result in:</p>

<p>$\frac{1}{m}\sum_{i=1}^mh_\theta(x^{(i)})-y^{(i)}$</p>

<p>But this is not what has been suggested. Instead the course suggests to take the square value of the difference, and to multiply by $\frac{1}{2m}$. So the formula is:</p>

<p>$\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$</p>

<p>Why is that? Why do we use the square function here, and why do we multiply by $\frac{1}{2m}$ instead of $\frac{1}{m}$?</p>
","<machine-learning><linear-regression><loss-function>"
"26388","Layman's Interpretation of XGBoost Importance","<p>I'm trying to come up with a good way to explain the 3 importance metrics <a href=""https://xgboost.readthedocs.io/en/latest/R-package/discoverYourData.html#build-the-feature-importance-data-table"" rel=""noreferrer"">(Gain, Cover, Frequency)</a> to a layman with only a basic understanding of XGBoost and trees in general.  How best would you frame this explanation?</p>
","<python><r><xgboost>","12318","How to interpret the output of XGBoost importance?","<p>I ran a xgboost model. I don't exactly know how to interpret the output of <code>xgb.importance</code>.</p>

<p>What is the meaning of Gain, Cover, and Frequency and how do we interpret them?</p>

<p>Also, what does Split, RealCover, and RealCover% mean? I have some extra parameters <a href=""https://raw.githubusercontent.com/pommedeterresautee/xgboost/master/R-package/vignettes/discoverYourData.Rmd"" rel=""noreferrer"">here</a></p>

<p>Are there any other parameters that can tell me more about feature importances?</p>

<p>From the R documentation, I have some understanding that Gain is something similar to Information gain and Frequency is number of times a feature is used across all the trees. I have no idea what Cover is.</p>

<p>I ran the example code given in the link (and also tried doing the same on the problem that I am working on), but the split definition given there did not match with the numbers that I calculated.</p>

<pre><code>importance_matrix
</code></pre>

<p>Output:</p>

<pre><code>           Feature         Gain        Cover    Frequence
  1:            xxx 2.276101e-01 0.0618490331 1.913283e-02
  2:           xxxx 2.047495e-01 0.1337406946 1.373710e-01
  3:           xxxx 1.239551e-01 0.1032614896 1.319798e-01
  4:           xxxx 6.269780e-02 0.0431682707 1.098646e-01
  5:          xxxxx 6.004842e-02 0.0305611830 1.709108e-02

214:     xxxxxxxxxx 4.599139e-06 0.0001551098 1.147052e-05
215:     xxxxxxxxxx 4.500927e-06 0.0001665320 1.147052e-05
216:   xxxxxxxxxxxx 3.899363e-06 0.0001536857 1.147052e-05
217: xxxxxxxxxxxxxx 3.619348e-06 0.0001808504 1.147052e-05
218:  xxxxxxxxxxxxx 3.429679e-06 0.0001792233 1.147052e-05
</code></pre>
","<machine-learning><xgboost>"
"28958","Re: Logistic Regression","<p>I am working on a dataset that has a dependent variable that is binary, but it contains 98% of 0's and 2% of 1's. I am trying to use Logistic regression to predict purchase of a product. But because of the huge number of 0's, the model is not predicting well and getting a large number of false positive result.</p>

<p>Kindly suggest how can I approach this.</p>
","<classification><logistic-regression>","24905","Best methods to solve class imbalance problem and why?","<p>I have a data set where I need to detect fraud. 99% are not fraud and 1% are.</p>

<p>What methods can be used to solve problems where classes are imbalanced?</p>
","<machine-learning><python><statistics><class-imbalance>"
"31288","Affect of multi-Col linearity in Machine Learning Algorithms","<p>This is my general understanding on the affect of Multi-Col linearity on Linear and Logistic regression. However I am still not able to understand whether multi-col linearity is a problem in decision tree (Classification and regression).</p>

<p>Linear Regression - It has a huge impact as the co-efficient might vary drastically if there are 2 variables with high correlation. And regularisation would overcome this issue.
Logistic Regression - Same as Linear regression.</p>

<p>Can anybody please give me some insight on how it affects in decision tree classifier and decision tree regressor. And how to overcome this problem if it affects. </p>
","<machine-learning><classification><regression><decision-trees>","31402","Multicollinearity in Decision Tree","<p>Can anybody please explain the affect of multicollinearity on Decision Tree algorithms (Classification and regression).
I have done some searching but was not able to find the right answer as some say it affects it and others say it doesn't.</p>
","<decision-trees>"
"31803","keras categorical CNN prediction always output [[ 0. 0. 0. 0. 0. 0. 1. 0.]] No probability for other classes?","<pre><code>print(model.predict(image))
print(model.predict_proba(image))
print(model.predict_classes(image))
</code></pre>

<p>Using the above different predict methods I always get predictions like</p>

<blockquote>
  <p>[[ 1.  0.  0.  0.]]</p>
</blockquote>

<p>but I want the probabilities of each classes something like this</p>

<blockquote>
  <p>[[ 0.8  0.05  0.05  0.1]]</p>
</blockquote>

<p>This is the model</p>

<pre><code>model = Sequential([
    Conv2D(32, (3, 3), input_shape=input_shape, padding='same',activation='relu'),
    Conv2D(32, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(64, (3, 3), activation='relu', padding='same', ),
    Conv2D(64, (3, 3), activation='relu', padding='same', ),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(128, (3, 3), activation='relu', padding='same', ),
    Conv2D(128, (3, 3), activation='relu', padding='same', ),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(256, (3, 3), activation='relu', padding='same', ),
    Conv2D(256, (3, 3), activation='relu', padding='same', ),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Conv2D(512, (3, 3), activation='relu', padding='same', ),
    Conv2D(512, (3, 3), activation='relu', padding='same', ),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(num_classes, activation='softmax')
])
</code></pre>

<p>Compiled as </p>

<pre><code>sgd = SGD(lr=0.0001,momentum=0.9)
model.compile(loss = ""categorical_crossentropy"", optimizer =sgd,metrics=['accuracy'])
</code></pre>

<p>Not sure if this has any effects, but this is how I loaded the data</p>

<blockquote>
  <p>train_datagen = ImageDataGenerator(
      rescale=1./255,
  )</p>
  
  <p>train_generator = train_datagen.flow_from_directory(folder+'apex/train',
                                                      target_size=(200, 400),
                                                      batch_size=batch_size,
                                                      class_mode='categorical')</p>
</blockquote>
","<machine-learning><keras><cnn><probability>","25841","Keras - no prediction probability for multiple output models?","<p>I have built the following model:</p>

<pre><code>def create_model(conv_kernels = 32, dense_nodes = 512):

    model_input=Input(shape=(img_channels, img_rows, img_cols))
    x = Convolution2D(conv_kernels, (3, 3), padding ='same', kernel_initializer='he_normal')(model_input)
    x = Activation('relu')(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
    x = Convolution2D(conv_kernels, (3, 3), kernel_initializer='he_normal')(x)
    x = Activation('relu')(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
    x = Dropout(0.25)(x)
    x = Flatten()(x)

    conv_out = (Dense(dense_nodes, activation='relu', kernel_constraint=maxnorm(3)))(x)

    x1 = Dense(nb_classes, activation='softmax')(conv_out)
    x2 = Dense(nb_classes, activation='softmax')(conv_out)
    x3 = Dense(nb_classes, activation='softmax')(conv_out)
    x4 = Dense(nb_classes, activation='softmax')(conv_out)

    lst = [x1, x2, x3, x4]

    model = Model(inputs=model_input, outputs=lst)
    sgd = SGD(lr=lrate, momentum=0.9, decay=lrate/nb_epoch, nesterov=False)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

    return model
</code></pre>

<p>When I do a prediction:</p>

<pre><code>model.predict(X_test)
</code></pre>

<p>it works properly. However, when I want to get prediction probability like this:</p>

<pre><code>model.predict_proba(X_test)
</code></pre>

<p>my model has no predict_proba function. Why not? Is it because of the multiple-output nature of the model?</p>
","<machine-learning><python><deep-learning><keras>"
"31880","Rephrase Neural Network: Where to Start?","<p>I'd like to solve the problem that denoted in the below picture.</p>

<p><a href=""https://i.stack.imgur.com/Fmf6M.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fmf6M.png"" alt=""enter image description here""></a></p>

<p>With which network could I try and start to solve this problem?</p>
","<deep-learning>","31274","Rephrase Neural Network : Find the Fittest Word for Given Meaning/Explanation","<p>I am a sophomore student who's interested in deep learning and its method layering up some linear/non-linear operations and constructs up the complex function through the network.</p>

<p>I'd like to comprise a network that finds the word upon the given explanations :</p>

<p>e.g.1 input: a spherically shaped fruit which tastes sour and sweet and gets red when matured well output: apple</p>

<p>e.g.2, input: to make up the boundary between multiple options for the base of one's behavior or decision output: determine.</p>

<p>So basically it's a classification problem but the class might be more than 20,000.</p>

<p>I'd like to name this task ""rephrase neural network"".</p>

<p>Any good reference or starting point, or, any network recommendation for comprising this neural network?</p>
","<deep-learning>"
"32129","How to come up with the splitting point in a decision tree?","<p>I read <a href=""https://www.researchgate.net/post/How_to_compute_impurity_using_Gini_Index"" rel=""nofollow noreferrer"">https://www.researchgate.net/post/How_to_compute_impurity_using_Gini_Index</a></p>

<p>I understand why choosing smallest gini index, but how do I come up with different candidate splits in the first place? How does R come up with the splits? Take the iris data as an instance:</p>

<pre><code>plot(iris$Sepal.Length, col=iris$Species, pch=20)
</code></pre>

<p><a href=""https://i.stack.imgur.com/ZSQsd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZSQsd.png"" alt=""enter image description here""></a></p>

<p>How do I determine where I want to draw the horizontal line to separate each class? I can draw infinitely amount of lines and then compare their gini index, but practically, it will not work.</p>
","<decision-trees>","24339","How is a splitting point chosen for continuous variables in decision trees?","<p>I have two questions related to decision trees:</p>

<ol>
<li><p>If we have a continuous attribute, how do we choose the splitting value?</p>

<p>Example: Age=(20,29,50,40....)</p></li>
<li><p>Imagine that we have a continuous attribute $f$ that have values in $R$. How can I  write an algorithm that finds the split point $v$, in order that when we split $f$ by $v$, we have a minimum gain for $f&gt;v$?</p></li>
</ol>
","<classification><data><decision-trees>"
"32232","What is the best references / books to get familiar with machine learning algorithms?","<p>I'm just starting off as a data scientist and i need to understand how regression, random forest and much more.. algorithms function under the hood, the maths behind them and when to use them. I started with some MOOC courses but not a fan, it's just not my style of learning as i need practical examples or books i can dive into with detailed examples i can just apply myself after understanding how each algorithm works and some 20 mins vids with scarce maths explanation is not what i aim for, i've done some research and ended up with these books:</p>

<p>-python for data analysis</p>

<p>-Python Crash Course</p>

<p>-oreilly hands on machine learning with scikit learn and tensorflow</p>

<p>-automate the boring stuff with python</p>

<p>kindly note that i have some python experience(less than a year) and  unfamiliar with tensorflow , keras, scikit-learn etc.. </p>

<p>Kindly also note that i didn't mention any maths books to explain how the machine learning algorithms work so i'm open to suggestions  .</p>
","<machine-learning><python><statistics><machine-learning-model>","26449","Beginner math books for Machine Learning","<p>I'm a Computer Science engineer with no background in statistics or advanced math.</p>

<p>I'm studying the book <a href=""https://sebastianraschka.com/books.html#python-machine-learning-3rd-edition"" rel=""nofollow noreferrer"">Python Machine Learning</a> by Raschka and Mirjalili, but when I tried to understand the math of the Machine Learning, I wasn't able to understand the great book that a friend suggest me <a href=""https://web.stanford.edu/~hastie/ElemStatLearn/"" rel=""nofollow noreferrer"">The Elements of Statistical Learning</a>.</p>

<p>Do you know any easier statistics and math books for Machine Learning? If you don't, how should I move?</p>
","<machine-learning><statistics><reference-request><mathematics><esl>"
"34178","Bias and Variance definitions","<p>Is it correct to say that, in the Bias-Variance trade-off:</p>

<p>the <strong>bias</strong> error represents the ability of the model to possibly map the trend of the training set ? </p>

<p>and the <strong>variance</strong> represents how much the model is prone to variations if the dataset changes ?</p>
","<machine-learning><deep-learning><data-mining>","10849","What are the relationships/differences between Bias, Variance and Residuals?","<p>I've been trying to find an answer to this question for a long time.</p>
<blockquote>
<p>What are the relationships/differences between Bias, Variance and
Residuals?</p>
</blockquote>
<p>I think I do understand Bias, Variance and Residuals as separate concepts. Correct me if I'm wrong - <br><br>
<b>Bias</b> is the difference between the average expected results from different runs of the model and the true values from data. <br><br>
<b>Variance</b> is the variability in the expected results (predictions) of a given data point between different runs of the model. <br><br>
<b>Residual</b> is the difference between the expected results from a model and the true values from data.<br> y - y^ <br>Residual seems somewhat similar to Bias. But is that my misunderstanding?<br><br></p>
<p>Please explain to me in plain words and also simple equations the relationships or differences between these three.</p>
","<machine-learning><predictive-modeling><statistics>"
"36656","Featuring scaling whole data set before spliting it.","<p>I am wondering why do we use scaling on train and test set separately. I understand that transform () on test data  μ and σ as computed from fit_transform() on  Train. But why can we compute μ and σ from all given data (before split) and then apply them on future data. </p>

<p>Do we do this because we don't know how the size of our future data?</p>
","<machine-learning><python><scikit-learn>","29055","Data Snooping, Information Leakage When Performing Feature Normalization","<p>Assume that we have a training data set (with both features and labels) and a test data set (with only features). </p>

<p>When we build a machine learning model that requires normalization of the features, the correct way of doing normalization (to prevent information leakage) is to only use the training data set. Namely, a wrong way of doing normalization is to stack the training (exclude the Y column) and test data set together, and perform normalization (i.e., using the mean and variance of the entire training + test data set). The intuition here is very clear: if you want to get an unbiased estimator of your model performance in production, then when you train your model, you shouldn't instill any information in the test data set that will be used to gauge the actual performance of your model.</p>

<p><strong>My question is the following:</strong> When we have trained the model correctly and about to use this model to do prediction, how should we normalize the test data set? I believe the correct way to normalize the test data set is: using the mean and variance obtained from the training data set to do normalization on the test data set.</p>

<p>However, why not just normalize the test data using its own mean and variance? Or why not stack train and test data set together and using the overall mean and variance to normalize the test data set? In the prediction stage, the idea and intuition of data snooping and information leakage is not clear to me.</p>
","<machine-learning><cross-validation>"
"36684","How to give names/labels to topics in LDA","<p>I want to give labels to different topics created using LDA. I don't want to do it manually. I saw some papers on automatic labeling but I am still confused. </p>

<p>How can I use the information produced by LDA to automatically generate names to the topics generated?</p>
","<data-mining><text-mining><topic-model><lda>","9663","How to give name to topics created using LDA?","<p>I have categorized 800,000 documents into 500 categories using the Mahout topic modelling.</p>

<p>Instead of representing the topic using the top 5/10 words for each topics, I want to infer a generic name for the group using any existing algorithm.
For the time being, I have used the following algorithm to arrive at the name for the topic:</p>

<p>For each topic</p>

<ul>
<li>Take all the documents belonging to the topic (using the document-topic distribution output)</li>
<li>Run python nltk to get the noun phrases</li>
<li>Create the TF file from the output</li>
<li>name for the topic is the phrase (limited towards max 5 words)</li>
</ul>

<p>Please suggest a approach to arrive at more relevant name for the topics.</p>
","<machine-learning><data-mining><nlp><text-mining><topic-model>"
"36953","How to determine the k in kNN","<p>I'm looking to use the k-Nearest Neighbors (kNN) algorithm.  What are the possible methods for determining the best K?  From what I have read, looking at many different values(say 10-100) should work, but that's a big range.   </p>

<p>How can I reduce the range of possible K's I should test?</p>
","<machine-learning><k-nn>","36049","How to adjust the hyperparameters of MLP classifier to get more perfect performance","<p>I am just getting touch with Multi-layer Perceptron. And, I got this accuracy when classifying the DEAP data with MLP. However, I have no idea how to adjust the hyperparameters for improving the result. </p>

<p>Here is the detail of my code and result:</p>

<p><a href=""https://i.stack.imgur.com/nD3Ow.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nD3Ow.png"" alt=""enter image description here""></a>.</p>

<pre><code>from sklearn.neural_network import MLPClassifier

import numpy as np
import scipy.io
x_vals = data['all_data'][:,0:320]

y_vals_new = np.array([0 if each=='Neg'  else 1 if each =='Neu' else 2 for each in data['all_data'][:,320]])
y_vals_Arousal = np.array([3 if each=='Pas'  else 4 if each =='Neu' else 5 for each in data['all_data'][:,321]])

DEAP_x_train = x_vals[:-256]        #using 80% of whole data for training
DEAP_x_test = x_vals[-256:]         #using 20% of whole data for testing
DEAP_y_train = y_vals_new[:-256]     ##Valence
DEAP_y_test = y_vals_new[-256:]
DEAP_y_train_A = y_vals_Arousal[:-256]   ### Arousal
DEAP_y_test_A = y_vals_Arousal[-256:]

mlp = MLPClassifier(solver='adam', activation='relu',alpha=1e-4,hidden_layer_sizes=(50,50,50), random_state=1,max_iter=11,verbose=10,learning_rate_init=.1)

mlp.fit(DEAP_x_train, DEAP_y_train)

print (mlp.score(DEAP_x_test,DEAP_y_test))
print (mlp.n_layers_)
print (mlp.n_iter_)
print (mlp.loss_)
</code></pre>
","<scikit-learn><hyperparameter-tuning><mlp>"
"37741","How is bias added in a convolutional layer","<p>In a typical neural network , bias is usally added like this</p>

<pre><code> v = activation ( w1*x1+ ... + Wb*b)
</code></pre>

<p>However, I am not really sure how it is done in convolutional layer. My one thought is that it is added with each convoltional operation for a neuron. Is that correct?</p>
","<cnn>","24494","Should the bias value be added after convolution operation in CNNs?","<p>Should we add bias to each entry of the convolution then sum, or add bias once at end of calculating the convolution in CNNs?</p>
","<machine-learning><deep-learning><cnn>"
"31725","How to understand backpropagation using derivative","<p>Before I was learning about gradient descent, but now I understand this. Now, I have a problem with the backpropagation algorithm. I know the idea - minimalize error in multilayer neural network using chain rule. However, I don't understand the role of the derivative of the sigmoid function. This derivative is described in the algorithm. What is the point of this? Can you explain this step by step using simple language?</p>

<p><a href=""https://i.stack.imgur.com/H5usA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H5usA.png"" alt=""enter image description here""></a></p>
","<machine-learning><neural-network><backpropagation>","30676","Role derivative of sigmoid function in neural networks","<p>I try to understand role of derivative of sigmoid function in neural networks. 
<a href=""https://i.stack.imgur.com/N8vbm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/N8vbm.png"" alt=""enter image description here""></a></p>

<p>First I plot sigmoid function, and derivative of all points from definition using python. What is the role of this derivative exactly? 
<a href=""https://i.stack.imgur.com/inMoa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/inMoa.png"" alt=""enter image description here""></a></p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def derivative(x, step):
    return (sigmoid(x+step) - sigmoid(x)) / step

x = np.linspace(-10, 10, 1000)

y1 = sigmoid(x)
y2 = derivative(x, 0.0000000000001)

plt.plot(x, y1, label='sigmoid')
plt.plot(x, y2, label='derivative')
plt.legend(loc='upper left')
plt.show()
</code></pre>
","<machine-learning><neural-network>"
"31349","plateaus and local minima in deep learning","<p>Why training deep learning models is more likely to suffer from plateaus than local minima? </p>

<p>Could anyone illustrate it.</p>
","<machine-learning><deep-learning>","22853","local minima vs saddle points in deep learning","<p>I heard Andrew Ng (in a video I unfortunately can't find anymore) talk about how the understanding of local minima in deep learning problems has changed in the sense that they are now regarded as less problematic because in high-dimensional spaces (encountered in deep learning) critical points are more likely to be saddle points or plateaus rather than local minima. </p>

<p>I've seen papers (e.g. <a href=""http://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf"" rel=""noreferrer"">this one</a>) that discuss assumptions under which ""every local minimum is a global minimum"". These assumptions are all rather technical, but from what I understand they tend to impose a structure on the neural network that make it somewhat linear.</p>

<p>Is it a valid claim that, in deep learning (incl. nonlinear architectures), plateaus are more likely than local minima? And if so, is there a (possibly mathematical) intuition behind it? </p>

<p><em>Is there anything particular about deep learning and saddle points?</em></p>
","<machine-learning><deep-learning><optimization><convergence>"
"42548","Deep learning: Training in batches","<p>How does training in batches help in obtaining a better deep learning model? What should one keep in mind while splitting data into batches?</p>
","<machine-learning><neural-network><deep-learning><tensorflow>","16807","Why mini batch size is better than one single ""batch"" with all training data?","<p>I often read that in case of Deep Learning models the usual practice is to apply mini batches (generally a small one, 32/64) over several training epochs. I cannot really fathom the reason behind this.</p>

<p>Unless I'm mistaken, the batch size is the number of training instances let seen by the model during a training iteration; and epoch is a full turn when each of the training instances have been seen by the model. If so, I cannot see the advantage of iterate over an almost insignificant subset of the training instances several times in contrast with applying a ""max batch"" by expose all the available training instances in each turn to the model (assuming, of course, enough the memory). What is the advantage of this approach?</p>
","<machine-learning><deep-learning>"
"42831","What are the assumptions of linear regression","<p>Can anyone explain the <strong>assumptions of linear regressions</strong>?  </p>

<p>If possible with an example?</p>

<p>Is this really important to check these assumptions before proceeding?</p>
","<machine-learning><linear-regression>","32295","Assumptions of linear regression","<p>In simple terms, what are the assumptions of Linear Regression?</p>

<p>I just want to know that when I can apply a linear regression model to our dataset.</p>
","<linear-regression>"
"43110","Problem upgrading pip command in Azure Jupyter notebook","<p>Trying to install CNTK, I received a message saying that I should upgrade pip command <a href=""https://datascience.stackexchange.com/questions/43068/problem-importing-cntk-in-azure-jupyter-notebook"">here</a></p>
<p>This one is another question related to the upgrading of pip.</p>
<p>After the installation with the command</p>
<pre><code>pip install cntk
</code></pre>
<p>I received another error message as shown in the attached image.</p>
<p><a href=""https://i.stack.imgur.com/ayg9c.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ayg9c.jpg"" alt=""enter image description here"" /></a></p>
","<machine-learning><jupyter><pip>","43068","Problem importing CNTK in Azure jupyter notebook","<p>Vineeth Sai indicated in <a href=""https://stackoverflow.com/questions/53316927/no-module-named-cntk"">this</a> that with the following code:</p>

<pre><code>pip install cntk 
</code></pre>

<p>the problem is solved. However, I am getting the error shown in attached image:</p>

<p><a href=""https://i.stack.imgur.com/RC6YJ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RC6YJ.jpg"" alt=""enter image description here""></a></p>
","<python><jupyter><azure-ml><pip>"
"45471","xgboost or lightgbm to handle Binomial problems","<p>I have a dataset containing a column of trials, a column of successes and other features; and, obviously, I can generate a probability column. I would like to use gradient boosting methods (like xgboost or lightgbm) to model the success probability. Which parameter shall I set to handle this in lightgbm or xgboost?</p>
","<xgboost><probability><hyperparameter>","57023","xgboost: Is there a way to perform regression on rates/percentages data?","<p>I have a dependent variable, <span class=""math-container"">$Y$</span>, that is made up of rates/percentages data, so each value is between <span class=""math-container"">$0$</span> and <span class=""math-container"">$1$</span>. I was attracted to the xgboost library because it allows focusing in on specific subsets of the data in training itself, but I am stumped as how to perform regression on the data I have.</p>

<p>Normal OLS regression will produce outputs that will be over and under the [0,1] range if you do not change the likelihood to be the <a href=""https://en.wikipedia.org/wiki/Beta_distribution"" rel=""noreferrer"">Beta distribution</a>, or something else bounded in the same range, but will xgboost suffer the same mistake?</p>

<p>Any advice on trying to get this to work would be greatly appreciated by me.</p>
","<regression><linear-regression><xgboost><distribution>"
"46208","Proper order of optimizing hyperparameters in ANN","<p>I have built a neural network for approximating a certain function and decided on a metric how to evaluate the performance. Now, where do I start with optimizing to get the optimal result? I need to decide on number of layers, hidden neurons, learning rate, learning algorithms and all other hyperparameters. What is the usual  procedure and order in which to optimize all network's parameters? I guess I am looking for some kind of general guidelines to follow to obtain the best possible performance</p>
","<machine-learning><neural-network>","27722","Is there a thumb-rule for designing neural-networks?","<p>I know that a neural-network architecture is mostly based on the problem itself and the types of input/output, but still - there's always a ""square one"" when starting to build one. So my question is - given a input dataset of <em>MxN</em> (M  is the number of records, N is the number of features) and a C possible output classes - is there a thumb-rule to how many layers/units should we start with? </p>
","<neural-network>"
"46218","For a multi-class classification problem, how to transform the target variable to a form that is usable by sklearn algorithms?","<p>I recently tried to create a model for predicting what class a sample belongs to out of 160 possible classes. These classes of the target variable are just simple strings describing workouts like ""Push-ups"", ""Sit-ups"" etc. I used sklearn's method LabelEncoder to transform the target variable, and it assigned every class a number between 0 and 159. I know that the reason why we are supposed to do one-hot encoding for categorical <em>features</em> is to avoid telling an algorithm something nonsensical like ""blue is greater than red"". My question then becomes, doesn't LabelEncoder tell the sklearn algorithm that ""push-ups"" is less than or greater than ""sit-ups"" if both of them are assigned distinct numbers between 0-159? Why or why not? This feels like a very stupid question, but I had real trouble finding an answer anywhere online. </p>
","<scikit-learn><multiclass-classification><encoding>","18456","Encode multi-class response variable","<p>In a classification problem when the response variable has multi-class, e.g., ""sunny"",""rainy"",""cloudy"", how should we encode it? I know that for predictors like this, usually we do <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""nofollow noreferrer"">One Hot Encoding</a>, and if a predictors have too many classes, then we might just use the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""nofollow noreferrer"">LabelEncode</a>. </p>

<p>When this multi-class issue occurs in the response variable, I guess we can just use LabelEncode() rather use using One Hot encoding right? Because if we use One Hot encoding, then we will have 2 variables as the response variable, and the machine learning algorithm in sklearn usually expects the response variable not to be a vector right? (I mean it expects a long 1D vector with length equal to the number of observations but not a 2D matrix). But on the other hand, if we just map ""sunny"", ""rainy"",""cloudy"" to {1,2,3} or {0,1,2}, or whatever 3 numbers, that will create a less than or greater than relationship among ""sunny"", ""rainy"",""cloudy"", which is not inherited in the original problem.</p>
","<scikit-learn><multiclass-classification><prediction>"
"27958","Perceptron learning rate irrelevant in convergence","<p>Via this <a href=""https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/lec2.pdf"" rel=""nofollow noreferrer"">MIT document</a> i studied the single layer Perceptron convergence proof (= maximum number of steps).</p>

<p>In the convergence proof inside this document , the learning rate is implicitly defined as 1 .
After studying it, by myself i tried to re-do the proof inserting this time a generic learning rate $\eta $.</p>

<p>The result is that the result remains the same:</p>

<p>$k\leq \frac{R^{2}\left \| \theta ^{*} \right \|^{2}}{\gamma ^{2}}$</p>

<p>that is the learning rate $\eta$ cancelled out in the proof.</p>

<p>Is it possible , or i make mistakes in my proof ?</p>
","<machine-learning><perceptron>","27898","Normalizing the final weights vector in the upper bound on the Perceptron's convergence","<p>The convergence of the ""simple"" <a href=""https://en.wikipedia.org/wiki/Perceptron#Convergence"" rel=""nofollow noreferrer"">perceptron</a> says that:</p>

<p>$$k\leqslant \left ( \frac{R\left \| \bar{\theta} \right \|}{\gamma } \right )^{2}$$</p>

<p>where $k$ is the number of iterations (in which the weights are updated), $R$ is the maximum distance of a sample from the origin, $\bar{\theta}$ is the final weights vector, and  $\gamma$ is the smallest distance from $\bar{\theta}$ to a sample (= the margin of hyperplane).</p>

<p>Many books implicitly say that  $\left \| \bar{\theta} \right \|$ is equal to 1. But why do they normalize it ?</p>
","<machine-learning><perceptron><convergence>"
"47372","How does back propagation works through layers like maxpooling and padding?","<p>I know back propagation takes derivatives (changing one quantity wrt other). But how this is applied when there is maxpooling layer in between two Conv2D layers? How it gains its original shape when there is padding added?   </p>
","<machine-learning><deep-learning><keras><tensorflow><backpropagation>","13587","Back-propagation through max pooling layers","<p>I have a small sub-question to <a href=""https://datascience.stackexchange.com/questions/11699/backprop-through-max-pooling-layers?newreg=a19724e3c92a4b71ad72b7128751da1c"">this question</a>. </p>

<p>I understand that when back-propagating through a max pooling layer the gradient is routed back in a way that the neuron in the previous layer which was selected as max gets all the gradient. What I'm not a 100% sure of is how the gradient in the next layer gets routed back to the pooling layer.</p>

<p>So the first question is if I have a pooling layer connected to a fully connected layer - like the image below.</p>

<p><a href=""https://i.stack.imgur.com/hJUT8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hJUT8.png"" alt=""example1""></a></p>

<p>When computing the gradient for the cyan ""neuron"" of the pooling layer do I sum all the gradients from the FC layer neurons? If this is correct then every ""neuron"" of the pooling layer has the same gradient?</p>

<p>For example if the first neuron of FC layer has a gradient of 2, second has a gradient of 3, and third a gradient of 6. What are the gradients of the blue and purple ""neurons"" in the pooling layer and why?</p>

<p>And the second question is when the pooling layer is connected to another convolution layer. How do I compute the gradient then? See the example below.</p>

<p><a href=""https://i.stack.imgur.com/7N4nM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7N4nM.png"" alt=""example2""></a></p>

<p>For the topmost rightmost ""neuron"" of the pooling layer (the outlined green one) I just take the gradient of the purple neuron in the next conv layer and route it back, right?</p>

<p>How about the filled green one? I need to multiply together the first column of neurons in the next layer because of the chain rule? Or do I need to add them?</p>

<p>Please do not post a bunch of equations and tell me that my answer is right in there because I've been trying to wrap my head around equations and I still don't understand it perfectly that's why I'm asking this question in a simple way.</p>
","<machine-learning><neural-network><convolutional-neural-network><backpropagation>"
"51499","Custom loss function for XGBoost","<p>I am looking for code to implement a custom loss function instead of just classification error, or cross entropy for gradient boosted classification trees.
We are trying to model regime detection in financial markets based on economic indicators. The intended loss function is that if the model outputs a probability of each class of risk as <code>p_low</code>, <code>p_medium</code>, and <code>p_high</code>, then the loss function we are looking for is <code>( p_low * Returns(portfolio_low_risk) + p_medium * Returns(portfolio_medium_risk) + p_high * Returns(portfolio_high_risk) )</code>
We already have a data frame of Returns(portfolio_i_risk).</p>
","<xgboost>","30630","changing cost function in xgboost","<p>I'm using the newest version of xgboost package in python 2.7 and based on my problem, I'm going to change xgboost cost function to use my own defined cost function. Couple of questions:</p>

<p>In which part of the xgboost package should I find and change the cost function?
After changing the cost function, how can I add the updated code to the xgboost package to use the new training function instead of xgboost's pre-defined function?</p>
","<machine-learning><python><xgboost>"
"26886","ValueError: Input contains NaN, infinity or a value too large for dtype('float64')","<p>I am trying to fit my data into my model which takes numpy as input, so I feed the model with the dataframe values</p>

<pre><code>stacked_averaged_models.fit(train.values, y_train1)
</code></pre>

<p>I am getting the following error</p>

<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-145-9ba69af8df05&gt; in &lt;module&gt;()
      1 X_traintrain = train.as_matrix().astype(np.float)
      2 from sklearn.metrics import r2_score
----&gt; 3 stacked_averaged_models.fit(train.values, y_train1)
      4 stacked_train_pred = stacked_averaged_models.predict(train.values)
      5 stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))

&lt;ipython-input-140-dfca4af6e9d1&gt; in fit(self, X, y)
     18                 instance = clone(model)
     19                 self.base_models_[i].append(instance)
---&gt; 20                 instance.fit(X[train_index], y[train_index])
     21                 y_pred = instance.predict(X[holdout_index])
     22                 out_of_fold_predictions[holdout_index, i] = y_pred

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\pipeline.py in fit(self, X, y, **fit_params)
    248         Xt, fit_params = self._fit(X, y, **fit_params)
    249         if self._final_estimator is not None:
--&gt; 250             self._final_estimator.fit(Xt, y, **fit_params)
    251         return self
    252 

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\linear_model\coordinate_descent.py in fit(self, X, y, check_input)
    705                              order='F', dtype=[np.float64, np.float32],
    706                              copy=self.copy_X and self.fit_intercept,
--&gt; 707                              multi_output=True, y_numeric=True)
    708             y = check_array(y, order='F', copy=False, dtype=X.dtype.type,
    709                             ensure_2d=False)

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\utils\validation.py in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)
    574     if multi_output:
    575         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,
--&gt; 576                         dtype=None)
    577     else:
    578         y = column_or_1d(y, warn=True)

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)
    451                              % (array.ndim, estimator_name))
    452         if force_all_finite:
--&gt; 453             _assert_all_finite(array)
    454 
    455     shape_repr = _shape_repr(array.shape)

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\utils\validation.py in _assert_all_finite(X)
     42             and not np.isfinite(X).all()):
     43         raise ValueError(""Input contains NaN, infinity""
---&gt; 44                          "" or a value too large for %r."" % X.dtype)
     45 
     46 

ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
</code></pre>

<p>I did a check on NaN and infinity, it did pass the test</p>

<pre><code>X_traintrain = train.as_matrix().astype(np.float)
print(np.any(np.isnan(X_traintrain)))
print(np.all(np.isfinite(X_traintrain)))
</code></pre>

<p>Output:</p>

<pre><code>False
True
</code></pre>

<p>How else can I solve, or at least, debug this?</p>

<pre><code>X1      X2       X3     X4       X5    X6   X7     X8   Y1      Y2
0.64    784.00  343.00  220.50  3.50    5   0.00    0   10.56   16.67
0.62    808.50  367.50  220.50  3.50    2   0.00    0   8.60    12.07
0.62    808.50  367.50  220.50  3.50    5   0.00    0   8.50    12.04
0.98    514.50  294.00  110.25  7.00    2   0.10    1   24.58   26.47
</code></pre>

<p>This is few rows of my dataset</p>
","<machine-learning><python><scikit-learn><pandas><numpy>","11928","ValueError: Input contains NaN, infinity or a value too large for dtype('float32')","<p>I got ValueError when predicting test data using a RandomForest model.</p>

<p>My code:</p>

<pre><code>clf = RandomForestClassifier(n_estimators=10, max_depth=6, n_jobs=1, verbose=2)
clf.fit(X_fit, y_fit)

df_test.fillna(df_test.mean())
X_test = df_test.values  
y_pred = clf.predict(X_test)
</code></pre>

<p>The error:</p>

<pre><code>ValueError: Input contains NaN, infinity or a value too large for dtype('float32').
</code></pre>

<p>How do I find the bad values in the test dataset? Also, I do not want to drop these records, can I just replace them with the mean or median?</p>

<p>Thanks.</p>
","<python><scikit-learn><pandas><random-forest><python-3.x>"
"52157","Why do we have to divide by 2 in the ML squared error cost function?","<p><a href=""https://i.stack.imgur.com/XtBWr.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/XtBWr.png"" alt=""""></a></p>

<p>I'm not sure why you need to multiply by <span class=""math-container"">$\frac1{2m}$</span> in the beginning. I understand that you would have to divide the whole sum by <span class=""math-container"">$\frac1{m}$</span>, but why do we have to multiply <span class=""math-container"">$m$</span> by two?</p>

<p>Is it because we have two <span class=""math-container"">$\theta$</span> here in the example?</p>
","<machine-learning><cost-function>","29526","Why is there a $2$ at the denominator of the mean squared error function?","<p>In the famous <a href=""http://neuralnetworksanddeeplearning.com/chap1.html"" rel=""noreferrer"">Deep Learning Book</a>, in chapter 1, equation 6, the Quadratic Cost (or Mean Squared Error) in a neural network is defined as</p>

<p><span class=""math-container"">$ C(w, b) = \frac{1}{2n}\sum_{x}||y(x)-a||^2 $</span></p>

<p>where <span class=""math-container"">$w$</span> is the set of all weights and <span class=""math-container"">$b$</span> the set of all biases, <span class=""math-container"">$n$</span> is the number of training inputs, x is the set of all training inputs, y(x) is the expected output of the network for input x, and <span class=""math-container"">$a$</span> is the actual output of the network for input <span class=""math-container"">$x$</span>, with respect to <span class=""math-container"">$w$</span> and <span class=""math-container"">$b$</span>.</p>

<p>Most of this formula seems very clear to be, except the <span class=""math-container"">$2$</span> in the denominator. If I understand it correctly, we are summing up the squared vector length of (the actual output minus its expected output), for each training input (giving us the total squared error for the training set) and then divide this by the number of training samples, to get the mean squared error of all training samples. Why do we divide this by <span class=""math-container"">$2$</span> as well then?</p>

<p>In other places I've seen that Andrew Ng's lecture defines the Mean Square cost in a similar way, also with the <span class=""math-container"">$2$</span> in the denominator, so this seems to be a common definition.</p>
","<neural-network><loss-function>"
"52222","When to consider a target Variable unbalanced?","<p>i'm performing a binary classification task , and after cheking the target variable , i saw that i had 69% of 0's and 31% of 1's , so , my question is , do i have in this case a unbalanced target variable ?
 and can somebody tell me , if there is a threshold , or when to consider a Target Variable unbalanced ?</p>

<p>Thanks to everyone </p>
","<classification><class-imbalance>","11788","When should we consider a dataset as imbalanced?","<p>I'm facing a situation where the numbers of positive and negative examples in a dataset are imbalanced.</p>

<p>My question is, are there any rules of thumb that tell us when we should subsample the large category in order to force some kind of balancing in the dataset.</p>

<p>Examples:</p>

<ul>
<li>If the number of positive examples is 1,000 and the number of negative examples is 10,000, should I go for training my classifier on the full dataset or I should subsample the negative examples?</li>
<li>The same question for 1,000 positive example and 100,000 negative.</li>
<li>The same question for 10,000 positive and 1,000 negative.</li>
<li>etc...</li>
</ul>
","<classification><dataset><sampling><class-imbalance>"
"53093","Define difference between feature selection and feature reduction","<p>What is the difference between feature selection and feature reduction? </p>

<p>When do we use feature selection and what happens when we don't use it? How is this different than feature reduction?</p>
","<feature-selection><linear-regression><feature-engineering>","130","What is dimensionality reduction? What is the difference between feature selection and extraction?","<p>From wikipedia:</p>
<blockquote>
<p>dimensionality reduction or dimension reduction is the process of
reducing the number of random variables under consideration, and
can be divided into feature selection and feature extraction.</p>
</blockquote>
<p>What is the difference between feature selection and feature extraction?</p>
<p>What is an example of dimensionality reduction in a Natural Language Processing task?</p>
","<feature-selection><feature-extraction><dimensionality-reduction>"
"25696","Text annotating process, quality vs quantity?","<p>I have a question regarding annotating text data for classification.</p>

<p>Assume we have ten volunteers who are about to annotate a large number of texts into label A or B. They probably won't have time to go through all the text samples, but at least a significant portion of them.</p>

<ol>
<li>Should we focus on generating new samples for each annotator? (They never see the same text samples as any other annotator) (quantity approach).</li>
<li>Or should all annotators see the same samples and the annotator agreement is taken to account? (quality approach).</li>
</ol>

<p>Thoughts,</p>

<ol>
<li>will generate more unique samples than 2. (more training samples for a classifier) - and hoping that in the feature extraction part, the useful features will appear by themselves.</li>
<li>will generate less unique samples, but with the annotator agreement taken into account. (less training samples for a classifier, but with higher quality)</li>
</ol>
","<machine-learning><nlp><classification><data-mining><text-mining>","24194","Text annotating process, quality vs quantity?","<p>I have a question regarding annotating text data for classification.</p>

<p>Assume we have ten volunteers who are about to annotate a large number of texts into label A or B. They probably won't have time to go through all the text samples, but at least a significant portion of them.</p>

<ol>
<li>Should we focus on generating new samples for each annotator? (They never see the same text samples as any other annotator) (quantity approach).</li>
<li>Or should all annotators see the same samples and the annotator agreement is taken to account? (quality approach).</li>
</ol>

<p>Thoughts,</p>

<ol>
<li>will generate more unique samples than 2. (more training samples for a classifier) - and hoping that in the feature extraction part, the useful features will appear by themselves.</li>
<li>will generate less unique samples, but with the annotator agreement taken into account. (less training samples for a classifier, but with higher quality)</li>
</ol>
","<machine-learning><classification><data-mining><nlp><text-mining>"
"55157","How does permutation of training data improve convergence time when training a perceptron or neural network model?","<p>I'm currently studying some basic concepts regarding Deep Learning and Neural Networks with <a href=""http://www.ciml.info/"" rel=""nofollow noreferrer"">this</a> material. </p>

<p>When discussing the training algorithm for a perceptron, the author states that looping through the training data in constant order is not always a good idea, specially in data sets where some labels could be somewhat ordered. </p>

<p>For example, say the model is being trained on a set of 1000 examples, the first 500 have a ""positive"" label, and the last 500 have a ""negative"" label. If these examples are iterated in their original order within the data set, in order to determine the weights of the perceptron, then after evaluating the error of the first few examples (which are ""positive"") the model would likely decide that every example is ""positive"", and would stop learning anything. It would perform well for a while, until it begins to evaluate the ""negative"" examples, when it would begin predicting everything as ""negative"". In the end, the model would've learned from only a handful of examples.</p>

<p>This makes perfect sense to me, and I understand the need to permute the data before iterating over it in the training algorithm. However, it is said that the convergence could be achieved even faster (about twice as fast), if the data is re-permuted in each iteration.</p>

<p>How does this improvement actually happen? Is there a way to actually quantify the difference in the number of epochs needed for convergence when the data is permuted once and when it is re-permuted with each iteration (maybe a statistical model, or something of the sort)? Also, are there any special cases where permuting the data with each iteration is actually ill-advised?</p>
","<neural-network><training><beginner><perceptron><convergence>","24511","Why should the data be shuffled for machine learning tasks","<p>In machine learning tasks it is common to shuffle data and normalize it. The purpose of normalization is clear (for having same range of feature values). But, after struggling a lot, I did not find any valuable reason for shuffling data.</p>

<p>I have read this post <a href=""https://stats.stackexchange.com/a/180847/179078"">here</a> discussing when we need to shuffle data, but it is not obvious why we should shuffle the data. Furthermore, I have frequently seen in algorithms such as Adam or SGD where we need batch gradient descent (data should be separated to mini-batches and batch size has to be specified). It is vital according to this <a href=""https://stats.stackexchange.com/questions/245502/shuffling-data-in-the-mini-batch-training-of-neural-network"">post</a> to shuffle data for each epoch to have different data for each batch. So, perhaps the data is shuffled and more importantly changed.</p>

<p>Why do we do this?</p>
","<machine-learning><neural-network><deep-learning>"
"55426","What is the Value of X in KNN and Why?","<p>I have a dataset of 25 instances these instances are divided into 2 classes Green Circles and Blue Squares</p>

<p>data distributed as this graph</p>

<p><a href=""https://i.stack.imgur.com/FGNk2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FGNk2.png"" alt=""enter image description here""></a></p>

<p>I want to predict X's class based on ""Likelihood Weighted KNN with k =3""</p>

<p>In normal KNN this is easy</p>

<p>the nearest 3 points are 2 Blue Squares and 1 Green Circle</p>

<p>which means X will be Blue Square </p>

<p>there are more Blue Squares neighbours than Green Circles (2 vs 1) </p>

<p>But What is needed is to find the Likelihood Weighted KNN with k =3</p>

<p>This is my try</p>

<p>In this case we have to calculate the weight (Likelihood) for each instance</p>

<p>Each Green Circle likelihood is <span class=""math-container"">$\frac{1}{5} $</span> , we have 5 Green Circles</p>

<p>While for Blue Squares it is  <span class=""math-container"">$\frac{1}{20} $</span> , we have 20 Blue Squares</p>

<p>Therefore the weights around X will be <span class=""math-container"">$\frac{1}{5} $</span>  Green Circle, and <span class=""math-container"">$\frac{2}{20} $</span> Blue Squares.</p>

<p>which means <span class=""math-container"">$\frac{1}{5} &gt; \frac{2}{20} $</span></p>

<p>Then X is Green Circle</p>

<p>Well, this is wrong :(</p>

<p>Can someone help me find the </p>
","<k-nn>","55419","logic behind weighted KNN","<p>I am reading about KNN</p>

<p><a href=""https://i.stack.imgur.com/yyCgi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yyCgi.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Ro1Tg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ro1Tg.png"" alt=""enter image description here""></a></p>

<p>So I made another example to make things clearer</p>

<p>In this example (Image attached)</p>

<p><a href=""https://i.stack.imgur.com/N3I5R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N3I5R.png"" alt=""enter image description here""></a></p>

<p>You can see there are in total 5 Greed Circles and 20 Blue Squares</p>

<p>by standard KNN (k=3) , X should be a Blue Square </p>

<p>This is obvious 2 Blue Squares vs 1 Green Circle.</p>

<p>But in weighted KNN things are difference</p>

<p>In this case we have to calculate the weight (Likelihood) for each instance</p>

<p>Each Green Circle likelihood is <span class=""math-container"">$\frac{1}{5} $</span> , we have 5 Green Circles</p>

<p>While for Blue Squares it is  <span class=""math-container"">$\frac{1}{20} $</span> , we have 20 Blue Squares</p>

<p>Therefore the weights around X will be <span class=""math-container"">$\frac{1}{5} $</span>  Green Circle, and <span class=""math-container"">$\frac{2}{20} $</span> Blue Squares.</p>

<p>which means <span class=""math-container"">$\frac{1}{5} &gt; \frac{2}{20} $</span></p>

<p>Then X is Green Circle</p>

<p>But if try to think of it logically then there are more Blue Squares than Green Circles which means X more likely to be Blue Square than Green Circle.</p>

<p>My question is :</p>

<p>Am I doing anything wrong here? Can someone explain why the equation is showing Green Circle while logic says Blue Square? </p>
","<k-nn>"
"56555","The Difference between One Hot Encoding and LabelEncoder?","<p>I am working on a ML problem to predict house prices and <code>Zip Code</code> is one feature which will be useful. I am also trying to use <code>Random Forest Regressor</code> to predict the <code>log</code> of the <code>price</code>.</p>

<p>However, should I use <code>One Hot Encoding</code> or <code>Label Encoder</code> for <code>Zip Code</code>? Because I have about 2000 <code>Zip Codes</code> in my dataset and performing <code>One Hot Encoding</code> will expand the columns significantly.</p>

<p><a href=""https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor"">When to use One Hot Encoding vs LabelEncoder vs DictVectorizor?</a></p>
","<scikit-learn><regression><random-forest><categorical-data><data-science-model>","9443","When to use One Hot Encoding vs LabelEncoder vs DictVectorizor?","<p>I have been building models with categorical data for a while now and when in this situation I basically default to using scikit-learn's LabelEncoder function to transform this data prior to building a model.</p>

<p>I understand the difference between <code>OHE</code>, <code>LabelEncoder</code> and <code>DictVectorizor</code> in terms of what they are doing to the data, but what is not clear to me is when you might choose to employ one technique over another.  </p>

<p>Are there certain algorithms or situations in which one has advantages/disadvantages with respect to the others?</p>
","<scikit-learn><categorical-data><feature-engineering>"
"58988","Difference between C and lambda in SVM","<p>I've been taking the coursera machine learning course and The instructor said that if ( C = 1/lambda )  then the learning algorithm would reach the same optimal value of theta</p>

<p>does this mean that using C instead of lambda is just a convention when using SVM or is there an actual difference ? </p>

<p>P.S 
Please dumb down the answer for me I'm kind of new to this :) Thanks</p>
","<machine-learning><svm>","22408","What is the difference between C and lambda in the context of an SVM?","<p>I don't understand the difference between the parameter $C$ and $\lambda$ in terms of the <a href=""https://en.wikipedia.org/wiki/Support_vector_machine"" rel=""nofollow noreferrer"">SVM</a>. It seems to me that they are both involved in regulating over-fitting of the data. </p>

<p>What difference between $C$ and $\lambda$?</p>
","<machine-learning><svm>"
"60164","R- stem and leaf plot","<p>I am having trouble constructing a stem and leaf plot in ""R""</p>

<p>The data is: 1717, 1719, 1645, 3739, 3024 (This is only a subset of a larger sample but it is sufficient to show my problem)</p>

<p>The output I get is:</p>

<p>The decimal point is 3 digit(s) to the right of the |</p>

<p>1 | 677</p>

<p>2 | </p>

<p>2 | </p>

<p>3 | 0</p>

<p>3 | 7</p>

<p>This does not reconstruct my data as the numbers would be 1600, 1700...
It seems my numbers have been rounded.
I want to make the stem to be one digit as the thousands number and the leaves to be 3 digits to represent the hundreds, tens and ones.
For ex) I want to 1717 to be 1|717</p>

<p>And I have tried scaling but that doesn't fix my problem, How do fix this issue?</p>
","<r>","60178","R stem and leaf values","<p>How to make a stem and leaf plot with the following data in ""R"":</p>

<p>1717 1719 1645 3739 3024 3664 3830 2991 2430 2730 3469 5086 2119 3021 3292 2844 3426 2067 3215 2767 3124 2573 2840 2449 2584 1505 1390 1645 2497 3466 3228 3192</p>

<p>The only way to get a reasonable amount of stems is to make the stem the first digit in each number and the leaves will all be 3 digit numbers. I am not able to split the data in this way, R represents the stem with 1 digit but round my numbers  so 2584 is shown as 2| 5 (3 decimals to the right) which would be 2500. How do I adjust my stem and leaf numbers?</p>
","<r>"
"60576","Unsupervised Algorithm for hybrid data","<p>I have a hybrid data that contains 15 categorical data and 4 continuous data.
I need to implement a prediction on the data. So as I don't have any labeled data, I need to implement the unsupervised algorithm.</p>

<p>Now Using PCA my dimensions reduced to 5 but still it has 3 categorical and 2 continuous data. Now to create a prediction algorithm, I can use KMeans for continuous data or KModes for categorical data, But I couldn't find any algorithm that works on both types of data.</p>

<p>Can anyone know any algorithm that works on both the data types for unsupervised prediction?</p>
","<machine-learning><algorithms><unsupervised-learning><data-science-model>","22","K-Means clustering for mixed numeric and categorical data","<p>My data set contains a number of numeric attributes and one categorical.</p>
<p>Say, <code>NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr</code>,</p>
<p>where <code>CategoricalAttr</code> takes one of three possible values: <code>CategoricalAttrValue1</code>, <code>CategoricalAttrValue2</code> or <code>CategoricalAttrValue3</code>.</p>
<p>I'm using default <a href=""https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/"" rel=""noreferrer"">k-means clustering algorithm implementation for Octave</a>.
It works with numeric data only.</p>
<p>So my question: is it correct to split the categorical attribute <code>CategoricalAttr</code> into three numeric (binary) variables, like <code>IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3</code> ?</p>
","<data-mining><clustering><octave><k-means><categorical-data>"
"61767","Where to get the Datascience Use cases for practice","<p>I just started  learning data science. I have gone through some of the courses in coursera &amp; udemy, now i want to practice what i have learned. What i want to know is from where can i get the Use cases (linear regression &amp; multiple linear regression) so that i could practice</p>
","<linear-regression><data-science-model><usecase>","842","Data Science Project Ideas","<p>I don't know if this is a right place to ask this question, but a community dedicated to Data Science should be the most appropriate place in my opinion.</p>

<p>I have just started with Data Science and Machine learning. I am looking for long term project ideas which I can work on for like 8 months.</p>

<p>A mix of Data Science and Machine learning would be great.</p>

<p>A project big enough to help me understand the core concepts and also implement them at the same time would be very beneficial.</p>
","<machine-learning><bigdata><dataset>"
"61814","Imbalanced dataset - Undersampling & multiple classifiers","<p>Let's suppose that my dataset in a classification problem looks like that:</p>

<ol>
<li>class A: 50000 observations</li>
<li>class B: 2000 observations</li>
<li>class C: 800 observations</li>
<li>class D: 200 observations</li>
</ol>

<p>These are some ways which I considered to deal with this imbalanced dataset:</p>

<ol>
<li><p>I reject straight away oversampling because it usually makes the model overfit (in the minority classes) by a lot.</p></li>
<li><p>Secondly, if I run the classifier with the data like that then it will be overclassifying documents in class A so I reject this method too.</p></li>
<li><p>Another approach is to do undersampling and reduce class A to let's say 4000 documents (where I tested it and it gives the best results so far).</p></li>
<li><p>However, in this way I am losing quite a lot of information. So I am wondering if building multiple classifiers with 4000 documents each for class A (different at each time) is a better solution (although I think that this approach resembles quite a lot the oversampling approach which I rejected).</p></li>
</ol>

<p>What do you think of method (4) comparing to method (3)?</p>
","<class-imbalance>","31685","weighted cross entropy for imbalanced dataset - multiclass classification","<p>I am trying to classify images to more then a 100 classes, of different sizes ranged from 300 to 4000 (mean size 1500 with std 600). I am using a pretty standard CNN where the last layer outputs a vector of length number of classes, and using pytorch's loss function CrossEntropyLoss.</p>

<p>I tried to use $weights = \frac{max(sizes)}{sizes}$ for the cross entropy loss which improved the unweighted version, not by much.</p>

<p>I also thought about duplicating images such that all classes ends up to be of the same size as the larges one.</p>

<p>Is there any standard way of handling this sort of imbalance? </p>
","<deep-learning><multiclass-classification><class-imbalance>"
"62394","is learning rate schedule a hyperparameter?","<p>I believe term ‘learning rate schedule’ is a certain solution for tuning the learning rate. But at the same time, every parameter evaluating the parameter itself can be called a hyperparameter. So can the learning rate schedule be called a hyperparameter?</p>
","<hyperparameter>","14187","What is the difference between model hyperparameters and model parameters?","<p>I have noticed that such terms as model <strong>hyperparameter</strong> and model <strong>parameter</strong> have been used interchangeably on the web without prior clarification. I think this is incorrect and needs explanation. Consider a machine learning model, an SVM/NN/NB based classificator or image recognizer, just anything that first springs to mind. </p>

<p>What are the <strong>hyperparameters</strong> and <strong>parameters</strong> of the model?<br>
Give your examples please.</p>
","<machine-learning><parameter><hyperparameter><language-model>"
"62926","Merging two pandas dataframes","<p>I have two csv files,and if I do:</p>

<pre><code>df_compiler = pd.read_csv('sub_compiler.csv')
</code></pre>

<p>I obtain:</p>

<p><a href=""https://i.stack.imgur.com/TzEBT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TzEBT.png"" alt=""enter image description here""></a></p>

<p>and for the other csv file I do:</p>

<pre><code>df_opt = pd.read_csv('sub_opt.csv')
</code></pre>

<p>I obtain:</p>

<p><a href=""https://i.stack.imgur.com/fPEuX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fPEuX.png"" alt=""enter image description here""></a></p>

<p>now what I would like to do is merge these two csv files and obtain something like:</p>

<p><a href=""https://i.stack.imgur.com/GXj0W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GXj0W.png"" alt=""enter image description here""></a></p>

<p>can somebody please help me doing this? Thank's in advance.</p>
","<pandas><dataframe>","62934","creating a csv file from two csv files","<p>I have two csv files:</p>

<pre><code>sub_compiler.to_csv('sub_compiler.csv')
sub_compiler.head()
</code></pre>

<p><a href=""https://i.stack.imgur.com/ZXRGC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZXRGC.png"" alt=""enter image description here""></a></p>

<p>and </p>

<pre><code>sub_opt = pd.read_csv('sub_opt.csv')
sub_opt.head()
</code></pre>

<p><a href=""https://i.stack.imgur.com/XRPQZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XRPQZ.png"" alt=""enter image description here""></a></p>

<p>and what I would like to do is to create a csv file where I have something of the form </p>

<p>compiler, opt </p>

<p>How could I do this? I need to do this to make a submission.</p>

<p>Thank's in advance.</p>

<p>[EDIT] Thank you for the answers. Now I have obtained :</p>

<p><a href=""https://i.stack.imgur.com/4UwKc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4UwKc.png"" alt=""enter image description here""></a></p>

<p>don't understand why I have the columns called unnamed. </p>

<p>My objective is to get a csv file that contains only the columns compiler and opt. How could I do this? Thank's again.</p>

<p>[EDIT 2] I have solved my problem but if I open the csv file manually, so just click on the file, I have the following:</p>

<p><a href=""https://i.stack.imgur.com/PQckG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PQckG.png"" alt=""enter image description here""></a></p>

<p>s it contains only 11 lines, but it contains 3000 lines in jupyter. Is it there something wrong?</p>

<p>[EDIT 3] I tried to to the following:</p>

<pre><code>import pandas as pd

test = pd.read_csv('1495927.csv')
test
</code></pre>

<p>and I have:</p>

<p><a href=""https://i.stack.imgur.com/iVGPS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iVGPS.png"" alt=""enter image description here""></a></p>

<p>where <code>1495927.csv</code> is the csv file I have created and that is in the image where I have 11 elements.</p>
","<pandas><dataframe>"
"63205","How to find and calculate correlation in a data set which has category and continuous variables?","<p>I am working on an Insurance domain use case to predict if an existing customer will buy a second insurance policy or not. I have a few personal details saved under different categories like Marital status, Smoker (Yes or No), Age (Young, Adult, Senior Citizen), Gender (Male/Female) and few are continuous variables like Premium Paid, Sum Insured.</p>

<p>My target is to use this mix set of categorical and continuous variables and predict the class ( 1 - Will buy a second policy, 0 - Will not buy a second policy). So how can I find/compute the correlation in this dataset and pick only the significant ones to use in Logistic Regression formula for classification?</p>

<p>Will appreciate if someone can provide articles, link to a similar piece of work done in Python.</p>
","<python><logistic-regression><categorical-data><correlation>","893","How to get correlation between two categorical variable and a categorical variable and continuous variable?","<p>I am building a regression model and I need to calculate the below to check for correlations</p>

<ol>
<li>Correlation between 2 Multi level categorical variables</li>
<li>Correlation between a Multi level categorical variable and
continuous variable </li>
<li>VIF(variance inflation factor) for a Multi
level categorical variables</li>
</ol>

<p>I believe its wrong to use Pearson correlation coefficient for the above scenarios because Pearson only works for 2 continuous variables. </p>

<p>Please answer the below questions</p>

<ol>
<li>Which correlation coefficient works best for the above cases ? </li>
<li>VIF calculation only works for continuous data so what is the
alternative? </li>
<li>What are the assumptions I need to check before I use the correlation coefficient you suggest? </li>
<li>How to implement them in SAS &amp; R?</li>
</ol>
","<r><statistics><correlation>"
"64723","K-Folds validation","<p>Suppose we make a linear regression model on each of the 10 folds with the same number of features (say 2 for simplification)
We will therefore have 10 sets of coefficients with the optimized values ​​of the parameters with each of the metrics (for example R2).</p>

<p>In the end what model do we retain? (the best, the average values ​​obtained for each parameter, the model that obtained the average R2?)
Ouvrir dans Google Traduction</p>
","<machine-learning><cross-validation>","13960","How to choose a classifier after cross-validation?","<p>When we do k-fold cross validation, should we just use the classifier that has the highest test accuracy? What is generally the best approach in getting a classifier from cross validation?</p>
","<machine-learning><cross-validation>"
"64745","AdaBoost vs Gradient Boost","<p>What is the difference? Under which criteria should each type of boost be used?</p>

<p>What is the theory behind each of these methods?</p>
","<python><adaboost>","39193","Adaboost vs Gradient Boosting","<p>How is AdaBoost different from a Gradient Boosting algorithm since both of them use a Boosting technique?</p>
<p>I could not figure out actual difference between these both algorithms from a theory point of view.</p>
","<algorithms><similarity><ensemble-modeling><boosting>"
"65147","What is the best way to encode features when clustering data?","<p>I have a dataset with numerical and categorical features. I am trying to run a k-means algorithm to find clusters of data.</p>

<p>What is the best way to encode categorical features?</p>

<p>I have been doing one hot encoding but I guess that it can be easily improved.</p>
","<machine-learning><python><unsupervised-learning><categorical-data><one-hot-encoding>","22","K-Means clustering for mixed numeric and categorical data","<p>My data set contains a number of numeric attributes and one categorical.</p>
<p>Say, <code>NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr</code>,</p>
<p>where <code>CategoricalAttr</code> takes one of three possible values: <code>CategoricalAttrValue1</code>, <code>CategoricalAttrValue2</code> or <code>CategoricalAttrValue3</code>.</p>
<p>I'm using default <a href=""https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/"" rel=""noreferrer"">k-means clustering algorithm implementation for Octave</a>.
It works with numeric data only.</p>
<p>So my question: is it correct to split the categorical attribute <code>CategoricalAttr</code> into three numeric (binary) variables, like <code>IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3</code> ?</p>
","<data-mining><clustering><octave><k-means><categorical-data>"
"65197","A cross-entropy loss explanation in simple words","<p>Suppose I build a FNN model. The last layer is a classification layer with softmax activation.<br>
A cross-entropy loss is used to classify a problems, such as logistic regression. 
How would I calculate the cross entropy loss ?</p>

<p>But I can not fully understand the cross-entropy loss Please any convenient explanation</p>

<p>Thanks in advance</p>
","<machine-learning><neural-network><deep-learning><rnn>","20296","Cross-entropy loss explanation","<p>Suppose I build a neural network for classification. The last layer is a dense layer with Softmax activation. I have five different classes to classify. Suppose for a single training example, the <code>true label</code> is <code>[1 0 0 0 0]</code> while the predictions be <code>[0.1 0.5 0.1 0.1 0.2]</code>. How would I calculate the cross entropy loss for this example?</p>
","<machine-learning><neural-network><deep-learning><softmax>"
"17927","Categorical data in Kmeans","<p>I need to perform clustering in a given dataset.
There are atrributes with numerical as well as categorical values.</p>

<p>What is the best way to convert categorical to numeric value ?</p>

<p>as an example one field is colour and the values are red, green, blue
so can I assign a  mapping like:</p>

<p>red : 1, green : 2, blue : 3 or
red : 11, green : 25, blue : 30 </p>

<p>and if I provide a mapping like this will this affect the Euclidian distance for clustering ?</p>

<p>or is there any other way ?</p>
","<clustering><k-means>","22","K-Means clustering for mixed numeric and categorical data","<p>My data set contains a number of numeric attributes and one categorical.</p>
<p>Say, <code>NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr</code>,</p>
<p>where <code>CategoricalAttr</code> takes one of three possible values: <code>CategoricalAttrValue1</code>, <code>CategoricalAttrValue2</code> or <code>CategoricalAttrValue3</code>.</p>
<p>I'm using default <a href=""https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/"" rel=""noreferrer"">k-means clustering algorithm implementation for Octave</a>.
It works with numeric data only.</p>
<p>So my question: is it correct to split the categorical attribute <code>CategoricalAttr</code> into three numeric (binary) variables, like <code>IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3</code> ?</p>
","<data-mining><clustering><octave><k-means><categorical-data>"
"66837","Cost function and minimization of error","<p>In linear regression model, how can we define cost function. also after defining cost function how to minimize the error term? </p>
","<machine-learning>","66662","How to reduce the error in linear regression","<p>In linear regression, how can we minimize the error term? </p>
","<machine-learning><linear-regression>"
"15847","How to tell if a problem should use regression or classification model?","<p>Once we know that the problem needs to be solved using supervised learning, how do we know if we have to use regression analysis or classification algorithm to solve it? Is there some thumb rules that can be used?</p>

<p>Most of the resources online give the standard example of prices of house for regression model and malignant/benign cancer plot for classification model. This isn't helping me much.</p>

<p>Is there come conceptual method to analyze the given problem?</p>
","<machine-learning><classification><regression><supervised-learning>","41740","What is the difference between classification and regression?","<p>I understand classification....a discrete response or category, like animal is dog or cat.</p>

<p>The author says...""Regression techniques predict continuous changes such as the change in temperature, power demand, or stock market prices.""</p>

<p>I can't wrap my head around what he means.</p>

<p>Thanks.</p>
","<machine-learning><classification><regression>"
"67830","How can I test my trained model on a completely new dataset?","<h2>Preface</h2>

<p>I have an annotated text dataset on hate speech. Simply put, the dataset consists of a column called <code>text</code> which includes a piece of text, and a column called <code>label</code> which can be either <code>0</code> (non-hateful) or <code>1</code> (hateful).  Let's call this dataset <code>Dataset A</code>. </p>

<p>I am using this dataset to train a very simple classifier using Logistic Regression. Code: </p>

<pre><code>dataset_1_df = pd.read_csv(d_1_path, sep='\t')

text = dataset_1_df.text   
X = text
y = dataset_1_df['label'].astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)

clf = Pipeline([
    ('vect', CountVectorizer(max_features=10000, ngram_range=(1, 2))),
    ('tfidf', TfidfTransformer(norm='l2')),
    ('clf', LogisticRegression()),
])

clf = clf.fit(X_train, y_train)
y_preds = clf.predict(X_test)
</code></pre>

<p>This classifier works fine. I can produce a classification report like this: </p>

<pre><code>report = classification_report(y_test, y_preds)
print(report)
</code></pre>

<p>Which outputs: </p>

<pre><code>              precision    recall  f1-score   support

           0       0.87      0.70      0.78       410
           1       0.94      0.98      0.96      2069

    accuracy                           0.93      2479
   macro avg       0.91      0.84      0.87      2479
weighted avg       0.93      0.93      0.93      2479
</code></pre>

<p>And I can now make predictions like this: </p>

<pre><code>print(clf.predict(['I hate you']))
print(clf.predict_proba(['I hate you']))
</code></pre>

<p>Which outputs: </p>

<pre><code>[1]
[[0.0749381 0.9250619]]
</code></pre>

<hr>

<h2>My Question</h2>

<p>I have <strong>another</strong> dataset, let's call this <code>Dataset B</code>, independent to <code>Dataset A</code>. <code>Dataset B</code> is also an annotated text dataset on hate speech with the same columns. </p>

<p>What I want to do is to test the model I trained using <code>Dataset A</code> on <code>Dataset B</code>. I.e., I want to produce a classification report on my model's performance on <code>Dataset B</code>. Can I do that? If yes, how?</p>

<p><strong>Clarification:</strong> I do <strong>not</strong> want to retrain my model using <code>Dataset B</code>. I want to see how good its performance is on the new dataset. </p>

<hr>

<h3>Similar Questions</h3>

<p>There is <a href=""https://datascience.stackexchange.com/questions/33256/how-to-apply-machine-learning-model-to-new-dataset"">this</a> question that looks similar based on the title but does not ask what I'm asking. </p>

<p>There's is also <a href=""https://datascience.stackexchange.com/questions/58766/how-to-use-a-a-trained-model"">this</a> question which asks how to make predictions. However, I know how to make predictions, what I want to do is to <strong><em>test</em></strong> my trained model on a totally new dataset. </p>
","<python><scikit-learn><logistic-regression><cross-validation><classifier>","58766","How to use a a trained model","<p>I just trained my first model in Python 3.7/scikitlearn (Linear Regression) (well I copied most of the code but its something ^^).</p>

<p>Now I want to actually Use the model. Specifically its about sons heights incorrelating to their fathers. So I now want to enter a new Father-height and get a predictions for its sons height.</p>

<p>How could something like this look like?</p>

<p>I read about ""Pickle"" to save a model and use it later, seems awsome but how would I use such a saved model?</p>

<p>If anybody can give me a simple answer or even just a link to atutorial would be great. Below is a piece of ""my"" code just for some context.</p>

<pre><code>#Spliting the data into test and train data
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)

#Doing a linear regression
lm=LinearRegression()
lm.fit(X_train,y_train)

#Predicting the height of Sons
y_test=lm.predict(X_test)
print(y_test)
</code></pre>
","<machine-learning><python><scikit-learn><linear-regression><data-science-model>"
"67921","is final fit with X,y or X_train , y_train?","<p>I split the dataset with <br/></p>

<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
</code></pre>

<p>and the fit </p>

<pre><code>from sklearn.metrics import log_loss
clf.fit(X_train, y_train)
clf_probs = clf.predict_proba(X_test)
score = log_loss(y_test, clf_probs)
print(score)
</code></pre>

<p>is final submission with </p>

<pre><code>clf.fit(X,y) or clf.fit(X_train,y_train)???
</code></pre>
","<machine-learning><cross-validation>","33008","Is it always better to use the whole dataset to train the final model?","<p>A common technique after training, validating and testing the Machine Learning model of preference is to use the complete dataset, including the testing subset, to train a <strong>final model</strong> to <strong>deploy</strong> it on, e.g. a product. </p>

<blockquote>
  <p>My question is: Is it always for the best to do so? What if the
  performance actually deteriorates?</p>
</blockquote>

<p>For example, let us assume a case where the model scores around 65% in classifying the testing subset. This could mean that either the model is trained insufficiently OR that the testing subset consists of outliers. In the latter case, training the final model with them would decrease its performance and you find out only after deploying it. </p>

<p>Re-phrasing my initial question:</p>

<blockquote>
  <p>If you had a <strong>one-time demonstration of a model</strong>, such as deploying it
  on embedded electronics on-board an expensive rocket experiment, would you trust a
  model that has been re-trained with the test subset in the final step without being
  re-tested on its new performance?</p>
</blockquote>
","<machine-learning><dataset><training><accuracy>"
"68347","Can we optimize regression problems that have categorical variables by encoding them if on the other hand we are inserting multicollinearity?","<p>Can we optimize regression problems that have categorical variables by encoding them if, on the other hand, we are inserting multicollinearity?</p>
","<regression><categorical-data><categorical-encoding>","27957","Why do we need to discard one dummy variable?","<p>I have learned that, for creating a regression model, we have to take care of categorical variables by converting them into dummy variables. As an example, if, in our data set, there is a variable like location: </p>

<pre><code>Location 
----------
Californian
NY
Florida
</code></pre>

<p>We have to convert them like:</p>

<pre><code>1  0  0
0  1  0
0  0  1
</code></pre>

<p>However, it was suggested that we have to discard one dummy variable, no matter how many dummy variables are there. </p>

<p>Why do we need to discard one dummy variable?</p>
","<machine-learning><regression><categorical-data>"
"69727","Use all available data to build Logistic Regression model","<p>Using K-Fold, I chose to use Logistic Regression for a project of mine.</p>

<p>I made it learn on my X_train (80% of data), and tested it on my X_test, with good results.</p>

<p>My question is : now that I need the model to be used, can I rebuilt the model with the same parameters but on all my data ? Will there be overfitting, or maybe as I saw that those parameters doesn't, will it be safe ?</p>
","<machine-learning><logistic-regression><overfitting>","33008","Is it always better to use the whole dataset to train the final model?","<p>A common technique after training, validating and testing the Machine Learning model of preference is to use the complete dataset, including the testing subset, to train a <strong>final model</strong> to <strong>deploy</strong> it on, e.g. a product. </p>

<blockquote>
  <p>My question is: Is it always for the best to do so? What if the
  performance actually deteriorates?</p>
</blockquote>

<p>For example, let us assume a case where the model scores around 65% in classifying the testing subset. This could mean that either the model is trained insufficiently OR that the testing subset consists of outliers. In the latter case, training the final model with them would decrease its performance and you find out only after deploying it. </p>

<p>Re-phrasing my initial question:</p>

<blockquote>
  <p>If you had a <strong>one-time demonstration of a model</strong>, such as deploying it
  on embedded electronics on-board an expensive rocket experiment, would you trust a
  model that has been re-trained with the test subset in the final step without being
  re-tested on its new performance?</p>
</blockquote>
","<machine-learning><dataset><training><accuracy>"
"13854","Why Logistic regression into Spark Mllib does not use Maximum likelihood estimation?","<p>During comparison estimates/ coefficients in 'R'&amp; Spark Mllib of Logistic regression, It has been observed that estimates are not same. </p>

<p>On further investigation, I found that R &amp; Mllib has different implementations for Logistic regression.</p>

<blockquote>
  <p>R's glm is return a maximum likelihood estimate of the model while
  Spark's LogisticRegressionWithLBFGS is return a regularized model
  estimate.</p>
</blockquote>

<p>Maximum likelihood estimates seems more efficient, as per literature available. </p>

<p>I am curious to know, why Spark Mllib developers has not chosen 'Maximum likelihood estimation' technique?</p>
","<machine-learning><r><apache-spark><logistic-regression>","5710","Why does logistic regression in Spark and R return different models for the same data?","<p>I've compared the logistic regression models on R (<code>glm</code>) and on Spark (<code>LogisticRegressionWithLBFGS</code>) on a dataset of 390 obs. of 14 variables.</p>

<p>The results are completely different in the intercept and the weights.
How to explain this?</p>

<p>Here is the results of Spark (LogisticRegressionWithLBFGS) :</p>

<pre><code>model.intercept  : 
 1.119830027739959
model.weights :
 GEST    0.30798496002530473
 DILATE  0.28121771009716895
 EFFACE  0.01780105068588628
 CONSIS -0.22782058111362183
 CONTR  -0.8094592237248102
 MEMBRAN-1.788173534959893
 AGE    -0.05285751197750732
 STRAT  -1.6650305527536942
 GRAVID  0.38324952943210994
 PARIT  -0.9463956993328745
 DIAB   0.18151162744507293
 TRANSF -0.7413500749909346
 GEMEL  1.5953124037323745
</code></pre>

<p>Here is the result of R :</p>

<pre><code>             Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  3.0682091  3.3944407   0.904 0.366052    
GEST         0.0086545  0.1494487   0.058 0.953821    
DILATE       0.4898586  0.2049361   2.390 0.016835 *  
EFFACE       0.0131834  0.0059331   2.222 0.026283 *  
CONSIS       0.1598426  0.2332670   0.685 0.493196    
CONTR        0.0008504  0.5788959   0.001 0.998828    
MEMBRAN     -1.5497870  0.4215416  -3.676 0.000236 ***   
AGE         -0.0420145  0.0326184  -1.288 0.197725    
STRAT       -0.3781365  0.5860476  -0.645 0.518777    
GRAVID       0.1866430  0.1522925   1.226 0.220366    
PARIT       -0.6493312  0.2357530  -2.754 0.005882 **  
DIAB         0.0335458  0.2163165   0.155 0.876760    
TRANSF      -0.6239330  0.3396592  -1.837 0.066219 .  
GEMEL        2.2767331  1.0995245   2.071 0.038391 *  
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
</code></pre>
","<machine-learning><r><logistic-regression><apache-spark>"
"13477","Are there any libraries for drawing a neural network in Python?","<p>I'd like to render a neural network in Python to make sure it is set up properly.  I'd like to verify that it has the correct # of nodes, correct edge configuration, and perhaps even correct edge weights after training.  I'd like to visually debug simple examples by printing them to a PNG/JPG but do not know any libraries that do this.  Is there a library for drawing a neural network for visual debugging?</p>

<p>(Such a library would of course make assumptions on the data types/structures - e.g., assuming the neural network comes from Theano, TensorFlow, or some other library.  I'm flexible on the assumptions made.)</p>
","<python><neural-network>","12851","How do you visualize neural network architectures?","<p>When writing a paper / making a presentation about a topic which is about neural networks, one usually visualizes the networks architecture.</p>

<p>What are good / simple ways to visualize common architectures automatically?</p>
","<machine-learning><neural-network><deep-learning><visualization>"
"71755","Should you perform feature scaling/mean normalization when predicting with a model?","<p>Should you also perform feature scaling/mean normalization when predicting with a model, that was trained and tested on with feature scaling/mean normalization?</p>
","<neural-network><deep-learning><training>","27615","Should we apply normalization to test data as well?","<p>I am doing a project on author identification problem. I had applied the tf-idf normalization to train data and then trained a svm on that data.</p>

<p>Now when using the classifier should I normalize test data as well. I feel that the basic aim of normalization is to make the learning algo give more weight to more important features while learning. So once it has trained it already knows which features are important, which are not. So is there any need of applying normalization to test data as well?</p>

<p>I am new to this field. So please ignore if the question appears silly?</p>
","<machine-learning><neural-network><deep-learning>"
"13273","Clustering categorical data","<p>I have a dataset with categorical features. I want to segment the data using clustering techniques. What could be the possible choices for this scenariogiven the fact that data has categorical features. Is there any variation of k-means which can be used here.</p>
","<data-mining>","12475","Clusering based on categorical variables?","<p>I am working on a project and currently experimenting cluster analysis. The dataset is mainly categorical variables and discrete numbers. Please pardon my poor programming skills as I am not very familiar with MathJax, but I will try to summarize the data set in words in the following three examples.</p>

<ol>
<li>imagine column 1 is participants names of course, from column2 - 5, each column's value range from 1 (least important) - 5 (most important). so in this case, column 2-8 only have discrete data. </li>
<li>column 6 for example, since this is a multiple choice question. row 1 chose ""nice"" as an answer, however, row 2 chose ""poor"". In this case, we have one column that contains multiple categorical answers.</li>
<li>for column 7-9, this is another type of multiple choice questions. However this time, each column represents only one answer. column 7 only allows string values ""true"", column 8 only allows string ""somewhat"". So in this case, we have multiple columns that represent multiple answers of the same question. </li>
</ol>

<p>Any ideas how to solve this? Thankful to any input! </p>

<pre><code>A2       A3       A4       A5       A6       A7         A8         A9
1        4        5        4        nice     true       somewhate  false
2        4        3        1        poor     true                  false
1        5        2        1        nice                somewhate    
3        2        1        5        nice     true                  false
</code></pre>
","<clustering>"
"72601","should we include or exclude a variable in a logistic regression based on the description below?","<p>should we include or exclude a variable in a logit regr. model which will only obtain values if a certain event takes place otherwise will show N/A? this variable tells whether or not a product will be bought based on calls made by the company. the variable tells: number of days that passed by after the client was last contacted from a previous campaign. the values (-1) are for users who</p>

<p><a href=""https://i.stack.imgur.com/yYT01.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yYT01.png"" alt=""enter image description here""></a></p>
","<r><regression><logistic-regression><preprocessing>","88242","Machine Learning with intended missing values","<p>I have a dataset relating to humans completing reviews, the target variable is whether the review decision is correct / incorrect and one of my features is a trailing 4 week accuracy score for the reviewer.</p>
<p>These accuracy scores are not always available however. My question is around how to model this data - the fact that there is no available accuracy score might be a signal. From my research into this - everything I see tells me that the missing values must be imputed or removed. I am wondering whether there are techniques to incorporate the fact that the data is missing into the dataset.</p>
<p>Perhaps I could convert the score into a categorical variable {low, medium, high, not available] - would this be common practice? I am open to suggestions and would love to hear what is commonly done in these scenarios</p>
","<machine-learning><statistics><feature-engineering>"
"73811","Is it possible for decision trees to consider less features than in my training set?","<p>I was looking at the decision tree algorithm and I wondered that for example if the training set has 20 features but only 5 features are important and classification can be done by using only them then will my end result decision tree has only 5 features? My understanding is that as decision tree is a kind of greedy based algorithm, so I believe it will only consider those features which can be used for the best fit of the training data and ignore the rest of them.</p>
","<decision-trees>","27569","Decision tree not using all features from training dataset","<p>I have built CART model using sklearn. I'm having total 6 features in training dataset and passing all of them in fit function. I've tested both criteria Gini and entropy. But whenever I plot tree using graphviz, the tree uses only 3 features in case of Gini and 4 features in case of entropy. I've also implemented CART from scratch for cross check purpose and still, Gini uses 3 features and entropy uses 4 features.</p>

<p>Everything else is working fine. I'm getting the result as expected in test dataset with accuracy 97%. I just want to confirm that, is this normal? Does cart chooses best features only and ignore other features to avoid the wrong classification? Also, my dataset is quite small in size, only 220 records.</p>
","<machine-learning><decision-trees>"
"74241","How do I find entropy of features having numerical data?","<p>I'm a newbie and I'm writing a decision tree from scratch using entropy and information gain.
I understand that entropy is the measure of impurity of a data set and also calculated entropy for categorical features but for continuous data, I do get an intuition that I have to take a range of values for each class label but how do I choose that range? Is my intuition correct?
What is the standard way of finding the entropy of a feature with numerical data.</p>
","<machine-learning><decision-trees>","24339","How is a splitting point chosen for continuous variables in decision trees?","<p>I have two questions related to decision trees:</p>

<ol>
<li><p>If we have a continuous attribute, how do we choose the splitting value?</p>

<p>Example: Age=(20,29,50,40....)</p></li>
<li><p>Imagine that we have a continuous attribute $f$ that have values in $R$. How can I  write an algorithm that finds the split point $v$, in order that when we split $f$ by $v$, we have a minimum gain for $f&gt;v$?</p></li>
</ol>
","<classification><data><decision-trees>"
"74634","Model stacking with instance attributes","<p>From what I have understood about model stacking: the meta estimator trains to combine the N-models predictions to fit the ground truth. Once trained, it combines the 1st level output to approach the ground truth.</p>

<p>The meta estimator is a model of type :
<span class=""math-container"">$ (y_{pred1}, y_{pred2}, y_{pred3})\rightarrow y_{pred-stack}$</span></p>

<p>So the combination is only based on the values of 1st level predictions. However, each line of the stacking data is also linked to other attributes: ""Brand"", ""Model"", ""Power"". Why won't we take those attributes to determine the optimal combination? So if the model 1 is the best when the brand is ""NaN"", the meta will learn it and redirect every prediction having NaN brand to model 1.</p>

<p>So the meta estimator I propose is as follow : 
<span class=""math-container"">$ (y_{pred1},y_{pred2},y_{pred3},$</span>brandIsNull<span class=""math-container"">$)\rightarrow y_{pred-stack}$</span> </p>

<ul>
<li>Does this approach exist?</li>
<li>If not, would it be a good or bad idea?</li>
</ul>
","<ensemble-modeling>","74301","What is the form of data used for prediction with generalized stacking ensemble?","<p>I am very confused as to how training data is split and on what data level 0 predictions are made when using generalized stacking. This question is similar to mine, but the answer is not sufficiently clear:</p>

<p><a href=""https://datascience.stackexchange.com/questions/19692/how-predictions-of-level-1-models-become-training-set-of-a-new-model-in-stacked"">How predictions of level 1 models become training set of a new model in stacked generalization.</a></p>

<p>My understanding is that the training set is split, base models trained on one split, and predictions are made on the other. These predictions now become features of a new dataset. One column for each model's prediction, plus a column that contains the ground truth for these predictions. </p>

<ol>
<li>Split training data into train/test. </li>
<li>Train base models on training split.</li>
<li>Make predictions on test split (according to linked answer, use k-fold CV for this).</li>
<li>Create a feature for each model, filling it with that model's predictions</li>
<li>Create a feature for the ground truth of those predictions.</li>
<li>Create a new model and train it on these predictions and ground truth features.</li>
</ol>

<p><strong>Question 1:</strong> Are these the only features used to train the ""meta"" model? In other words, are none of the actual features of the original data included? The linked answer says it is common to include the original data, but I have not read about it elsewhere. </p>

<p><strong>Question 2:</strong> If the above algorithm is correct, What is the form of the data when making predictions? It seems like it would also have to have predictions as independent variables. If so, that means running all new incoming data through all base models again, right?</p>

<p><strong>Question 3:</strong> I keep seeing an ""out-of-fold"" requirement for the first level predictions. It seems that doing a simple train/test split as mentioned above would fulfill this. However, would you not want a 3rd split to test the combined model's generalization? Or is this type of ensemble bulletproof enough not to worry about it?</p>
","<ensemble-modeling><generalization><ensemble-learning>"
"74682","Is over fitting okay if test accuracy is high enough?","<p>I am trying to build a binary classifier. I have tried deep neural networks with various different structures and parameters and I was not able to get anything better than</p>

<pre><code>Train set accuracy : 0.70102
Test set accuracy : 0.70001
</code></pre>

<p>Then I tried machine learning algorithms such as KNN and Decision Trees etc. And I found that Random forest Classifier from <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"" rel=""noreferrer"">Scikit-learn</a> with <code>n_estimators=100</code> gave me</p>

<pre><code>Train set accuracy : 1.0
Test set accuracy : 0.924068
</code></pre>

<p>I tried adjusting other parameters such as <code>max_depth</code>, <code>criterion</code> But the decrease in training set accuracy also caused the test set accuracy to drop. Like </p>

<pre><code>Train set accuracy : 0.82002
Test set accuracy : 0.75222
</code></pre>

<p>My question is, is this</p>

<pre><code>Train set accuracy : 1.0
Test set accuracy : 0.924068
</code></pre>

<p>acceptable ? Even thought the model is over fitting, the test set accuracy is better. </p>
","<scikit-learn><random-forest><overfitting>","66350","What would I prefer - an over-fitted model or a less accurate model?","<p>Let's say we have two models trained. And let's say we are looking for good accuracy. 
The first has an accuracy of 100% on training set and 84% on test set. Clearly over-fitted.
The second has an accuracy of 83% on training set and 83% on test set. </p>

<p>On the one hand, model #1 is over-fitted but on the other hand it still yields better performance on an unseen test set than the good general model in #2. </p>

<p>Which model would you choose to use in production? The First or the Second and why?</p>
","<machine-learning-model><training><supervised-learning><accuracy><overfitting>"
"74696","Compute Accuracy of k-means","<p>Could you please provide me an example of how I can compute the accuracy for a kmeans clustering? 
I split my dataset into train and test sets and computed the predicted clusters for the train set. However, I do not know how to compiute its level of accuracy and see if it is good not also for the test.</p>

<p>An example with a sample dataset of your choice it would be extremely useful.</p>
","<python><clustering><k-means>","17461","How to test accuracy of an unsupervised clustering model output?","<p>I am trying to test how well my unsupervised K-Means clustering properly clusters my data.  I have an unsupervised K-Means clustering model output (as shown in the first photo below) and then I clustered my data using the actual classifications.</p>

<p><a href=""https://i.stack.imgur.com/1y0zS.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1y0zS.jpg"" alt=""enter image description here""></a></p>

<p>The photo below are the actual classifications.  I am trying to test, in Python, how well my K-Means classification (above) did against the actual classification.</p>

<p><a href=""https://i.stack.imgur.com/AeHnt.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/AeHnt.jpg"" alt=""enter image description here""></a></p>

<p>For my K-Means code, I am using a simple model, as follows:</p>

<pre><code>kmeans = KMeans(n_clusters=4, random_state=0).fit(myData)
labels = kmeans.labels_
</code></pre>

<p>What would be the best way for me to compare how well my unsupervised KMeans clustering model did against the actual classifications?</p>
","<clustering><k-means>"
"76258","Why would GradientBoostClassifier do better than XGBoostClassifier?","<p>I am working on the Kaggle home loan model and interestingly enough, the GradientBoostClassifier has a considerably better score than XGBClassifier. At the same time it seems to not overfit as much. (note, I am running both algos with default settings).
From what I've been reading XGBClassifier is the same as GradientBoostClassifier, just much faster and more robust. Therefore I am now confused on why would XGB overfit so much more than GradientBoostClassifier, when it should do the contrary? What would be a good reason why this is happening?</p>
","<python><xgboost><natural-gradient-boosting>","16904","GBM vs XGBOOST? Key differences?","<p>I am trying to understand the key differences between GBM and XGBOOST. I tried to google it, but could not find any good answers explaining the differences between the two algorithms and why xgboost almost always performs better than GBM. What makes XGBOOST so fast?</p>
","<machine-learning><algorithms><xgboost><ensemble-modeling><gbm>"
"76465","what is difference between fit and fit_transform in sklearn while applying feature scaling","<p>I have seen few post related to this question but i am not quite clear about my confusions as mention bellow.</p>
<p>I have some confusion related to fit and fit_transform.
suppose, I have X_train and X_test data, and let my scaling function is standard scalar.
I am using following code for scaling,
<code>sc_X =StandardScaler()</code>
<code>X_train = sc_X.fit_transform(X_train)</code>
X_test = sc_X.fit_transform`</p>
<p>My question is, if i use same scalar on bot trainin and testing data, wont it creat problem of data leakage?</p>
<p>What if I use the code like below,</p>
<p><code>sc_X_train =StandardScaler()</code>
<code>sc_X_test =StandardScaler()</code>
<code>X_train = sc_X_train.fit_transform(X_train)</code>
<code>X_test = sc_X_test.fit_transform(X_test)</code></p>
<p>Does the both code give different results?</p>
","<machine-learning><scikit-learn><feature-scaling>","12321","What's the difference between fit and fit_transform in scikit-learn models?","<p>I do not understand the difference between the <code>fit</code> and <code>fit_transform</code> methods in scikit-learn. Can anybody explain simply why we might need to transform data?</p>
<p>What does it mean, fitting a model on training data and transforming to test data? Does it mean, for example, converting categorical variables into numbers in training and transforming the new feature set onto test data?</p>
","<python><scikit-learn>"
"77530","Clustering for Categorical Data?","<p>How exactly does k-means clustering for categorical data work?</p>
<p>I have a dataset which has several categorical features that can have 2,3,4,..,n values. I could one hot encode them, but I'm not sure if it even makes sense because k-means uses a distance metric.</p>
<p>I looked into k-prototype clustering, but tried it on my dataset, but plotting it does not make sense when the y-axis and y-axis are just integer values that are small.</p>
","<machine-learning><clustering><k-means>","22","K-Means clustering for mixed numeric and categorical data","<p>My data set contains a number of numeric attributes and one categorical.</p>
<p>Say, <code>NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr</code>,</p>
<p>where <code>CategoricalAttr</code> takes one of three possible values: <code>CategoricalAttrValue1</code>, <code>CategoricalAttrValue2</code> or <code>CategoricalAttrValue3</code>.</p>
<p>I'm using default <a href=""https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/"" rel=""noreferrer"">k-means clustering algorithm implementation for Octave</a>.
It works with numeric data only.</p>
<p>So my question: is it correct to split the categorical attribute <code>CategoricalAttr</code> into three numeric (binary) variables, like <code>IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3</code> ?</p>
","<data-mining><clustering><octave><k-means><categorical-data>"
"78483","Entity Recognition to extract question,options and diagrams from a question paper?","<p>Here is a sample question paper. I want to extract questions, their options and diagrams for the corresponding question(if any). Thinking of doing entity recogntion for the same. But I am not sure if that is possible for diagrams? Can someone help me out.</p>
<p>I tried OCR techniques previously but they are not doing well.</p>
<p><a href=""https://i.stack.imgur.com/sQK05.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sQK05.jpg"" alt=""enter image description here"" /></a></p>
","<python><deep-learning><nlp><named-entity-recognition>","78322","Image segmentation network to extract questions from an image of a test paper?","<p>This is the sample document -&gt;
<a href=""https://i.stack.imgur.com/kvswZ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kvswZ.jpg"" alt=""enter image description here"" /></a></p>
<p>I want to extract questions along with the options. There are other question papers as which have questions with diagrams in them. I want to be able to extract them as well.</p>
<p>End goal is to store them in a question bank.</p>
<p>I am planning to run an image segmentation algorithm to extract the questions along with their options.
Will that be feasible?
Suggestions are more than welcome.</p>
","<python><nlp><computer-vision><ocr><image-segmentation>"
"78513","Can mini-batch gradient descent outperform batch gradient descent?","<p>As I was reading and going through the second course of Andrew Ng's deep learning course, I came across a sentence that said,</p>
<blockquote>
<p><em><strong>With a well-turned mini-batch size, usually it outperforms either
gradient descent or stochastic gradient descent (particularly when the
training set is large).</strong></em></p>
</blockquote>
<p>But how is it possible? can mini-batch gradient descend really give us a better set of weights and biases even though it's not updating them based on the whole dataset? I can only think that it's maybe not overfitting and in that way, it can give better results.</p>
","<machine-learning><python><deep-learning><gradient-descent><gradient>","16807","Why mini batch size is better than one single ""batch"" with all training data?","<p>I often read that in case of Deep Learning models the usual practice is to apply mini batches (generally a small one, 32/64) over several training epochs. I cannot really fathom the reason behind this.</p>

<p>Unless I'm mistaken, the batch size is the number of training instances let seen by the model during a training iteration; and epoch is a full turn when each of the training instances have been seen by the model. If so, I cannot see the advantage of iterate over an almost insignificant subset of the training instances several times in contrast with applying a ""max batch"" by expose all the available training instances in each turn to the model (assuming, of course, enough the memory). What is the advantage of this approach?</p>
","<machine-learning><deep-learning>"
"79744","Object detection/recognition, pre-processing error","<h1>My Imports:</h1>
<pre><code># Importing modules 
import numpy as np 
import pandas as pd 
import os
import matplotlib.pyplot as plt
import cv2

from keras.utils import to_categorical
from keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout
from keras.models import Sequential

from sklearn.model_selection import train_test_split
</code></pre>
<h1>My Code:</h1>
<pre><code>np.random.seed(1)


train_images = []
train_labels = []

shape = (108,108)

label_path = 'Beer/ModelSet/'

train_labels.append('miller_lite')
train_labels.append('stella_artois')
train_labels.append('michelob_ultra')
train_labels.append('belgian_blue')

for folder in os.listdir(label_path):
    for files in os.listdir(label_path+folder):
        img = cv2.imread(os.path.join(label_path,folder,files))
        train_images.append(img)


train_labels = np.asarray(pd.get_dummies(train_labels).values)

train_images = np.asarray(train_images)


x_train, x_val, y_train, y_val = train_test_split(train_images, train_labels, random_state=1)
</code></pre>
<h2>The part that fails</h2>
<pre><code>    x_train, x_val, y_train, y_val = train_test_split(train_images, train_labels, 
random_state=1)
</code></pre>
<h2>The reason it fails</h2>
<pre><code>ValueError: Found input variables with inconsistent numbers of samples: [20000, 4]
</code></pre>
<h1>My Error:</h1>
<pre><code>2020-08-03 23:47:11.117431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File &quot;/Attempt-2.py&quot;, line 40, in &lt;module&gt;
    x_train, x_val, y_train, y_val = train_test_split(train_images, train_labels, random_state=1)
  File &quot;python3.8/site-packages/sklearn/model_selection/_split.py&quot;, line 2127, in train_test_split
    arrays = indexable(*arrays)
  File &quot;python3.8/site-packages/sklearn/utils/validation.py&quot;, line 293, in indexable
    check_consistent_length(*result)
  File &quot;python3.8/site-packages/sklearn/utils/validation.py&quot;, line 256, in check_consistent_length
    raise ValueError(&quot;Found input variables with inconsistent numbers of&quot;
ValueError: Found input variables with inconsistent numbers of samples: [20000, 4]
</code></pre>
<h2>The Goal:</h2>
<p>Create a classification model that distinguishes between 4 different brands of beer bottles. MillerLite,StellaArtois,MichelobUltra, and BelgianBlue.</p>
<h3>Background Note:</h3>
<p>I have never had any practical/work experience with either software engineering, data science, software development, machine learning etc. I am just a student messing around for my own amusement/fun.</p>
<h2>The Question:</h2>
<p>How exactly do I fix this? I understand the problem is that matrix x is not the same size as matrix y. X is 20000 images of size 108,108 and 3 channels RGB. Y is the label matrix:
[[1 0 0 0][0 1 0 0][0 0 1 0][0 0 0 1]]
in order to split into train/test images the error says I need to have the same len/size array/matrix for both x and y.</p>
","<keras><scikit-learn><object-detection><python-3.x><object-recognition>","20199","train_test_split() error: Found input variables with inconsistent numbers of samples","<p>Fairly new to Python but building out my first RF model based on some classification data. I've converted all of the labels into int64 numerical data and loaded into X and Y as a numpy array, but I am hitting an error when I am trying to train the models. </p>

<p>Here is what my arrays look like:</p>

<pre><code>&gt;&gt;&gt; X = np.array([[df.tran_cityname, df.tran_signupos, df.tran_signupchannel, df.tran_vmake, df.tran_vmodel, df.tran_vyear]])

&gt;&gt;&gt; Y = np.array(df['completed_trip_status'].values.tolist())

&gt;&gt;&gt; X
array([[[   1,    1,    2,    3,    1,    1,    1,    1,    1,    3,    1,
            3,    1,    1,    1,    1,    2,    1,    3,    1,    3,    3,
            2,    3,    3,    1,    1,    1,    1],
        [   0,    5,    5,    1,    1,    1,    2,    2,    0,    2,    2,
            3,    1,    2,    5,    5,    2,    1,    2,    2,    2,    2,
            2,    4,    3,    5,    1,    0,    1],
        [   2,    2,    1,    3,    3,    3,    2,    3,    3,    2,    3,
            2,    3,    2,    2,    3,    2,    2,    1,    1,    2,    1,
            2,    2,    1,    2,    3,    1,    1],
        [   0,    0,    0,   42,   17,    8,   42,    0,    0,    0,   22,
            0,   22,    0,    0,   42,    0,    0,    0,    0,   11,    0,
            0,    0,    0,    0,   28,   17,   18],
        [   0,    0,    0,   70,  291,   88,  234,    0,    0,    0,  222,
            0,  222,    0,    0,  234,    0,    0,    0,    0,   89,    0,
            0,    0,    0,    0,   40,  291,  131],
        [   0,    0,    0, 2016, 2016, 2006, 2014,    0,    0,    0, 2015,
            0, 2015,    0,    0, 2015,    0,    0,    0,    0, 2015,    0,
            0,    0,    0,    0, 2016, 2016, 2010]]])

&gt;&gt;&gt; Y
array(['NO', 'NO', 'NO', 'YES', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'NO',
       'NO', 'YES', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',
       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO'], 
      dtype='|S3')

&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)
</code></pre>

<blockquote>
  <p>Traceback (most recent call last):</p>

<pre><code>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Library/Python/2.7/site-packages/sklearn/cross_validation.py"", line
</code></pre>
  
  <p>2039, in train_test_split
          arrays = indexable(*arrays)
        File ""/Library/Python/2.7/site-packages/sklearn/utils/validation.py"", line
  206, in indexable
          check_consistent_length(*result)
        File ""/Library/Python/2.7/site-packages/sklearn/utils/validation.py"", line
  181, in check_consistent_length
          "" samples: %r"" % [int(l) for l in lengths])</p>

<pre><code>ValueError: Found input variables with inconsistent numbers of samples: [1, 29]
</code></pre>
</blockquote>
","<python><scikit-learn><sampling>"
"80954","Recommended Tutorial Videos or Books on Feature Engineering Using Python","<p>I will appreciate it if you guys can recommend for me a good hands-on tutorial videos or books on feature engineering using Python. I do not want videos or books that teach only the theory behind feature engineering but one that will show me some codes and how to do it.</p>
<p>Thanks.</p>
","<machine-learning><feature-selection><feature-engineering><feature-extraction><feature-scaling>","12984","List of feature engineering techniques","<p>Is there any resource with a list of feature engineering techniques? A mapping of type of data, model and feature engineering technique would be a gold mine.</p>
","<feature-selection><feature-extraction><feature-engineering><feature-construction><featurization>"
"80970","Exclude regression constant from non-linear regression","<p>For example, suppose I've data set and I want to perform a non-linear regression on the data set as follows:</p>
<p><span class=""math-container"">$Y=B_1e^{B_2 X} + B_3$</span></p>
<p>and I found that if I exclude <span class=""math-container"">$B_3$</span> the <span class=""math-container"">$R^2$</span> increased also standard error of the regression decreased, can I exclude <span class=""math-container"">$B_3$</span>?</p>
","<regression>","80812","Removing constant from the regression model","<p>I am trying to calibrate two variables <span class=""math-container"">$(X,Y)$</span> of different measuring techniques from two instruments, the result of the linear regression analysis appears as shown in the image.</p>
<p>The result shows the regression constant is not statistically significant but the model is significant. I have tried to remove the regression constant (it is a very small value close to zero) and <span class=""math-container"">$R$</span> of the new model is raised to 90%. Is it correct to remove the regression constant?</p>
<p><a href=""https://i.stack.imgur.com/MOkcs.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MOkcs.jpg"" alt=""Regression result"" /></a></p>
","<r><regression><predictive-modeling><linear-algebra>"
"81797","How can a ML algorithm learn to classify fake news?","<p>I am new in Machine learning techniques and in fake news detection by using these algorithms (SVM, nn, logistic regression,..).
I would like to understand how an algorithm can learn from a training set which include news and fake news, what it will be necessary to have (target), what type of information can be relevant for a good analysis and learning ...</p>
<p>I read many papers and most of them do not mention the type of news used, the criteria for building the classifiers, but they only show the results of the prediction when a new news is generated.
This could be perfectly fine, but it could be also easily to say: I got this amazing results!, without understanding how these results were got.</p>
<p>I would appreciate if you could show me an example of how an algorithm can learn from text (not necessarily news, but also tweets, or something else would be fine).</p>
<p>Following discussion with Erwan: one of his previous answer partially has answered my question. However I would like to understand the following. One needs to have a corpus, then label news/tweets in fake/not fake, then run the model. But how the algorithm works on texts and takes relevant words or features for detecting fake news?</p>
<p>So my question is:
If I have a corpus on Trump, would the algorithm be able to detect fake news on Vitamin C, without any words (verb, adj, noun,..) in common between the two dataset, except stopwords?</p>
","<machine-learning><data-science-model><algorithms><text-classification>","66391","Fake News Detection problem","<p>I would like to work on a project for Fake News Detection especially for Indians news which are in different languages and different formats.</p>

<ol>
<li>Fake news as image with no or very less text</li>
<li>Fake news on a blog site</li>
<li>Fake news as Tweets</li>
<li>Fake news in Hindi</li>
<li>Fake news in the watsapp group and shared across.</li>
</ol>

<p>Need your help on the approach. One approach I can think of is using OCR we can read the content of the post, then search those content in the google. If the news is not present in any of the famous print media then we can tag it as fake. However there can be many challenges in this. What if the print media itself gives any fake news shared by someone.</p>

<p>How to handle the scenario where there is no text in the image but the information shown as image is fake.</p>

<p>How to handle posts written in Hindi. ?</p>

<p>And even if we detect fake news, is there any way to make the person accountable for sharing it. ? I know it is little difficult problem to solve. But is there any work currently done by any company on this. ? 
Any starting point for me to get into this domain ?</p>
","<machine-learning><deep-learning><ocr>"
"81884","accuracy and loss NAN for keras multi-label Neural network learning","<p>When I ran a Neural Network modeling for multi-class labeling using Keras, the <strong>accuracy</strong>, <strong>loss</strong>, <strong>val_accuracy</strong>, and <strong>val_loss</strong> all seems to have <em>nan</em> at some point or other during the training process...</p>
<p>Running environment and other info</p>
<ul>
<li>Keras version: '2.4.0'</li>
<li>TensorFlow version: '2.3.0'</li>
<li>OS: Ubuntu 20.04.1    LTS</li>
<li>GPU: Radeon RX 580 Series</li>
<li>Python running in Jupyter Notebook    (there is no library version issue)</li>
</ul>
<p>Thank you for you kind help.</p>
<p>Code I used:</p>
<pre><code># data import n prep
data_irr = pd.read_csv( 'https://www.dropbox.com/s/97awk5f9rgn5k87/irrigation_machine.csv?dl=1')
df_TRAIN = data_irr.sample( frac=  78./100 )
df_TEST  = data_irr.drop( df_TRAIN.index )

# TRAIN data
X_train = df_TRAIN.drop( df_TRAIN.columns[ [0,21,22,23] ], axis= 1 ); X_train
Y_train = df_TRAIN[['parcel_0','parcel_1','parcel_2']]; Y_train

X_train = X_train.to_numpy(); Y_train = Y_train.to_numpy()


# TEST data
X_test = df_TEST.drop( df_TEST.columns[[0,21,22,23]], axis= 1 ); X_test
Y_test = df_TEST[['parcel_0','parcel_1','parcel_2']]; Y_test.head()

X_test = X_test.to_numpy();  Y_test = Y_test.to_numpy()


## model architecture

model1 = keras.Sequential()
model1.add(
            keras.layers.Dense( 64, activation= 'relu', input_shape= (X_train.shape[1],) )
)
model1.add(
            keras.layers.Dense( 3, activation= 'sigmoid' )
)

model1.compile( 
                optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy']
)
model1.summary()


model1.fit( 
            x= X_train, y= Y_train,
            epochs= 15, validation_split= 20./100,
            verbose= 2
)
</code></pre>
<p>The result of the training is:</p>
<pre><code>Epoch 1/15
39/39 - 0s - accuracy: nan - loss: nan - val_accuracy: 0.5481 - val_loss: -5.4487e-02
Epoch 2/15
39/39 - 0s - accuracy: nan - loss: nan - val_accuracy: nan - val_loss: nan
Epoch 3/15
39/39 - 0s - accuracy: 0.5433 - loss: nan - val_accuracy: 0.7981 - val_loss: 1.8837e-07
Epoch 4/15
39/39 - 0s - accuracy: nan - loss: nan - val_accuracy: 1.1190 - val_loss: 0.3816
Epoch 5/15
39/39 - 0s - accuracy: nan - loss: nan - val_accuracy: 0.8221 - val_loss: nan
Epoch 6/15
39/39 - 0s - accuracy: 0.5529 - loss: nan - val_accuracy: 0.9802 - val_loss: 0.2807
Epoch 7/15
39/39 - 0s - accuracy: 0.5473 - loss: nan - val_accuracy: 0.8893 - val_loss: 0.0128
Epoch 8/15
39/39 - 0s - accuracy: 0.5537 - loss: nan - val_accuracy: 0.8287 - val_loss: inf
Epoch 9/15
39/39 - 0s - accuracy: 0.6186 - loss: nan - val_accuracy: 0.7756 - val_loss: 1522.9906
Epoch 10/15
39/39 - 0s - accuracy: 0.5905 - loss: nan - val_accuracy: 0.8085 - val_loss: 0.1824
Epoch 11/15
39/39 - 0s - accuracy: 0.6250 - loss: nan - val_accuracy: 0.8007 - val_loss: 0.0545
Epoch 12/15
39/39 - 0s - accuracy: 0.6514 - loss: nan - val_accuracy: 0.8980 - val_loss: 13932.9346
Epoch 13/15
39/39 - 0s - accuracy: 0.5457 - loss: nan - val_accuracy: 0.8000 - val_loss: nan
Epoch 14/15
39/39 - 0s - accuracy: 0.5545 - loss: nan - val_accuracy: 1.9837e-07 - val_loss: -1.8476e+10
Epoch 15/15
39/39 - 0s - accuracy: 0.5409 - loss: nan - val_accuracy: 2.0768e-07 - val_loss: 0.5513
&lt;tensorflow.python.keras.callbacks.History at 0x7f92602e0f98&gt;
</code></pre>
","<neural-network><deep-learning><keras><loss-function>","68331","Keras Sequential model returns loss 'nan'","<p>I'm implementing a neural network with Keras, but the <code>Sequential</code> model returns <code>nan</code> as loss value.
I have sigmoid activation function in the output layer to squeeze output between 0 and 1, but maybe doesn't work properly.</p>

<p>This is the code:</p>

<pre><code>def data_generator(batch_count, training_dataset, training_dataset_labels):
  while True:
    start_range = 0
    for batch in batch_count:
      end_range = (start_range + batch[1])
      batch_dataset = training_dataset[start_range:end_range]
      batch_labels = training_dataset_labels[start_range:end_range]
      start_range = end_range
      yield batch_dataset, batch_dataset

mlp = keras.models.Sequential()

# add input layer
mlp.add(
    keras.layers.Input(
        shape = (training_dataset.shape[1], )
    )
)
# add hidden layer
mlp.add(
    keras.layers.Dense(
        units=training_dataset.shape[1] + 10,
        input_shape = (training_dataset.shape[1] + 10,),
        kernel_initializer='random_uniform',
        bias_initializer='zeros',
        activation='relu')
    )
# add output layer
mlp.add(
    keras.layers.Dense(
        units=1,
        input_shape = (1, ),
        kernel_initializer='glorot_uniform',
        bias_initializer='zeros',
        activation='sigmoid')
    )

print('Compiling model...\n')

mlp.compile(
    optimizer='adam',
    loss=listnet_loss
)

mlp.summary() # print model settings


# Training
with tf.device('/GPU:0'):
  print('Start training')
  #mlp.fit(training_dataset, training_dataset_labels, epochs=50, verbose=2, batch_size=3, workers=10)
  mlp.fit_generator(data_generator(groups_id_count, training_dataset, training_dataset_labels),
                    steps_per_epoch=len(training_dataset), epochs=50, verbose=2, workers=10, use_multiprocessing=True)
</code></pre>

<p>How can I do?</p>
","<python><neural-network><keras><loss-function>"
"82073","Why you shouldn't upsample before cross validation","<p>I have an imbalanced dataset and I am trying different methods to address the data imbalance. I found this <a href=""https://kiwidamien.github.io/how-to-do-cross-validation-when-upsampling-data.html"" rel=""nofollow noreferrer"">article</a> that explains the correct way to cross-validate when oversampling data using SMOTE technique.</p>
<p>I have created a model using AdaBoost algorithm and set the following parametres to be used in Grid Search:</p>
<pre class=""lang-py prettyprint-override""><code>    ada = AdaBoostClassifier(n_estimators=100, random_state=42)
    params = {
        'n_estimators': [50, 100, 200],
        'random_state': [42]
    }
</code></pre>
<p>According to the article, this is the <strong>wrong</strong> way to oversample:</p>
<pre class=""lang-py prettyprint-override""><code>    X_train_upsample, y_train_upsample = SMOTE(random_state=42).fit_sample(X_train, y_train)
    
    # cross-validate using grid search
    
    grid_naive_up = GridSearchCV(ada, param_grid=params, cv=kf, 
                                 scoring='recall').fit(X_train_upsample, 
                                                       y_train_upsample)
    grid_naive_up.best_score_
</code></pre>
<p>0.6715940782827282</p>
<pre class=""lang-py prettyprint-override""><code>    # test set
    recall_score(y_test, grid_naive_up.predict(X_test))
</code></pre>
<p>0.2824858757062147</p>
<p>Whereas the <strong>correct</strong> way to oversample is like so:</p>
<pre class=""lang-py prettyprint-override""><code>    from imblearn.pipeline import Pipeline, make_pipeline
    
    imba_pipeline = make_pipeline(SMOTE(random_state=42), 
                                  AdaBoostClassifier(n_estimators=100, random_state=42))
    cross_val_score(imba_pipeline, X_train, y_train, scoring='recall', cv=kf)
    new_params = {'adaboostclassifier__' + key: params[key] for key in params}
    grid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring='recall',
                            return_train_score=True)
    grid_imba.fit(X_train, y_train);


    # How well do we do on our validation set?
    grid_imba.best_score_
</code></pre>
<p>0.29015614186873506</p>
<pre class=""lang-py prettyprint-override""><code>    # compare this to the test set:
    y_test_predict = grid_imba.predict(X_test)
</code></pre>
<p>0.2824858757062147</p>
<p>So, according to the article, the first method is wrong because when upsampling before cross validation, the validation recall isn't a good measure of the test recall (28.2%).
However, when using the imblearn pipeline for upsampling as part of the cross validation, the validation set recall (29%) was a good estimate of the test set recall (28.3%). According to the article, the reason for this is:</p>
<blockquote>
<p>When upsampling before cross validation, you will be picking the most
oversampled model, because the oversampling is allowing data to leak
from the validation folds into the training folds.</p>
</blockquote>
<p>Can anyone explain to me simply how the oversampling allows data to leak into the validation and causes the overfitting? And why does this problem not occur in the imblearn pipeline?</p>
","<python><scikit-learn><cross-validation><class-imbalance><smote>","60764","Why did sampling boost the performance of my model?","<p>I have an imbalanced dataset with 88 positive samples and 128575 negative samples. I was reluctant to over/undersample the data since it's a biological dataset and I didn't want to introduce synthetic data. I built a Random Forest Classifier with this original dataset. I got an F1 score of 0 for the positive class. Zero precision. Zero recall. I cross-checked the predictions and test data. The model predicts some positives none of which are actually positive. Worst performance.</p>

<p>So, I tried to oversample the positive class. I upsampled the positives to 1000 samples. To my surprise, the F1 score for this dataset was 0.97, for the positive class. Then I tried lesser samples. I was able to achieve an F1 score of 0.83 with 200 positive samples, which is just 2.25 times of the original positive samples.</p>

<p>I would like to know why this occurs. For 88 samples, F1 score is 0.00 (rounded off to two digits). For 200 samples it's 0.83. There is no data leakage. All the features are engineered. I used <code>imbalanced-learn</code> module for oversampling. Can someone explain why is this difference in performance?</p>
","<random-forest><class-imbalance><sampling>"
"2651","Deep learning basics","<p>I am looking for a paper detailing the very basics of deep learning. Ideally like the Andrew Ng course for deep learning. Do you know where I can find this ?</p>
","<machine-learning><deep-learning>","6068","How can an undergraduate learn more about deep learning?","<p>I am an undergraduate who needs to submit a thesis for graduation. I am fairly interested in deep learning, and am working on a project that uses deep learning methods extensively (rCNNs to be precise). The caveat here is that I am working alone, with little help from my advisor. There are hardly any experts on deep learning in my university. How do I go about finishing my project. It is specially intimidating to see that most papers in deep learning are published by multiple accomplished scientists with very little involvement of undergraduate level students. </p>

<p>PS: I am really interested in this topic, so please try not to advise me to switch to an easier topic.</p>
","<machine-learning><deep-learning><research>"
"84442","Keras is adding dimension to shape","<p>When I create an InputLayer, a <code>None</code> dimension gets added. I don't know why this is.</p>
<pre><code>    image_shape = ds_info.features['image'].shape
    input_shape = (ds_info.features['label'].num_classes, image_shape[0], image_shape[1], image_shape[2])

    print('input_shape: ', input_shape)
    
    ## Create the CNN
    # Create the input layer
    input_layer = Input(shape=input_shape)
    
    print('input_layer: ', input_layer.shape)
</code></pre>
<h2>Results</h2>
<pre><code>input_shape:  (1623, 105, 105, 3)
input_layer:  (None, 1623, 105, 105, 3)
</code></pre>
","<keras><tensorflow>","54159","What does an Input layer of shape=(None,) or (None,12) actually mean?","<p>Is this telling the model that there are two dimensions (i.e. it’s a matrix) but we don’t yet know the size of that particular dimension? If so, how can the model be compiled? Doesn’t the size of each dimension affect the number of nodes in middle layers?</p>
","<neural-network><keras><tensorflow>"
"84742","SMOTE train test split with validation data","<p>Would like to ask, in which way to use SMOTE?
My dataset is imbalanced and a multiclass problem. As I read in many posts, use SMOTE method only for the training dataset (X_train and y_train). Not for the test dataset (X_test and y_test). There I include validation data. How do you handle SMOTE with validation data?</p>
<pre><code>df = pd.read_excel...

X=df.drop('column1',axis=1)
y=df.column1


#Training part
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

#validation part
X_train_smote, X_val, y_train_smote, y_val = train_test_split(X_train_smote, y_train_smote, test_size=0.5, random_state=42)
</code></pre>
<p>Is this correct?</p>
<p>and is it right, that the validation datasets (X_val and y_val) have both SMOTE inside?  or should I make it out of the normal train test split:
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=42)?
Im confused.</p>
","<training><smote><validation>","82073","Why you shouldn't upsample before cross validation","<p>I have an imbalanced dataset and I am trying different methods to address the data imbalance. I found this <a href=""https://kiwidamien.github.io/how-to-do-cross-validation-when-upsampling-data.html"" rel=""nofollow noreferrer"">article</a> that explains the correct way to cross-validate when oversampling data using SMOTE technique.</p>
<p>I have created a model using AdaBoost algorithm and set the following parametres to be used in Grid Search:</p>
<pre class=""lang-py prettyprint-override""><code>    ada = AdaBoostClassifier(n_estimators=100, random_state=42)
    params = {
        'n_estimators': [50, 100, 200],
        'random_state': [42]
    }
</code></pre>
<p>According to the article, this is the <strong>wrong</strong> way to oversample:</p>
<pre class=""lang-py prettyprint-override""><code>    X_train_upsample, y_train_upsample = SMOTE(random_state=42).fit_sample(X_train, y_train)
    
    # cross-validate using grid search
    
    grid_naive_up = GridSearchCV(ada, param_grid=params, cv=kf, 
                                 scoring='recall').fit(X_train_upsample, 
                                                       y_train_upsample)
    grid_naive_up.best_score_
</code></pre>
<p>0.6715940782827282</p>
<pre class=""lang-py prettyprint-override""><code>    # test set
    recall_score(y_test, grid_naive_up.predict(X_test))
</code></pre>
<p>0.2824858757062147</p>
<p>Whereas the <strong>correct</strong> way to oversample is like so:</p>
<pre class=""lang-py prettyprint-override""><code>    from imblearn.pipeline import Pipeline, make_pipeline
    
    imba_pipeline = make_pipeline(SMOTE(random_state=42), 
                                  AdaBoostClassifier(n_estimators=100, random_state=42))
    cross_val_score(imba_pipeline, X_train, y_train, scoring='recall', cv=kf)
    new_params = {'adaboostclassifier__' + key: params[key] for key in params}
    grid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring='recall',
                            return_train_score=True)
    grid_imba.fit(X_train, y_train);


    # How well do we do on our validation set?
    grid_imba.best_score_
</code></pre>
<p>0.29015614186873506</p>
<pre class=""lang-py prettyprint-override""><code>    # compare this to the test set:
    y_test_predict = grid_imba.predict(X_test)
</code></pre>
<p>0.2824858757062147</p>
<p>So, according to the article, the first method is wrong because when upsampling before cross validation, the validation recall isn't a good measure of the test recall (28.2%).
However, when using the imblearn pipeline for upsampling as part of the cross validation, the validation set recall (29%) was a good estimate of the test set recall (28.3%). According to the article, the reason for this is:</p>
<blockquote>
<p>When upsampling before cross validation, you will be picking the most
oversampled model, because the oversampling is allowing data to leak
from the validation folds into the training folds.</p>
</blockquote>
<p>Can anyone explain to me simply how the oversampling allows data to leak into the validation and causes the overfitting? And why does this problem not occur in the imblearn pipeline?</p>
","<python><scikit-learn><cross-validation><class-imbalance><smote>"
"838","Pig latin code error","<p>While running the below pig script I am getting error in line4: If it is <code>GROUP</code> then I am getting error. If I change from <code>GROUP</code> TO <code>group</code> in line4, then the script is running. What is the difference between group and GROUP.</p>

<pre><code>LINES = LOAD '/user/cloudera/datapeople.csv' USING PigStorage(',') AS ( firstname:chararray, lastname:chararray, address:chararray, city:chararray, state:chararray, zip:chararray );

WORDS = FOREACH LINES GENERATE FLATTEN(TOKENIZE(zip)) AS ZIPS;

WORDSGROUPED = GROUP WORDS BY ZIPS;

WORDBYCOUNT = FOREACH WORDSGROUPED GENERATE GROUP AS ZIPS, COUNT(WORDS);

WORDSSORT = ORDER WORDBYCOUNT BY $1 DESC;

DUMP WORDSSORT;
</code></pre>
","<apache-hadoop><bigdata>","829","Pig script code error?","<p>While running the below pig script I am getting an error in line4: 
If it is GROUP then I am getting error.
If I change from 'GROUP' TO 'group' in line4, then the script is running.</p>

<p>What is the difference between group and GROUP?</p>

<pre><code>LINES = LOAD '/user/cloudera/datapeople.csv' USING PigStorage(',') AS ( firstname:chararray, lastname:chararray, address:chararray, city:chararray, state:chararray, zip:chararray );

WORDS = FOREACH LINES GENERATE FLATTEN(TOKENIZE(zip)) AS ZIPS;

WORDSGROUPED = GROUP WORDS BY ZIPS;

WORDBYCOUNT = FOREACH WORDSGROUPED GENERATE GROUP AS ZIPS, COUNT(WORDS);

WORDSSORT = ORDER WORDBYCOUNT BY $1 DESC;

DUMP WORDSSORT;
</code></pre>
","<bigdata><apache-hadoop>"
"85972","BERT data cleaning","<p>I am wondering which data cleaning steps should be performed if you want to re-fine a BERT model on custom text data.</p>
<p>Which steps should be performed?</p>
<p>Does it make sense to perform a stemming or lemmatization if it has not been applied to the initial training of the BERT Base/Large model?</p>
","<nlp><preprocessing><bert><transformer>","62862","Preprocessing for Text Classification in Transformer Models (BERT variants)","<p>This might be silly to ask, but I am wondering if one should carry out the conventional text preprocessing steps for training one of the transformer models?</p>
<p>I remember for training a Word2Vec or Glove, we needed to perform an extensive text cleaning like: tokenize, remove stopwords, remove punctuations, stemming or lemmatization and more. However, during last few days I have had a quick jump into transformer models (fascinated btw), and what I have noticed that most of these models have a built-in tokenizer (cool), but none of the demos, examples, or tutorials are performing any of the these text preprocessing steps. You may take <a href=""https://github.com/kaushaltrivedi/fast-bert"" rel=""nofollow noreferrer"">fast-bert</a> for instance, there are no text preprocessing involved for the demos (maybe it is just a demo), but at <a href=""https://github.com/kaushaltrivedi/fast-bert#5-model-inference"" rel=""nofollow noreferrer"">inference</a> the whole sentences are passed without any cleaning:</p>
<pre><code>texts = ['I really love the Netflix original movies',
         'this movie is not worth watching']
predictions = learner.predict_batch(texts)
</code></pre>
<p>The same is true for the <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">original transformer</a> by HuggingFace. Or many tutorials that I have looked at (take <a href=""https://github.com/kaushaltrivedi/bert-toxic-comments-multilabel/blob/master/toxic-bert-multilabel-classification.ipynb"" rel=""nofollow noreferrer"">this</a> or <a href=""https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d"" rel=""nofollow noreferrer"">another one</a>). I can imagine that depending on the task this might not be required, e.g. next work prediction or machine translation and more. More importantly I think this is part of the contextual-based approach that these models offer (that is the innovation so to say) that are meant to keep most of the text and we may obtain a minimum but still good representation of the each token (out of vocabulary word). Borrowed from <a href=""https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d"" rel=""nofollow noreferrer"">medium article</a> by HuggingFace:</p>
<blockquote>
<p><strong>Tokenisation</strong>
BERT-Base, uncased uses a vocabulary of 30,522 words. The
processes of tokenisation involves splitting the input text into list
of tokens that are available in the vocabulary. In order to deal with
the words not available in the vocabulary, BERT uses a technique
called BPE based WordPiece tokenisation. In this approach an out of
vocabulary word is progressively split into subwords and the word is
then represented by a group of subwords. Since the subwords are part
of the vocabulary, we have learned representations an context for
these subwords and the context of the word is simply the combination
of the context of the subwords.</p>
</blockquote>
<p>But does that hold true for tasks like multi-label text classification? In my use case the text is full of not useful stopwords, punctuation, characters and abbreviations and it is multi-label text classification as mentioned earlier. And in fact the prediction accuracy is not good (after a few rounds of training using <a href=""https://github.com/kaushaltrivedi/fast-bert"" rel=""nofollow noreferrer"">fast-bert</a>). What do I miss here?</p>
","<python><nlp><preprocessing><bert><transformer>"
"86155","How to deal with a binary classification problem, where the instances in the negative class are very similar?","<p>Let's say, one wants to detect, whether a picture of a fixed size contains a cat or not. But as a dataset, you have 10000 pictures of cats, and 30000 pictures which don't contain a cat, but are very similar to each other. For example, let's assume, the 30000 pictures in the &quot;not cat&quot; class contain <strong>only</strong> pictures of one or two kinds of spiders.</p>
<p>When training a CNN, you will find that you achieve a high score on the test set (here high score = almost fully diagonal confusion matrix) but when you want to use the CNN in the real world, you find that almost everything gets classified as a cat.</p>
<p>Why does the network generalize badly in this case? Even if the dataset doesn't represent the kind of data, the CNN would see in the real world, shouldn't it be easy for the CNN to say &quot;I have seen 10000 examples of cats, therefore anything which doesn't look like a cat is not a cat&quot; ?</p>
<p>How would one deal with this problem (besides gathering more data)?</p>
","<neural-network><cnn><image-classification><binary>","81313","How to use a dataset with only one category of data","<p>I am performing a classification task, to try to detect an object. A picture of the environment is taken, candidates are generated of this possible object using vision algorithms, and once isolated, these candidates will be passed through a CNN for the final decision on whether the object has been detected or not. I am attempting to use transfer learning on InceptionV3 but am having difficulty training it, as I only have one set/class of images.</p>
<p>The dilemma is that I only have one class of data and when I pass it through the network, I get a 100% accuracy (because there is nothing to compare it to). How should I overcome this? Should I find more categories online to add to my dataset? What should these categories be?</p>
<p>Just to clarify, as an example, I have class &quot;cat&quot;.</p>
<p>Not &quot;cat&quot; and &quot;dog&quot;.</p>
<p>Not &quot;cat&quot; and &quot;no cat&quot;.</p>
<p>Just &quot;cat&quot;. That is what my dataset consists of at the moment.</p>
","<keras><cnn>"
"87328","Keep retweets during topic-modelling","<p>I got a dataset made out of tweets and I need to classify them into topics. For topic modelling with LDA I have cleaned out the dataset (removing stopwords, mentions, symbols, etc). Do I need to remove also retweets in order not to influence the LDA? If so, how should I finally classify all tweets since I could not use per-document-per-topic probability?</p>
","<machine-learning><nlp><r><data-mining><lda>","87321","Tweet Classification into topics- What to do with data","<p>Good evening,
First of all, I want to apologize if the title is misleading.
I have a dataset made of around 60000 tweets, their date and time as well as the username. I need to classify them into topics. I am working on topic modelling with LDA getting the right number of topics (I guess) thanks to <a href=""https://cran.r-project.org/web/packages/ldatuning/vignettes/topics.html"" rel=""nofollow noreferrer"">this R package</a>, which calculates the value of three metrics(&quot;CaoJuan2009&quot;, &quot;Arun2010&quot;, &quot;Deveaud2014&quot;). Since I am very new to this, I just thought about a few questions that might be obvious for some of you, but I can't find online.</p>
<ol>
<li><p>I have removed, before cleaning the data (removing mentions, stopwords, weird characters, numbers etc), all duplicate instances (having all three columns in common), in order to avoid them influencing the results of topic modelling. Is this right?</p>
</li>
<li><p>Should I, for the same reason mentioned before, remove also all retweets?</p>
</li>
<li><p>Until now, I thought about classifing using the &quot;per-document-per-topic&quot; probability. If I get rid of so many instances, do I have to classify them based on the &quot;per-word-per-topic&quot; probability?</p>
</li>
<li><p>Do I have to divide the dataset into testing and training? I thought that is a thing only in supervised training, since I cannot really use the testing dataset to measure quality of classification.</p>
</li>
<li><p>Antoher goal would be to classify twitterers based the topic they most are passionate about. Do you have any idea about how to implement this?</p>
</li>
</ol>
<p>Thank you all very much in advance.</p>
","<machine-learning><nlp><r><topic-model><lda>"
"87542","OneHotEncoding target variable?","<p>I'm working on a multiclass classifier with 6 classes on the target column and I was thinking about Hot Encoding the classes, thus having 6 target columns. Will this improve efficiency? I am using <code>sklearn</code>.</p>
<p>L.E: improve efficiency compared to just label encode</p>
","<machine-learning><multiclass-classification><one-hot-encoding>","18456","Encode multi-class response variable","<p>In a classification problem when the response variable has multi-class, e.g., ""sunny"",""rainy"",""cloudy"", how should we encode it? I know that for predictors like this, usually we do <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""nofollow noreferrer"">One Hot Encoding</a>, and if a predictors have too many classes, then we might just use the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"" rel=""nofollow noreferrer"">LabelEncode</a>. </p>

<p>When this multi-class issue occurs in the response variable, I guess we can just use LabelEncode() rather use using One Hot encoding right? Because if we use One Hot encoding, then we will have 2 variables as the response variable, and the machine learning algorithm in sklearn usually expects the response variable not to be a vector right? (I mean it expects a long 1D vector with length equal to the number of observations but not a 2D matrix). But on the other hand, if we just map ""sunny"", ""rainy"",""cloudy"" to {1,2,3} or {0,1,2}, or whatever 3 numbers, that will create a less than or greater than relationship among ""sunny"", ""rainy"",""cloudy"", which is not inherited in the original problem.</p>
","<scikit-learn><multiclass-classification><prediction>"
"88607","Use Categorical features in BERT model","<p>I am trying to fine-tune BERT-base model for binary text classification using multiple features. 3 text features, 4 categorical features. Text features having more than 500 tokens length, and four categorical features having binary values 0,1. For each categorical features created its corresponding derived feature. for ex. 'Impact_Code' having values 0 / 1, i derived new feature as 'Impact_Code_Derived' with the value 'Impact Code Not Available' for '0', and 'Impact Code Available' for 1. Like this, derived new features for all the 4 categorical features and using text features as it is.</p>
<p>While fine-tuning, i am getting bert embeddings for all the 8 features. i.e., I am taking last-hidden-state of BERT for each feature. The size is (Batch_Size x 512 x 768). I do avg. pooling for each token (1 token x 768 features) then the size become (Batch_Size x 512). and then concatenate all the 8 features avg. pooled output (Batch_size x 512 x 8) then pass it to Fully connected layer with tanh activation. This FC layer output size is 1024. Then this 1024 will be passed as input to another FC layer it will provides 2 outputs.</p>
<p>Since, i could see accuracy around 60%, I am not sure, whether</p>
<ol>
<li>it is right approach to handle categorical features like this</li>
<li>it is right approach to handle multiple features in bert like this.</li>
</ol>
<p>Please let me know your valuable suggestions. I searched in net, but couldn't find much clear solutions to clear my doubts.</p>
","<deep-learning><nlp><bert>","54888","How can I add custom numerical features for training to BERT fine tuning?","<p>I have currently fine tuned the BERT model on some custom data and I want to conduct some more experiments to increase the accuracy. </p>

<p>My original dataset consists of a pair of sentences (like MRPC dataset). I want to increase the accuracy of classification by adding some numerical features (which I will separately calculate). I wanted to know if I could train bert using this after I have already fine tuned it ?</p>

<p>I have read about some solutions people have proposed in the past like :
<em>'Extracting word embeddings (extract_features.py on Bert GITHUB) and combining that with my custom data to feed a single layer CNN network.'</em> I dont want to lose out on the accuracy the BERT network is providing me by only extracting features from the pretrained model. </p>

<p>So is there any way I can create a kind of hybrid model which first fine tunes BERT and then I add my features and it feeds into another model for an improved classification ?</p>

<p>P.S</p>

<p>As I am new to tensorflow and Deep learning, please let me know if there is something fundamentally wrong in my understanding. Thank you for your help</p>
","<deep-learning><tensorflow><bert>"
"38611","How can I make use of the labels subdivision in a Deep Learning Image classification?","<p>I want to solve an image classification problem. I want to recognize <code>cats</code> and <code>dogs</code>. However my labels are more detailed than this - they contain information about the dog/cat <code>breed</code>.
 In other words I want to classify images into A and B, but for each photo I know labels such as A.1, A.2, B.1, B.2.</p>

<p>If I simply cast the labels into A and B I lose some of the expertise I have. 
I can try to classify into detailed labels first and then cast the results.</p>

<p>However I don't want my neural network to focus on distinguishing between particular sublabels.</p>

<p>Are there any rules of a thumb on it? Any name for such a problem I don't know?</p>
","<neural-network><deep-learning><classification><labels>","29798","How to Classify an Image in a Class and a Subclass?","<p>I am a beginner in Machine Learning and I have a situation where an Image needs to be classified into first a super class, then a subclass.
For Example,
I have a set of images of fruits, containing images of Mangoes, Banana, Peaches, Apples etc. Then each fruit can either be fresh or rotten. I have training set containing images of rotten and fresh fruits for each Fruit type.
So for a test image I need to classify which fruit it is (Mango, Banana, etc) and then whether it is fresh or rotten. 
I want to know how to approach this kind of problem</p>
","<machine-learning><image-classification>"
"38676","Maximum Layers in ""any"" Neural Network","<p>I have about 6 months of experience in building and using Neural Networks with no prior/formal training.  As I explore this field further, I see a lot of discussions about determining how many layers/neurons to use and some rules of thumb of where to start.  In development of my network models, I use a brute-force approach by incrementing the layers and neurons by 1 for several cycles of epoch training and then select the ""best"" model out of those.  My understanding at this point, is that the layers/neurons represent the relationship between the NN inputs and the NN outputs, and while training my networks, I can determine the optimal number of layers to be 5 or more.  I determine optimal based on using cross validation data the network was not trained on.</p>

<p>Given the above, I have read from various sources statements like ""you never need more than 2 layers"" and ""you don't get better performance out of more than 2 layers"", etc.  I have also read comments that indicate more than 2 layers is expected.  Is the idea of 2 layers could be ""too much"" really correct and I should be focusing on expanding the number of neurons used while capping the number of layer at 2 for my brute-force determination of optimal layers?</p>

<p>EDIT:</p>

<p>Is it is true that in the vast majority of cases no more than 2 layers is warranted as it seems some are claiming?  My specific interest is in the area of numeric prediction, however, comments about other segments are welcome as well.</p>
","<neural-network><training>","26597","How to set the number of neurons and layers in neural networks","<p>I am a beginner to neural networks and have had trouble grasping two concepts:</p>

<ol>
<li>How does one decide the number of middle layers a given neural network have? 1 vs. 10 or whatever.</li>
<li>How does one decide the number of neurons in each middle layer? Is it recommended having an equal number of neurons in each middle layer or does it vary with the application?</li>
</ol>
","<machine-learning><neural-network><deep-learning><hyperparameter><hyperparameter-tuning>"
"38752","Are there cases where tree based algorithms can do better than neural networks?","<p>I trained an image auto-encoder on a large dataset, and now have for every image, an n-dimensional feature vector. This vector is not spatially correlated to the image. I now used this embedding space directly for classification, using fully connected dense layers. I have 55 classes, and the vector dimensionality is 1024. Even a single classification layer on top of the embedding, means 1024*55 parameters and it very easily overfits.</p>

<p>Since my embedding is not spatial in any sense, does it make sense to use Trees for classifying over the embedding space? I.e, feed the feature vectors to a tree based classification method, instead of a fully connected network. Are there cases, depending on the data, where trees outperform neural networks. If yes, what are such cases?</p>
","<machine-learning><neural-network><classification><decision-trees>","38328","When does decision tree perform better than the neural network?","<p>I was experimenting with different modelling methods including KNN, Decision Trees, Neural Networks and SVN and trying to fit my data to see which works the best. To my surprise, the decision tree works the best with training accuracy of 1.0 and test accuracy of 0.5. The neural networks, which I believed would always perform the best no matter what has a training accuracy of 0.92 and test accuracy of 0.42 which is 8% less than the decision tree classifier. </p>

<p>Could someone please explain the circumstances/cases where neural networks could have low accuracy when compared to a modelling technique like the decision tree. I had tried my neural network with different configurations like:</p>

<pre><code>1 hidden layer and 1 neuron : Train Accuracy 34% Test Accuracy 42%
7 hidden layers and 5 neurons in each layer: Train Accuracy 79% Test Accuracy 42%
1 hidden layer and 100 neurons: Train Accuracy 34% and Test Accuracy 35%
</code></pre>

<p>but not in a single case, I found the neural network to beat the decision tree test accuracy of 50%.</p>
","<machine-learning><python><neural-network><decision-trees>"
"90107","Results Random Forest Regression Optimization","<p>I'm going to optimize the hyperparameters of a Random Forest Regressor using scikit-learn and GridSearchCV, with the following code:</p>
<pre><code>from sklearn.model_selection import GridSearchCV
param_grid = {
    'bootstrap': [True],
    'max_depth': [30,40,50,60,79],
    'min_samples_leaf': [5,10,15,20],
    'min_samples_split': [10,12,14,16,20],
    'n_estimators': [80,100,110,120]
}

rf = RandomForestRegressor()

grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                          cv = 10, n_jobs = -1, verbose = 2)
grid_search.fit(X_train, y_train)
def evaluate(model, test_features, test_labels):
    predictions = model.predict(test_features)
    errors = abs(predictions - test_labels)
    mape = 100 * np.mean(errors / test_labels)
    accuracy = 100 - mape
    print('Model Performance')
    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))
    print('Accuracy = {:0.2f}%.'.format(accuracy))
    
    return accuracy
    
print (grid_search.best_params_)

best_grid = grid_search.best_estimator_
grid_accuracy = evaluate(best_grid, X_test, y_test)
print (grid_accuracy)
</code></pre>
<p>The performances after the optimization decrease in terms of RMSE. Basically, just using the standard library call the RMSE is lower than the RMSE with hyperparameters optimization. is it possible?</p>
","<scikit-learn><random-forest><hyperparameter-tuning>","82028","Is it possible to get worse model after optimization?","<p>I am trying recently to optimize models but for some reason, whenever I try to run the optimization the model score in the end is worse than before, so I believe I do something wrong.</p>
<p>in order to optimize my model I define param grid and than fit with the train data and then according to the results run again with nre parameters, e.g-</p>
<pre><code>#ROUND 1
param_grid={
    'max_depth': [3,4,5],
    'learning_rate':[0.1,0.01,0.05],
    'gamma': [0,0.25,1.0],
    'reg_lambda':[0,1.0,10.0],
    'scale_pos_weight':[1,3,5]
}

grid_search = GridSearchCV(estimator = clf_xgb, param_grid = param_grid, 
                          cv = 3, n_jobs = -1, verbose = 2)
grid_search.fit(X_train,y_train)
grid_search.best_params_

&gt;&gt;&gt;.....

</code></pre>
<p>(and now based on the result changing the params...)</p>
<p>after this step I choose the best hyperparameters and run the model;</p>
<pre><code>clf_xgb=xgb.XGBClassifier(seed=42,
                         objective='binary:logistic',
                         gamma=0,
                         learn_rate=0.7,
                         max_depth=6,
                         reg_lambda=0.8,
                         scale_pos_weight=1,
                         subsample=0.9,
                         cilsample_bytree=0.5)

clf_xgb.fit(X_train,
           y_train,
           verbose=True,
           early_stopping_rounds=10,
           eval_metric='aucpr',
           eval_set=[(X_test,y_test)])
</code></pre>
<p>The problem is that when I check the model score</p>
<pre><code>clf_xgb.score(X_test,y_test)
</code></pre>
<p>I always get lower score than what I got before the optimization which makes me suspect that I'm missing something in the way doing it/basic principle in this process.</p>
<p><strong>Is it possible that after running the optimization my score won't get better (and even worse?) ? Where is my mistake? Are there other parameters that could influence or improve my model?</strong></p>
","<python><scikit-learn><xgboost><optimization>"
"90164","Deep learning with Imbalanced classes","<p>I am trying to model a packet data with 1 dimensional CNN but I have a very imbalanced classes in my target. I have 3 classes as class 0 has 53000 cases, class 1 has 300 cases and class 2 has 150 cases. Thanks in advance!</p>
<p>I have tried what you have suggested but I do not think I am getting a good results(loss and accuracy), from the model.</p>
<p><a href=""https://i.stack.imgur.com/DtA3t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DtA3t.png"" alt=""enter image description here"" /></a></p>
","<neural-network><deep-learning><cnn><class-imbalance>","13490","How to set class weights for imbalanced classes in Keras?","<p>I know that there is a possibility in Keras with the <code>class_weights</code> parameter dictionary at fitting, but I couldn't find any example. Would somebody so kind to provide one?</p>

<p>By the way, in this case the appropriate praxis is simply to weight up the minority class proportionally to its underrepresentation?</p>
","<deep-learning><classification><keras><weighted-data>"
"90549","Looking for CART/ML model that works with relative data","<p>I am a beginner at AI and ML. I have been given a dataset, where I have noticed the columns are relative to one another. So is there any CART or ML model that can work with relative data ?</p>
<p>For example in Decision Tree, the tree looks like :</p>
<pre><code>  if X[0]&lt;192:
     if X[1]&gt;24:
       if X[2]&lt;12:
       ...
</code></pre>
<p>I'm looking for a Decision Tree, that works like this :</p>
<pre><code>  if X[0]&gt;X[1]:
   if X[1]&lt;X[2]:
      ...
</code></pre>
<p>Is there any such Machine Learning Model ?</p>
","<machine-learning><random-forest><decision-trees><boosting><cart>","60848","Can a decision in a node of a decision tree be based on comparison between 2 columns of the dataset?","<p>Assume the features in the dataframe are columns - A,B,C and my target is Y</p>

<p>Can my decision tree have a decision node which looks for say, <code>if A&gt;B then true else false</code>?</p>
","<classification><decision-trees>"
"39320","In supervised learning, how to get info from correlation?","<p>I am trying to build a classification model so I have tried to check the correlation between the features.</p>

<p><a href=""https://i.stack.imgur.com/nwzSs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nwzSs.png"" alt=""enter image description here""></a></p>

<p>Here Loan_Status is my target variable. </p>

<p>I just don't know how to extract information from this? Please help.
I have questions like.
Is -0.0047 corelation of ApplicantIncome with Loan Status useful?</p>
","<machine-learning><python><correlation>","893","How to get correlation between two categorical variable and a categorical variable and continuous variable?","<p>I am building a regression model and I need to calculate the below to check for correlations</p>

<ol>
<li>Correlation between 2 Multi level categorical variables</li>
<li>Correlation between a Multi level categorical variable and
continuous variable </li>
<li>VIF(variance inflation factor) for a Multi
level categorical variables</li>
</ol>

<p>I believe its wrong to use Pearson correlation coefficient for the above scenarios because Pearson only works for 2 continuous variables. </p>

<p>Please answer the below questions</p>

<ol>
<li>Which correlation coefficient works best for the above cases ? </li>
<li>VIF calculation only works for continuous data so what is the
alternative? </li>
<li>What are the assumptions I need to check before I use the correlation coefficient you suggest? </li>
<li>How to implement them in SAS &amp; R?</li>
</ol>
","<r><statistics><correlation>"
"91119","How to tune the proportion of minority class after oversampling in imbalanced data with cross-validation?","<p>I have an imbalanced data set and I want to balance them with <a href=""https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.html#imblearn.over_sampling.SMOTENC.fit_resample"" rel=""nofollow noreferrer"">SMOTENC</a> with cross-validation. In order to determine the performance of the classifier on the original data, I will cross-validation as follows:</p>
<p>Here, feature <code>X</code> is divided into <code>X_train</code> and <code>X_val</code>.</p>
<pre><code>scoring = {'accuracy' : make_scorer(accuracy_score), 
           'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}

model=RandomForestClassifier(n_estimators=50,random_state=10) 

results = cross_validate(estimator=model, X=X, y=Y, cv=10, scoring=scoring)
</code></pre>
<p>Based on this code, I can find the performance of random forest on the original data.</p>
<p>However, I am a bit confused about how I should compare the oversampled data with the original data. I want to determine the best proportion of data and I use the following code. Should I split the original data to <strong>train</strong> and <strong>val</strong> and oversample <strong>train</strong> data and find the performance on <strong>val</strong> data?
I am wondering if the results of these codes are comparable?</p>
<pre><code>percentage = np.arange(0.20,0.5,0.02)
Measurements = np.zeros( (np.size(percentage),4) )


skf = StratifiedKFold(n_splits=10)

for train_index, test_index in skf.split(X, y):
    X_train, X_val = X[train_index], X[test_index]
    y_train, y_val = y[train_index], y[test_index]



    counter = 0
    for per in percentage:

        smote_nc = SMOTENC(sampling_strategy = per, categorical_features=[0,1,2,3,4,6,8,9], random_state=0)
        X_resampled, y_resampled = smote_nc.fit_resample(X_train, y_train)

        X_resampled = pd.DataFrame(X_resampled, columns=X_val_train.columns)


        after_X_resampled = encoding(X_resampled)
        
        RF = RandomForestClassifier(n_estimators=50,random_state=10) 
        RF.fit(after_X_resampled, y_resampled)
        
        prediction = RF.predict(X_val)

        
        Measurements[counter,0] += accuracy_score(y_val, prediction)
        Measurements[counter,1] += precision_score(y_val, prediction)
        Measurements[counter,2] += recall_score(y_val, prediction)
        Measurements[counter,3] += f1_score(y_val, prediction)

        counter+=1
</code></pre>
","<classification><class-imbalance><imbalanced-learn><imbalanced-data>","82073","Why you shouldn't upsample before cross validation","<p>I have an imbalanced dataset and I am trying different methods to address the data imbalance. I found this <a href=""https://kiwidamien.github.io/how-to-do-cross-validation-when-upsampling-data.html"" rel=""nofollow noreferrer"">article</a> that explains the correct way to cross-validate when oversampling data using SMOTE technique.</p>
<p>I have created a model using AdaBoost algorithm and set the following parametres to be used in Grid Search:</p>
<pre class=""lang-py prettyprint-override""><code>    ada = AdaBoostClassifier(n_estimators=100, random_state=42)
    params = {
        'n_estimators': [50, 100, 200],
        'random_state': [42]
    }
</code></pre>
<p>According to the article, this is the <strong>wrong</strong> way to oversample:</p>
<pre class=""lang-py prettyprint-override""><code>    X_train_upsample, y_train_upsample = SMOTE(random_state=42).fit_sample(X_train, y_train)
    
    # cross-validate using grid search
    
    grid_naive_up = GridSearchCV(ada, param_grid=params, cv=kf, 
                                 scoring='recall').fit(X_train_upsample, 
                                                       y_train_upsample)
    grid_naive_up.best_score_
</code></pre>
<p>0.6715940782827282</p>
<pre class=""lang-py prettyprint-override""><code>    # test set
    recall_score(y_test, grid_naive_up.predict(X_test))
</code></pre>
<p>0.2824858757062147</p>
<p>Whereas the <strong>correct</strong> way to oversample is like so:</p>
<pre class=""lang-py prettyprint-override""><code>    from imblearn.pipeline import Pipeline, make_pipeline
    
    imba_pipeline = make_pipeline(SMOTE(random_state=42), 
                                  AdaBoostClassifier(n_estimators=100, random_state=42))
    cross_val_score(imba_pipeline, X_train, y_train, scoring='recall', cv=kf)
    new_params = {'adaboostclassifier__' + key: params[key] for key in params}
    grid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring='recall',
                            return_train_score=True)
    grid_imba.fit(X_train, y_train);


    # How well do we do on our validation set?
    grid_imba.best_score_
</code></pre>
<p>0.29015614186873506</p>
<pre class=""lang-py prettyprint-override""><code>    # compare this to the test set:
    y_test_predict = grid_imba.predict(X_test)
</code></pre>
<p>0.2824858757062147</p>
<p>So, according to the article, the first method is wrong because when upsampling before cross validation, the validation recall isn't a good measure of the test recall (28.2%).
However, when using the imblearn pipeline for upsampling as part of the cross validation, the validation set recall (29%) was a good estimate of the test set recall (28.3%). According to the article, the reason for this is:</p>
<blockquote>
<p>When upsampling before cross validation, you will be picking the most
oversampled model, because the oversampling is allowing data to leak
from the validation folds into the training folds.</p>
</blockquote>
<p>Can anyone explain to me simply how the oversampling allows data to leak into the validation and causes the overfitting? And why does this problem not occur in the imblearn pipeline?</p>
","<python><scikit-learn><cross-validation><class-imbalance><smote>"
"92968","Follow Up Question - Why use fit when already have fit_transform?","<p>This is a follow up question to: <a href=""https://datascience.stackexchange.com/questions/12321/whats-the-difference-between-fit-and-fit-transform-in-scikit-learn-models/92921"">What&#39;s the difference between fit and fit_transform in scikit-learn models?</a></p>
<p>I want to know why should we use <code>fit</code> at all when we have <code>fit_transform</code> which is much faster than using <code>fit</code> and <code>transform</code> separately? After all we will always transform the training data after fitting it. Do we have any use of <code>fit</code> all by itself?</p>
","<python><scikit-learn>","12321","What's the difference between fit and fit_transform in scikit-learn models?","<p>I do not understand the difference between the <code>fit</code> and <code>fit_transform</code> methods in scikit-learn. Can anybody explain simply why we might need to transform data?</p>
<p>What does it mean, fitting a model on training data and transforming to test data? Does it mean, for example, converting categorical variables into numbers in training and transforming the new feature set onto test data?</p>
","<python><scikit-learn>"
"96522","Incorrect derivative","<p>Currently I'm reading a book named &quot;Grokking Deep Learning&quot; and I'm confused with the way author takes derivative from function. Let me explain.</p>
<p>We have loss function, where <em><strong>pred</strong></em> is prediction and <em><strong>goal_pred</strong></em> is goal prediction (constant value that is equal to 1)</p>
<p><a href=""https://i.stack.imgur.com/D1WtZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D1WtZ.png"" alt=""enter image description here"" /></a></p>
<p>Author of the book writes that the derivative is <em><strong>weight_delta</strong></em></p>
<p><a href=""https://i.stack.imgur.com/13Xqg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/13Xqg.png"" alt=""enter image description here"" /></a></p>
<h2>But!</h2>
<p><span class=""math-container"">$
  \begin{cases}
    error(weight) = (pred - goalPred)^2   \\
    pred = input \cdot weight  \\
  \end{cases}  =&gt; error(weight) = (input \cdot weight - goalPred)^2
$</span></p>
<p><span class=""math-container"">$error(weight)' = 2(input \cdot weight - goalPred) \cdot input$</span> <br />
<span class=""math-container"">$error(weight)' = 2(pred - goalPred) \cdot input$</span> <br />
<span class=""math-container"">$error(weight)' = 2 \cdot delta \cdot input$</span></p>
<p><strong>Here is the question: why the derivative in the book is not correct or I'm wrong somewhere?</strong></p>
","<loss-function><mathematics>","53171","Why does putting a 1/2 in front of the squared error make the math easier?","<p>Per wiki, the mean squared error (MSE) <a href=""https://en.wikipedia.org/wiki/Mean_squared_error#Predictor"" rel=""nofollow noreferrer"">looks</a> like:</p>

<p><span class=""math-container"">$$
\operatorname {MSE} ={\frac {1}{m}}\sum _{i=1}^{m}(y_{i}-{\hat y_{i}})^{2}
$$</span></p>

<p>The <a href=""https://youtu.be/K69jbdvMPMc?t=302"" rel=""nofollow noreferrer"">professor</a> added a <span class=""math-container"">$1\over2$</span> in front of the formula and explained it a little bit. I am a little bit confused. How does putting a <span class=""math-container"">$1\over2$</span> in front of the squared error make the math easier?</p>
","<machine-learning><loss-function>"
"96822","Why positional embeddings are summed with word embeddings instead of concatenation?","<p>I was trying to understand the architecture of transformers especially for the &quot;Positional encoding&quot; part and more precisely on &quot;Why positional embeddings are summed with word embeddings instead of concatenation?&quot;</p>
<p>I came across an article saying that:</p>
<blockquote>
<p>&quot;Initially, if we pay attention to the figure 2, we will find out that
only the first few dimensions of the whole embedding are used to store
the information about the positions (Note that the reported embedding
dimension is 512 despite our small toy example). And since the
embeddings in the Transformer are trained from scratch, the parameters
are probably set in a way that the semantic of words does not get
stored in the first few dimensions to avoid interfering with the
positional encoding.&quot;</p>
</blockquote>
<p>Here, I have a little problem understanding what it means when it says &quot;the parameters are probably set in a way that the semantics of words does not get stored in the first few dimensions?&quot;
<a href=""https://i.stack.imgur.com/JY5Xv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JY5Xv.png"" alt=""figure2"" /></a>
I did not really get it.<br />
I would be very grateful if someone could explain this to me or if anyone could recommend an article or any other resource that could help.</p>
","<deep-learning><nlp><transformer>","55901","In a Transformer model, why does one sum positional encoding to the embedding rather than concatenate it?","<p>While reviewing the Transformer architecture, I realized something I didn't expect, which is that :</p>

<ul>
<li>the positional encoding is summed to the word embeddings </li>
<li>rather than concatenated to it.</li>
</ul>

<blockquote>
  <p><a href=""https://i.stack.imgur.com/bFPI9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bFPI9.png"" alt=""positional encoding summed to word embedding""></a></p>
  
  <p><a href=""http://jalammar.github.io/images/t/transformer_positional_encoding_example.png"" rel=""noreferrer"">http://jalammar.github.io/images/t/transformer_positional_encoding_example.png</a></p>
</blockquote>

<p>Based on the graphs I have seen wrt what the encoding looks like, that means that :</p>

<ul>
<li>the first few bits of the embedding are completely unusable by the network because the position encoding will distort them a lot, </li>
<li>while there is also a large amount of positions in the embedding that are only slightly affected by the positional encoding (when you move further towards the end).</li>
</ul>

<blockquote>
  <p><a href=""https://i.stack.imgur.com/XLT9V.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/XLT9V.png"" alt=""graph shows positional encoding affects firsts logits a lot, last logits hardly not""></a></p>
  
  <p><a href=""https://www.tensorflow.org/beta/tutorials/text/transformer_files/output_1kLCla68EloE_1.png"" rel=""noreferrer"">https://www.tensorflow.org/beta/tutorials/text/transformer_files/output_1kLCla68EloE_1.png</a></p>
</blockquote>

<p>So, why not instead have smaller word embeddings (reduce memory usage) and a smaller positional encoding retaining only the most important bits of the encoding, and instead of summing the positional encoding of words keep it concatenated to word embeddings?</p>
","<nlp><encoding><transformer><attention-mechanism>"
"97947","Don't understand Channels in Covolutional Layers","<p>I'm struggling to understand the concept of 'Channels'. What does a channel mean in the context of an image. I understand that a grey scale image only has 1 channel, and a RGB has 3, but then I see people saying you can have 100+ channels. This doesn't make sense to me because a channel seems like a representation of the image. So if you use grey scale, you only need a single representation, and if you have a colour image, you have Red,Green,Blue to represent the image. How on earth can you have more then 3 channels? I know you can but I don't know why or how, or even what it means to have more then 3 channels.</p>
<p>I've done extensive research, but no one explains what exactly a channel is beyond 3.</p>
","<neural-network><cnn><image-classification>","64278","What is a channel in a CNN?","<p>I was reading an article about convolutional neural networks, and I found something that I don't understand, which is:</p>
<blockquote>
<p>The filter must have the same number of channels as the input image so that the element-wise multiplication can take place.</p>
</blockquote>
<p>Now, what I don't understand is: What is a channel in a convolutional neural network? I have tried looking for the answer, but can't understand what is it yet.</p>
<p>Can someone explain it to me?</p>
<p>Thanks in advance.</p>
","<machine-learning><neural-network><cnn>"
"45465","How do we go about imbalanced data for prediction problem?","<p>As in classification we have imbalanced classes, we use up-sampling or down-sampling and other techniques, what do we do when we have imbalanced data in prediction problems, for example, I have distribution of outputs like 90% value1, 5% value2, 3%value3, 2% value4?</p>
","<regression><prediction><class-imbalance>","33188","Class imbalance strategies","<p>When dealing with the class imbalance problem in a binary classifier, there are three ways I know of to address it: over-sampling, under-sampling and using cost-sensitive methods. </p>

<p>Are there any guidelines, rules of thumb or general strategies to choose among these methods? A possible answer would be: over-sample when the positive class has more than 100 instances (I just made it up).</p>
","<machine-learning><classification><class-imbalance>"
"102176","Can I retrain my best model on all available data?","<p>I split data on Zillow single-unit properties into train-validation-test 70-15-15 and trained a few different sklearn linear models to predict selling price. I chose the best one based on validation data without tuning the hyperparameters. This left the test set on which the model performed the same as it did on the validation set.</p>
<p>If I want to put the model into production and make predictions on new home sales, wouldn't I get the best results if I retrained the best model on all available data prior to the last data point? Am I not wasting data that could improve the model's weights?</p>
","<machine-learning><linear-regression>","33008","Is it always better to use the whole dataset to train the final model?","<p>A common technique after training, validating and testing the Machine Learning model of preference is to use the complete dataset, including the testing subset, to train a <strong>final model</strong> to <strong>deploy</strong> it on, e.g. a product. </p>

<blockquote>
  <p>My question is: Is it always for the best to do so? What if the
  performance actually deteriorates?</p>
</blockquote>

<p>For example, let us assume a case where the model scores around 65% in classifying the testing subset. This could mean that either the model is trained insufficiently OR that the testing subset consists of outliers. In the latter case, training the final model with them would decrease its performance and you find out only after deploying it. </p>

<p>Re-phrasing my initial question:</p>

<blockquote>
  <p>If you had a <strong>one-time demonstration of a model</strong>, such as deploying it
  on embedded electronics on-board an expensive rocket experiment, would you trust a
  model that has been re-trained with the test subset in the final step without being
  re-tested on its new performance?</p>
</blockquote>
","<machine-learning><dataset><training><accuracy>"
"102509","How does Gradient Descent work?","<p>I know the calculus and the famous hill and valley analogy (so to say) of gradient descent. However, I find the update rule of the weights and biases quite terrible. Let's say we have a couple of parameters, one weight 'w' and one bias 'b'. Using SGD, we can update both w and b after the evaluation of each mini-batch. If the size of the mini-batch is 1, we give way to online learning. <em><strong>What if I do not want to use any of these methods and simply want to use &quot;Gradient descent&quot; in its entirety? What is the update rule in that case? To be more precise; at what step does w and b get updated? And at what step do we stop?</strong></em></p>
<p>That said, the elephant in the room is the initial value of w and b. <em><strong>What is the parameter for choosing the first values of w and b?</strong></em></p>
","<machine-learning><neural-network><gradient-descent>","44703","How does Gradient Descent and Backpropagation work together?","<p>Please forgive me as I am new to this. I have attached a diagram trying to model my understanding of neural network and Back-propagation? From videos on Coursera and resources online I formed the following understanding of how neural network works:</p>

<ol>
<li>Input is given, which gets weight assigned to it using a probability distribution.</li>
<li>The activation functions use the weights to provide the predicted value.</li>
<li>The cost or loss functions calculate the error of the prediction between the actual class and the predicted value.</li>
<li>The optimization functions such as gradient descent use the results of the cost function to minimize the error.</li>
</ol>

<p>If the above is correct then I am struggling to understand the connection between Gradient Descent and Backpropagation?</p>

<p>Here is an image of my understanding so far: <img src=""https://i.stack.imgur.com/7Ui1C.png""></p>
","<machine-learning><neural-network><gradient-descent><backpropagation><cost-function>"
"102519","Best Way to find the important features for the model","<p>I have data with 245 Features and almost all of the features are categorical. I would like to know what will be the best approach to find the important features for training the model. I know I can use tree approach for finding important features, but is there any other way around this?</p>
<p>This might sound vague, but would love to know your input.</p>
","<machine-learning><feature-selection><decision-trees><training><feature-engineering>","78298","Feature Importance without Random Forest Feature Importances","<p>Is their an intuitive way of finding feature importances without just using the random forest feature importances method?</p>
<p>I have a binary logistic regression problem where I have binary features (1 or 0) and a binary target (1 or 0).</p>
<p>I want to see which features are most important towards predicting the target and somehow rank them.</p>
<p>I did an odds ratio for each feature, which gave me some idea of importance.</p>
<p>Are there any other methods?</p>
","<machine-learning><feature-selection><logistic-regression>"
"103087","Different AUC values for xgb and sklearn built in functions","<p>The model is trained with early stopping on a validation set:</p>
<pre><code>#TRAINING
model = xgb.train(param, dtrain,
                  evals = eval_set,
                  num_boost_round=1000,
                  early_stopping_rounds=5,
                  verbose_eval = 20,
                  )
</code></pre>
<p>last iteration: [440]   train-auc:0.62159   valid-auc:0.62563</p>
<p>But when computing the AUC in train with scikit-learn function</p>
<pre><code>from sklearn.metrics import roc_auc_score
roc_auc_score( y_train, model.predict(dtrain) )
</code></pre>
<p>a new value is obtained: 0.6261919302504069</p>
<p>What could be the cause?</p>
","<scikit-learn><xgboost><training><auc>","43247","AUC ROC in keras is different when using tensorflow or scikit functions.","<p>Two solutions for using AUC-ROC to train keras models, proposed <a href=""https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras"">here</a> worked for me. But using tensorflow or scikit rocauc functions I get different results.    </p>

<pre><code>def auc(y_true, y_pred):
    auc = tf.metrics.auc(y_true, y_pred)[1]
    K.get_session().run(tf.local_variables_initializer())
    return auc
</code></pre>

<p>and </p>

<pre><code>def auc(y_true, y_pred):
    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)
</code></pre>

<p>Based on the history, it looks like both are being applied to train and validation.
When I plot history metrics, tensorflow curve looks very smoothed compared to scikit.</p>

<p>Shouldn't I get about the same results using both functions?</p>
","<deep-learning><keras><scikit-learn><performance>"
"46941","Loss is decreasing but val_loss not!","<p>If loss is decreasing but val_loss not, what is the problem and how can I fix it?</p>

<p>I get such vague result:
<a href=""https://i.stack.imgur.com/2ykWK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2ykWK.png"" alt=""enter image description here""></a></p>
","<lstm><loss-function>","43191","Validation loss is not decreasing","<p>I am trying to train a LSTM model. Is this model suffering from overfitting? </p>

<p>Here is train and validation loss graph: </p>

<p><a href=""https://i.stack.imgur.com/l3fw7.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/l3fw7.png"" alt=""loss""></a></p>
","<machine-learning><neural-network><regression><lstm><rnn>"
"103647","Why is the signed gradient of the image used for adversarial examples","<p>In this <a href=""https://arxiv.org/pdf/1412.6572.pdf"" rel=""nofollow noreferrer"">paper</a>, the gradient of the loss w.r.t. to the image is computed, but its <a href=""https://en.wikipedia.org/wiki/Sign_function"" rel=""nofollow noreferrer"">sign</a> is used. Why is using the <em>sign</em>-method better?</p>
","<deep-learning><neural-network><gradient><adversarial-ml>","88061","Why a sign of gradient (plus or minus) is not enough for finding a steepest ascend?","<p>Consider a simple 1-D function <span class=""math-container"">$y = x^2$</span> to find a maximum with the gradient ascent method.</p>
<p>If we start in point 3 on <code>x</code>-axis:
<span class=""math-container"">$$ \frac{\partial f}{\partial x} \biggr\rvert_{x=3}   = 2x \biggr\rvert_{x=3} = 6 $$</span></p>
<p>This means that a direction in which we should move is a <span class=""math-container"">$6$</span>.</p>
<p>Gradient ascent gives rule to update:
x = old_x + learning_rate * gradient</p>
<p>What I can't understand why we need to multiply a <code>learing_rate</code> with <code>gradient</code>. Why we can't just use <code>x = old_x + learning_rate * sign(gradient)</code>.</p>
<p>Because if we made a  <code>learning_rate</code> step in a positive direction it is already a maximum switch of <code>x</code> we can make.</p>
<p>I know the reasoning behind finding maximum direction in this equation:</p>
<p><span class=""math-container"">$$grad(𝑓(𝑎))⋅𝑣⃗=|grad(𝑓(𝑎))||𝑣⃗|cos(𝜃) $$</span></p>
<p>But I can't undestand why just to accept a sign of gradient (plus or minus) is not enough for ascending.</p>
","<gradient-descent><learning-rate><gradient>"
"48235","Date transformation for KNN to get distance","<p>I have data set with date features like 01/01/2019 and I would like to use KNN. However, I cannot find a good transformation for dates that has a meaningful distance result for the last feature.</p>

<p>For example:</p>

<p>f1 | 1 | 2 | 3 | 4 | 01/01/2019</p>

<p>f2 | 10 | 3 | 12 | 1 | 14/01/2019</p>

<p>Does anyone have any recommendations?</p>
","<machine-learning><data><distance><k-nn>","48234","Date transformation for KNN","<p>I have data set with date features like <code>01/01/2019</code> and I would like to use KNN. However, I cannot find a good transformation for dates that has a meaningful distance result for the last feature.</p>

<p>For example:</p>

<p>f1 | 1 | 2 | 3 | 4 | 01/01/2019</p>

<p>f2 | 10 | 3 | 12 | 1 | 14/01/2019</p>

<p>Does anyone have any recommendations?</p>
","<machine-learning><data><distance><k-nn>"
"51542","Using Matplotlib","<p>I am new to data science. I use Anaconda on windows 7.
I plotted a sine curve by doing the following on iPython:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0, 10, 1000)
y = np.sin(x)
plt.plot(x, y)
</code></pre>

<p>And I got this:</p>

<p><a href=""https://i.stack.imgur.com/IDfA1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IDfA1.png"" alt=""enter image description here""></a></p>

<p>But when I got ready to name the axes, the curve disappeared. I wrote this code:</p>

<pre><code>plt.xlabel(""Time"")
</code></pre>

<p>And I got this:</p>

<p><a href=""https://i.stack.imgur.com/72HH9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/72HH9.png"" alt=""enter image description here""></a></p>

<p>I also wrote this:</p>

<pre><code>plt.ylabel(""Speed"")
</code></pre>

<p>And got this:</p>

<p><a href=""https://i.stack.imgur.com/v4dpi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v4dpi.png"" alt=""enter image description here""></a></p>

<p>The lecture video I use as a guide does not display images until the tutor enters the code ""plt.show()"". That gives me the impression that iPython adds all the information together behind the scene and displays the final result on request. But as you can see, my iPython displays the image without the ""plt.show()"" code and does not add previous instructions.</p>

<p>So, what should I do? (i.e. how can I plot a curve with labelled axes?) </p>
","<deep-learning><numpy><plotting><matplotlib><ipython>","51509","Naming the Axes of my Graph on iPython Using Matplotlib.pyplot","<p>I am new to data science. I use Anaconda on windows 7.
I plotted a sine curve by doing the following on iPython:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0, 10, 1000)
y = np.sin(x)
plt.plot(x, y)
</code></pre>

<p>And I got this:
<a href=""https://i.stack.imgur.com/sUEr3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sUEr3.png"" alt=""enter image description here""></a>
But when I got ready to name the axes, the curve disappeared. I wrote this code:</p>

<pre><code>plt.xlabel(""Time"")
</code></pre>

<p>And I got this:
<a href=""https://i.stack.imgur.com/5d3RW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5d3RW.png"" alt=""enter image description here""></a>
I also wrote this:</p>

<pre><code>plt.ylabel(""Speed"")
</code></pre>

<p>And got this:
<a href=""https://i.stack.imgur.com/IZJX5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IZJX5.png"" alt=""enter image description here""></a></p>

<p>So my question is, how can I plot a curve with labelled axes? (In other words, I will like the x-axis to be <em>Time</em>, the y-axis to be <em>Speed</em> and the curve intact)</p>
","<numpy><plotting><matplotlib>"
"104317","how to choose the best machine learning algorithms from all kinds of algorithms?","<p>guys, I am a beginner at data science and I’ve been learning machine learning for a while with some courses online without any help of a teacher and after I’ve got to work with some real projects on my own, I have found some questions and couldn’t find the answers so if you be could help me in this problem and guide me to a better path, I would be thankful,
Here is my question:
When I want to find a model for my dataset that I find that there are lots of algorithms that I can use.
I know how to minimize selection choices by separating supervised and unsupervised algorithms and the purpose of the problem I am trying to solve.
But after that, there are also lots of algorithms to choose from, even in the sklearn library that I currently use, and there are lots of algorithms that I don’t know and there might work better in my problem and also there are deep learning algorithms that are stronger than machine learning algorithms and after looking for them I’ve got tired and a simple project cost me whole 2 weeks but I wasn’t satisfied with the result at the end neither.
So, what should I do?
Do I have to memorize all the algorithms in machine learning libraries like sklearn?
Or should I abandon learning machine learning algorithms and start learning deep learning?</p>
","<machine-learning><deep-learning><scikit-learn><machine-learning-model><model-selection>","104318","How can I choose the best machine learning algorithms from all kinds of algorithms?","<p>I am a beginner at data science and I’ve been learning machine learning for a while with some courses online without any help of a teacher. After I’ve got to work with some real projects on my own, I have found some questions and couldn’t find the answers so if you be could help me in this problem and guide me to a better path, I would be thankful,</p>
<p>Here is my question:</p>
<p>When I want to find a model for my data set, I find that there are lots of algorithms that I can use. I know how to minimize selection choices by separating supervised and unsupervised algorithms and the purpose of the problem I am trying to solve.</p>
<p>But after that, there are also lots of algorithms to choose from, even in the <a href=""https://en.wikipedia.org/wiki/Scikit-learn"" rel=""nofollow noreferrer"">scikit-learn</a> library that I currently use, and there are lots of algorithms that I don’t know. They might work better in my problem and also there are deep learning algorithms that are stronger than machine learning algorithms. After looking for them, I’ve got tired and a simple project cost me a whole two weeks, but I wasn’t satisfied with the result at the end either.</p>
<ul>
<li>So, what should I do?</li>
<li>Do I have to memorize all the algorithms in machine learning libraries, like scikit-learn?</li>
<li>Or should I abandon learning machine learning algorithms and start learning deep learning?</li>
</ul>
","<machine-learning><deep-learning><scikit-learn><machine-learning-model><model-selection>"
"52915","Labeling audio dataset","<p>I would like to try and create my own audio dataset which I can then use to train machine learning models for classification. The data that I've gathered consists of multiple long audio files of around 1 hour each, containing audio of different categories. Since this is my first time working with audio files instead of data in a tabular format, I am a bit lost on how the do the labeling/preparation.</p>

<p>Most information I find on the internet is mainly related to applying machine learning models to existing pre-labeled datasets. I am hoping to find some more information specifically about what would be the best way to approach the labeling and, if possible, also some information regarding the division of the long audio files into much shorter audio snippets.</p>
","<dataset><preprocessing><audio-recognition>","55566","Tool for labeling audio","<p>I have few thousand audio signals to label into 2 different classes and save them to numpy array for further training of models. MATLAB recently released <code>Signal Labeler</code> for their <code>Signal Analyzer</code>, that could help to label time series, but for certain reasons, I can't use it. </p>

<p>Is there any specific tool for analysis and labeling of time series for Python? It is not necessary to save data and labels into numpy arrays, <code>.csv</code> format or anything similar is suitable as well.</p>

<p>Will appreciate any help!</p>
","<time-series><labels><audio-recognition>"
"57708","What should be the criteria to select features using correlation factors between features?","<p>For the titanic dataset, I have done some feature engineering (one-hot encoded the features) and now I have developed a heatmap to view the <em>correlation</em> between different features. </p>

<ol>
<li><p>I'm not able to understand what to do with them. Lets say two features are highly correlated, eg, in the image, <code>Name_title_mr</code> and <code>sex_0</code> (ie. male) are having correlation factor of 0.84. So, does that mean I should drop <code>Name_title_mr</code>, since <code>sex_0</code> is a very important feature. (This we know with experience, that sex in titanic is very important but is there any way we could view this as well by just observing the heatmap?).</p></li>
<li><p>One more doubt I have is: How would I know that I can just add two features, like <code>sibsp</code> and <code>parch</code> ? I have seen many kernels where they just create one feature with <code>no_of_family_members</code> by just adding <code>sibsp</code> and <code>parch</code>.</p></li>
</ol>

<p>Will that adding of two features helpful? and should I drop the individual features <code>sibsp</code> and <code>parch</code> in that case?<a href=""https://i.stack.imgur.com/klyvr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/klyvr.png"" alt=""enter image description here""></a></p>

<ol start=""3"">
<li><code>sibsp_1</code> and <code>sibsp_0</code> (<code>sibsp_1</code> is when <code>sibsp=2</code> and <code>sibsp_0</code> is when <code>sibsp=1</code>) seems to be highly negatively correlated. Should I consider some action on this as well?</li>
</ol>
","<machine-learning><feature-engineering><correlation><kaggle><heatmap>","13104","How to further improve the kaggle titanic submission accuracy?","<p>I am working on the Titanic dataset. So far my submission has 0.78 score using soft majority voting with logistic regression and random forest. As for the features, I used Pclass, Age, SibSp, Parch, Fare, Sex, Embarked.</p>

<p>My question is how to further boost the score for this classification problem? </p>

<p>One thing I tried is to add more classifiers for the majority voting, but it does not help, it even worthens the result. How do I understand this worthening effect?</p>

<p>Thanks for your insight.</p>
","<machine-learning><classification><beginner>"
"60731","correlation between categorical(ordinal) and discrete(continuous) value","<p>I am doing my bi variate analysis  but right now looking to see the correlation between  my atributes</p>

<p>some are categorical 5 levels and others amount of money.</p>

<p>how can I see the correlation between them ?</p>

<p>do I have to create class for my money amount?</p>
","<correlation>","893","How to get correlation between two categorical variable and a categorical variable and continuous variable?","<p>I am building a regression model and I need to calculate the below to check for correlations</p>

<ol>
<li>Correlation between 2 Multi level categorical variables</li>
<li>Correlation between a Multi level categorical variable and
continuous variable </li>
<li>VIF(variance inflation factor) for a Multi
level categorical variables</li>
</ol>

<p>I believe its wrong to use Pearson correlation coefficient for the above scenarios because Pearson only works for 2 continuous variables. </p>

<p>Please answer the below questions</p>

<ol>
<li>Which correlation coefficient works best for the above cases ? </li>
<li>VIF calculation only works for continuous data so what is the
alternative? </li>
<li>What are the assumptions I need to check before I use the correlation coefficient you suggest? </li>
<li>How to implement them in SAS &amp; R?</li>
</ol>
","<r><statistics><correlation>"
"61123","Apply a clustering algorithm on categorical data with features of multiple values","<p>Let us I have a people data like gender, age, marital status, education, employment, hobbies.</p>

<p>I want to make clusters of those people, having some similarity/common among them (for example they have common hobby, education, age etc.).</p>

<p>Here there is a sample of my dataset:</p>

<p><a href=""https://i.stack.imgur.com/1arR0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1arR0.png"" alt=""enter image description here""></a></p>

<p>I should use an algorithm which works with categorical data like K-Prototypes but I am not sure how to specifically handle the hobbies, because that feature may have many values from 1 to N.</p>
","<python><clustering><k-means><categorical-data>","22","K-Means clustering for mixed numeric and categorical data","<p>My data set contains a number of numeric attributes and one categorical.</p>
<p>Say, <code>NumericAttr1, NumericAttr2, ..., NumericAttrN, CategoricalAttr</code>,</p>
<p>where <code>CategoricalAttr</code> takes one of three possible values: <code>CategoricalAttrValue1</code>, <code>CategoricalAttrValue2</code> or <code>CategoricalAttrValue3</code>.</p>
<p>I'm using default <a href=""https://blog.west.uni-koblenz.de/2012-07-14/a-working-k-means-code-for-octave/"" rel=""noreferrer"">k-means clustering algorithm implementation for Octave</a>.
It works with numeric data only.</p>
<p>So my question: is it correct to split the categorical attribute <code>CategoricalAttr</code> into three numeric (binary) variables, like <code>IsCategoricalAttrValue1, IsCategoricalAttrValue2, IsCategoricalAttrValue3</code> ?</p>
","<data-mining><clustering><octave><k-means><categorical-data>"
"61487","Getting worse results after Hyperparameter Tuning(Grid/Random Search/TPOT)","<p>I have a problem with Hyperparameter Tuning. Usually I getting almost the same results(or worse) than before tuning. Usually default parameters of classificator(regressor) give me a best score. Anybody could recommend other techniques or give me a tip what I should improve?</p>
","<machine-learning><hyperparameter-tuning>","82028","Is it possible to get worse model after optimization?","<p>I am trying recently to optimize models but for some reason, whenever I try to run the optimization the model score in the end is worse than before, so I believe I do something wrong.</p>
<p>in order to optimize my model I define param grid and than fit with the train data and then according to the results run again with nre parameters, e.g-</p>
<pre><code>#ROUND 1
param_grid={
    'max_depth': [3,4,5],
    'learning_rate':[0.1,0.01,0.05],
    'gamma': [0,0.25,1.0],
    'reg_lambda':[0,1.0,10.0],
    'scale_pos_weight':[1,3,5]
}

grid_search = GridSearchCV(estimator = clf_xgb, param_grid = param_grid, 
                          cv = 3, n_jobs = -1, verbose = 2)
grid_search.fit(X_train,y_train)
grid_search.best_params_

&gt;&gt;&gt;.....

</code></pre>
<p>(and now based on the result changing the params...)</p>
<p>after this step I choose the best hyperparameters and run the model;</p>
<pre><code>clf_xgb=xgb.XGBClassifier(seed=42,
                         objective='binary:logistic',
                         gamma=0,
                         learn_rate=0.7,
                         max_depth=6,
                         reg_lambda=0.8,
                         scale_pos_weight=1,
                         subsample=0.9,
                         cilsample_bytree=0.5)

clf_xgb.fit(X_train,
           y_train,
           verbose=True,
           early_stopping_rounds=10,
           eval_metric='aucpr',
           eval_set=[(X_test,y_test)])
</code></pre>
<p>The problem is that when I check the model score</p>
<pre><code>clf_xgb.score(X_test,y_test)
</code></pre>
<p>I always get lower score than what I got before the optimization which makes me suspect that I'm missing something in the way doing it/basic principle in this process.</p>
<p><strong>Is it possible that after running the optimization my score won't get better (and even worse?) ? Where is my mistake? Are there other parameters that could influence or improve my model?</strong></p>
","<python><scikit-learn><xgboost><optimization>"
"63680","Binary Classification - One Hot Encoding preventing me using Test Set","<p>I have a preprocessing pipeline that includes replacing missing values and onehotencoding for the categorical variables. </p>

<p>When I try to use my model on the test set, it explains that the number of columns it expects differs. This is due to one hot encoding</p>

<p>One option I considered was passing the full dataset into the pipeline and then seperating into test and split. However, this causes data leakage as the missing values it capturing values from the testset.</p>

<p>Please let me know how to prevent this. </p>

<p>Thanks, </p>
","<machine-learning><encoding><one-shot-learning>","54052","Obtaining consistent one-hot encoding of train / production data","<p>I'm building an app that will require user input. Currently, on the training set, I run the following code, in which <code>data</code> is a pandas dataframe with a combination of categorical and numerical columns.</p>

<pre><code>dummified_data = data.get_dummies()
train_data = dummified_data[:10000]
test_data = dummified_data[10000:12000]
</code></pre>

<p>Currently, I have a hand-written function that takes user-inputted data and transforms it into a format like dummy data. This doesn't seem sustainable as the number of columns/the size of my categorical variables grows.</p>

<p>Is there a way to dummify training data and production data consistently?</p>
","<python><pandas><dummy-variables>"
"64262","Can someone please explain Lovasz softmax loss? as its a bit difficult to understand why it works well from the original paper","<p>Lovasz Softmax is used a lot these days for segmentation problem and the original paper is really bad at explaining why it works.</p>
","<loss-function><softmax>","57081","Intuitive explanation of Lovasz Softmax loss for Image Segmentation problems","<p>Lovasz Softmax is used a lot these days for segmentation problem and the original <a href=""https://arxiv.org/pdf/1705.08790.pdf"" rel=""nofollow noreferrer"">paper</a> is really bad at <em>explaining</em> why it works. </p>
","<deep-learning><image-classification><loss-function>"
"64934","Dummy encoding the categorical variables using the changed version of OneHotEncoder","<p>This is my code, I was trying to dummy encode the first column of X using OneHotEncoder but it was showing error and the documentation page of OneHotEncoder says that it has been changed and I wasn't able to figure out how I can do it using the changed version. the data type of X is numpy.ndarray.</p>

<pre><code>from sklearn.preprocessing import LabelEncoder, OneHotEncoder
onehotencoder=OneHotEncoder(categorical_features = [0])
X[:, 0]=onehotencoder.fit_transform(X).toarray()
</code></pre>
","<python><categorical-data><dummy-variables><one-hot-encoding><categorical-encoding>","56831","String handling by OneHotEncoder","<p>I am reading everywhere on new questions and blogs that since version 0.20, OneHotEncoder is able to handle string features.</p>
<p>Moreover, the documentation is what looks more ambiguous. Here are the first two lines from the documentation:</p>
<blockquote>
<p><em><strong>Encode categorical integer features as a one-hot numeric array. The input to this transformer should be an array-like of integers or
strings, denoting the values taken on by categorical (discrete)
features.</strong></em></p>
</blockquote>
<p>First line says it</p>
<blockquote>
<p>encodes categorical integer features</p>
</blockquote>
<p>and the next line says</p>
<blockquote>
<p>input should be array like of integers or strings.</p>
</blockquote>
<p>When i tried it, i still got the value error.</p>
<pre><code>print(X.columns)
encoder = OneHotEncoder(categorical_features=[1,4,5])
encoder.fit(X)

Index(['age', 'sex', 'bmi', 'children', 'smoker', 'region'], dtype='object')
ValueError: could not convert string to float: 'female'
</code></pre>
<p>I am aware of the means to handle encoding of string features with <code>LabelEncoder</code>, <code>ColumnTransfomer</code> and <code>pd.getDummies()</code> but specifically want to understand about this.</p>
","<python><scikit-learn><pandas><encoding>"
"66473","What is the difference between ""fit_transform"" and ""transform"" methods when using ""SimpleImputer""?","<p>I have following code, I am not able to understand the difference between use of <code>fit_transform()</code> and <code>transform()</code> method in this case.</p>

<p>In the following code, when I am using <code>fit_transform(X_valid)</code> instead of <code>transform(X_valid)</code> it gives me different outputs. </p>

<pre><code>from sklearn.impute import SimpleImputer

# Imputation
my_imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))
</code></pre>
","<machine-learning><python><data-science-model><missing-data><data-imputation>","12321","What's the difference between fit and fit_transform in scikit-learn models?","<p>I do not understand the difference between the <code>fit</code> and <code>fit_transform</code> methods in scikit-learn. Can anybody explain simply why we might need to transform data?</p>
<p>What does it mean, fitting a model on training data and transforming to test data? Does it mean, for example, converting categorical variables into numbers in training and transforming the new feature set onto test data?</p>
","<python><scikit-learn>"
"67698","Isn't one-hot encoding a waste of information?","<p>I was just playing around with one-hot representations of features and thought of the following:</p>

<p>Say that we're having 4 categories for a given feature (e.g. fruit) {Apple, Orange, Pear, Melon}. In this case the one-hot encoding would yield:</p>

<pre><code>Apple:  [1 0 0 0]
Orange: [0 1 0 0]
Pear:   [0 0 1 0]
Melon:  [0 0 0 1]
</code></pre>

<p>The above means that we quadruple the feature space as we go from having one feature to having four.</p>

<p>This looks like it's wasting a few bits, as we can represent 4 values with <span class=""math-container"">$\log_{2}4=2$</span> bits/features:</p>

<pre><code>Apple:  [0 0]
Orange: [0 1]
Pear:   [1 0]
Melon:  [1 1]
</code></pre>

<p>Would there be a problem with this representation in any of the most common machine learning models?</p>
","<machine-learning><feature-extraction><one-hot-encoding>","54384","Mapping of categorical features into binary indicator features","<p>I am currently reading an introductory machine learning book by Daumé (<a href=""http://www.ciml.info/dl/v0_99/ciml-v0_99-ch03.pdf"" rel=""nofollow noreferrer"">ch. 03, p. 30</a>) and when discussing the mapping of categorical features with ""n"" possible values into ""n"" binary indicator features, the following question is proposed:</p>

<blockquote>
  <p>Is it a good idea to map a categorical feature with n values to log2(n) binary features?</p>
</blockquote>

<p>Why wouldn't that be the case, seeing as how much resources could be spared by working with fewer features? Does this approach depend on the model that is being used?</p>
","<machine-learning><categorical-data><beginner><encoding><k-nn>"
"71800","Applying a keras model working with greyscale images to RGB images","<p>I followed this <a href=""https://www.tensorflow.org/tutorials/keras/classification"" rel=""nofollow noreferrer"">basic classification TensorFlow tutorial</a> using the <strong>Fashion MNIST</strong> dataset.
The training set contains 60,000 28x28 pixels greyscale images, split into 10 classes (trouser, pullover, shoe, etc...).
The tutorial uses a simple model:</p>

<pre><code>model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10)
])
</code></pre>

<p>This model reaches 91% accuracy after 10 epochs.</p>

<p>I am now practicing with another dataset called <strong>CIFAR-10</strong>, which consists of 50,000 32*32 pixels RGB images, also split into 10 classes (frog, horse, boat, etc...). </p>

<p>Considering that both the Fashion MNIST and CIFAR-10 datasets are pretty similar in terms of number of images and image size and that they have the same number of classes, I naively tried training a similar model, simply adjusting the input shape:</p>

<pre><code>  model = keras.Sequential([
     keras.layers.Flatten(input_shape=(32, 32, 3)),
     keras.layers.Dense(128, activation='relu'),
     keras.layers.Dense(10)
  ])
</code></pre>

<p>Alas, after 10 epochs, the model reaches an accuracy of 45%. What am I doing wrong?</p>

<p>I am aware that I have thrice as many samples in an RGB image than in a grayscale image, so I tried increasing the number of epochs as well as the size of the intermediate dense layer, but to no avail.</p>

<hr>

<p>Below is my full code:</p>

<pre><code>import tensorflow as tf
import IPython.display as display
from PIL import Image
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import pdb
import pathlib
import os
from tensorflow.keras import layers #Needed to make the model
from tensorflow.keras import datasets, layers, models

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

IMG_HEIGHT = 32
IMG_WIDTH = 32

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']


train_images = train_images / 255.0
test_images = test_images / 255.0

def make_model():
      model = keras.Sequential([
         keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),
         keras.layers.Dense(512, activation='relu'),
         keras.layers.Dense(10)
      ])
      model.compile(optimizer='adam',
                   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                   metrics=['accuracy'])
      return model

model=make_model()
history = model.fit(train_images, train_labels, epochs=10)
</code></pre>

<hr>

<p><strong>EDIT</strong>: In order to address colt.exe's suggestion below, I converted the CIFAR-10 images from RGB to greyscale, using:</p>

<pre><code>rgb_weights = [0.2989, 0.5870, 0.1140]
train_images = np.dot(train_images, rgb_weights)
test_images = np.dot(test_images, rgb_weights)
</code></pre>

<p>However, using the same model with 10 epochs yields an accuracy of about 38%, even worse than before! 
I am starting to think that the Fashion MNIST is ""easier"" to work with since all images have a light background, whereas the CIFAR-10 consist of pictures taken outdoors in a wide variety of contexts.</p>
","<python><tensorflow><keras>","71751","Applying a keras model working with greyscale images to RGB images","<p>I followed this <a href=""https://www.tensorflow.org/tutorials/keras/classification"" rel=""nofollow noreferrer"">basic classification TensorFlow tutorial</a> using the <strong>Fashion MNIST</strong> dataset.
The training set contains 60,000 28x28 pixels greyscale images, split into 10 classes (trouser, pullover, shoe, etc...).
The tutorial uses a simple model:</p>

<pre><code>model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10)
])
</code></pre>

<p>This model reaches 91% accuracy after 10 epochs.</p>

<p>I am now practicing with another dataset called <strong>CIFAR-10</strong>, which consists of 50,000 32*32 pixels RGB images, also split into 10 classes (frog, horse, boat, etc...). </p>

<p>Considering that both the Fashion MNIST and CIFAR-10 datasets are pretty similar in terms of number of images and image size and that they have the same number of classes, I naively tried training a similar model, simply adjusting the input shape:</p>

<pre><code>  model = keras.Sequential([
     keras.layers.Flatten(input_shape=(32, 32, 3)),
     keras.layers.Dense(128, activation='relu'),
     keras.layers.Dense(10)
  ])
</code></pre>

<p>Alas, after 10 epochs, the model reaches an accuracy of 45%. What am I doing wrong?</p>

<p>I am aware that I have thrice as many samples in an RGB image than in a grayscale image, so I tried increasing the number of epochs as well as the size of the intermediate dense layer, but to no avail.</p>

<hr>

<p>Below is my full code:</p>

<pre><code>import tensorflow as tf
import IPython.display as display
from PIL import Image
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import pdb
import pathlib
import os
from tensorflow.keras import layers #Needed to make the model
from tensorflow.keras import datasets, layers, models

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

IMG_HEIGHT = 32
IMG_WIDTH = 32

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']


train_images = train_images / 255.0
test_images = test_images / 255.0

def make_model():
      model = keras.Sequential([
         keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),
         keras.layers.Dense(512, activation='relu'),
         keras.layers.Dense(10)
      ])
      model.compile(optimizer='adam',
                   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                   metrics=['accuracy'])
      return model

model=make_model()
history = model.fit(train_images, train_labels, epochs=10)
</code></pre>
","<python><keras><tensorflow>"
"72456","Is shuffling data really necessary for training?","<p>I don't mean if we had a dataset where if sequentially sampled, the labels would be [1111122223333]. In this case, the network learns to predict everything as 1, then 2, and so on and it's impossible to learn.</p>

<p>I mean: Assume you have Imagenet 2012 dataset. You shuffle it once. So now the labels and the images are shuffled. Since the dataset is huge, can the network really remember the previous epoch's predictions and overfit?</p>

<p>OR, I shuffle data 5 times and use each ordering in epochs 1,2,..5, and then at epoch 6 I use the Shuffled data#1 again.</p>

<p>Everybody talks about the importance of shuffling but I never read anything that addresses these problems.</p>

<p>BTW, this question was prompted by me using a database where accessing data sequentially is a lot faster than random access. If I knew that even a pseudo-shuffling helps, it would save me 6-7 hours per training epoch.</p>
","<machine-learning><randomized-algorithms>","24511","Why should the data be shuffled for machine learning tasks","<p>In machine learning tasks it is common to shuffle data and normalize it. The purpose of normalization is clear (for having same range of feature values). But, after struggling a lot, I did not find any valuable reason for shuffling data.</p>

<p>I have read this post <a href=""https://stats.stackexchange.com/a/180847/179078"">here</a> discussing when we need to shuffle data, but it is not obvious why we should shuffle the data. Furthermore, I have frequently seen in algorithms such as Adam or SGD where we need batch gradient descent (data should be separated to mini-batches and batch size has to be specified). It is vital according to this <a href=""https://stats.stackexchange.com/questions/245502/shuffling-data-in-the-mini-batch-training-of-neural-network"">post</a> to shuffle data for each epoch to have different data for each batch. So, perhaps the data is shuffled and more importantly changed.</p>

<p>Why do we do this?</p>
","<machine-learning><neural-network><deep-learning>"
"72460","Machine Learning: Missing Data in Dataset and Imputer","<p>I am a newbie in ML, and I am learning how to fill missing data in a dataset using Imputer. These are the few lines of code that I came across</p>

<pre><code>from sklearn.preprocessing import Imputer
imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)
imputer = imputer.fit(X[:, 1:3])
X[:, 1:3] = imputer.transform(X[:, 1:3])
</code></pre>

<p>Now I am not able to understand what is the role of the <strong>fit</strong> and the <strong>transform</strong> function. It will be great if someone can help. Thank you.</p>
","<machine-learning><scikit-learn>","12321","What's the difference between fit and fit_transform in scikit-learn models?","<p>I do not understand the difference between the <code>fit</code> and <code>fit_transform</code> methods in scikit-learn. Can anybody explain simply why we might need to transform data?</p>
<p>What does it mean, fitting a model on training data and transforming to test data? Does it mean, for example, converting categorical variables into numbers in training and transforming the new feature set onto test data?</p>
","<python><scikit-learn>"
"72713","L1 vs. L2 Loss in XGBoost","<p>I understand the concepts of L1 and L2 loss (i.e. L1 loss will force some parameter coefficients to zero while L2 will only make them approach zero). What do these do when implemented in XGBoost? Does L1 loss prune the tree more significantly than L2 loss?</p>
","<xgboost>","57255","L1 & L2 Regularization in Light GBM","<p>This question pertains to L1 &amp; L2 regularization parameters in Light GBM. As per official documentation:</p>

<p><code>reg_alpha (float, optional (default=0.))</code> – L1 regularization term on weights.</p>

<p><code>reg_lambda (float, optional (default=0.))</code> – L2 regularization term on weights</p>

<p>I have seen data scientists using both of these parameters at the same time, ideally either you use L1 or L2 not both together. </p>

<p>While reading about tuning LGBM parameters I cam across one such case: Kaggle official GBDT Specification and Optimization Workshop in Paris where Instructors are ML experts. And these experts have used positive values of both L1 &amp; L2 params in LGBM model. Link below (Ctrl+F 'search_spaces' to directly reach parameter grid in this long kernel) </p>

<p><a href=""http://www.kaggle.com/lucamassaron/kaggle-days-paris-gbdt-workshop"" rel=""noreferrer"">http://www.kaggle.com/lucamassaron/kaggle-days-paris-gbdt-workshop</a> </p>

<p>I have seen same in XGBoost implementations.</p>

<p>My question is why use both at the same time in LGBM/XGBoost.</p>

<p>Thanks.</p>
","<xgboost><regularization><lightgbm>"
"74396","Logistic regression based prediction model using flask(python) to predict if Student will pass or fail. Error","<p>I am trying to create a web application on Python using Flask that predicts if a student is likely to pass or fail using a <a href=""https://www.kaggle.com/spscientist/students-performance-in-exams"" rel=""nofollow noreferrer"">Kaggle dataset</a>.
I changed the dataset a little and want to predict if the student will Pass or Fail using Logistic Regression by setting all students with Average marks (calculated as (math score+reading score+writing score)/3) below 45 as fail and others as pass. I also dropped the lunch column.
I am getting an error when I run the following code---</p>

<p>model.py</p>

<pre><code>import pandas as pd
import pickle
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

data = pd.read_csv('1. StudentsPerformance main.csv')

target_name='Average Score'
x = data.iloc[:, :3].values
y=data[target_name].values

le = LabelEncoder()  
x['Parental Education Level']= le.fit_transform(x['Parental Education Level']) 
x['Race/Ethnicity']= le.fit_transform(x['Race/Ethnicity'])
x['Test Preparation Course']= le.fit_transform(x['Test Preparation Course'])

onehotencoder = OneHotEncoder() 
x = onehotencoder.fit_transform(x).toarray()

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state = 40)
print('The number of samples into the train data is {}.'.format(x_train.shape[0]))
print('The number of samples into the test data is {}.'.format(x_test.shape[0]))

from sklearn.linear_model import LogisticRegression
reg=LogisticRegression(n_jobs=-1, random_state=15, solver='lbfgs')
reg.fit(x_train,y_train)


pickle.dump(reg, open('model.pkl','wb'))
#model = pickle.load(open('model.pkl','rb'))
</code></pre>

<p>app.py</p>

<pre><code>app = Flask(__name__)
model = pickle.load(open('model.pkl', 'rb'))

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/predict',methods=['POST'])
def predict():
    '''
    For rendering results on HTML GUI
    '''
    int_features = [int(x) for x in request.form.values()]
    final_features = [np.array(int_features)]
    prediction = model.predict(final_features)

    output = round(prediction[0], 2)


    #return render_template('index.html', prediction_text='The student is more likely to PASS')
    return render_template('index.html', prediction_text='The student is more likely to $ {}'.format(output))                       

@app.route('/predict_api',methods=['POST'])
def predict_api():
    '''
    For direct API calls trought request
    '''
    data = request.get_json(force=True)
    prediction = model.predict([np.array(list(data.values()))])

    output = prediction[0]
    return jsonify(output)

if __name__ == ""__main__"":
    app.run(debug=True)
</code></pre>

<p>index.html</p>

<pre><code>&lt;html&gt;
&lt;head&gt;
&lt;link rel=""stylesheet"" href=""a.css""&gt;
&lt;title&gt;Prediction Model&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;
&lt;h4&gt;Prediction Model for StudentPerformance.csv&lt;/h4&gt;
&lt;form action=""{{ url_for('predict')}}""method=""post""&gt;
&lt;label for=""tpc""&gt;Test Preparation Course Status-&lt;/label&gt;&lt;br&gt;
    &lt;select id=""tpc"" name=""tpc""&gt; 
      &lt;option value=""0""&gt;Complete&lt;/option&gt; 
      &lt;option value=""1""&gt;None&lt;/option&gt;  
    &lt;/select&gt;&lt;br&gt; 
&lt;label for=""pel""&gt;Parental Education Level-&lt;/label&gt;&lt;br&gt;
    &lt;select id=""pel"" name=""pel""&gt; 
      &lt;option value=""0""&gt;High School&lt;/option&gt; 
      &lt;option value=""1""&gt;Some College&lt;/option&gt; 
      &lt;option value=""2""&gt;Bachelor's Degree&lt;/option&gt; 
      &lt;option value=""3""&gt;Associate's Degree&lt;/option&gt; 
      &lt;option value=""4""&gt;Master's Degree&lt;/option&gt; 
      &lt;/select&gt;&lt;br&gt;
&lt;label for=""re""&gt;Race/Ethnicity-&lt;/label&gt;&lt;br&gt;
    &lt;select id=""re"" name=""re""&gt; 
      &lt;option value=""0""&gt;Group A&lt;/option&gt; 
      &lt;option value=""1""&gt;Group B&lt;/option&gt; 
      &lt;option value=""2""&gt;Group C&lt;/option&gt; 
      &lt;option value=""3""&gt;Group D&lt;/option&gt; 
      &lt;option value=""4""&gt;Group E&lt;/option&gt; 
    &lt;/select&gt;&lt;br&gt;
&lt;input type=""Submit"" value=""Submit""&gt;&lt;br&gt;&lt;/form&gt;
&lt;p&gt;Output: &lt;/p&gt; &lt;br&gt;
{{ prediction_text }}
&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>request.py</p>

<pre><code>import requests

url = 'http://localhost:5000/predict_api'
r = requests.post(url,json={'tpc':""completed"", 'pel':""high school"", 're':""group A""})

print(r.json())
</code></pre>

<p>Traceback---</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\admin\anaconda3\lib\site-packages\flask\app.py"", line 2463, in __call__
    return self.wsgi_app(environ, start_response)
  File ""C:\Users\admin\anaconda3\lib\site-packages\flask\app.py"", line 2449, in wsgi_app
    response = self.handle_exception(e)
  File ""C:\Users\admin\anaconda3\lib\site-packages\flask\app.py"", line 1866, in handle_exception
    reraise(exc_type, exc_value, tb)
  File ""C:\Users\admin\anaconda3\lib\site-packages\flask\_compat.py"", line 39, in reraise
    raise value
  File ""C:\Users\admin\anaconda3\lib\site-packages\flask\app.py"", line 2446, in wsgi_app
    response = self.full_dispatch_request()
  File ""C:\Users\admin\anaconda3\lib\site-packages\flask\app.py"", line 1951, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""C:\Users\admin\anaconda3\lib\site-packages\flask\app.py"", line 1820, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""C:\Users\admin\anaconda3\lib\site-packages\flask\_compat.py"", line 39, in reraise
    raise value
  File ""C:\Users\admin\anaconda3\lib\site-packages\flask\app.py"", line 1949, in full_dispatch_request
    rv = self.dispatch_request()
  File ""C:\Users\admin\anaconda3\lib\site-packages\flask\app.py"", line 1935, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""C:\Users\admin\shreyaflask\app.py"", line 25, in predict
    prediction = model.predict(final_features)
  File ""C:\Users\admin\anaconda3\lib\site-packages\sklearn\linear_model\_base.py"", line 293, in predict
    scores = self.decision_function(X)
  File ""C:\Users\admin\anaconda3\lib\site-packages\sklearn\linear_model\_base.py"", line 273, in decision_function
    % (X.shape[1], n_features))
ValueError: X has 3 features per sample; expecting 12
</code></pre>
","<python><scikit-learn><predictive-modeling><logistic-regression><spyder>","57243","Sklearn ValueError: X has 2 features per sample; expecting 11","<p>I try to visualizing multiple logistic regression but I get the above error. </p>

<p>I'm practicing on <a href=""https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009"" rel=""nofollow noreferrer"">red wine quality</a> data set from kaggle.</p>

<p>Here is a full traceback:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-88-230199fd3a97&gt; in &lt;module&gt;
      4 X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
      5                      np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
----&gt; 6 plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
      7              alpha = 0.75, cmap = ListedColormap(('red', 'green')))
      8 plt.xlim(X1.min(), X1.max())

/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/base.py in predict(self, X)
    287             Predicted class label per sample.
    288         """"""
--&gt; 289         scores = self.decision_function(X)
    290         if len(scores.shape) == 1:
    291             indices = (scores &gt; 0).astype(np.int)

/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/base.py in decision_function(self, X)
    268         if X.shape[1] != n_features:
    269             raise ValueError(""X has %d features per sample; expecting %d""
--&gt; 270                              % (X.shape[1], n_features))
    271 
    272         scores = safe_sparse_dot(X, self.coef_.T,

ValueError: X has 2 features per sample; expecting 11
</code></pre>

<p>Here is the code:</p>

<pre><code>#Split the variables
X = dataset.iloc[:, :11].values
y = dataset.iloc[:, -1].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting Logistic Regression to the Training set
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
acc_score = accuracy_score(y_test, y_pred)
print(acc_score*100)
</code></pre>

<p>Below is the visualization code:</p>

<pre><code># Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()
</code></pre>

<p>I know that the error is that the model has been trained using 11 functions, but it is envisaged to use 2 functions, but I do not know exactly what to change.</p>
","<python><scikit-learn>"
"74571","my xgboost model accuracy decreases after grid search with","<p>I tried grid search for hyperparameter tuning in XGBoost classifier but the best accuracy is less than the accuracy without any tuning </p>

<pre><code>// this is the code before the grid search
xg_cl = xgb.XGBClassifier(objective='binary:logistic', seed = 22)
xg_cl.fit(x_train, y_train)
y_pred = xg_cl.predict(x_test)
print(metrics.confusion_matrix(y_test, y_pred))
print(metrics.accuracy_score(y_test, y_pred))
[[11  1]
[ 1 26]]
0.9487179487179487
</code></pre>

<p>also, worth mentioning that the dataset shape is (195, 11) after PCA, and i am trying to classify whether a patient has a parkinsons disease or not. </p>

<pre><code>// this is the grid search code
clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')
params__grid = {
    'n_estimators' : range(50,150,10),
    'max_depth': range(2, 12),
    'colsample_bytree': np.arange(0.5,1,0.1),
    'reg_alpha' : np.arange(0,0.6,0.1),
    'reg_lambda' : np.arange(0,0.8,0.1)


}
search = GridSearchCV(estimator=clf_xgb, param_grid=params__grid, scoring = 'accuracy',
                            cv = 4 )
search.fit(x_train,y_train)

print('best score:/n',search.best_score_)
print('bestparams:/n' ,search.best_params_)

best score:
0.9038461538461539

best params:
{'colsample_bytree': 0.5,
     'max_depth': 7,
     'n_estimators': 50,
     'reg_alpha': 0.2,
     'reg_lambda': 0.1}
</code></pre>

<p>then I used these parameters to build and train a new classifier</p>

<pre><code>clf_xgb_1 = xgb.XGBClassifier(objective = 'binary:logistic', max_depth = 7, n_estimators = 50, reg_alpha = 0.2 ,
                              reg_lambda = 0.1, colsample_bytree = 0.5 )
clf_xgb_1.fit(x_train,y_train)
y_pred_2 = clf_xgb_1.predict(x_test)
print('accuracy:/n', metrics.accuracy_score(y_test,y_pred_2))
print('confusion matrix:/n', metrics.confusion_matrix(y_test,y_pred_2))

accuracy:
0.8974358974358975

confusion matrix:
array([[ 9,  3],
       [ 1, 26]], dtype=int64)
</code></pre>

<p>How come my results are worse? I would expect that GridSearch would improve on the results.</p>
","<xgboost><grid-search>","82028","Is it possible to get worse model after optimization?","<p>I am trying recently to optimize models but for some reason, whenever I try to run the optimization the model score in the end is worse than before, so I believe I do something wrong.</p>
<p>in order to optimize my model I define param grid and than fit with the train data and then according to the results run again with nre parameters, e.g-</p>
<pre><code>#ROUND 1
param_grid={
    'max_depth': [3,4,5],
    'learning_rate':[0.1,0.01,0.05],
    'gamma': [0,0.25,1.0],
    'reg_lambda':[0,1.0,10.0],
    'scale_pos_weight':[1,3,5]
}

grid_search = GridSearchCV(estimator = clf_xgb, param_grid = param_grid, 
                          cv = 3, n_jobs = -1, verbose = 2)
grid_search.fit(X_train,y_train)
grid_search.best_params_

&gt;&gt;&gt;.....

</code></pre>
<p>(and now based on the result changing the params...)</p>
<p>after this step I choose the best hyperparameters and run the model;</p>
<pre><code>clf_xgb=xgb.XGBClassifier(seed=42,
                         objective='binary:logistic',
                         gamma=0,
                         learn_rate=0.7,
                         max_depth=6,
                         reg_lambda=0.8,
                         scale_pos_weight=1,
                         subsample=0.9,
                         cilsample_bytree=0.5)

clf_xgb.fit(X_train,
           y_train,
           verbose=True,
           early_stopping_rounds=10,
           eval_metric='aucpr',
           eval_set=[(X_test,y_test)])
</code></pre>
<p>The problem is that when I check the model score</p>
<pre><code>clf_xgb.score(X_test,y_test)
</code></pre>
<p>I always get lower score than what I got before the optimization which makes me suspect that I'm missing something in the way doing it/basic principle in this process.</p>
<p><strong>Is it possible that after running the optimization my score won't get better (and even worse?) ? Where is my mistake? Are there other parameters that could influence or improve my model?</strong></p>
","<python><scikit-learn><xgboost><optimization>"
"74620","Plot Decision boundary in 3D plot","<p>I am running logistic regression on iris dataset.  I computed thetas and this is how I draw a decision boundary line.</p>

<pre><code>x_values = ([min(X_train[:,0]), max(X_train[:,0])])
y_values = - (theta[0] + np.dot(theta[1], x_values)) / theta[2]
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Set1, edgecolor='k')
plt.plot(x_values, y_values )
</code></pre>

<p><a href=""https://i.stack.imgur.com/JvM9G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JvM9G.png"" alt=""enter image description here""></a></p>

<p>I tried this, but the result is odd. </p>

<pre><code>X= np.c_[ X_train, np.zeros(100) ]   

theta = theta.reshape(3)

d=0

xx, yy = np.meshgrid(np.arange(np.min(X_reduced[:, 0]), np.max(X_reduced[:, 0])), np.arange(np.min(X_reduced[:, 1]), np.max(X_reduced[:, 1])))

z = (-theta[0] * xx - theta[1] * yy - d) * 1. / theta[2]

fig = plt.figure(1, figsize=(8, 6))
ax = Axes3D(fig, elev=-150, azim=110)

ax.scatter(X[:100, 0], X[:100, 1], X[:100, 2], c=y_train, cmap=plt.cm.Set1, edgecolor='k', s=40)
ax.plot_surface(xx, yy, z, alpha = 0.5)
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/tHO88.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tHO88.png"" alt=""enter image description here""></a></p>

<p>I guess d in plane equation (a<em>x+b</em>y+c*z = d) shouldn't be equal to 0. So I'm completely confused about this.</p>
","<machine-learning><visualization><matplotlib>","49573","How to plot logistic regression decision boundary?","<p>I am running logistic regression on a small dataset which looks like this:</p>

<p><a href=""https://i.stack.imgur.com/a9pH5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a9pH5.png"" alt=""enter image description here""></a></p>

<p>After implementing gradient descent and the cost function, I am getting a 100% accuracy in the prediction stage, However I want to be sure that everything is in order so I am trying to plot the decision boundary line which separates the two datasets.</p>

<p>Below I present plots showing the cost function and theta parameters. As can be seen, currently I am printing the decision boundary line incorrectly.</p>

<p><a href=""https://i.stack.imgur.com/hHtWK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hHtWK.png"" alt=""enter image description here""></a></p>

<p><strong>Extracting data</strong></p>

<pre><code>clear all; close all; clc;

alpha = 0.01;
num_iters = 1000;

%% Plotting data
x1 = linspace(0,3,50);
mqtrue = 5;
cqtrue = 30;
dat1 = mqtrue*x1+5*randn(1,50);

x2 = linspace(7,10,50);
dat2 = mqtrue*x2 + (cqtrue + 5*randn(1,50));

x = [x1 x2]'; % X

subplot(2,2,1);
dat = [dat1 dat2]'; % Y

scatter(x1, dat1); hold on;
scatter(x2, dat2, '*'); hold on;
classdata = (dat&gt;40);
</code></pre>

<p><strong>Computing Cost, Gradient and plotting</strong></p>

<pre><code>%  Setup the data matrix appropriately, and add ones for the intercept term
[m, n] = size(x);

% Add intercept term to x and X_test
x = [ones(m, 1) x];

% Initialize fitting parameters
theta = zeros(n + 1, 1);
%initial_theta = [0.2; 0.2];

J_history = zeros(num_iters, 1); 

plot_x = [min(x(:,2))-2,  max(x(:,2))+2]

for iter = 1:num_iters 
% Compute and display initial cost and gradient
    [cost, grad] = logistic_costFunction(theta, x, classdata);
    theta = theta - alpha * grad;
    J_history(iter) = cost;

    fprintf('Iteration #%d - Cost = %d... \r\n',iter, cost);


    subplot(2,2,2);
    hold on; grid on;
    plot(iter, J_history(iter), '.r');  title(sprintf('Plot of cost against number of iterations. Cost is %g',J_history(iter)));
    xlabel('Iterations')
    ylabel('MSE')
    drawnow

    subplot(2,2,3);
    grid on;
    plot3(theta(1), theta(2), J_history(iter),'o')
    title(sprintf('Tita0 = %g, Tita1=%g', theta(1), theta(2)))
    xlabel('Tita0')
    ylabel('Tita1')
    zlabel('Cost')
    hold on;
    drawnow

    subplot(2,2,1);
    grid on;    
    % Calculate the decision boundary line
    plot_y = theta(2).*plot_x + theta(1);  % &lt;--- Boundary line 
    % Plot, and adjust axes for better viewing
    plot(plot_x, plot_y)
    hold on;
    drawnow

end

fprintf('Cost at initial theta (zeros): %f\n', cost);
fprintf('Gradient at initial theta (zeros): \n');
fprintf(' %f \n', grad);
</code></pre>

<p>The above code is implementing gradient descent correctly (I think) but I am still unable to show the boundary line plot. Any suggestions would be appreciated.</p>

<hr>

<p><strong>logistic_costFunction.m</strong></p>

<pre><code>function [J, grad] = logistic_costFunction(theta, X, y)

    % Initialize some useful values
    m = length(y); % number of training examples

    grad = zeros(size(theta));

    h = sigmoid(X * theta);
    J = -(1 / m) * sum( (y .* log(h)) + ((1 - y) .* log(1 - h)) );

    for i = 1 : size(theta, 1)
        grad(i) = (1 / m) * sum( (h - y) .* X(:, i) );
    end

end
</code></pre>

<p><strong>EDIT:</strong></p>

<p>As per the below answer by @Esmailian, now I have something like this:</p>

<pre><code>[m, n] = size(x);

x1_class = [ones(m, 1) x1' dat1'];
x2_class = [ones(m, 1) x2' dat2'];

x = [x1_class ; x2_class]
</code></pre>
","<machine-learning><logistic-regression>"
"74868","What is difference between Standard Normal Distribution and Mean Normalization approaches to feature-scaling?","<p>The tag <em>feature-scaling</em> seems to convey that one of the scaling methods is Standard Normal Distribution. Further, I read an Answer on this site saying that Mean Normalization is a form of feature scaling.</p>
<p>What is the difference between two approaches to scaling?</p>
<p>Note: I think that statistics and mathematics of normalization do differ.</p>
","<statistics><normalization><feature-scaling><sampling><definitions>","8946","Feature Scaling and Mean Normalization","<p>In my machine learning class these two methods were discussed and mentioned that both should be used. I have a couple questions about this:</p>

<p>1) Can I mix and match these two approaches? e.g. Feature Scale x1 and Mean Normalize x2?</p>

<p>2) How do you determine which of these options to apply? It seems that either could accomplish the task of increasing your convergence rates... I suppose you just need to know your data set to understand which will reliably reduce your values while leaving as few outliers as possible?</p>
","<data-cleaning><gradient-descent><linear-regression>"
"75161","Training accuracy is ~97% but validation accuracy is stuck at ~40%. What does it imply?","<p>Training accuracy is ~97% but validation accuracy is stuck at ~40%.</p>

<p>I can not understand the meaning of two concepts and their relationship. </p>
","<cross-validation><accuracy>","19124","How to know the model has started overfitting?","<p>I hope the following excerpts will provide an insight into what my question is going to be. These are from <a href=""http://neuralnetworksanddeeplearning.com/chap3.html"" rel=""nofollow noreferrer"">here</a>.</p>
<blockquote>
<p>The learning then gradually slows down. Finally, at around epoch 280 the classification accuracy pretty much stops improving. Later epochs merely see small stochastic fluctuations near the value of the accuracy at epoch 280. Contrast this with the earlier graph, where the cost associated to the training data continues to smoothly drop. If we just look at that cost, it appears that our model is still getting &quot;better&quot;. But the test accuracy results show the improvement is an illusion. Just like the model that Fermi disliked, what our network learns after epoch 280 no longer generalizes to the test data. And so it's not useful learning. We say the network is overfitting or overtraining beyond epoch 280.</p>
</blockquote>
<p>We are training a neural network and the cost (on training data) is dropping till epoch 400 but the classification accuracy is becoming static (barring a few stochastic fluctuations) after epoch 280 so we conclude that model is overfitting on training data post epoch 280.</p>
<blockquote>
<p>We can see that the cost on the test data improves until around epoch 15, but after that it actually starts to get worse, even though the cost on the training data is continuing to get better. This is another sign that our model is overfitting. It poses a puzzle, though, which is whether we should regard epoch 15 or epoch 280 as the point at which overfitting is coming to dominate learning? From a practical point of view, what we really care about is improving classification accuracy on the test data, while the cost on the test data is no more than a proxy for classification accuracy. And so it makes most sense to regard epoch 280 as the point beyond which overfitting is dominating learning in our neural network.</p>
</blockquote>
<p>As opposed to classification accuracy on test data compared with training cost previously we are now placing cost on test data against training cost.</p>
<p>Then the book goes on to explain why 280 is the right epoch where the overfitting has started. That is what I have an issue with. I can't wrap my head around this.</p>
<p>We are asking model to minimize the cost and thus cost is the metric it uses as a measure of its own strength to classify correctly. If we consider 280 as the right epoch where the overfitting has started, have we not in a way created a biased model that though is a  better classifier on the particular test data but nonetheless is making decisions with low confidence and hence is more prone to deviate from the results shown on the test data?</p>
","<neural-network><overfitting>"
"75931","ValueError: Found input variables with inconsistent numbers of samples","<p>I am trying to do svm model training and it gives this error:</p>

<pre><code> ValueError: Found input variables with inconsistent numbers of samples: [91, 212]
</code></pre>

<p>Code:</p>

<pre><code>target = data.target
target.head()

##splitiing data into training and test data
X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.3, random_state=101)
print(""X_train size ===&gt;"", X_train.shape)
print(""y_train size ===&gt;"", y_train.shape)
print(""X_test size ===&gt;"", X_test.shape)
print(""y_test size ===&gt;"", y_test.shape)

#Create a svm Classifier

clf = svm.SVC(kernel='linear')

#training the model using the training sets

clf.fit(X_train, y_train)

#predict the response for test dataset

y_pred = clf.predict(X_train)
</code></pre>
","<machine-learning><deep-learning><classification><svm><machine-learning-model>","20199","train_test_split() error: Found input variables with inconsistent numbers of samples","<p>Fairly new to Python but building out my first RF model based on some classification data. I've converted all of the labels into int64 numerical data and loaded into X and Y as a numpy array, but I am hitting an error when I am trying to train the models. </p>

<p>Here is what my arrays look like:</p>

<pre><code>&gt;&gt;&gt; X = np.array([[df.tran_cityname, df.tran_signupos, df.tran_signupchannel, df.tran_vmake, df.tran_vmodel, df.tran_vyear]])

&gt;&gt;&gt; Y = np.array(df['completed_trip_status'].values.tolist())

&gt;&gt;&gt; X
array([[[   1,    1,    2,    3,    1,    1,    1,    1,    1,    3,    1,
            3,    1,    1,    1,    1,    2,    1,    3,    1,    3,    3,
            2,    3,    3,    1,    1,    1,    1],
        [   0,    5,    5,    1,    1,    1,    2,    2,    0,    2,    2,
            3,    1,    2,    5,    5,    2,    1,    2,    2,    2,    2,
            2,    4,    3,    5,    1,    0,    1],
        [   2,    2,    1,    3,    3,    3,    2,    3,    3,    2,    3,
            2,    3,    2,    2,    3,    2,    2,    1,    1,    2,    1,
            2,    2,    1,    2,    3,    1,    1],
        [   0,    0,    0,   42,   17,    8,   42,    0,    0,    0,   22,
            0,   22,    0,    0,   42,    0,    0,    0,    0,   11,    0,
            0,    0,    0,    0,   28,   17,   18],
        [   0,    0,    0,   70,  291,   88,  234,    0,    0,    0,  222,
            0,  222,    0,    0,  234,    0,    0,    0,    0,   89,    0,
            0,    0,    0,    0,   40,  291,  131],
        [   0,    0,    0, 2016, 2016, 2006, 2014,    0,    0,    0, 2015,
            0, 2015,    0,    0, 2015,    0,    0,    0,    0, 2015,    0,
            0,    0,    0,    0, 2016, 2016, 2010]]])

&gt;&gt;&gt; Y
array(['NO', 'NO', 'NO', 'YES', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'NO',
       'NO', 'YES', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO',
       'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO'], 
      dtype='|S3')

&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)
</code></pre>

<blockquote>
  <p>Traceback (most recent call last):</p>

<pre><code>  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/Library/Python/2.7/site-packages/sklearn/cross_validation.py"", line
</code></pre>
  
  <p>2039, in train_test_split
          arrays = indexable(*arrays)
        File ""/Library/Python/2.7/site-packages/sklearn/utils/validation.py"", line
  206, in indexable
          check_consistent_length(*result)
        File ""/Library/Python/2.7/site-packages/sklearn/utils/validation.py"", line
  181, in check_consistent_length
          "" samples: %r"" % [int(l) for l in lengths])</p>

<pre><code>ValueError: Found input variables with inconsistent numbers of samples: [1, 29]
</code></pre>
</blockquote>
","<python><scikit-learn><sampling>"
"75996","Trusting p-values when errors are not normally distributed","<p>Suppose I have a multiple linear regression model and the errors are not distributed normally.Does the central limit theorem hold true in this case? should i trust the p values of the coefficients of variables assuming the residuals are not skewed?</p>

<p>Will really appreciate any help on this, thank you :) </p>
","<statistics><linear-regression>","73502","Confidence interval interpretation in linear regression when errors are not normally distributed","<p>I've read that ""<em>If the error distribution is significantly non-normal, confidence intervals may be too wide or too narrow</em>"" (<a href=""http://people.duke.edu/~rnau/testing.htm"" rel=""nofollow noreferrer"">source</a>). So, can anyone elaborate on this? When are the confidence intervals narrow and when are they wide? Does it have anything to do with skewness?</p>
","<regression><linear-regression>"
"77349","Why is count encoding effective in improving accuracy?","<p>Can someone please explain why/how Count encoding of categorical features improve accuracy in classification  when compared to simply label encoding them ?</p>
<p>I found one explanation in kaggle &quot; Rare values tend to have similar counts (with values like 1 or 2), so you can classify rare values together at prediction time. Common values with large counts are unlikely to have the same exact count as other values. So, the common/important values get their own grouping. &quot; which doesn't seem convincing or I don't understand the reasoning .</p>
<p>Can someone please explain why it performs better than label encoding ? Label encoding can also find frequent patterns to correlate with the target variable , right ?</p>
","<machine-learning><feature-engineering><categorical-data><categorical-encoding>","63749","Why does frequency encoding work?","<p>Frequency encoding is a widely used technique in Kaggle competitions, and many times proves to be a very reasonable way of dealing with categorical features with high cardinality. I really don't understand why it works. </p>

<p>Does it work in very specific cases where frequencies are correlated with the target or is it more general? What's the rationale behind it?</p>
","<machine-learning><feature-engineering><categorical-data><encoding>"
"78095","confused AUC ROC score","<p>I am working on binary classification problem, I try to evaluate the performance of some classification algorithms (<strong>LR,Decission Tree , Random forest</strong> ...). I am using a <strong>10 fold cross-validation</strong> technique (to avoid over-fitting) with <strong>AUC ROC</strong> as scoring function to compare the performance of the algorithms, but I am getting a weird results with <strong>Random forest</strong> and <strong>AdbBoost</strong>, I have a perfect <strong>AUC_ROC</strong> score (i.e. =1) despite the fact that the <strong>Recall</strong>(TPR) and <strong>FPR</strong> of this algorithms are different from 1 and 0 respectively .</p>
<p><a href=""https://i.stack.imgur.com/4DtuA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4DtuA.png"" alt=""results"" /></a></p>
","<classification><cross-validation><random-forest><auc>","78032","Confused AUC ROC score","<p>I am working on binary classification problem, I try to evaluate the performance of some classification algorithms (<strong>LR,Decission Tree , Random forest</strong> ...).
I am using  a <strong>cross validation</strong> technique (to avoid over-fitting) with <strong>AUC ROC</strong> as scoring function to compare the performance of the algorithms, but I am getting a weird results with <strong>Random forest</strong> and <strong>AdbBoost</strong>, I have a perfect <strong>AUC_ROC</strong> score (i.e. =1) despite the fact that the recall(TPR) and FPR of this algorithms are different from 1 and 0 respectively .</p>
<p><a href=""https://i.stack.imgur.com/8Udgh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Udgh.png"" alt=""enter image description here"" /></a></p>
<pre><code>def FPR(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    result = fp / (fp+tn)
    return result
def FNR(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    result = fn / (tp+fn)
    return result
FPR_scorer = make_scorer(FPR)
FNR_scorer = make_scorer(FNR)   

def get_CrossValResults2(model,cv_rst,bestIndx):
    best=pd.DataFrame.from_dict(cv_rst).iloc[bestIndx]
    roc=&quot;{:.12f}&quot;.format(best['mean_test_roc_auc'])
    acc =&quot;{:.0%}&quot;.format(best['mean_test_accuracy'])
    prec =&quot;{:.0%}&quot;.format(best['mean_test_precision'])
    rec =&quot;{:.0%}&quot;.format( best['mean_test_recall'])
    f1 =&quot;{:.0%}&quot;.format(best['mean_test_f1'])
    r2=&quot;{:.2f}&quot;.format(best['mean_test_r2'])
    g_mean=&quot;{:.2f}&quot;.format(best['mean_test_gmean'])
    pr_auc=&quot;{:.8f}&quot;.format(best['mean_test_pr'])
    fnr=&quot;{:.0%}&quot;.format(best['mean_test_fnr'])
    fpr=&quot;{:.0%}&quot;.format(best['mean_test_fpr'])
    rst = pd.DataFrame([[ model, acc,prec,rec,fpr,fnr,f1,roc,pr_auc,g_mean,r2]],columns = ['Model', 'Accuracy', 'Precision', 'Recall','FPR','FNR', 'F1-Score','ROC_auc','PR_auc','gmean','r2'])
    return rst
      
cross_val_rst = pd.DataFrame(columns = ['Model', 'Accuracy', 'Precision', 'Recall','FPR','FNR', 'F1-Score','ROC_auc','PR_auc','gmean','r2'])    
        
scoring = {'accuracy':'accuracy','recall':'recall','precision':'precision','fpr':FPR_scorer,'fnr':FNR_scorer,'f1':'f1' ,'roc_auc':'roc_auc','pr':'average_precision','gmean':Gmean_scorer,'r2':'r2'}    
param_grid = {'n_estimators': [200], 
             'max_depth': [80,90],
              'min_samples_leaf': [2,3, 4],
            'min_samples_split': [2,5,12],
            'criterion': [ 'gini'],
             'class_weight' : [class_weights], 'n_jobs' : [-1]} 
clf = GridSearchCV(RandomForestClassifier(class_weight=class_weights), param_grid, cv=kfold,scoring=scoring,refit=refit)#Fit the model
bestmodel = clf.fit(X,Y)
     
cross_val_rst = cross_val_rst.append(get_CrossValResults2(model='Random Forrest',bestIndx=bestmodel.best_index_,cv_rst=bestmodel.cv_results_),ignore_index=True)
</code></pre>
","<classification><random-forest><logistic-regression><cross-validation><auc>"
"78369","Which model is better, one just before overfitting with higher accuracy or one with no overfitting and lower accuracy?","<p>I am training a CNN model.</p>
<p>In the first one I got a training accuracy of 87%(0.29 loss) and validation accuracy of 87%(0.30 loss) at 5th epoch, I kept training it for total of 15 epochs and as expected it started overfitting with training accuracy increasing to 97%(0.01 loss) and validation remaining at 87%(0.35 loss).</p>
<p>In the second model, I used Data Augmentation and Dropout layer to deal with overfitting (trained for total 10 epochs). These are the results : 5th Epoch : Train Accuracy 77% (0.45 loss) and Validation accuracy 77% (0.41 loss). 10th Epoch : Train Accuracy 82% (0.38 loss) and Validation accuracy 82% (0.35 loss)</p>
<p>From the loss and accuracy graph which you can see below, it's clear that Model is overfitting in first scenario, but in second it's not overfitting.</p>
<p><strong>Scenario One</strong></p>
<p><a href=""https://i.stack.imgur.com/DCpmE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DCpmE.png"" alt=""enter image description here"" /></a></p>
<p><strong>Scenario Two</strong></p>
<p><a href=""https://i.stack.imgur.com/UjJRK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UjJRK.png"" alt=""enter image description here"" /></a></p>
<p>My question is, which model is better in real world based on accuracy ? Model one stopped at epoch 5 with 87% accuracy or model 2 which has not overfit with 82% accuracy (validation)? I understand just based on accuracy model 1 sounds better but it's ultimately overfitting, but if I stop the training using early stopping or something similar, will this be a better model than my second one ?</p>
","<machine-learning><cnn><accuracy><overfitting>","66350","What would I prefer - an over-fitted model or a less accurate model?","<p>Let's say we have two models trained. And let's say we are looking for good accuracy. 
The first has an accuracy of 100% on training set and 84% on test set. Clearly over-fitted.
The second has an accuracy of 83% on training set and 83% on test set. </p>

<p>On the one hand, model #1 is over-fitted but on the other hand it still yields better performance on an unseen test set than the good general model in #2. </p>

<p>Which model would you choose to use in production? The First or the Second and why?</p>
","<machine-learning-model><training><supervised-learning><accuracy><overfitting>"
"82126","Why do people prefer $(target-actual)^2$ over $|(target-actual)|$","<p>When computing loss functions, people use <span class=""math-container"">$(target-actual)^2$</span>. They sqaure it to prevent any negative loss. But we can even use <span class=""math-container"">$|(target-actual)|$</span> to prevent any negative loss. So, why do people prefer the first option more than the second?</p>
","<neural-network><deep-learning><loss-function><loss>","12728","Minimize absolute values of errors instead of squares","<p>Calculating absolute values is much more efficient than calculating squares. Is there any advantage, then, to using the latter as a cost function over the former? Squares are easier to treat analytically, but in practice that doesn't matter.</p>
","<optimization>"
"83972","Predict real world data after modelling with scaled features","<p>I trained and test a model with scaled features.
Now, I want to predict a single real world sample. If I have one sample alone, I can't scale it to fit into the model like I did with the test data.
I imagine it's a pretty common situation.
What are the steps I need to do to predict this single sample?</p>
","<machine-learning><predictive-modeling><training><prediction><feature-scaling>","82285","ANN regression accuracy and loss stuck","<p>I have a data set on predicting solar power generation, the dataset has 20 independent var and 1 dependent. The accuracy of my model is stuck at 60%. I have tried several models but this accuracy is the best I could get other sucks even more.
Here is my code:</p>
<pre><code>data_path = r'drive/My Drive/Proj/S.P.F./solarpowergeneration.csv'
dts = pd.read_csv('solarpowergeneration.csv')
dts.head()
X = dts.iloc[:, :-1].values
y = dts.iloc[:, -1].values
print(X.shape, y.shape)
y = np.reshape(y, (-1,1))
y.shape
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
from sklearn.preprocessing import StandardScaler
sc= StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
y_train = sc.fit_transform(y_train)
y_test = sc.transform(y_test)

import keras.backend as K  
def calc_accu(y_true, y_pred):
  return K.mean(K.equal(K.round(y_true), K.round(y_pred)))

def get_spfnet():
  ann = tf.keras.models.Sequential()

  ann.add(Dense(X_train.shape[1], activation='relu'))
# ann.add(BatchNormalization())
  ann.add(Dropout(0.3))
  ann.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
 # ann.add(BatchNormalization())
  ann.add(Dropout(0.3))
  ann.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
 # ann.add(BatchNormalization())
  ann.add(Dropout(0.3))

  ann.add(Dense(1))
  ann.compile(loss='mse',
              optimizer='adam',
              metrics=[tf.keras.metrics.RootMeanSquaredError(), calc_accu])
  return ann

spfnet = get_spfnet()
#spfnet.summary()
hist = spfnet.fit(X_train, y_train, batch_size=32, epochs=250, verbose=2)
</code></pre>
<p>the accuracy and loss graphs are:</p>
<pre><code>plt.plot(hist.history['calc_accu'])
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.show()
plt.plot(hist.history['root_mean_squared_error'])
plt.title('Model error')
plt.xlabel('Epochs')
plt.ylabel('error')
plt.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/B3eH1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/B3eH1.png"" alt=""enter image description here"" /></a>    <a href=""https://i.stack.imgur.com/eNog0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eNog0.png"" alt=""enter image description here"" /></a></p>
<p>after 50 epochs nothing seems to improve, neither curve seems to overfit on the data
I tried other models like reducing layers and removing kernel regularizes, using</p>
<pre><code>kernel_initlizers='normal' and 'he-normal'  
</code></pre>
<p>but they perform poorly stuck at 20%.</p>
<p>Dataset:</p>
<p><a href=""https://i.stack.imgur.com/SZu4u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SZu4u.png"" alt=""enter image description here"" /></a> <a href=""https://i.stack.imgur.com/fxLEO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fxLEO.png"" alt=""enter image description here"" /></a></p>
","<python><neural-network><tensorflow><regression>"
"87547","Cleaning NaNs with averages pre or post split?","<p>I have a column with some NaNs in it and I want to replace those NaNs with the average/median/mode.</p>
<p>Technically, the validation/ test data has never been <em>seen</em> before - so how could I include it in the average? That would bias the values.</p>
<p>Do I &quot;fit&quot; the average to my training data only, just like scaling? Or do I take the average using the entire dataset?</p>
","<data-cleaning><preprocessing><normalization><encoding>","86778","How data are prepared during training, testing and in production?","<p>Most of real world datasets have features with missing values. Replacing missing values with an appropriate value such as its mean, is considered as a good step in feature engineering. Some times we also standardize/normalize feature columns before feeding it to train an model.</p>
<p>Before modelling  we also split our dataset to training and testing sets.</p>
<p>My first query is how do we do feature engineering in this splitted dataset?</p>
<p>Do we use a global mean of the unsplitted features  to replace the missing value of those features in both training and testing set or should we use local means of those sets?</p>
<p>Like the above question how do we do normalization to a train, test dataset?</p>
<p>The last but an important question, in productions we mostly get feature values one at a time (think a row of features), how do we feature engineer such data rows?</p>
","<predictive-modeling><training><feature-engineering><preprocessing><missing-data>"
"88067","Representing the architecture of a deep CNN","<p>Suppose I am feeding a <span class=""math-container"">$60\times60$</span> RGB image as the input to deep CNN with the first layer created using the following Keras code <code>model.add(Conv2D(64, (3, 3), input_shape=(60, 60, 3)))</code>.</p>
<ol>
<li>Will 64 filters be created for each channel (red, green and blue) of the image?</li>
<li>Is the following representation of the network correct? If 64 filters are created for each channel, then should I write &quot;3X64@58X58&quot;?</li>
</ol>
<p><a href=""https://i.stack.imgur.com/1SgT3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1SgT3.png"" alt=""Representation of the CNN architecture"" /></a></p>
","<machine-learning><deep-learning><cnn>","85366","Math behind 2D convolution for RGB images","<p>I read many threads discussing why 2D convolutional layer is typically used for RGB images in neural network.
I read that it is possible to use 3D conv layer.</p>
<p>What I do not understand is the math behind it.</p>
<p>Say your image is 300 by 300, and the <code>kernel_size = (3, 3)</code> and <code>filter = 16</code> for the <code>Conv2D</code> layer. <code>Input_shape</code> would be (300, 300, 3) because there are 3 channels(RGB).</p>
<ol>
<li>Since the kernel is 2D, the convolution can only be done at 1 channel at a time. Is that correct?</li>
<li>Are the same kernel applied/convolved for the 3 channels? If so there should be 3 output but the dimension of the output would be (298, 298, 16). Is it averaged over the 3 channels?</li>
</ol>
","<neural-network><cnn>"
"88702","Find correlation between grades from two raters","<p>The question is whether we can find a correlation between two sets of grades (categorical data).</p>
<p>Let’s say we have a dog competition and there are 1000 dogs participating.</p>
<p>There are two rounds of assessment</p>
<p>first round
dog owners give their assessment on the scale from A to C. Where A is excellent and C is bad. There are four criteria for assessment during both tours (behaviour etc).</p>
<p>second round
one judge gives his assessment of one dog based on the same assessment criteria as in round 1. however, grades vary from M - meeting expectation, E - exceeding expectation, B - Bellow expectation.</p>
<p>We understand that M is B, E is A and B is C.</p>
<p>After two rounds our table would look like:</p>
<pre><code>
| dog             | round one | round two |
| --------------- | --------- | --------- |
| Dog1_criteria1  | A         | B         |
| Dog1_criteria2  | A         | E         |
| Dog1_criteria3  | A         | E         |
| Dog1_criteria4  | B         | M         |
| Dog2_criteria1  | A         | E         |
| Dog2_criteria2  | B         | M         |
| Dog2_criteria3  | A         | E         |
| Dog2_criteria4  | C         | B         |
....
</code></pre>
<p>How do we find a correlation between the two sets of answers? Thank you!</p>
","<r><categorical-data>","893","How to get correlation between two categorical variable and a categorical variable and continuous variable?","<p>I am building a regression model and I need to calculate the below to check for correlations</p>

<ol>
<li>Correlation between 2 Multi level categorical variables</li>
<li>Correlation between a Multi level categorical variable and
continuous variable </li>
<li>VIF(variance inflation factor) for a Multi
level categorical variables</li>
</ol>

<p>I believe its wrong to use Pearson correlation coefficient for the above scenarios because Pearson only works for 2 continuous variables. </p>

<p>Please answer the below questions</p>

<ol>
<li>Which correlation coefficient works best for the above cases ? </li>
<li>VIF calculation only works for continuous data so what is the
alternative? </li>
<li>What are the assumptions I need to check before I use the correlation coefficient you suggest? </li>
<li>How to implement them in SAS &amp; R?</li>
</ol>
","<r><statistics><correlation>"
"89323","Validation set after hyperparameter tuning","<p>Let's say I'm comparing few models, and for my dataset I'm using train/validation/test split, and not cross validation. Let's say I'm completely done with parameter tuning for one of them and want to evaluate on the test set. Will I train a new model, on both the training and the validation datasets combined with the best configuration, or will I just run the same model on the test data?</p>
","<machine-learning><machine-learning-model><validation>","45200","Using the validation data","<p>I'm unclear on the exact process of using the validation data.</p>

<p>Let's say that I fit my neural network model and adjust hyperparameters using the training set and validation set. Do I then evaluate the test set on this model? Or do I recombine the validation and training sets and fit a fresh model with the hyperparameters that I found during the validation phase, and then evaluate on the test data? I have seen a number of different notebooks and examples that do both ways.</p>

<p>Surely, once I've found my hyperparameters, it makes sense to fit a fresh model using the full training set (recombined with validation set), since the validation loss has no effect on the weights.</p>
","<machine-learning><neural-network><cnn>"
"93223","Which method to use to remove correlation between independent variables comprising of both categorical and numerical variables?","<p>The independent variables in the dataset contains categorical variables such as</p>
<ul>
<li>Gender ( 2 levels)</li>
<li>Mode of Shipment ( 3 levels)</li>
<li>Product Importance ( 4 levels)</li>
</ul>
<p>and Numerical Variables such as</p>
<ul>
<li>Customer care calls</li>
<li>Discount Offered</li>
<li>Package weight</li>
</ul>
<p>How do I find the correlations between these variables?</p>
<ol>
<li>Converting categorical variables in to dummy variables and then using pearson correlation? What if the dummy variable categories also shows correlations too? such as correlation between Mode of shipment categories, Flight, ship, road? Do I need to remove the highly correlated dummy variable category with the other mode of shipment category?
or</li>
<li>doing separate correlations between numerical variables using pearson correlation, and for categorical variables using chi sq statistics?</li>
</ol>
<p>How to go about it?</p>
<p>Thank you! It's a long question, but really need this clarity. Would appreciate any additional links too. Thanks again!</p>
","<correlation><pearsons-correlation-coefficient>","893","How to get correlation between two categorical variable and a categorical variable and continuous variable?","<p>I am building a regression model and I need to calculate the below to check for correlations</p>

<ol>
<li>Correlation between 2 Multi level categorical variables</li>
<li>Correlation between a Multi level categorical variable and
continuous variable </li>
<li>VIF(variance inflation factor) for a Multi
level categorical variables</li>
</ol>

<p>I believe its wrong to use Pearson correlation coefficient for the above scenarios because Pearson only works for 2 continuous variables. </p>

<p>Please answer the below questions</p>

<ol>
<li>Which correlation coefficient works best for the above cases ? </li>
<li>VIF calculation only works for continuous data so what is the
alternative? </li>
<li>What are the assumptions I need to check before I use the correlation coefficient you suggest? </li>
<li>How to implement them in SAS &amp; R?</li>
</ol>
","<r><statistics><correlation>"
"93273","Getting a neural network to approximate x^2","<p>I don't get why it is so hard to get my neural network to learn such a simple function. I've tried all sorts of combinations of layer numbers, number of neurons but it doesn't seem to want to learn. Any suggestions would be greatly appreciated!
Here is my code:</p>
<pre><code>import numpy as np
import numpy
from tensorflow import keras
from tensorflow.keras import layers
import random

inputs = keras.Input(shape=(1,))
dense = layers.Dense(50, activation=&quot;relu&quot;)
x = dense(inputs)
x = layers.Dense(50, activation=&quot;relu&quot;)(x)
x = layers.Dense(50, activation=&quot;relu&quot;)(x)
x = layers.Dense(50, activation=&quot;relu&quot;)(x)
x = layers.Dense(50, activation=&quot;relu&quot;)(x)
outputs = layers.Dense(1)(x)

model = keras.Model(inputs=inputs, outputs=outputs, name=&quot;kinetic_energy&quot;)
adam = keras.optimizers.Adam(lr=0.0002,beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False,)
model.compile(
    loss='mean_squared_error',
    optimizer='adam',
    metrics=[&quot;mean_squared_error&quot;],
)



for j in range(10): 
    x = np.array([])
    y = np.array([])
    for i in range(1000):
        randint = random.randint(0, 10000)
        square = randint * randint
        x = numpy.append(x, randint)
        y = numpy.append(y, square)
    model.fit(x, y, epochs = 100)
    
    
x = np.array([20])
z = model.predict(x)
print (z)
<span class=""math-container"">```</span>
</code></pre>
","<machine-learning><neural-network><keras><tensorflow><predictive-modeling>","47787","Can a neural network compute $y = x^2$?","<p>In spirit of the famous <a href=""http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/"" rel=""noreferrer"">Tensorflow Fizz Buzz</a> joke and <a href=""https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b"" rel=""noreferrer"">XOr problem</a> I started to think, if it's possible to design a neural network that implements <span class=""math-container"">$y = x^2$</span> function?</p>

<p>Given some representation of a number (e.g. as a vector in binary form, so that number <code>5</code> is represented as <code>[1,0,1,0,0,0,0,...]</code>), the neural network should learn to return its square - 25 in this case.</p>

<p>If I could implement <span class=""math-container"">$y=x^2$</span>, I could probably implement <span class=""math-container"">$y=x^3$</span> and generally any polynomial of x, and then with Taylor series I could approximate <span class=""math-container"">$y=\sin(x)$</span>, which would solve the Fizz Buzz problem - a neural network that can find remainder of the division.</p>

<p>Clearly, just the linear part of NNs won't be able to perform this task, so if we could do the multiplication, it would be happening thanks to activation function.</p>

<p>Can you suggest any ideas or reading on subject?</p>
","<machine-learning><neural-network>"
"94201","Framework for Genetic Algorithms on python","<p>I'm trying to use a framework implemented in python to use GA (Genetic algorithms) and other related algorithms . But I'm not sure about what framework to use, I've found two interesting options Pymoo and JmetalPy, the former it seems well documented it and the latter isn't looks like it (it's under construction), but the version in Java is very popular and well know. I need choose a framework to make a comparation with an algorithm made by me.</p>
<p>For this reason a reliable framework is needed and with small learning curve.</p>
<p>Do you have any good or  bad experience using it?,
Do you know another framework?</p>
<p>Pymoo:
<a href=""https://ieeexplore.ieee.org/document/9078759?denied="" rel=""nofollow noreferrer"">Paper</a>
<a href=""https://pymoo.org/"" rel=""nofollow noreferrer"">Docs</a></p>
<p>JmetalPy: <a href=""https://arxiv.org/abs/1903.02915"" rel=""nofollow noreferrer"">Paper</a>
<a href=""https://jmetal.github.io/jMetalPy/index.html"" rel=""nofollow noreferrer"">Docs</a></p>
","<python><genetic-algorithms>","58713","Python metaheuristic packages","<p>I need to use a metaheuristic algorithm to solve an optimization problem on a Python codebase.</p>

<p>Metaheuristics usually need to be written in C++ or Java as they involve a lot of iterations, while Python is weak from this point of view.</p>

<p>Questions:</p>

<ol>
<li>do any Python metaheuristic packages which wrap faster languages as
C++/Java exist?</li>
<li>do any Python metaheuristic packages based on maybe
cython on numba exist?</li>
<li>other solutions?</li>
</ol>
","<python><optimization><genetic-algorithms>"
"94232","Neural network activation function","<p>im fairly new to neural networks.
I want to ask what exactly does the activation output, is it the probability the combined summation of inputs and weights lead to a match for the next neuron?</p>
<p>Thanks</p>
<p><a href=""https://i.stack.imgur.com/XHI93.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XHI93.png"" alt=""enter image description here"" /></a></p>
","<machine-learning><neural-network>","14349","Difference of Activation Functions in Neural Networks in general","<p>I have studied the activation function types for neural networks. The functions themselves are quite straightforward, but the application difference is not entirely clear.</p>

<p>It's reasonable that one differentiates between logical and linear type functions, depending on the desired binary/continuous output but what is the advantage of sigmoid function over the simple linear one?</p>

<p>ReLU is especially difficult to understand for me, for instance: what is the point to use a function that behaves like linear in case of positive inputs but is ""flat"" in case of negatives? What is the intuition behind this? Or is it just a simple trial-error thing, nothing more?</p>
","<neural-network><activation-function>"
"97275","Should the test dataset be scaled with respect to its distribution or with respect to the distribution of the training dataset?","<p>I have applied data scaling techniques on my training dataset during training.</p>
<p>For evaluation, when scaling the test dataset, should it be scaled using the scalers fitted to the training dataset or the test dataset?</p>
","<machine-learning><scikit-learn><preprocessing>","39932","Feature Scaling both training and test data","<p>It is stated that for:
Feature Normalization - </p>

<blockquote>
  <p>The test set must use identical scaling to the training set.</p>
</blockquote>

<p>And the point is given that:</p>

<blockquote>
  <p>Do not scale the training and test sets using different scalars: this
  could lead to random skew in the data.</p>
</blockquote>

<p>Could someone explain what that means?</p>
","<machine-learning><data-science-model>"
"97524","How to swap the clothes of person with some clothes using GAN?","<p>I have one source image of person , the persons clothes should swap with destination image (contain the picture of any clothes) .I want to use GAN , like use StyleGan for it . I am trying to find out the repository on google if somebody has implemented it but I am not finding it , If you have an idea kindly share the link with me . I don't want to use video.
If you have any helpful link , please share with me here</p>
","<deep-learning><algorithms><gan>","97473","How to replace the clothes of person using GAN?","<p>I have one source video, let us say if the person is standing or walking in the video, the person's clothes should swap with the destination image (contain the picture of any clothes). I would like to use a GAN, like StyleGan for it. I am trying to find out the repository on Google if somebody has implemented it but I am not finding it. If you have an idea kindly share the link with me.</p>
","<deep-learning><gan><image><deep-fakes>"
"98368","Null and 0 Values Handling - Basic Statistics","<p>I am trying to do some descriptive statistics and basic graphs on relatively small data sets derived from environmental sample collection and/or chemical research. It turns out that sample points end up being recorded as a <code>0</code> if the analyte was not detected, or they are just left blank (<code>Null</code>) if a sample wasn't collected.</p>
<p>The problem is that this does not happen truly randomly, so I probably shouldn't delete any data. Also, there is no apparent predictability in the results, as environmental samples, for example, may not be collected for a number of reasons (inclement weather, equipment failure, etc.). I know that inferential statistics or modelling through ML is may be used here; however, I'm not well versed with these methods (and the data sets would probably be not large enough anyway, only a few dozen/hundred points). Is there is any generally accepted advice on possibly replacing the missing values or 0's with the population mean, or any other more basic statistical tricks like that? Otherwise, are there any cases in which excluding data points is acceptable?</p>
","<statistics><data-cleaning><descriptive-statistics>","98144","What is the Better approach to handle Missing Values?","<p>I read up some books on missing values. They have mentioned that listwise deletion is the least preferred method even though the sample size maybe be large <em>(Newman, D. A. 2014. Missing Data: Five Practical Guide-lines.Organizational Research Methods 17: 372–411)</em></p>
<p><a href=""https://i.stack.imgur.com/dsScC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dsScC.png"" alt=""enter image description here"" /></a></p>
<p>Next, I went to Kaggle to checkout how the missing value is handled for this data set <a href=""https://www.kaggle.com/c/house-prices-advanced-regression-techniques"" rel=""nofollow noreferrer"">https://www.kaggle.com/c/house-prices-advanced-regression-techniques</a>. However the top rated code handle it by totally removing it. Some other top code use mean, mode etc. but so far I have not found any use Multiple Imputation, Maximum Likelihood etc. Moreover,  the missingness (MCAR / MAR) was not determined.</p>
<p>Hence, I am confused in which situation do we need to go into details to handle missing value. What is the industry standard approach?</p>
","<missing-data>"
"100449","How to compute similarity matrix for strings efficiently?","<p><strong>Here I'm trying to compute similarity between 1000 cross 10000 strings (using Levenshtein distance), I'm using a dataframe approach where you just need to compare n(n-1)/2 comparisons instead of n*n. But even this took a lot of time, is there a better way to optimise further?</strong></p>
<pre><code>import time
import random, string
import Levenshtein
import pandas as pd

# random alphanumeric strings of length 10
rand_ls = [''.join(random.choices(string.ascii_letters + string.digits, k=10)) for i in range(1000)]

# a dataframe filled with 0's but with shape n * n where n = len(rand_ls)
df = pd.DataFrame(0, index = rand_ls, columns = rand_ls)

s = time.time()
for i in range(len(df)):
    for j in range(len(df)):
        if i &gt; j:
            dist = Levenshtein.ratio(rand_ls[i], rand_ls[j])
            df.iloc[i, j] = dist
            df.iloc[j, i] = dist
            
e = time.time()
print(e-s)
# took 130 sec for 1000*1000 comparisons 


</code></pre>
","<python><pandas><numpy><similarity><cosine-distance>","54570","How to efficiently iterate a supervised model over the Cartesian product of very large number of records?","<p><strong>The problem:</strong> </p>

<p>Two large databases, with ~1M records each, ""old customer data"" and ""new customer data"". The data came from different sources and was ingested at different times, so there are many duplicates, but the duplicates might not be exact matches. For example In the old data, the customer was listed as ""Michael Smith"" but in the new data, they were listed as ""Mike Smith"" or ""M. Smith"", or the names match, but the address field is different: How do we now if it is different people with the same name or the same person who changed address? </p>

<p><strong>Approach:</strong></p>

<p>It seems possible to use a supervised learning approach to classify a pair of records as either being duplicates or not. </p>

<p><strong>Question:</strong></p>

<p>Assuming such a model is possible and we have trained it, how would we iterate over the entire data set to produce our predictions? </p>

<p>To apply a supervised approach (or any ML/Probabilistic approach), I can only think of naively examining each pair of records one-by-one, but this means our models would have to iterate through <span class=""math-container"">$10^{12}$</span> records, which doesn't seem feasible even with advanced compute capabilities? </p>

<p>How can one efficiently iterate/search through the combined data set in a situation like this?  </p>
","<algorithms><supervised-learning><search>"
"104261","Feature engineering using XGBoost regressor","<p>If I want to train a regression model through tree based algorithms like XGBoost. Suppose that there have 5 features x1, x2, x3, x4, x5 and a target y. And some experts said x2 minus x3 is highly correlate to y. Should I put x2-x3 in the model as the sixth feature, or XGBoost will automatically learn it by just put x1~x5 in model.</p>
<p>As I know, a linear mode can learn a formula from features, and how about tree based methods? If tree based can also do the same thing, does the size of data matter?</p>
","<xgboost><feature-engineering>","17710","Is feature engineering still useful when using XGBoost?","<p>I was reading the material related to XGBoost. It seems that this method does not require any variable scaling since it is based on trees and this one can capture complex non-linearity pattern, interactions. And it can handle both numerical and categorical variables and it also seems that redundant variables does not affect this method too much. </p>

<p>Usually, in predictive modeling, you may do some selection among all the features you have and you may also create some new features from the set of features you have. So select a subset of features means you think there are some redundancy in your set of features; create some new features from the current feature set means you do some functional transformations on your current features. Then, both of these two points should be covered in XGBoost. Then, does it mean that to use XGBoost, you only need to choose those tunning parameters wisely? What is the value of doing feature engineering using XGBoost?</p>
","<xgboost><feature-engineering>"